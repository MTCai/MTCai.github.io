[{"title":"00-环境/00-MySQL安装","date":"2021-04-12T02:11:07.946Z","path":"2021/04/12/00-环境/00-MySQL安装.html","text":"MySQL安装安装文件：mysql-5.5.15-winx64.msi 接下来，需要注意：1）Typical 是经典安装，包含服务端和自带的客户端；2）Custom 是自定义安装；3）Complete 是完全安装。这里选择自定义安装。同时修改安装路径。 然后期间会出现mysql的广告，点击下一步即可。然后会继续安装，安装成功后，保证下面是勾选状态（默认也是勾选的），到这里仅是安装好了服务，还没配置。 如果取消了勾选，或配置时中途退出，也可以在安装目录下重新运行配置程序。 D:\\Program Files\\MySQL\\MySQL Server 5.5\\bin\\MySQLInstanceConfig.exe 配置界面有两个选项：1）Detailed XX 是精确配置；2）Standard XXX是标准配置。这里使用精确配置。然后选择服务类型，从上到下依次是开发机、服务器和专用服务器，占用的内存也依次递增。一般选择开发机即可。 接下来选择数据库类型，1）多功能型数据库；2）事务型数据库；3）非事务型数据库。存储引擎有事务性和非事务性，多功能型数据库在两种存储引擎速度都比较快，事务型数据库在事务型引擎较快，非事务型数据库在非事务型引擎速度较快。一般选择多功能型数据库。下一个界面直接下一步。 接下来配置数据库并发连接数：1）策略式，支持20个连接；2）在线式，允许500个连接；3）自定义，自己设定连接数。一般选择第一个即可。然后是配置端口号，开发中一般需要修改，防止别人恶意攻击。学习使用可先不改。 接下来选择字符集：使用第三个，然后下拉选择utf8。下一个界面中起一个服务名，其中绿线是开机自启，红线是添加环境变量。 接下来，设置root账户密码，同时勾选允许远程连接。然后点击“Execute”执行，等待完成，如下图。 图像化界面安装文件：SQLyog-10.0.0-0.exe 激活码： Name: any key: dd987f34-f358-4894-bd0f-21f3f04be9c1 一路下一步即可。然后新建一个连接，输入刚才设置的密码，连接。 连接成功就完成全部配置。 linux 安装mysql 查看mysql 是否安装，如果安装了，卸载mysql 12rpm -qa|grep mysql # 查询rpm -e --nodeps mysql-libs-XXXXX.x86_64 # 卸载 安装mysql 服务端：rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm 查看产生的随机密码：cat /root/.mysql_secret 查看mysql 状态：service mysql status 启动mysql服务：service mysql start 安装MySql 客户端：rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm 链接mysql：mysql -uroot -pOEXaQuS8IWkG19Xs 修改密码：mysql&gt;SET PASSWORD=PASSWORD(‘000000’); 退出：mysql&gt;exit; 配置只要是root 用户+密码，在任何主机上都能登录MySQL 数据库。 12345678910111213[root@hadoop102 mysql-libs]# mysql -uroot -p000000mysql&gt;use mysql;mysql&gt;show tables; mysql&gt;select User, Host, Password from user;mysql&gt;update user set host='%' where host='localhost'; mysql&gt;delete from user where Host='hadoop102'; mysql&gt;delete from user where Host='127.0.0.1'; mysql&gt;delete from user where Host='::1'; mysql&gt;flush privileges; mysql&gt;quit;","tags":[]},{"title":"安装Hadoop","date":"2021-04-12T02:00:50.000Z","path":"2021/04/12/00-环境/01-Hadoop安装.html","text":"安装Hadoop 解压安装包 [atguigu@hadoop102 software]$ tar -zxvf hadoop-3.1.3.tar.gz -C/opt/module 添加环境变量 [atguigu@hadoop102 hadoop-3.1.3]$ sudo vim/etc/profile.d/my_env.sh 在 my_env.sh文件末尾添加如下内容 (shift+g) :’ #HADOOP_HOME ‘ export HADOOP_HOME=/opt/module/hadoop-3.1.3 export PATH=\\$PATH:\\$HADOOP_HOME/bin export PATH=\\$PATH:\\$HADOOP_HOME/sbin 测试是否安装成功 [atguigu@hadoop102 hadoop-3.1.3 ]$ hadoop version Hadoop 3.1.3 配置SSH免密登陆SSH无密登录配置 生成公钥和私钥 [atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登录的目标机器上 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104 注意： 还需要在hadoop102上采用 root 账号，配置一下无密登录到hadoop102、hadoop103、hadoop104； 还需要在hadoop103上采用 atguigu 账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。 还需要在hadoop104上采用 atguigu 账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。 配置集群核心配置文件配置core-site.xml [atguigu@hadoop102 hadoop]$ vi core-site.xml 在该文件中编写如下配置 123456789101112131415161718&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt;&lt;!-- Hadoop3.x 配置 HDFS 网页登录使用的静态用户为 atguigu --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt; HDFS配置文件配置hadoop-env.sh [atguigu@hadoop102 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置hdfs-site.xml [atguigu@hadoop102 hadoop]$ vi hdfs-site.xml 在该文件中编写如下配置 1234567891011121314151617&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- nn web 端访问地址 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http address &lt;/&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;!-- 2 nn web 端访问地址 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt; YARN配置文件配置yarn-env.sh [atguigu@hadoop102 hadoop]$ vi yarn-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置yarn-site.xml [atguigu@hadoop102 hadoop]$ vi yarn-site.xml 在该文件中增加如下配置 123456789101112131415161718192021222324252627282930313233343536&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置日志聚集服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;&lt;!-- 环境变量的继承 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE, HADOOP_YARN_HOME, HADOOP_MAPRED_HOME &lt;/value&gt;&lt;/property&gt; MapReduce配置文件配置mapred-env.sh [atguigu@hadoop102 hadoop]$ vi mapred-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置mapred-site.xml [atguigu@hadoop102 hadoop]$ cp mapred-site.xml.template mapred-site.xml [atguigu@hadoop102 hadoop]$ vi mapred-site.xml 在该文件中增加如下配置 12345678910111213141516&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop102:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器 web 端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;19888&lt;/value&gt;&lt;/property&gt; 在集群上分发配置好的Hadoop配置文件 [atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/ 配置slaves/workers12345678hadoop2.x修改 /opt/module/hadoop-2.7.2/etc/hadoop/slaveshadoop3.x修改 /opt/module/hadoop-3.1.3/etc/hadoop/workers[atguigu@hadoop102 hadoop]$ vi slaves在该文件中增加如下内容：注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。hadoop102hadoop103hadoop104 同步所有节点配置文件: [atguigu@hadoop102 hadoop]$ xsync slaves 启动如果集群是第一次启动，需要格式化NameNode，==启动前要关闭所有服务==（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据） 注意：格式化 NameNode 会产生新的集群 id，导致 NameNode 和 DataNode的集群 id不一致，集群找不到已往数据。 如果集群在运行过程中报错，需要重新格式化 NameNode的话， 一定要 先停止 namenode和 datanode进程， 并且要 删除 所有机器的 data和 logs目录，然后再进行格式化。 [atguigu@hadoop102 hadoop-2.7.2]$ hadoop namenode -format 集群启动/停止HDFS start-dfs.sh / stop-dfs.sh 集群启动/停止YARN start-yarn.sh / stop-yarn.sh","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"Hadoop编译源码","date":"2021-04-12T02:00:50.000Z","path":"2021/04/12/00-环境/02-Hadoop编译源码.html","text":"Hadoop编译源码（面试重点）准备工作 （1）系统联网，或者有yum源 （2）hadoop-2.7.2-src.tar.gz 进入hadoop-2.7.2-src文件夹，查看BUILDING.txt cd hadoop-2.7.2-src more BUILDING.txt 可以看到编译所需的库或者工具 （3）jdk-8u144-linux-x64.tar.gz （4）apache-ant-1.9.9-bin.tar.gz（build工具，打包用的） （5）apache-maven-3.0.5-bin.tar.gz （6）protobuf-2.5.0.tar.gz（序列化的框架） （7）apache-tomcat-6.0.44.tar.gz 配置jdk验证命令：java -version 配置Maven [root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml 1234567891011121314151617181920212223242526&lt;mirrors&gt;&lt;!-- mirror| Specifies a repository mirror site to use instead of a given repository. The repository that| this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used| for inheritance and direct lookup purposes, and must be unique across the set of mirrors.|&lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt;&lt;/mirror&gt;--&gt;&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;&lt;/mirrors&gt; [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile 123:' #MAVEN_HOME 'export MAVEN_HOME=/opt/module/apache-maven-3.0.5export PATH=$PATH:$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn -version 配置ant [root@hadoop101 apache-ant-1.9.9]# vi /etc/profile 123:' #ANT_HOME 'export ANT_HOME=/opt/module/apache-ant-1.9.9export PATH=$PATH:$ANT_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：ant -version 安装 g++、make、cmake等库 [root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers [root@hadoop101 apache-ant-1.9.9]# yum -y install svn ncurses-devel gcc* [root@hadoop101 apache-ant-1.9.9]# yum install make [root@hadoop101 apache-ant-1.9.9]# yum install cmake [root@hadoop101 apache-ant-1.9.9]# yum -y install lzo-devel zlib-devel autoconf automake libtool cmake openssl-devel 安装protobuf [root@hadoop101 protobuf-2.5.0]#./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make check [root@hadoop101 protobuf-2.5.0]# make install [root@hadoop101 protobuf-2.5.0]# ldconfig [root@hadoop101 hadoop-dist]# vi /etc/profile 123:' #LD_LIBRARY_PATH 'export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0export PATH=$PATH:$LD_LIBRARY_PATH [root@hadoop101 software]#source /etc/profile 验证命令：protoc —version 安装findbugs 解压：tar -zxvf findbugs-3.0.1.tar.gz -C /opt/moudles/ 配置环境变量: 123456在 /etc/profile 文件末尾添加：export FINDBUGS_HOME=/opt/findbugs-3.0.1export PATH=$PATH:$FINDBUGS_HOME/bin保存退出，并使更改生效。 验证命令：findbugs -version 编译源码 1.进入到源码目录 [root@hadoop101 hadoop-2.7.2-src]# pwd /opt/hadoop-2.7.2-src 2.通过maven执行编译命令 [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 编译过程中会下载 apache-tomcat-6.0.44.tar.gz，速度非常慢，把提前下载好的文件放到如下目录： 注：编译前这两个目录并不存在，编译过程中及时中断，然后复制文件 hadoop-2.7.2-src/hadoop-common-project/hadoop-kms/downloads/ hadoop-2.7.2-src/hadoop-hdfs-project/hadoop-hdfs-httpfs/downloads 等待时间30分钟左右，最终成功是全部SUCCESS，如图 成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下 编译源码过程中常见的问题及解决方案（1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） （2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar （3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐2.7.0版本的问题汇总帖子 http://www.tuicool.com/articles/IBn63qf","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"Zookeeper安装","date":"2021-04-12T02:00:50.000Z","path":"2021/04/12/00-环境/03-Zookeeper安装.html","text":"官网首页：https://zookeeper.apache.org/ 本地模式安装部署安装 安装JDK 解压Zookeeper安装包 [atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 修改zoo.cfg配置、创建zkData目录 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件，修改dataDir路径： dataDir=/opt/module/zookeeper-3.4.10/zkData 在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹 mkdir zkData 操作Zookeeper 启动Zookeeper：bin/zkServer.sh start 查看进程是否启动 123[atguigu@hadoop102 zookeeper-3.4.10]$ jps4020 Jps4001 QuorumPeerMain 查看状态 1234[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: standalone 启动客户端：bin/zkCli.sh 退出客户端：[zk: localhost:2181(CONNECTED) 0] quit 停止Zookeeper：bin/zkServer.sh stop 配置参数解读Zookeeper中的配置文件zoo.cfg中参数含义解读如下： tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) initLimit =10：Leader 和 Fllower初始通信时限 集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 syncLimit =5：LF同步通信时限 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。 dataDir：数据文件目录+数据持久化路径 主要用于保存Zookeeper中的数据。 clientPort =2181：客户端连接端口 监听客户端连接的端口。 分布式安装部署在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper 解压安装1234# 解压Zookeeper安装包到/opt/module/目录下[atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/# 同步/opt/module/zookeeper-3.4.10目录内容到hadoop103、hadoop104[atguigu@hadoop102 module]$ xsync zookeeper-3.4.10/ 配置服务器编号 在/opt/module/zookeeper-3.4.10/这个目录下创建zkData：mkdir -p zkData 在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件：touch myid 注：添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 在myid文件中添加与server对应的编号：2 可以不从0开始，但是需要是唯一编号！ 拷贝配置好的zookeeper到其他机器上：[atguigu@hadoop102 zkData]$ xsync myid 并分别在hadoop102、hadoop103上修改myid文件中内容为3、4 修改zoo.cfg配置 重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg 修改zoo.cfg配置 12345dataDir=/opt/module/zookeeper-3.4.10/zkData#######################cluster##########################server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888 同步zoo.cfg配置文件 配置参数解读：server.A=B:C:D。 A是一个数字，表示这个是第几号服务器；集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群操作分别启动Zookeeper123[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start[atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start[atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start 查看状态123456789101112[atguigu@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[atguigu@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader[atguigu@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Flume安装","date":"2021-04-12T02:00:50.000Z","path":"2021/04/12/00-环境/05-Flume安装.html","text":"安装地址 Flume 官网地址：http://flume.apache.org/ 文档查看地址：http://flume.apache.org/FlumeUserGuide.html 下载地址：http://archive.apache.org/dist/flume/ 安装部署 解压 apache flume 1.7.0 bin.tar.gz 到 /opt/ 目录下 将 flume/conf 下的 flume env.sh.template 文件修改为 flume env.sh ，并 配置 flumeenv.sh 文件 123[atguigu@hadoop102 conf]$ mv flume env.sh.template flume env.sh[atguigu@hadoop102 conf]$ vi flume env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144","tags":[{"name":"Flume","slug":"Flume","permalink":"https://mtcai.github.io/tags/Flume/"}]},{"title":"数据结构32-拓扑排序","date":"2021-03-09T12:12:12.000Z","path":"2021/03/09/01-数据结构/数据结构32-拓扑排序 - 副本 (2).html","text":"[toc]","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构32-拓扑排序","date":"2021-03-09T12:12:12.000Z","path":"2021/03/09/01-数据结构/数据结构32-拓扑排序 - 副本.html","text":"[toc]","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构32-拓扑排序","date":"2021-03-09T12:12:12.000Z","path":"2021/03/09/01-数据结构/数据结构32-拓扑排序 - 副本 (3).html","text":"[toc]","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构32-拓扑排序","date":"2021-03-09T12:12:12.000Z","path":"2021/03/09/01-数据结构/数据结构32-拓扑排序.html","text":"[toc]","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构31-动态规划实战","date":"2021-03-09T08:49:12.000Z","path":"2021/03/09/01-数据结构/数据结构31-动态规划实战.html","text":"[toc] 搜索引擎中的拼写纠错功能利用 Trie 树，可以实现搜索引擎的关键词提示功能，这样可以节省用户输入搜索关键词的时间。实际上，搜索引擎在用户体验方面的优化还有很多，比如你可能经常会用的拼写纠错功能。当你在搜索框中，一不小心输错单词时，搜索引擎会非常智能地检测出你的拼写错误，并且用对应的正确单词来进行搜索。 编辑距离（Edit Distance）指的是将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。可以用来量化两个字符串之间的相似程度。编辑距离越大，说明两个字符串的相似程度越小；相反，编辑距离就越小，说明两个字符串的相似程度越大。对于两个完全相同的字符串来说，编辑距离就是 0。 根据所包含的编辑操作种类的不同，编辑距离有多种不同的计算方式，比较著名的有莱文斯坦距离（Levenshtein distance）和最长公共子串长度（Longest common substring length）。其中，莱文斯坦距离允许增加、删除、替换字符这三个编辑操作，最长公共子串长度只允许增加、删除字符这两个编辑操作。莱文斯坦距离和最长公共子串长度，从两个截然相反的角度，分析字符串的相似程度。莱文斯坦距离的大小，表示两个字符串差异的大小；而最长公共子串的大小，表示两个字符串相似程度的大小。 当用户在搜索框内，输入一个拼写错误的单词时，就拿这个单词跟词库中的单词一一进行比较，计算编辑距离，将编辑距离最小的单词，作为纠正之后的单词，提示给用户。 这就是拼写纠错最基本的原理。不过，真正用于商用的搜索引擎，拼写纠错功能显然不会就这么简单。一方面，单纯利用编辑距离来纠错，效果并不一定好；另一方面，词库中的数据量可能很大，搜索引擎每天要支持海量的搜索，所以对纠错的性能要求很高。 针对纠错效果不好的问题，有很多种优化思路，这里介绍几种。 我们并不仅仅取出编辑距离最小的那个单词，而是取出编辑距离最小的 TOP 10，然后根据其他参数，决策选择哪个单词作为拼写纠错单词。比如使用搜索热门程度来决定哪个单词作为拼写纠错单词。 我们还可以用多种编辑距离计算方法，比如今天讲到的两种，然后分别编辑距离最小的 TOP 10，然后求交集，用交集的结果，再继续优化处理。 我们还可以通过统计用户的搜索日志，得到最常被拼错的单词列表，以及对应的拼写正确的单词。搜索引擎在拼写纠错的时候，首先在这个最长被拼错单词列表中查找。如果一旦找到，直接返回对应的正确的单词。这样纠错的效果非常好。 我们还有更加高级一点的做法，引入个性化因素。针对每个用户，维护这个用户特有的搜索喜好，也就是常用的搜索关键词。当用户输入错误的单词的时候，我们首先在这个用户常用的搜索关键词中，计算编辑距离，查找编辑距离最小的单词。 针对纠错性能方面，我也有相应的优化方式。有两种分治的优化思路。 如果纠错功能的 TPS 不高，我们可以部署多台机器，每台机器运行一个独立的纠错功能。当有一个纠错请求的时候，我们通过负载均衡，分配到其中一台机器，来计算编辑距离，得到纠错单词。 如果纠错系统的响应时间太长，也就是，每个纠错请求处理时间过长，我们可以将纠错的词库，分割到很多台机器。当有一个纠错请求的时候，我们就将这个拼写错误的单词，同时发送到这多台机器，让多台机器并行处理，分别得到编辑距离最小的单词，然后再比对合并，最终决定出一个最优的纠错单词。 编程计算莱文斯坦距离回溯是一个递归处理的过程。如果 a[i] 与 b[j] 匹配，我们递归考察 a[i+1] 和 b[j+1]。如果 a[i] 与 b[j] 不匹配，那我们有多种处理方式可选： 可以删除 a[i]，然后递归考察 a[i+1] 和 b[j]； 可以删除 b[j]，然后递归考察 a[i] 和 b[j+1]； 可以在 a[i] 前面添加一个跟 b[j] 相同的字符，然后递归考察 a[i] 和 b[j+1]; 可以在 b[j] 前面添加一个跟 a[i] 相同的字符，然后递归考察 a[i+1] 和 b[j]； 可以将 a[i] 替换成 b[j]，或者将 b[j] 替换成 a[i]，然后递归考察 a[i+1] 和 b[j+1]。 12345678910111213141516171819202122// 回溯算法private char[] a = \"mitcmu\".toCharArray();private char[] b = \"mtacnu\".toCharArray();private int n = 6;private int m = 6;private int minDist = Integer.MAX_VALUE; // 存储结果// 调用方式 lwstBT(0, 0, 0);public lwstBT(int i, int j, int edist) &#123; if (i == n || j == m) &#123; if (i &lt; n) edist += (n-i); if (j &lt; m) edist += (m - j); if (edist &lt; minDist) minDist = edist; return; &#125; if (a[i] == b[j]) &#123; // 两个字符匹配 lwstBT(i+1, j+1, edist); &#125; else &#123; // 两个字符不匹配 lwstBT(i + 1, j, edist + 1); // 删除 a[i] 或者 b[j] 前添加一个字符 lwstBT(i, j + 1, edist + 1); // 删除 b[j] 或者 a[i] 前添加一个字符 lwstBT(i + 1, j + 1, edist + 1); // 将 a[i] 和 b[j] 替换为相同字符 &#125;&#125; 根据回溯算法的代码实现，我们可以画出递归树，看是否存在重复子问题。如果存在重复子问题，那我们就可以考虑能否用动态规划来解决；如果不存在重复子问题，那回溯就是最好的解决方法。 在递归树中，每个节点代表一个状态，状态包含三个变量 (i, j, edist)，其中，edist 表示处理到 a[i] 和 b[j] 时，已经执行的编辑操作的次数。 在递归树中，(i, j) 两个变量重复的节点很多，比如 (3, 2) 和 (2, 3)。对于 (i, j) 相同的节点，我们只需要保留 edist 最小的，继续递归处理就可以了，剩下的节点都可以舍弃。所以，状态就从 (i, j, edist) 变成了 (i, j, min_edist)，其中 min_edist 表示处理到 a[i] 和 b[j]，已经执行的最少编辑次数。 看到这里，你有没有觉得，这个问题跟上两节讲的动态规划例子非常相似？不过，这个问题的状态转移方式，要比之前两节课中讲到的例子都要复杂很多。上一节我们讲的矩阵最短路径问题中，到达状态 (i, j) 只能通过 (i-1, j) 或 (i, j-1) 两个状态转移过来，而今天这个问题，状态 (i, j) 可能从 (i-1, j)，(i, j-1)，(i-1, j-1) 三个状态中的任意一个转移过来。 12345678910111213141516171819202122232425262728293031323334353637383940如果：a[i]!=b[j]，那么：min_edist(i, j) 就等于：min(min_edist(i-1,j)+1, min_edist(i,j-1)+1, min_edist(i-1,j-1)+1) 如果：a[i]==b[j]，那么：min_edist(i, j) 就等于：min(min_edist(i-1,j)+1, min_edist(i,j-1)+1，min_edist(i-1,j-1)) 其中，min 表示求三数中的最小值。 // 动态规划public int lwstDP(char[] a, int n, char[] b, int m) &#123; int[][] minDist = new int[n][m]; for (int j = 0; j &lt; m; ++j) &#123; // 初始化第 0 行:a[0..0] 与 b[0..j] 的编辑距离 if (a[0] == b[j]) minDist[0][j] = j; else if (j != 0) minDist[0][j] = minDist[0][j-1]+1; else minDist[0][j] = 1; &#125; for (int i = 0; i &lt; n; ++i) &#123; // 初始化第 0 列:a[0..i] 与 b[0..0] 的编辑距离 if (a[i] == b[0]) minDist[i][0] = i; else if (i != 0) minDist[i][0] = minDist[i-1][0]+1; else minDist[i][0] = 1; &#125; for (int i = 1; i &lt; n; ++i) &#123; // 按行填表 for (int j = 1; j &lt; m; ++j) &#123; if (a[i] == b[j]) minDist[i][j] = min( minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]); else minDist[i][j] = min( minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]+1); &#125; &#125; return minDist[n-1][m-1];&#125; private int min(int x, int y, int z) &#123; int minv = Integer.MAX_VALUE; if (x &lt; minv) minv = x; if (y &lt; minv) minv = y; if (z &lt; minv) minv = z; return minv;&#125; 编程计算最长公共子串长度每个状态还是包括三个变量 (i, j, max_lcs)，max_lcs 表示 a[0…i] 和 b[0…j] 的最长公共子串长度。那 (i, j) 这个状态都是由哪些状态转移过来的呢？ 我们先来看回溯的处理思路。我们从 a[0] 和 b[0] 开始，依次考察两个字符串中的字符是否匹配。 如果 a[i] 与 b[j] 互相匹配，我们将最大公共子串长度加一，并且继续考察 a[i+1] 和 b[j+1]。 如果 a[i] 与 b[j] 不匹配，最长公共子串长度不变，这个时候，有两个不同的决策路线： 删除 a[i]，或者在 b[j] 前面加上一个字符 a[i]，然后继续考察 a[i+1] 和 b[j]； 删除 b[j]，或者在 a[i] 前面加上一个字符 b[j]，然后继续考察 a[i] 和 b[j+1]。 反过来也就是说，如果我们要求 a[0…i] 和 b[0…j] 的最长公共长度 max_lcs(i, j)，我们只有可能通过下面三个状态转移过来： (i-1, j-1, max_lcs)，其中 max_lcs 表示 a[0…i-1] 和 b[0…j-1] 的最长公共子串长度； (i-1, j, max_lcs)，其中 max_lcs 表示 a[0…i-1] 和 b[0…j] 的最长公共子串长度； (i, j-1, max_lcs)，其中 max_lcs 表示 a[0…i] 和 b[0…j-1] 的最长公共子串长度。 12345678910111213141516171819202122232425262728293031323334353637383940如果：a[i]==b[j]，那么：max_lcs(i, j) 就等于：max(max_lcs(i-1,j-1)+1, max_lcs(i-1, j), max_lcs(i, j-1))； 如果：a[i]!=b[j]，那么：max_lcs(i, j) 就等于：max(max_lcs(i-1,j-1), max_lcs(i-1, j), max_lcs(i, j-1))； 其中 max 表示求三数中的最大值。 // 动态规划public int lcs(char[] a, int n, char[] b, int m) &#123; int[][] maxlcs = new int[n][m]; for (int j = 0; j &lt; m; ++j) &#123;// 初始化第 0 行：a[0..0] 与 b[0..j] 的 maxlcs if (a[0] == b[j]) maxlcs[0][j] = 1; else if (j != 0) maxlcs[0][j] = maxlcs[0][j-1]; else maxlcs[0][j] = 0; &#125; for (int i = 0; i &lt; n; ++i) &#123;// 初始化第 0 列：a[0..i] 与 b[0..0] 的 maxlcs if (a[i] == b[0]) maxlcs[i][0] = 1; else if (i != 0) maxlcs[i][0] = maxlcs[i-1][0]; else maxlcs[i][0] = 0; &#125; for (int i = 1; i &lt; n; ++i) &#123; // 填表 for (int j = 1; j &lt; m; ++j) &#123; if (a[i] == b[j]) maxlcs[i][j] = max( maxlcs[i-1][j], maxlcs[i][j-1], maxlcs[i-1][j-1]+1); else maxlcs[i][j] = max( maxlcs[i-1][j], maxlcs[i][j-1], maxlcs[i-1][j-1]); &#125; &#125; return maxlcs[n-1][m-1];&#125; private int max(int x, int y, int z) &#123; int maxv = Integer.MIN_VALUE; if (x &gt; maxv) maxv = x; if (y &gt; maxv) maxv = y; if (z &gt; maxv) maxv = z; return maxv;&#125;","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构30-动态规划理论","date":"2021-03-09T03:41:12.000Z","path":"2021/03/09/01-数据结构/数据结构29-动态规划理论.html","text":"[toc] 一个模型三个特征一个模型：多阶段决策最优解模型。一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。 三个特征： 最优子结构 最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，后面阶段的状态可以通过前面阶段的状态推导出来。 无后效性 无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。 重复子问题 不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。 假设我们有一个 n 乘以 n 的矩阵 w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？ 从 (0, 0) 走到 (n-1, n-1)，总共要走 2*(n-1) 步，也就对应着 2*(n-1) 个阶段。每个阶段都有向右走或者向下走两种决策，并且每个阶段都会对应一个状态集合。 我们把状态定义为 min_dist(i, j)，其中 i 表示行，j 表示列。min_dist 表达式的值表示从 (0, 0) 到达 (i, j) 的最短路径长度。所以，这个问题是一个多阶段决策最优解问题，符合动态规划的模型。 可以用回溯算法来解决这个问题，画一下递归树，就会发现，递归树中有重复的节点。重复的节点表示，从左上角到节点对应的位置，有多种路线，这也能说明这个问题中存在重复子问题。 如果我们走到 (i, j) 这个位置，我们只能通过 (i-1, j)，(i, j-1) 这两个位置移动过来，也就是说，我们想要计算 (i, j) 位置对应的状态，只需要关心 (i-1, j)，(i, j-1) 两个位置对应的状态，并不关心棋子是通过什么样的路线到达这两个位置的。而且，我们仅仅允许往下和往右移动，不允许后退，所以，前面阶段的状态确定之后，不会被后面阶段的决策所改变，所以，这个问题符合“无后效性”这一特征。 刚刚定义状态的时候，我们把从起始位置 (0, 0) 到 (i, j) 的最小路径，记作 min_dist(i, j)。因为我们只能往右或往下移动，所以，我们只有可能从 (i, j-1) 或者 (i-1, j) 两个位置到达 (i, j)。也就是说，到达 (i, j) 的最短路径要么经过 (i, j-1)，要么经过 (i-1, j)，而且到达 (i, j) 的最短路径肯定包含到达这两个位置的最短路径之一。换句话说就是，min_dist(i, j) 可以通过 min_dist(i, j-1) 和 min_dist(i-1, j) 两个状态推导出来。这就说明，这个问题符合“最优子结构”。 1min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j)) 两种动态规划解题思路总结状态转移表法在递归树中检查是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。 找到重复子问题之后，接下来，我们有两种处理思路，第一种是直接用回溯加“备忘录”的方法，来避免重复子问题。从执行效率上来讲，这跟动态规划的解决思路没有差别。第二种是使用动态规划的解决方法，状态转移表法。 我们先画出一个状态表。状态表一般都是二维的，所以你可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。 尽管大部分状态表都是二维的，但是如果问题的状态比较复杂，需要很多变量来表示，那对应的状态表可能就是高维的，比如三维、四维。那这个时候，我们就不适合用状态转移表法来解决了。一方面是因为高维状态转移表不好画图表示，另一方面是因为人脑确实很不擅长思考高维的东西。 现在，我们来看一下，如何套用这个状态转移表法，来解决之前那个矩阵最短路径的问题？ 从起点到终点，我们有很多种不同的走法。我们可以穷举所有走法，然后对比找出一个最短走法。不过如何才能无重复又不遗漏地穷举出所有走法呢？我们可以用回溯算法这个比较有规律的穷举算法。 回溯算法的代码实现如下所示。代码很短。 123456789101112131415private int minDist = Integer.MAX_VALUE; // 全局变量或者成员变量// 调用方式：minDistBacktracing(0, 0, 0, w, n);public void minDistBT(int i, int j, int dist, int[][] w, int n) &#123; // 到达了 n-1, n-1 这个位置了，这里看着有点奇怪哈，你自己举个例子看下 if (i == n &amp;&amp; j == n) &#123; if (dist &lt; minDist) minDist = dist; return; &#125; if (i &lt; n) &#123; // 往下走，更新 i=i+1, j=j minDistBT(i + 1, j, dist+w[i][j], w, n); &#125; if (j &lt; n) &#123; // 往右走，更新 i=i, j=j+1 minDistBT(i, j+1, dist+w[i][j], w, n); &#125;&#125; 有了回溯代码之后，接下来，我们要画出递归树，以此来寻找重复子问题。在递归树中，一个状态（也就是一个节点）包含三个变量 (i, j, dist)，其中 i，j 分别表示行和列，dist 表示从起点到达 (i, j) 的路径长度。从图中，我们看出，尽管 (i, j, dist) 不存在重复的，但是 (i, j) 重复的有很多。对于 (i, j) 重复的节点，我们只需要选择 dist 最小的节点，继续递归求解，其他节点就可以舍弃了。 既然存在重复子问题，我们就可以尝试看下，用动态规划来解决。我们画出一个二维状态表，表中的行、列表示棋子所在的位置，表中的数值表示从起点到这个位置的最短路径。我们按照决策过程，通过不断状态递推演进，将状态表填好。为了方便代码实现，我们按行来进行依次填充。 1234567891011121314151617181920public int minDistDP(int[][] matrix, int n) &#123; int[][] states = new int[n][n]; int sum = 0; for (int j = 0; j &lt; n; ++j) &#123; // 初始化 states 的第一行数据 sum += matrix[0][j]; states[0][j] = sum; &#125; sum = 0; for (int i = 0; i &lt; n; ++i) &#123; // 初始化 states 的第一列数据 sum += matrix[i][0]; states[i][0] = sum; &#125; for (int i = 1; i &lt; n; ++i) &#123; for (int j = 1; j &lt; n; ++j) &#123; states[i][j] = matrix[i][j] + Math.min(states[i][j-1], states[i-1][j]); &#125; &#125; return states[n-1][n-1];&#125; 状态转移方程法状态转移方程法有点类似递归的解题思路。我们需要分析，某个问题如何通过子问题来递归求解，也就是所谓的最优子结构。根据最优子结构，写出递归公式，也就是所谓的状态转移方程。有了状态转移方程，代码实现就非常简单了。一般情况下，我们有两种代码实现方法，一种是递归加“备忘录”，另一种是迭代递推。 1min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j)) 状态转移方程是解决动态规划的关键。如果我们能写出状态转移方程，那动态规划问题基本上就解决一大半了，而翻译成代码非常简单。但是很多动态规划问题的状态本身就不好定义，状态转移方程也就更不好想到。 12345678910111213141516171819private int[][] matrix = &#123;&#123;1，3，5，9&#125;, &#123;2，1，3，4&#125;，&#123;5，2，6，7&#125;，&#123;6，8，4，3&#125;&#125;;private int n = 4;private int[][] mem = new int[4][4];public int minDist(int i, int j) &#123; // 调用 minDist(n-1, n-1); if (i == 0 &amp;&amp; j == 0) return matrix[0][0]; if (mem[i][j] &gt; 0) return mem[i][j]; int minLeft = Integer.MAX_VALUE; if (j-1 &gt;= 0) &#123; minLeft = minDist(i, j-1); &#125; int minUp = Integer.MAX_VALUE; if (i-1 &gt;= 0) &#123; minUp = minDist(i-1, j); &#125; int currMinDist = matrix[i][j] + Math.min(minLeft, minUp); mem[i][j] = currMinDist; return currMinDist;&#125; 四种算法思想比较分析贪心、回溯、动态规划可以归为一类，而分治单独可以作为一类。前三个算法解决问题的模型，都可以抽象成我们今天讲的那个多阶段决策最优解模型，而分治算法解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。 回溯算法是个“万金油”。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。回溯算法相当于穷举搜索。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。对于大规模数据的问题，用回溯算法解决的执行效率就很低了。 尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。 贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性。其中，最优子结构、无后效性跟动态规划中的无异。“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构19-递归树：分析递归算法时间复杂度","date":"2021-03-02T03:15:12.000Z","path":"2021/03/02/01-数据结构/数据结构19-递归树.html","text":"[toc] 递归树与时间复杂度分析递归代码的时间复杂度分析起来很麻烦，除了用递推公式这种比较复杂的分析方法，还可以借助递归树来分析递归算法的时间复杂度。 递归的思想就是，将大问题分解为小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止。 如果我们把这个一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫作递归树。节点里的数字表示数据的规模，一个节点的求解可以分解为左右子节点两个问题的求解。 把归并排序画成递归树，就是下面这个样子： 因为每次分解都是一分为二，所以代价很低，我们把时间上的消耗记作常量 1。归并算法中比较耗时的是归并操作，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记作 n。 现在，我们只需要知道这棵树的高度 h，用高度 h 乘以每一层的时间消耗 n，就可以得到总的时间复杂度 O(n*h)。 从归并排序的原理和递归树，可以看出来，归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是 log~2~n，所以，归并排序递归实现的时间复杂度就是 O(nlogn)。这里的时间复杂度都是估算的，对树的高度的计算也没有那么精确，但是这并不影响复杂度的计算结果。 实战一：分析快速排序的时间复杂度快速排序在最好情况下，每次分区都能一分为二，这个时候用递推公式 T(n)=2T(\\frac{h}{2})+n，很容易就能推导出时间复杂度是 O(nlogn)。但是，我们并不可能每次分区都这么幸运，正好一分为二。 假设平均情况下，每次分区之后，两个分区的大小比例为 1:k。当 k=9 时，如果用递推公式的方法来求解时间复杂度的话，递推公式就写成T(n)=T(\\frac{n}{10})+T(\\frac{9n}{10})+n。 快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是 n。我们现在只要求出递归树的高度 h，这个快排过程遍历的数据个数就是 h∗n ，也就是说，时间复杂度就是 O(h∗n)。 快速排序结束的条件就是待排序的小区间，大小为 1，也就是说叶子节点里的数据规模是 1。从根节点 n 到叶子节点 1，递归树中最短的一个路径每次都乘以 1/10，最长的一个路径每次都乘以 9/10。通过计算，我们可以得到，从根节点到叶子节点的最短路径是 $log_{10}n$，最长的路径是 $log_{\\frac{10}{9}}⁡n$。 所以，遍历数据的个数总和就介于 $nlog_{10}n$ 和 $nlog_{\\frac{10}{9}}⁡n$ 之间。根据复杂度的大 O 表示法，对数复杂度的底数不管是多少，我们统一写成 logn，所以，当分区大小比例是 1:9 时，快速排序的时间复杂度仍然是 O(nlogn)。 也就是说，对于 k 等于 99，9999，甚至是 999999，99999999……，只要 k 的值不随 n 变化，是一个事先确定的常量，那快排的时间复杂度就是 O(nlogn)。所以，从概率论的角度来说，快排的平均时间复杂度就是 O(nlog⁡n)。 实战二：分析斐波那契数列的时间复杂度 从根节点走到叶子节点，每条路径是长短不一的。如果每次都是 −1，那最长路径大约就是 n；如果每次都是 −2，那最短路径大约就是 n/2。 每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作 1。所以，从上往下，第一层的总时间消耗是 1，第二层的总时间消耗是 2，第三层的总时间消耗就是 2^2^。依次类推，第 k 层的时间消耗就是 2^k−1^，那整个算法的总的时间消耗就是每一层时间消耗之和。 如果路径长度都为 n，那这个总和就是 2^n^−1。 如果路径长度都是 n/2 ，那整个算法的总的时间消耗就是 2^n/2^−1。 实战三：分析全排列的时间复杂度1234567891011121314151617181920212223// 调用方式：// int[]a = a=&#123;1, 2, 3, 4&#125;; printPermutations(a, 4, 4);// k 表示要处理的子数组的数据个数public void printPermutations(int[] data, int n, int k) &#123; if (k == 1) &#123; for (int i = 0; i &lt; n; ++i) &#123; System.out.print(data[i] + \" \"); &#125; System.out.println(); &#125; for (int i = 0; i &lt; k; ++i) &#123; int tmp = data[i]; data[i] = data[k-1]; data[k-1] = tmp; printPermutations(data, n, k - 1); tmp = data[i]; data[i] = data[k-1]; data[k-1] = tmp; &#125;&#125; 如果我们确定了最后一位数据，那就变成了求解剩下 n−1 个数据的排列问题。而最后一位数据可以是 n 个数据中的任意一个，因此它的取值就有 nn种情况。所以，“n 个数据的排列”问题，就可以分解成 n 个“n−1 个数据的排列”的子问题。 第一层分解有 nn次交换操作，第二层有 n 个节点，每个节点分解需要 n−1 次交换，所以第二层总的交换次数是 n∗(n−1)。第三层有 n∗(n−1)n∗(n−1) 个节点，每个节点分解需要 n−2次交换，所以第三层总的交换次数是 n∗(n−1)∗(n−2)n∗(n−1)∗(n−2)。第 kk 层总的交换次数就是 n∗(n−1)∗(n−2)∗…∗(n−k+1)。最后一层为n∗(n−1)∗(n−2)∗…∗2∗1等于 n!。也就是说，全排列的递归算法的时间复杂度大于O(n!)，小于 O(n∗n!)。 小结有些代码比较适合用递推公式来分析，比如归并排序的时间复杂度、快速排序的最好情况时间复杂度；有些比较适合采用递归树来分析，比如快速排序的平均时间复杂度。而有些可能两个都不怎么适合使用，比如二叉树的递归前中后序遍历。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构18-红黑树","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构18-红黑树.html","text":"[toc] 平衡二叉查找树平衡二叉树的定义：二叉树中任意一个节点的左右子树的高度相差不能大于 1。完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。 发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。所以，平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。 如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比 log~2~n 大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。 什么是红黑树红黑树的英文是“Red-Black Tree”，简称 R-B Tree。它是一种不严格的平衡二叉查找树。 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点； 这里的第二点要求“叶子节点都是黑色的空节点”，稍微有些奇怪，它主要是为了简化红黑树的代码实现而设置的，只要满足这一条要求，那在任何时刻，红黑树的平衡操作都可以归结为特定的几类。暂时不考虑这一点，所以，在画图和讲解的时候，将黑色的、空的叶子节点都省略掉了。 红黑树“近似平衡”平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化的太严重。 如果我们将红色节点从红黑树中去掉，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。 红黑树的定义里有这么一条：从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。 完全二叉树的高度近似 log~2~n，这里的四叉“黑树”的高度要低于完全二叉树，所以去掉红色节点的“黑树”的高度也不会超过 log~2~n。 现在把红色节点加回去，在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过 log~2~n，所以加入红色节点之后，最长路径不会超过 2log~2~n，也就是说，红黑树的高度近似 2log~2~n。 所以，红黑树的高度只比高度平衡的 AVL 树的高度（log~2~n）仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。 Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。 AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了。 红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 AVL 树要低。 所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。 实现红黑树的基本思想左旋（rotate left）全称其实是叫围绕某个节点的左旋，右旋（rotate right）的全称叫围绕某个节点的右旋。 在插入、删除节点的过程中，第三、第四点要求可能会被破坏。 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点； 插入操作的平衡调整红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。 关于插入操作的平衡调整，有这样两种特殊情况： 如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。 如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。 除此之外，其他情况都会违背红黑树的定义，于是就需要进行调整，调整的过程包含两种基础的操作：左右旋转和改变颜色。 红黑树的平衡调整过程是一个迭代的过程。把正在处理的节点叫作关注节点。关注节点会随着不停地迭代处理，而不断发生变化。最开始的关注节点就是新插入的节点。 新节点插入之后，如果红黑树的平衡被打破，那一般会有下面三种情况。我们只需要根据每种情况的特点，不停地调整，就可以让红黑树继续符合定义，也就是继续保持平衡。 我们下面依次来看每种情况的调整过程。为了简化描述，把父节点的兄弟节点叫作叔叔节点，父节点的父节点叫作祖父节点。 CASE 1：如果关注节点是 a，它的叔叔节点 d 是红色 将关注节点 a 的父节点 b、叔叔节点 d 的颜色都设置成黑色； 将关注节点 a 的祖父节点 c 的颜色设置成红色； 关注节点变成 a 的祖父节点 c； 跳到 CASE 2 或者 CASE 3。 CASE 2：如果关注节点是 a，它的叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的右子节点 关注节点变成节点 a 的父节点 b； 围绕新的关注节点 b 左旋； 跳到 CASE 3。 CASE 3：如果关注节点是 a，它的叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的左子节点 围绕关注节点 a 的祖父节点 c 右旋； 将关注节点 a 的父节点 b、兄弟节点 c 的颜色互换。 调整结束。 删除操作的平衡调整红黑树插入操作的平衡调整还不是很难，但是它的删除操作的平衡调整相对就要难多了。不过原理都是类似的，依旧只需要根据关注节点与周围节点的排布特点，按照一定的规则去调整就行了。 删除操作的平衡调整分为两步，第一步是针对删除节点初步调整。初步调整只是保证整棵红黑树在一个节点删除之后，仍然满足最后一条定义的要求，也就是说，每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；第二步是针对关注节点进行二次调整，让它满足红黑树的第三条定义，即不存在相邻的两个红色节点。 针对删除节点初步调整红黑树的定义中“只包含红色节点和黑色节点”，经过初步调整之后，为了保证满足红黑树定义的最后一条要求，有些节点会被标记成两种颜色，“红 - 黑”或者“黑 - 黑”。如果一个节点被标记为了“黑 - 黑”，那在计算黑色节点个数的时候，要算成两个黑色节点。 在下面的讲解中，如果一个节点既可以是红色，也可以是黑色，在画图的时候，我会用一半红色一半黑色来表示。如果一个节点是“红 - 黑”或者“黑 - 黑”，我会用左上角的一个小黑点来表示额外的黑色。 CASE 1：如果要删除的节点是 a，它只有一个子节点 b 删除节点 a，并且把节点 b 替换到节点 a 的位置，这一部分操作跟普通的二叉查找树的删除操作一样； 节点 a 的位置只能是黑色，节点 b的位置也只能是红色（任意节点到可达子节点经过的黑色节点数量一致，任意节点的左右子树高度相差不超过1），其他情况均不符合红黑树的定义。这种情况下，我们把节点 b 改为黑色； 调整结束，不需要进行二次调整。 CASE 2：如果要删除的节点 a 有两个非空子节点，并且它的后继节点就是节点 a 的右子节点 c 如果节点 a 的后继节点就是右子节点 c，那右子节点 c 肯定没有左子树。我们把节点 a 删除，并且将节点 c 替换到节点 a 的位置。这一部分操作跟普通的二叉查找树的删除操作无异； 然后把节点 c 的颜色设置为跟节点 a 相同的颜色； 如果节点 c 是黑色，为了不违反红黑树的最后一条定义，我们给节点 c 的右子节点 d 多加一个黑色，这个时候节点 d 就成了“红 - 黑”或者“黑 - 黑”； 这个时候，关注节点变成了节点 d，第二步的调整操作就会针对关注节点来做。 CASE 3：如果要删除的是节点 a，它有两个非空子节点，并且节点 a 的后继节点不是右子节点 找到后继节点 d，并将它删除，删除后继节点 d 的过程参照 CASE 1； 将节点 a 替换成后继节点 d； 把节点 d 的颜色设置为跟节点 a 相同的颜色； 如果节点 d 是黑色，为了不违反红黑树的最后一条定义，我们给节点 d 的右子节点 c 多加一个黑色，这个时候节点 c 就成了“红 - 黑”或者“黑 - 黑”； 这个时候，关注节点变成了节点 c，第二步的调整操作就会针对关注节点来做。 针对关注节点进行二次调整 经过初步调整之后，关注节点变成了“红 - 黑”或者“黑 - 黑”节点。针对这个关注节点，我们再分四种情况来进行二次调整。二次调整是为了让红黑树中不存在相邻的红色节点。 CASE 1：如果关注节点是 a，它的兄弟节点 c 是红色的 围绕关注节点 a 的父节点 b 左旋； 关注节点 a 的父节点 b 和祖父节点 c 交换颜色； 关注节点不变； 继续从四种情况中选择适合的规则来调整。 CASE 2：如果关注节点是 a，它的兄弟节点 c 是黑色的，并且节点 c 的左右子节点 d、e 都是黑色的 将关注节点 a 的兄弟节点 c 的颜色变成红色； 从关注节点 a 中去掉一个黑色，这个时候节点 a 就是单纯的红色或者黑色； 给关注节点 a 的父节点 b 添加一个黑色，这个时候节点 b 就变成了“红 - 黑”或者“黑 - 黑”； 关注节点从 a 变成其父节点 b； 继续从四种情况中选择符合的规则来调整。 CASE 3：如果关注节点是 a，它的兄弟节点 c 是黑色，c 的左子节点 d 是红色，c 的右子节点 e 是黑色 围绕关注节点 a 的兄弟节点 c 右旋； 节点 c 和节点 d 交换颜色； 关注节点不变； 跳转到 CASE 4，继续调整。 CASE 4：如果关注节点 a 的兄弟节点 c 是黑色的，并且 c 的右子节点是红色的 围绕关注节点 a 的父节点 b 左旋； 将关注节点 a 的兄弟节点 c 的颜色，跟关注节点 a 的父节点 b 设置成相同的颜色； 将关注节点 a 的父节点 b 的颜色设置为黑色； 从关注节点 a 中去掉一个黑色，节点 a 就变成了单纯的红色或者黑色； 将关注节点 a 的叔叔节点 e 设置为黑色； 调整结束。 小结第一点，把红黑树的平衡调整的过程比作魔方复原，不要过于深究这个算法的正确性。你只需要明白，只要按照固定的操作步骤，保持插入、删除的过程，不破坏平衡树的定义就行了。 第二点，找准关注节点，不要搞丢、搞错关注节点。因为每种操作规则，都是基于关注节点来做的，只有弄对了关注节点，才能对应到正确的操作规则中。在迭代的调整过程中，关注节点在不停地改变，所以，这个过程一定要注意，不要弄丢了关注节点。 第三点，插入操作的平衡调整比较简单，但是删除操作就比较复杂。针对删除操作，我们有两次调整，第一次是针对要删除的节点做初步调整，让调整后的红黑树继续满足第四条定义，“每个节点到可达叶子节点的路径都包含相同个数的黑色节点”。但是这个时候，第三条定义就不满足了，有可能会存在两个红色节点相邻的情况。第二次调整就是解决这个问题，让红黑树不存在相邻的红色节点。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构21-图的表示","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构21-图的表示.html","text":"[toc] 图 图中的元素我们就叫作顶点（vertex）。 图中的一个顶点可以与任意其他顶点建立连接关系，这种建立的关系叫作边（edge）。 无向图中顶点相连接的边的条数叫作顶点的度（degree）。 有向图中，度分为入度（In-degree）和出度（Out-degree）：顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。 在带权图中，每条边都有一个权重（weight）。 邻接矩阵存储方法图最直观的一种存储方法就是，邻接矩阵（Adjacency Matrix）。 邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 $A[i][j]$ 和 $A[j][i]$ 标记为 1；对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 $A[i][j]$ 标记为 1。同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 $A[j][i]$ 标记为 1。对于带权图，数组中就存储相应的权重。 用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。对于无向图来说，如果 $A[i][j]$ 等于 1，那 $A[j][i]$ 也肯定等于 1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。如果我们存储的是稀疏图（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。 邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。 用邻接矩阵存储图的另外一个好处是方便计算。这是因为，用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。 邻接表存储方法邻接表（Adjacency List）每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。 图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点， 邻接表存储起来比较节省空间，但是使用起来就比较耗时间。可以将邻接表中的链表改成平衡二叉查找树。实际开发中，可以选择用红黑树。这样，可以更加快速地查找两个顶点之间是否存在边。二叉查找树也可以换成其他动态数据结构，比如跳表、散列表等。除此之外，还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构20-堆排序","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构20-堆和堆排序.html","text":"堆 堆是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。 第一点，堆必须是一个完全二叉树。完全二叉树要求除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。 第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。 对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆”。 其中第 1 个和第 2 个是大顶堆，第 3 个是小顶堆，第 4 个不是堆。 一个包含 n 个节点的完全二叉树，树的高度不会超过 log~2~⁡n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)。 插入如果把新插入的元素放到堆的最后，就不符合堆的特性了，于是就需要进行调整，让其重新满足堆的特性，这个过程起了一个名字，就叫作堆化（heapify）。 堆化实际上有两种，从下往上和从上往下。这里先讲从下往上的堆化方法。 让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。 12345678910111213141516171819202122public class Heap &#123; private int[] a; // 数组，从下标 1 开始存储数据 private int n; // 堆可以存储的最大数据个数 private int count; // 堆中已经存储的数据个数 public Heap(int capacity) &#123; a = new int[capacity + 1]; n = capacity; count = 0; &#125; public void insert(int data) &#123; if (count &gt;= n) return; // 堆满了 ++count; a[count] = data; int i = count; while (i/2 &gt; 0 &amp;&amp; a[i] &gt; a[i/2]) &#123; // 自下往上堆化 swap(a, i, i/2); // swap() 函数作用：交换下标为 i 和 i/2 的两个元素 i = i/2; &#125; &#125; &#125; 删除假设我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。 实际上，我们稍微改变一下思路，就可以解决这个问题。你看我画的下面这幅图。我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。 1234567891011121314151617public void removeMax() &#123; if (count == 0) return -1; // 堆中没有数据 a[1] = a[count]; --count; heapify(a, count, 1);&#125; private void heapify(int[] a, int n, int i) &#123; // 自上往下堆化 while (true) &#123; int maxPos = i; if (i*2 &lt;= n &amp;&amp; a[i] &lt; a[i*2]) maxPos = i*2; if (i*2+1 &lt;= n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; &#125;&#125; 堆排序建堆我们首先将数组原地建成一个堆。所谓“原地”就是，不借助另一个数组，就在原数组上操作。建堆的过程，有两种思路。 第一种是借助前面讲的，在堆中插入一个元素的思路。尽管数组中包含 n 个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为 1 的数据。然后，我们调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含 n 个数据的数组，组织成了堆。 第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且每个数据都是从上往下堆化。因为叶子节点往下堆化只能自己跟自己比较，所以直接从第一个非叶子节点开始，依次堆化就行了。 对于完全二叉树来说，下标从n/2+1 到 n 的节点都是叶子节点。 12345678910111213141516private static void buildHeap(int[] a, int n) &#123; for (int i = n/2; i &gt;= 1; --i) &#123; heapify(a, n, i); &#125;&#125; private static void heapify(int[] a, int n, int i) &#123; while (true) &#123; int maxPos = i; if (i*2 &lt;= n &amp;&amp; a[i] &lt; a[i*2]) maxPos = i*2; if (i*2+1 &lt;= n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; &#125;&#125; 因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度 kk成正比。 将每个非叶子节点的高度求和： S=1*h+2^1*(h-1)+2^2*(h-2)+...+2^{h-1}*1\\\\ S=2^{h+1}-h-2\\\\ h=\\log_2n所以，建堆的时间复杂度就是 O(n)。 排序建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。这个过程有点类似上面讲的“删除堆顶元素”的操作，当堆顶元素移除之后，我们把下标为 nn的元素放到堆顶，然后再通过堆化的方法，将剩下的 n−1 个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是 n−1 的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。 12345678910// n 表示数据的个数，数组 a 中的数据从下标 1 到 n 的位置。public static void sort(int[] a, int n) &#123; buildHeap(a, n); int k = n; while (k &gt; 1) &#123; swap(a, 1, k); --k; heapify(a, k, 1); &#125;&#125; 整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlog⁡n)，所以，堆排序整体的时间复杂度是 O(nlogn)。 堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。 在前面的讲解以及代码中，我都假设，堆中的数据是从数组下标为 1 的位置开始存储。那如果从 0 开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了.如果节点的下标是 i，那左子节点的下标就是 2∗i+1，右子节点的下标就是 2∗i+2，父节点的下标就是 $\\frac{i-1}{2}$。 特点 堆排序数据访问的方式没有快速排序友好。 对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。 比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶节点进行堆化，会依次访问数组下标是 1，2，4，8 的元素，而不是像快速排序那样，局部顺序访问，所以，这样对 CPU 缓存是不友好的。 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序 在讲排序的时候，提过两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多。 但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。 堆的应用优先级队列优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。 堆和优先级队列非常相似，一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。 合并有序小文件假设有 100 个小文件，每个文件的大小是 100MB，每个文件中存储的都是有序的字符串。希望将这些 100 个小文件合并成一个有序的大文件。这里就会用到优先级队列。 整体思路有点像归并排序中的合并函数。从这 100 个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。 假设，这个最小的字符串来自于 13.txt 这个小文件，我们就再从这个小文件取下一个字符串，并且放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，并且将它从数组中删除。依次类推，直到所有的文件中的数据都放入到大文件为止。 这里用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。 很好的方法是用优先级队列，将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中。 删除堆顶数据和往堆中插入数据的时间复杂度都是 O(logn)，n 表示堆中的数据个数，这里就是 100。比原来数组存储的方式高效了很多 高性能定时器假设有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。 但是，这样每过 1 秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。 针对这些问题，就可以用优先级队列来解决。按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。 这样，定时器就不需要每隔 1 秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 T。 这个时间间隔 T 就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在 T 秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。 当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。 这样，定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。 求 Top K求 Top K 的问题抽象成两类。一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。 针对静态数据，如何在一个包含 n 个数据的数组中，查找前 K 大数据呢？我们可以维护一个大小为 K 的小顶堆，顺序遍历数组，从数组中取出取数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。遍历数组需要 O(n) 的时间复杂度，一次堆化操作需要 O(logK) 的时间复杂度，所以最坏情况下，n 个元素都入堆一次，所以时间复杂度就是 O(nlogK)。 针对动态数据求得 Top K 就是实时 Top K。怎么理解呢？举一个例子。一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前 K 大数据。 如果每次询问前 K 大数据，我们都基于当前的全部数据重新计算的话，那时间复杂度就是 O(nlogK)，n 表示当前的数据的大小。实际上，可以一直都维护一个 K 大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 K 大数据，我们都可以里立刻返回给他。 求中位数中位数，顾名思义，就是处在中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第 $\\frac{n}{2}+1$ 个数据就是中位数；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第 $\\frac{n}{2}$ 个和第 $\\frac{n}{2}+1$个数据，这个时候，可以随意取一个作为中位数。 对于一组静态数据，中位数是固定的，我们可以先排序，第 $\\frac{n}{2}$个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。所以，尽管排序的代价比较大，但是边际成本会很小。但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。 借助堆这种数据结构，不用排序，就可以非常高效地实现求中位数操作，需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。 也就是说，如果有 n 个数据，n 是偶数，我们从小到大排序，那前 n/2个数据存储在大顶堆中，后 n/2个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果 n 是奇数，情况是类似的，大顶堆就存储 n/2+1 个数据，小顶堆中就存储 n/2 个数据。 如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；如果新加入的数据大于等于小顶堆的堆顶元素，我们就将这个新数据插入到小顶堆。 这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果 n 是偶数，两个堆中的数据个数都是 n/2；如果 n 是奇数，大顶堆有 n/2+1 个数据，小顶堆有 n/2 个数据。这个时候，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。 于是，我们就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间复杂度变成了 O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是 O(1)。 实际上，利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 n*99% 个数据，小顶堆中保存 n*1% 个数据。大顶堆堆顶的数据就是我们要找的 99% 分位数据。 每次插入一个数据的时候，要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。 但是，为了保持大顶堆中的数据占 99%，小顶堆中的数据占 1%，在每次新插入数据之后，都要重新计算大顶堆和小顶堆中的数据个数，是否还符合 99:1 这个比例。如果不符合，就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法。 通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是 O(logn)。每次求 99% 响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是 O(1)。 12345678910111213141516171819class MedianFinder &#123; Queue&lt;Integer&gt; A, B; public MedianFinder() &#123; A = new PriorityQueue&lt;&gt;(); // 小顶堆，保存较大的一半 B = new PriorityQueue&lt;&gt;((x, y) -&gt; (y - x)); // 大顶堆，保存较小的一半 &#125; public void addNum(int num) &#123; if(A.size() != B.size()) &#123; A.add(num); B.add(A.poll()); &#125; else &#123; B.add(num); A.add(B.poll()); &#125; &#125; public double findMedian() &#123; return A.size() != B.size() ? A.peek() : (A.peek() + B.peek()) / 2.0; &#125;&#125;","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构22-深度和广度优先搜索","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构22-深度和广度优先搜索.html","text":"[toc] 深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。 1234567891011121314151617public class Graph &#123; // 无向图 private int v; // 顶点的个数 private LinkedList&lt;Integer&gt; adj[]; // 邻接表 public Graph(int v) &#123; this.v = v; adj = new LinkedList[v]; for (int i=0; i&lt;v; ++i) &#123; adj[i] = new LinkedList&lt;&gt;(); &#125; &#125; public void addEdge(int s, int t) &#123; // 无向图一条边存两次 adj[s].add(t); adj[t].add(s); &#125;&#125; 广度优先搜索广度优先搜索（Breadth-First-Search）,简称为 BFS，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。 12345678910111213141516171819202122232425262728293031323334public void bfs(int s, int t) &#123; //其中 s 表示起始顶点，t 表示终止顶点。 if (s == t) return; boolean[] visited = new boolean[v]; visited[s]=true; Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); queue.add(s); int[] prev = new int[v]; for (int i = 0; i &lt; v; ++i) &#123; prev[i] = -1; &#125; while (queue.size() != 0) &#123; int w = queue.poll(); for (int i = 0; i &lt; adj[w].size(); ++i) &#123; int q = adj[w].get(i); if (!visited[q]) &#123; prev[q] = w; if (q == t) &#123; print(prev, s, t); return; &#125; visited[q] = true; queue.add(q); &#125; &#125; &#125;&#125; private void print(int[] prev, int s, int t) &#123; // 递归打印 s-&gt;t 的路径 if (prev[t] != -1 &amp;&amp; t != s) &#123; print(prev, s, prev[t]); &#125; System.out.print(t + \" \");&#125; visited是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点 q 被访问，那相应的 visited[q] 会被设置为 true。 queue是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第 k 层的顶点都访问完成之后，才能访问第 k+1 层的顶点。当我们访问到第 k 层的顶点的时候，我们需要把第 k 层的顶点记录下来，稍后才能通过第 k 层的顶点来找第 k+1 层的顶点。所以，我们用这个队列来实现记录的功能。 prev用来记录搜索路径。当我们从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。不过，这个路径是反向存储的。prev[w] 存储的是，顶点 w 是从哪个前驱顶点遍历过来的。比如，我们通过顶点 2 的邻接表访问到顶点 3，那 prev[3] 就等于 2。为了正向打印出路径，我们需要递归地来打印，你可以看下 print() 函数的实现方式。 最坏情况下，终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是 O(V+E)，其中，V 表示顶点的个数，E 表示边的个数。当然，对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，E 肯定要大于等于 V-1，所以，广度优先搜索的时间复杂度也可以简写为 O(E)。 广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是 O(V)。 深度优先搜索深度优先搜索（Depth-First-Search），简称 DFS。 最直观的例子就是“走迷宫”。假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。 这里面实线箭头表示遍历，虚线箭头表示回退。从图中可以看出，深度优先搜索找出来的路径，并不是顶点 s 到顶点 t 的最短路径。 实际上，深度优先搜索用的是一种比较著名的算法思想，回溯思想。这种思想解决问题的过程，非常适合用递归来实现。 12345678910111213141516171819202122232425262728boolean found = false; // 全局变量或者类成员变量 public void dfs(int s, int t) &#123; found = false; boolean[] visited = new boolean[v]; int[] prev = new int[v]; for (int i = 0; i &lt; v; ++i) &#123; prev[i] = -1; &#125; recurDfs(s, t, visited, prev); print(prev, s, t);&#125; private void recurDfs(int w, int t, boolean[] visited, int[] prev) &#123; if (found == true) return; visited[w] = true; if (w == t) &#123; found = true; return; &#125; for (int i = 0; i &lt; adj[w].size(); ++i) &#123; int q = adj[w].get(i); if (!visited[q]) &#123; prev[q] = w; recurDfs(q, t, visited, prev); &#125; &#125;&#125; 深度优先搜索代码实现也用到了 prev、visited 变量以及 print() 函数，它们跟广度优先搜索代码实现里的作用是一样的。不过，深度优先搜索代码实现里，有个比较特殊的变量 found，它的作用是，当已经找到终止顶点 t 之后，就不再递归地继续查找了。 从我面画的图可以看出，每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是 O(E)，E 表示边的个数。 深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是 O(V)。 小结广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，比如 A、IDA 等，要简单粗暴，没有什么优化，所以，也被叫作暴力搜索算法。所以，这两种搜索算法仅适用于状态空间不大，也就是说图不大的搜索。 广度优先搜索，通俗的理解就是，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先搜索的时间复杂度都是 O(E)，空间复杂度是 O(V)。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构23-字符串匹配","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构23-字符串匹配.html","text":"[toc] BF (Brute Force) 算法BF 算法( Brute Force )，中文叫作暴力匹配算法，也叫朴素匹配算法。 在字符串 A 中查找字符串 B，那字符串 A 就是主串，字符串 B 就是模式串。把主串的长度记作 n，模式串的长度记作 m。因为我们是在主串中查找模式串，所以 n&gt;m。 BF 算法的思想可以用一句话来概括，那就是，在主串中，检查起始位置分别是 0、1、2…n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的。 从上面的算法思想和例子，可以看出，在极端情况下，比如主串是“aaaaa…aaaaaa”（省略号表示有很多重复的字符 a），模式串是“aaaaab”。我们每次都比对 m 个字符，要比对 n-m+1 次，所以，这种算法的最坏情况时间复杂度是 O(n*m)。 尽管理论上，BF 算法的时间复杂度很高，是 O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。 第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把 m 个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。 第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有 bug 也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。 所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。 RK (Rabin-Karp) 算法RK 算法的全称叫 Rabin-Karp 算法，是由它的两位发明者 Rabin 和 Karp 的名字来命名的。 RK 算法的思路是这样的：通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。 不过，通过哈希算法计算子串的哈希值的时候，需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。有没有方法可以提高哈希算法计算子串哈希值的效率呢？ 假设要匹配的字符串的字符集中只包含 K 个字符，我们可以用一个 K 进制数来表示一个子串，这个 K 进制数转化成十进制数，作为子串的哈希值。比如要处理的字符串只包含 a～z 这 26 个小写字母，那我们就用二十六进制来表示一个字符串。我们把 a～z 这 26 个字符映射到 0～25 这 26 个数字，a 就表示 0，b 就表示 1，以此类推，z 表示 25。在十进制的表示法中，一个数字的值是通过下面的方式计算出来的。对应到二十六进制，一个包含 a 到 z 这 26 个字符的字符串，计算哈希的时候，我们只需要把进位从 10 改成 26 就可以。 这种哈希算法有一个特点，在主串中，相邻两个子串的哈希值的计算公式有一定关系：相邻两个子串 s[i-1] 和 s[i]（i 表示子串在主串中的起始位置，子串的长度都为 m），对应的哈希值计算公式有交集，也就是说，我们可以使用 s[i-1] 的哈希值很快的计算出 s[i] 的哈希值。 不过，这里有一个小细节需要注意，那就是 26^(m-1) 这部分的计算，我们可以通过查表的方法来提高效率。我们事先计算好 26^0^、26^1^、26^2^……26^(m-1)，并且存储在一个长度为 m 的数组中，公式中的“次方”就对应数组的下标。当我们需要计算 26 的 x 次方的时候，就可以从数组的下标为 x 的位置取值，直接使用，省去了计算的时间。 整个 RK 算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是 O(n)。 模式串哈希值与每个子串哈希值之间的比较的时间复杂度是 O(1)，总共需要比较 n-m+1 个子串的哈希值，所以，这部分的时间复杂度也是 O(n)。所以，RK 算法整体的时间复杂度就是 O(n)。 这里还有一个问题就是，模式串很长，相应的主串中的子串也会很长，通过上面的哈希算法计算得到的哈希值就可能很大，如果超过了计算机中整型数据可以表示的范围，那该如何解决呢？比如将每一个字母从小到大对应一个素数，而不是 1，2，3……这样的自然数，这样冲突的概率就会降低一些。 RK 算法的时间复杂度是 O(n)，跟 BF 算法相比，效率提高了很多。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为 O(n*m)。 BM (Boyer-Moore)算法核心思想模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF 算法和 RK 算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。 在这个例子里，主串中的 c，在模式串中是不存在的，所以，模式串向后滑动的时候，只要 c 与模式串有重合，肯定无法匹配。所以，我们可以一次性把模式串往后多滑动几位，把模式串移动到 c 的后面。 BM 算法本质上其实就是在寻找这种规律。借助这种规律，在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位。 原理分析BM 算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）。 坏字符规则BM 算法的匹配顺序是按照模式串下标从大到小的顺序，倒着匹配的。 当发生不匹配的时候，把坏字符对应的模式串中的字符下标记作 si。如果坏字符在模式串中存在，把这个坏字符在模式串中的下标记作 xi。如果不存在，我们把 xi 记作 -1。那模式串往后移动的位数就等于 si-xi。（注意，我这里说的下标，都是字符在模式串的下标）。如果坏字符在模式串里多处出现，那我们在计算 xi 的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。 利用坏字符规则，BM 算法在最好情况下的时间复杂度非常低，是 O(n/m)。比如，主串是 aaabaaabaaabaaab，模式串是 aaaa。每次比对，模式串都可以直接后移四位，所以，匹配具有类似特点的模式串和主串的时候，BM 算法非常高效。 不过，单纯使用坏字符规则还是不够的。因为根据 si-xi 计算出来的移动位数，有可能是负数，比如主串是 aaaaaaaaaaaaaaaa，模式串是 baaa。不但不会向后滑动模式串，还有可能倒退。所以，BM 算法还需要用到“好后缀规则”。 好后缀规则依然是倒序匹配，我们把已经匹配的 bc 叫作好后缀，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。 如果在模式串中找不到另一个等于{u}的子串，我们不仅要看好后缀在模式串中，是否有另一个匹配的子串，我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。 我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的，假设是{v}，然后将模式串滑动到如图所示的位置。 当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？ 我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。 代码实现假设字符串的字符集不是很大，每个字符长度是 1 字节，我们用大小为 256 的数组，来记录每个字符在模式串中最后出现的位置。数组的下标对应字符的 ASCII 码值，数组中存储这个字符在模式串中出现的位置。 掌握了坏字符规则之后，先把 BM 算法代码的大框架写好，先不考虑好后缀规则，仅用坏字符规则，并且不考虑 si-xi 计算得到的移动位数可能会出现负数的情况。 1234567891011121314151617public int bm(char[] a, int n, char[] b, int m) &#123; int[] bc = new int[SIZE]; // 记录模式串中每个字符最后出现的位置 generateBC(b, m, bc); // 构建坏字符哈希表 int i = 0; // i 表示主串与模式串对齐的第一个字符 while (i &lt;= n - m) &#123; int j; for (j = m - 1; j &gt;= 0; --j) &#123; // 模式串从后往前匹配 if (a[i+j] != b[j]) break; // 坏字符对应模式串中的下标是 j &#125; if (j &lt; 0) &#123; return i; // 匹配成功，返回主串与模式串第一个匹配的字符的位置 &#125; // 这里等同于将模式串往后滑动 j-bc[(int)a[i+j]] 位 i = i + (j - bc[(int)a[i+j]]); &#125; return -1;&#125; 因为好后缀也是模式串本身的后缀子串，所以，我们可以在模式串和主串正式匹配之前，通过预处理模式串，预先计算好模式串的每个后缀子串，对应的另一个可匹配子串的位置。 因为后缀子串的最后一个字符的位置是固定的，下标为 m-1，只需要记录长度就可以了。通过长度，可以确定一个唯一的后缀子串。 现在，要引入最关键的变量 suffix 数组。suffix 数组的下标 k，表示后缀子串的长度，下标对应的数组值存储的是，在模式串中跟好后缀{u}相匹配的子串{u*}的起始下标值。如，在模式串中和后一个b匹配的下标是2。如果模式串中有多个（大于 1 个）子串跟后缀子串{u}匹配，存储模式串中最靠后的那个子串的起始位置，也就是下标最大的那个子串的起始位置，避免模式串往后滑动得过头了。 123456789101112131415161718// b 表示模式串，m 表示长度，suffix，prefix 数组事先申请好了private void generateGS(char[] b, int m, int[] suffix, boolean[] prefix) &#123; for (int i = 0; i &lt; m; ++i) &#123; // 初始化 suffix[i] = -1; prefix[i] = false; &#125; for (int i = 0; i &lt; m - 1; ++i) &#123; // b[0, i] int j = i; int k = 0; // 公共后缀子串长度 while (j &gt;= 0 &amp;&amp; b[j] == b[m-1-k]) &#123; // 与 b[0, m-1] 求公共后缀子串 --j; ++k; suffix[k] = j+1; //j+1 表示公共后缀子串在 b[0, i] 中的起始下标 &#125; i if (j == -1) prefix[k] = true; // 如果公共后缀子串也是模式串的前缀子串 &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637// a,b 表示主串和模式串；n，m 表示主串和模式串的长度。public int bm(char[] a, int n, char[] b, int m) &#123; int[] bc = new int[SIZE]; // 记录模式串中每个字符最后出现的位置 generateBC(b, m, bc); // 构建坏字符哈希表 int[] suffix = new int[m]; boolean[] prefix = new boolean[m]; generateGS(b, m, suffix, prefix); int i = 0; // j 表示主串与模式串匹配的第一个字符 while (i &lt;= n - m) &#123; int j; for (j = m - 1; j &gt;= 0; --j) &#123; // 模式串从后往前匹配 if (a[i+j] != b[j]) break; // 坏字符对应模式串中的下标是 j &#125; if (j &lt; 0) &#123; return i; // 匹配成功，返回主串与模式串第一个匹配的字符的位置 &#125; int x = j - bc[(int)a[i+j]]; int y = 0; if (j &lt; m-1) &#123; // 如果有好后缀的话 y = moveByGS(j, m, suffix, prefix); &#125; i = i + Math.max(x, y); &#125; return -1;&#125; // j 表示坏字符对应的模式串中的字符下标 ; m 表示模式串长度private int moveByGS(int j, int m, int[] suffix, boolean[] prefix) &#123; int k = m - 1 - j; // 好后缀长度 if (suffix[k] != -1) return j - suffix[k] +1; for (int r = j+2; r &lt;= m-1; ++r) &#123; if (prefix[m-r] == true) &#123; return r; &#125; &#125; return m;&#125; 性能分析及优化整个算法用到了额外的 3 个数组，其中 bc 数组的大小跟字符集大小有关，suffix 数组和 prefix 数组的大小跟模式串长度 m 有关。 如果我们处理字符集很大的字符串匹配问题，bc 数组对内存的消耗就会比较多。因为好后缀和坏字符规则是独立的，如果我们运行的环境对内存要求苛刻，可以只使用好后缀规则，不使用坏字符规则，这样就可以避免 bc 数组过多的内存消耗。不过，单纯使用好后缀规则的 BM 算法效率就会下降一些了。 BM 算法的时间复杂度分析起来是非常复杂，这篇论文“A new proof of the linearity of the Boyer-Moore string searching algorithm”证明了在最坏情况下，BM 算法的比较次数上限是 5n。这篇论文“Tight bounds on the complexity of the Boyer-Moore string matching algorithm”证明了在最坏情况下，BM 算法的比较次数上限是 3n。 BM 算法核心思想是，利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。BM 算法构建的规则有两类，坏字符规则和好后缀规则。好后缀规则可以独立于坏字符规则使用。因为坏字符规则的实现比较耗内存，为了节省内存，我们可以只用好后缀规则来实现 BM 算法。 KMP 算法基本原理KMP 算法是根据三位作者（D.E.Knuth，J.H.Morris 和 V.R.Pratt）的名字来命名的，算法的全称是 Knuth Morris Pratt 算法，简称为 KMP 算法。 在模式串和主串匹配的过程中，把不能匹配的那个字符仍然叫作坏字符，把已经匹配的那段字符串叫作好前缀。 看不懂！！！","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构24-Trie 树","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构24-Trie树.html","text":"[toc] Trie 树Trie 树，也叫“字典树”。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。 这样一个问题可以有多种解决方法，比如散列表、红黑树，或者前面几节的一些字符串匹配算法，但是，Trie 树在这个问题的解决上，有它特有的优点。不仅如此，Trie 树能解决的问题也不限于此。 有 6 个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这 6 个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？ 可以先对这 6 个字符串做一下预处理，组织成 Trie 树的结构，之后每次查找，都是在 Trie 树中进行匹配查找。Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。 其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）。 12345678910111213141516171819202122232425262728293031323334353637383940public class Trie &#123; private TrieNode root = new TrieNode('/'); // 存储无意义字符 // 往 Trie 树中插入一个字符串 public void insert(char[] text) &#123; TrieNode p = root; for (int i = 0; i &lt; text.length; ++i) &#123; int index = text[i] - 'a'; if (p.children[index] == null) &#123; TrieNode newNode = new TrieNode(text[i]); p.children[index] = newNode; &#125; p = p.children[index]; &#125; p.isEndingChar = true; &#125; // 在 Trie 树中查找一个字符串 public boolean find(char[] pattern) &#123; TrieNode p = root; for (int i = 0; i &lt; pattern.length; ++i) &#123; int index = pattern[i] - 'a'; if (p.children[index] == null) &#123; return false; // 不存在 pattern &#125; p = p.children[index]; &#125; if (p.isEndingChar == false) return false; // 不能完全匹配，只是前缀 else return true; // 找到 pattern &#125; public class TrieNode &#123; public char data; public TrieNode[] children = new TrieNode[26]; public boolean isEndingChar = false; public TrieNode(char data) &#123; this.data = data; &#125; &#125;&#125; 如果要在一组字符串中，频繁地查询某些字符串，用 Trie 树会非常高效。构建 Trie 树的过程，需要扫描所有的字符串，时间复杂度是 O(n)（n 表示所有字符串的长度和）。但是一旦构建成功之后，后续的查询操作会非常高效。 每次查询时，如果要查询的字符串长度是 k，那我们只需要比对大约 k 个节点，就能完成查询操作。跟原本那组字符串的长度和个数没有任何关系。所以说，构建好 Trie 树后，在其中查找字符串的时间复杂度是 O(k)，k 表示要查找的字符串的长度。 Trie 树是非常耗内存的，用的是一种空间换时间的思路 Trie 树的实现的时候，讲到用数组来存储一个节点的子节点的指针。如果字符串中包含从 a 到 z 这 26 个字符，那每个节点都要存储一个长度为 26 的数组，并且每个数组存储一个 8 字节指针（或者是 4 字节，这个大小跟 CPU、操作系统、编译器等有关）。而且，即便一个节点只有很少的子节点，远小于 26 个，比如 3、4 个，我们也要维护一个长度为 26 的数组。 Trie 树的本质是避免重复存储一组字符串的相同前缀子串，但是现在每个字符（对应一个节点）的存储远远大于 1 个字节。在重复的前缀并不多的情况下，Trie 树不但不能节省内存，还有可能会浪费更多的内存。 将每个节点中的数组换成其他数据结构，比如有序数组、跳表、散列表、红黑树等。 假设我们用有序数组，数组中的指针按照所指向的子节点中的字符的大小顺序排列。查询的时候，我们可以通过二分查找的方法，快速查找到某个字符应该匹配的子节点的指针。但是，在往 Trie 树中插入一个字符串的时候，我们为了维护数组中数据的有序性，就会稍微慢了点。 实际上，Trie 树的变体有很多，都可以在一定程度上解决内存消耗的问题。比如，缩点优化，就是对只有一个子节点的节点，而且此节点不是一个串的结束节点，可以将此节点与子节点合并。这样可以节省空间，但却增加了编码难度。 Trie 树与散列表、红黑树的比较实际上，字符串的匹配问题，笼统上讲，其实就是数据的查找问题。对于支持动态数据高效操作的数据结构，比如散列表、红黑树、跳表等等。实际上，这些数据结构也可以实现在一组字符串中查找字符串的功能。 在一组字符串中查找字符串，Trie 树实际上表现并不好。它对要处理的字符串有严苛的要求。 第一，字符串中包含的字符集不能太大。如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。 第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。 第三，如果要用 Trie 树解决问题，那就要自己从零开始实现一个 Trie 树，还要保证没有 bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。 第四，通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。 综合这几点，针对在一组字符串中查找字符串的问题，在工程中，更倾向于用散列表或者红黑树。因为这两种数据结构，都不需要自己去实现，直接利用编程语言中提供的现成类库就行了。 实际上，Trie 树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适合的是查找前缀匹配的字符串。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构25-AC自动机","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构25-AC自动机.html","text":"[toc] AC自动机实现一个高性能的敏感词过滤系统 基于单模式串和 Trie 树实现的敏感词过滤BF 算法、RK 算法、BM 算法、KMP 算法，还有 Trie 树。前面四种算法都是单模式串匹配算法，只有 Trie 树是多模式串匹配算法。 单模式串匹配算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串。多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。 对敏感词字典进行预处理，构建成 Trie 树结构。这个预处理的操作只需要做一次，如果敏感词字典动态更新了，比如删除、添加了一个敏感词，那只需要动态更新一下 Trie 树就可以了。 当用户输入一个文本内容后，把用户输入的内容作为主串，从第一个字符（假设是字符 C）开始，在 Trie 树中匹配。当匹配到 Trie 树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符 C 的下一个字符开始，重新在 Trie 树中匹配。 基于 Trie 树的这种处理方法，有点类似单模式串匹配的 BF 算法。我们知道，单模式串匹配算法中，KMP 算法对 BF 算法进行改进，引入了 next 数组，让匹配失败时，尽可能将模式串往后多滑动几位。借鉴单模式串的优化改进方法，能否对多模式串 Trie 树进行改进，进一步提高 Trie 树的效率呢？这就要用到 AC 自动机算法了。 经典的多模式串匹配算法：AC 自动机AC 自动机算法，全称是 Aho-Corasick 算法。其实，Trie 树跟 AC 自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟 KMP 算法之间的关系一样，只不过前者针对的是多模式串而已。所以，AC 自动机实际上就是在 Trie 树之上，加了类似 KMP 的 next 数组，只不过此处的 next 数组是构建在树上罢了。 所以，AC 自动机的构建，包含两个操作： 将多个模式串构建成 Trie 树； 在 Trie 树上构建失败指针（相当于 KMP 中的失效函数 next 数组）。 构建好 Trie 树之后，如何在它之上构建失败指针？","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构22-深度和广度优先搜索","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构26-贪心算法.html","text":"[toc] 贪心算法（greedy algorithm）贪心算法有很多经典的应用，比如霍夫曼编码（Huffman Coding）、Prim 和 Kruskal 最小生成树算法、还有 Dijkstra 单源最短路径算法。 假设我们有一个可以容纳 100kg 物品的背包，可以装各种物品。我们有以下 5 种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？ 只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆，所以，我们可以往背包里装 20kg 黑豆、30kg 绿豆、50kg 红豆。 总结一下贪心算法解决问题的步骤： 第一步，当看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。类比到刚刚的例子，限制值就是重量不能超过 100kg，期望值就是物品的总价值。这组数据就是 5 种豆子。我们从中选出一部分，满足重量不超过 100kg，并且总价值最大。 第二步，尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。类比到刚刚的例子，我们每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。 第三步，举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。 实际上，用贪心算法解决问题的思路，并不总能给出最优解。 在一个有权图中，从顶点 S 开始，找一条到顶点 T 的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点 T。按照这种思路，我们求出的最短路径是 S-&gt;A-&gt;E-&gt;T，路径长度是 1+4+4=9。 但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径 S-&gt;B-&gt;D-&gt;T 才是最短路径，因为这条路径的长度是 2+2+2=6。 在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果我们第一步从顶点 S 走到顶点 A，那接下来面对的顶点和边，跟第一步从顶点 S 走到顶点 B，是完全不同的。所以，即便我们第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。 贪心算法实战分析分糖果我们有 m 个糖果和 n 个孩子。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m&lt;n），所以糖果只能分配给一部分孩子。 每个糖果的大小不等，这 m 个糖果的大小分别是 s1，s2，s3，……，sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这 n 个孩子对糖果大小的需求分别是 g1，g2，g3，……，gn。如何分配糖果，能尽可能满足最多数量的孩子？ 我们可以把这个问题抽象成，从 n 个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数（期望值）是最大的。这个问题的限制值就是糖果个数 m。 对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。 我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。 钱币找零这个问题在我们的日常生活中更加普遍。假设我们有 1 元、2 元、5 元、10 元、20 元、50 元、100 元这些面额的纸币，它们的张数分别是 c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付 K 元，最少要用多少张纸币呢？ 在生活中，我们肯定是先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用 1 元来补齐。 在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。直觉告诉我们，这种处理方法就是最好的。实际上，要严谨地证明这种贪心算法的正确性，需要比较复杂的。 区间覆盖假设我们有 n 个区间，区间的起始端点和结束端点分别是 [l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。我们从这 n 个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？ 这个问题的处理思路稍微不是那么好懂，不过，我建议你最好能弄懂，因为这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。 这个问题的解决思路是这样的：我们假设这 n 个区间中最左端点是 lmin，最右端点是 rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将 [lmin, rmax] 覆盖上。我们按照起始端点从小到大的顺序对这 n 个区间排序。 我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。 霍夫曼编码霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在 20%～90% 之间。 霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。 对于等长的编码来说，我们解压缩起来很简单。用 3 个 bit 表示一个字符，在解压缩的时候，我们每次从文本中读取 3 位二进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取 1 位还是 2 位、3 位等等来解压缩呢？这个问题就导致霍夫曼编码解压缩起来比较复杂。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。 假设这 6 个字符出现的频率从高到低依次是 a、b、c、d、e、f。我们把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀，在解压缩的时候，我们每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这 1000 个字符只需要 2100bits 就可以了。 尽管霍夫曼编码的思想并不难理解，但是如何根据字符出现频率的不同，给不同的字符进行不同长度的编码呢？这里的处理稍微有些技巧。 我们把每个字符看作一个节点，并且辅带着把频率放到优先级队列中。我们从队列中取出频率最小的两个节点 A、B，然后新建一个节点 C，把频率设置为两个节点的频率之和，并把这个新节点 C 作为节点 A、B 的父节点。最后再把 C 节点放入到优先级队列中。重复这个过程，直到队列中没有数据。 现在，我们给每一条边加上画一个权值，指向左子节点的边我们统统标记为 0，指向右子节点的边，我们统统标记为 1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构27-分治算法","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构27-分治算法.html","text":"[toc] 分治算法分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。 分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作： 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题。 分治算法能解决的问题，一般需要满足下面这几个条件： 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。 应用举例分析假设我们有 n 个数据，我们期望数据从小到大排列，那完全有序的数据的有序度就是 n(n-1)/2，逆序度等于 0；相反，倒序排列的数据的有序度就是 0，逆序度是 n(n-1)/2。除了这两种极端情况外，我们通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。 现在的问题是，如何编程求出一组数据的有序对个数或者逆序对个数呢？因为有序对个数和逆序对个数的求解方式是类似的，所以你可以只思考逆序对个数的求解方法。 最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作 k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的 k 值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是 O(n^2)。那有没有更加高效的处理方法呢？ 套用分治的思想来求数组 A 的逆序对个数。我们可以将数组分成前后两半 A1 和 A2，分别计算 A1 和 A2 的逆序对个数 K1 和 K2，然后再计算 A1 与 A2 之间的逆序对个数 K3。那数组 A 的逆序对个数就等于 K1+K2+K3。 12345678910111213141516171819202122232425262728293031323334353637private int num = 0; // 全局变量或者成员变量 public int count(int[] a, int n) &#123; num = 0; mergeSortCounting(a, 0, n-1); return num;&#125; private void mergeSortCounting(int[] a, int p, int r) &#123; if (p &gt;= r) return; int q = (p+r)/2; mergeSortCounting(a, p, q); mergeSortCounting(a, q+1, r); merge(a, p, q, r);&#125; private void merge(int[] a, int p, int q, int r) &#123; int i = p, j = q+1, k = 0; int[] tmp = new int[r-p+1]; while (i&lt;=q &amp;&amp; j&lt;=r) &#123; if (a[i] &lt;= a[j]) &#123; tmp[k++] = a[i++]; &#125; else &#123; num += (q-i+1); // 统计 p-q 之间，比 a[j] 大的元素个数 tmp[k++] = a[j++]; &#125; &#125; while (i &lt;= q) &#123; // 处理剩下的 tmp[k++] = a[i++]; &#125; while (j &lt;= r) &#123; // 处理剩下的 tmp[k++] = a[j++]; &#125; for (i = 0; i &lt;= r-p; ++i) &#123; // 从 tmp 拷贝回 a a[p+i] = tmp[i]; &#125;&#125; 有很多同学经常说，某某算法思想如此巧妙，我是怎么也想不到的。实际上，确实是的。有些算法确实非常巧妙，并不是每个人短时间都能想到的。比如这个问题，并不是每个人都能想到可以借助归并排序算法来解决，不夸张地说，如果之前没接触过，绝大部分人都想不到。但是，如果我告诉你可以借助归并排序算法来解决，那你就应该要想到如何改造归并排序，来求解这个问题了，只要你能做到这一点，我觉得就很棒了。 关于分治算法，我这还有两道比较经典的问题，你可以自己练习一下。 二维平面上有 n 个点，如何快速计算出两个距离最近的点对？ 有两个 n*n 的矩阵 A，B，如何快速求解两个矩阵的乘积 C=A*B？ 分治思想在海量数据处理中的应用比如，给 10GB 的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有 10GB，而我们的机器的内存可能只有 2、3GB 这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。 要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。我们可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。 可以先扫描一遍订单，根据订单的金额，将 10GB 的文件划分为几个金额区间。比如订单金额为 1 到 100 元的放到一个小文件，101 到 200 之间的放到另一个文件，以此类推。这样每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终有序的 10GB 订单数据了。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构28-回溯算法","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构28-回溯算法.html","text":"[toc] 深度优先搜索算法利用的是回溯算法思想。这个算法思想非常简单，但是应用却非常广泛。它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。 理解“回溯算法”笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。 回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。 理论的东西还是过于抽象，老规矩，我还是举例说明一下。我举一个经典的回溯例子，我想你可能已经猜到了，那就是八皇后问题。 我们有一个 8x8 的棋盘，希望往里放 8 个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。你可以看我画的图，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。 我们把这个问题划分成 8 个阶段，依次将 8 个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前的方法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种方法，继续尝试。 123456789101112131415161718192021222324252627282930313233343536373839int[] result = new int[8];// 全局或成员变量, 下标表示行, 值表示 queen 存储在哪一列public void cal8queens(int row) &#123; // 调用方式：cal8queens(0); if (row == 8) &#123; // 8 个棋子都放置好了，打印结果 printQueens(result); return; // 8 行棋子都放好了，已经没法再往下递归了，所以就 return &#125; for (int column = 0; column &lt; 8; ++column) &#123; // 每一行都有 8 中放法 if (isOk(row, column)) &#123; // 有些放法不满足要求 result[row] = column; // 第 row 行的棋子放到了 column 列 cal8queens(row+1); // 考察下一行 &#125; &#125;&#125; private boolean isOk(int row, int column) &#123;// 判断 row 行 column 列放置是否合适 int leftup = column - 1, rightup = column + 1; for (int i = row-1; i &gt;= 0; --i) &#123; // 逐行往上考察每一行 if (result[i] == column) return false; // 第 i 行的 column 列有棋子吗？ if (leftup &gt;= 0) &#123; // 考察左上对角线：第 i 行 leftup 列有棋子吗？ if (result[i] == leftup) return false; &#125; if (rightup &lt; 8) &#123; // 考察右上对角线：第 i 行 rightup 列有棋子吗？ if (result[i] == rightup) return false; &#125; --leftup; ++rightup; &#125; return true;&#125; private void printQueens(int[] result) &#123; // 打印出一个二维矩阵 for (int row = 0; row &lt; 8; ++row) &#123; for (int column = 0; column &lt; 8; ++column) &#123; if (result[row] == column) System.out.print(\"Q \"); else System.out.print(\"* \"); &#125; System.out.println(); &#125; System.out.println();&#125; 经典应用0-1 背包0-1 背包问题有很多变体，这里介绍一种比较基础的。我们有一个背包，背包总的承载重量是 Wkg。现在我们有 n 个物品，每个物品的重量不等，并且不可分割。我们现在期望选择几件物品，装载到背包中。在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大？ 今天讲的这个背包问题，物品是不可分割的，要么装要么不装，所以叫 0-1 背包问题。显然，这个问题已经无法通过贪心算法来解决了。我们现在来看看，用回溯算法如何来解决。 对于每个物品来说，都有两种选择，装进背包或者不装进背包。对于 n 个物品来说，总的装法就有 2^n 种，去掉总重量超过 Wkg 的，从剩下的装法中选择总重量最接近 Wkg 的。不过，我们如何才能不重复地穷举出这 2^n 种装法呢？ 这里就可以用回溯的方法。我们可以把物品依次排列，整个问题就分解为了 n 个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。描述起来很费劲，我们直接看代码，反而会更加清晰一些。 这里还稍微用到了一点搜索剪枝的技巧，就是当发现已经选择的物品的重量超过 Wkg 之后，我们就停止继续探测剩下的物品。 123456789101112131415public int maxW = Integer.MIN_VALUE; // 存储背包中物品总重量的最大值// cw 表示当前已经装进去的物品的重量和；i 表示考察到哪个物品了；// w 背包重量；items 表示每个物品的重量；n 表示物品个数// 假设背包可承受重量 100，物品个数 10，物品重量存储在数组 a 中，那可以这样调用函数：// f(0, 0, a, 10, 100)public void f(int i, int cw, int[] items, int n, int w) &#123; if (cw == w || i == n) &#123; // cw==w 表示装满了 ;i==n 表示已经考察完所有的物品 if (cw &gt; maxW) maxW = cw; return; &#125; f(i+1, cw, items, n, w);// 借助回溯过程，实现了以每一个可能的物品，作为第一个装入背包的，以尝试所有物品组合。 if (cw + items[i] &lt;= w) &#123;// 已经超过可以背包承受的重量的时候，就不要再装了 f(i+1,cw + items[i], items, n, w); &#125;&#125; 正则表达式正则表达式中，最重要的就是通配符，通配符结合在一起，可以表达非常丰富的语义。为了方便讲解，我假设正表达式中只包含 “*” 和 “?” 这两种通配符，并且对这两个通配符的语义稍微做些改变，其中，“*”匹配任意多个（大于等于 0 个）任意字符，“\\?”匹配零个或者一个任意字符。基于以上背景假设，我们看下，如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？ 我们依次考察正则表达式中的每个字符，当是非通配符时，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。 如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口，比如“*”有多种匹配方案，可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。 12345678910111213141516171819202122232425262728293031323334public class Pattern &#123; private boolean matched = false; private char[] pattern; // 正则表达式 private int plen; // 正则表达式长度 public Pattern(char[] pattern, int plen) &#123; this.pattern = pattern; this.plen = plen; &#125; public boolean match(char[] text, int tlen) &#123; // 文本串及长度 matched = false; rmatch(0, 0, text, tlen); return matched; &#125; private void rmatch(int ti, int pj, char[] text, int tlen) &#123; if (matched) return; // 如果已经匹配了，就不要继续递归了 if (pj == plen) &#123; // 正则表达式到结尾了 if (ti == tlen) matched = true; // 文本串也到结尾了 return; &#125; if (pattern[pj] == '*') &#123; // * 匹配任意个字符 for (int k = 0; k &lt;= tlen-ti; ++k) &#123; rmatch(ti+k, pj+1, text, tlen); &#125; &#125; else if (pattern[pj] == '?') &#123; // ? 匹配 0 个或者 1 个字符 rmatch(ti, pj+1, text, tlen); rmatch(ti+1, pj+1, text, tlen); &#125; else if (ti &lt; tlen &amp;&amp; pattern[pj] == text[ti]) &#123; // 纯字符匹配才行 rmatch(ti+1, pj+1, text, tlen); &#125; &#125;&#125; 回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。 尽管回溯算法的原理非常简单，但是却可以解决很多问题，比如我们开头提到的深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配等等。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构29-动态规划入门","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构30-动态规划入门案例.html","text":"[toc] 动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。 大部分动态规划能解决的问题，都可以通过回溯算法来解决，只不过回溯算法解决起来效率比较低，时间复杂度是指数级的。动态规划算法，在执行效率方面，要高很多。尽管执行效率提高了，但是动态规划的空间复杂度也提高了，所以，很多时候，我们会说，动态规划是一种空间换时间的算法思想。 正月点灯笼—动态规划 0-1 背包问题对于一组不同重量、不可分割的物品，我们需要选择一些装入背包，在满足背包最大重量限制的前提下，求背包中物品总重量的最大值。 把整个求解过程分为 n 个阶段，每个阶段会决策一个物品是否放到背包中。每个物品决策（放入或者不放入背包）完之后，背包中的物品的重量会有多种情况，也就是说，会达到多种不同的状态，对应到递归树中，就是有很多不同的节点。 把每一层重复的状态（节点）合并，只记录不同的状态，然后基于上一层的状态集合，来推导下一层的状态集合。我们可以通过合并每一层重复的状态，这样就保证每一层不同状态的个数都不会超过 w 个（w 表示背包的承载重量）。于是，我们就成功避免了每层状态个数的指数级增长。 我们用一个二维数组 states[n][w+1]，来记录每层可以达到的不同状态。 123private int[] weight = &#123;2，2，4，6，3&#125;; // 物品重量private int n = 5; // 物品个数private int w = 9; // 背包承受的最大重量 第 0 个（下标从 0 开始编号）物品的重量是 2，要么装入背包，要么不装入背包，决策完之后，会对应背包的两种状态，背包中物品的总重量是 0 或者 2。我们用 states[0][0]=true 和 states[0][2]=true 来表示这两种状态。 第 1 个物品的重量也是 2，基于之前的背包状态，在这个物品决策完之后，不同的状态有 3 个，背包中物品总重量分别是 0(0+0)，2(0+2 or 2+0)，4(2+2)。我们用 states[1][0]=true，states[1][2]=true，states[1][4]=true 来表示这三种状态。 以此类推，直到考察完所有的物品后，整个 states 状态数组就都计算好了。我把整个计算的过程画了出来，你可以看看。图中 0 表示 false，1 表示 true。我们只需要在最后一层，找一个值为 true 的最接近 w（这里是 9）的值，就是背包中物品总重量的最大值。 123456789101112131415161718weight: 物品重量，n: 物品个数，w: 背包可承载重量public int knapsack(int[] weight, int n, int w) &#123; boolean[][] states = new boolean[n][w+1]; // 默认值 false states[0][0] = true; // 第一行的数据要特殊处理，可以利用哨兵优化 states[0][weight[0]] = true; for (int i = 1; i &lt; n; ++i) &#123; // 动态规划状态转移 for (int j = 0; j &lt;= w; ++j) &#123;// 不把第 i 个物品放入背包 if (states[i-1][j] == true) states[i][j] = states[i-1][j]; &#125; for (int j = 0; j &lt;= w-weight[i]; ++j) &#123;// 把第 i 个物品放入背包 if (states[i-1][j]==true) states[i][j+weight[i]] = true; &#125; &#125; for (int i = w; i &gt;= 0; --i) &#123; // 输出结果 if (states[n-1][i] == true) return i; &#125; return 0;&#125; 时间复杂度是 O(n*w)。n 表示物品个数，w 表示背包可以承载的总重量。用回溯算法解决这个问题的时间复杂度 O(2^n^)，是指数级的。 实际上，我们只需要一个大小为 w+1 的一维数组就可以解决这个问题。动态规划状态转移的过程，都可以基于这个一维数组来操作。 1234567891011121314public static int knapsack2(int[] items, int n, int w) &#123; boolean[] states = new boolean[w+1]; // 默认值 false states[0] = true; // 第一行的数据要特殊处理，可以利用哨兵优化 states[items[0]] = true; for (int i = 1; i &lt; n; ++i) &#123; // 动态规划 for (int j = w-items[i]; j &gt;= 0; --j) &#123;// 把第 i 个物品放入背包 if (states[j]==true) states[j+items[i]] = true; &#125; &#125; for (int i = w; i &gt;= 0; --i) &#123; // 输出结果 if (states[i] == true) return i; &#125; return 0;&#125; 强调一下代码中的第 6 行，j 需要从大到小来处理。如果我们按照 j 从小到大处理的话，会出现 for 循环重复计算的问题。 0-1 背包问题升级版于一组不同重量、不同价值、不可分割的物品，我们选择将某些物品装入背包，在满足背包最大重量限制的前提下，求背包中可装入物品的最大总价值 回溯算法: 123456789101112131415private int maxV = Integer.MIN_VALUE; // 结果放到 maxV 中private int[] items = &#123;2，2，4，6，3&#125;; // 物品的重量private int[] value = &#123;3，4，8，9，6&#125;; // 物品的价值private int n = 5; // 物品个数private int w = 9; // 背包承受的最大重量public void f(int i, int cw, int cv) &#123; // 调用 f(0, 0, 0) if (cw == w || i == n) &#123; // cw==w 表示装满了，i==n 表示物品都考察完了 if (cv &gt; maxV) maxV = cv; return; &#125; f(i+1, cw, cv); // 选择不装第 i 个物品 if (cw + weight[i] &lt;= w) &#123; f(i+1,cw+weight[i], cv+value[i]); // 选择装第 i 个物品 &#125;&#125; 动态规划: 用一个二维数组 states[n][w+1]，来记录每层可以达到的不同状态。不过这里数组存储的值不再是 boolean 类型的了，而是当前状态对应的最大总价值。我们把每一层中 (i, cw) 重复的状态（节点）合并，只记录 cv 值最大的那个状态，然后基于这些状态来推导下一层的状态。 1234567891011121314151617181920212223242526272829public static int knapsack3(int[] weight, int[] value, int n, int w) &#123; int[][] states = new int[n][w+1]; for (int i = 0; i &lt; n; ++i) &#123; // 初始化 states for (int j = 0; j &lt; w+1; ++j) &#123; states[i][j] = -1; &#125; &#125; states[0][0] = 0; states[0][weight[0]] = value[0]; for (int i = 1; i &lt; n; ++i) &#123; // 动态规划，状态转移 for (int j = 0; j &lt;= w; ++j) &#123; // 不选择第 i 个物品 if (states[i-1][j] &gt;= 0) states[i][j] = states[i-1][j]; &#125; for (int j = 0; j &lt;= w-weight[i]; ++j) &#123; // 选择第 i 个物品 if (states[i-1][j] &gt;= 0) &#123; int v = states[i-1][j] + value[i]; if (v &gt; states[i][j+weight[i]]) &#123; states[i][j+weight[i]] = v; &#125; &#125; &#125; &#125; // 找出最大值 int maxvalue = -1; for (int j = 0; j &lt;= w; ++j) &#123; if (states[n-1][j] &gt; maxvalue) maxvalue = states[n-1][j]; &#125; return maxvalue;&#125; 时间复杂度是 O(n*w)，空间复杂度也是 O(n*w)。 满减淘宝的“双十一”购物节有各种促销活动，比如“满 200 元减 50 元”。假设你女朋友的购物车中有 n 个（n&gt;100）想买的商品，她希望从里面选几个，在凑够满减条件的前提下，让选出来的商品价格总和最大程度地接近满减条件（200 元），这样就可以极大限度地“薅羊毛”。 不过，这个问题不仅要求大于等于 200 的总价格中的最小的，我们还要找出这个最小总价格对应都要购买哪些商品。实际上，我们可以利用 states 数组，倒推出这个被选择的商品序列。 123456789101112131415161718192021222324252627// items 商品价格，n 商品个数, w 表示满减条件，比如 200public static void double11advance(int[] items, int n, int w) &#123; boolean[][] states = new boolean[n][3*w+1];// 超过 3 倍就没有薅羊毛的价值了 states[0][0] = true; // 第一行的数据要特殊处理 states[0][items[0]] = true; for (int i = 1; i &lt; n; ++i) &#123; // 动态规划 for (int j = 0; j &lt;= 3*w; ++j) &#123;// 不购买第 i 个商品 if (states[i-1][j] == true) states[i][j] = states[i-1][j]; &#125; for (int j = 0; j &lt;= 3*w-items[i]; ++j) &#123;// 购买第 i 个商品 if (states[i-1][j]==true) states[i][j+items[i]] = true; &#125; &#125; int j; for (j = w; j &lt; 3*w+1; ++j) &#123; if (states[n-1][j] == true) break; // 输出结果大于等于 w 的最小值 &#125; if (j == 3*w+1) return; // 没有可行解 for (int i = n-1; i &gt;= 1; --i) &#123; // i 表示二维数组中的行，j 表示列 if(j-items[i] &gt;= 0 &amp;&amp; states[i-1][j-items[i]] == true) &#123; System.out.print(items[i] + \" \"); // 购买这个商品 j = j - items[i]; &#125; // else 没有购买这个商品，j 不变。 &#125; if (j != 0) System.out.print(items[0]);&#125;","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构17-二叉树基础","date":"2020-12-26T03:19:17.000Z","path":"2020/12/26/01-数据结构/数据结构17-二叉树基础.html","text":"[toc] 二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？ 树（Tree）比如下面这幅图，A 节点就是 B 节点的父节点，B 节点是 A 节点的子节点。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫作根节点，也就是图中的节点 E。我们把没有子节点的节点叫作叶子节点或者叶节点，比如图中的 G、H、I、J、K、L 都是叶子节点。 节点的高度（Height）：节点到叶子节点的最长路径（边数） 节点的深度（Depth）：根节点到这个节点所经历的边的个数 节点的层（Level）：节点的深度+1 树的高度：根节点的高度 二叉树（Binary Tree）二叉树，顾名思义，每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子\\**节**\\点**。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。 编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。 编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作完全二叉树。 要理解完全二叉树定义的由来，我们需要先了解，如何表示（或者存储）一棵二叉树？ 想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。 我们先来看比较简单、直观的链式存储法。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。 我们再来看，基于数组的顺序存储法。我们把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置。如果节点 X 存储在数组中下标为 i 的位置，下标为 2 i 的位置存储的就是左子节点，下标为 2 i + 1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为 1 的位置），这样就可以通过下标计算，把整棵树都串起来。 刚刚举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为 0 的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。 所以，如果某棵二叉树是一棵==完全二叉树，那用数组存储无疑是最节省内存==的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。 当我们讲到堆和堆排序的时候，你会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。 二叉树的遍历如何将所有节点都遍历打印出来呢？经典的方法有三种，前序遍历、中序遍历和后序遍历。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。 实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。 1234567891011121314151617181920void preOrder(Node* root) &#123; if (root == null) return; print root // 此处为伪代码，表示打印 root 节点 preOrder(root-&gt;left); preOrder(root-&gt;right);&#125; void inOrder(Node* root) &#123; if (root == null) return; inOrder(root-&gt;left); print root // 此处为伪代码，表示打印 root 节点 inOrder(root-&gt;right);&#125; void postOrder(Node* root) &#123; if (root == null) return; postOrder(root-&gt;left); postOrder(root-&gt;right); print root // 此处为伪代码，表示打印 root 节点&#125; 从前、中、后序遍历的顺序图，可以看出来，每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数 n 成正比，也就是说二叉树遍历的时间复杂度是 O(n)。 二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。满二叉树又是完全二叉树的一种特殊情况。 二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。 二叉查找树（Binary Search Tree）二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。 这些都依赖于二叉查找树的特殊结构。二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。 二叉查找树的查找操作我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。 1234567891011121314151617181920212223public class BinarySearchTree &#123; private Node tree; public Node find(int data) &#123; Node p = tree; while (p != null) &#123; if (data &lt; p.data) p = p.left; else if (data &gt; p.data) p = p.right; else return p; &#125; return null; &#125; public static class Node &#123; private int data; private Node left; private Node right; public Node(int data) &#123; this.data = data; &#125; &#125;&#125; 二叉查找树的插入操作二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。 如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。 1234567891011121314151617181920212223public void insert(int data) &#123; if (tree == null) &#123; tree = new Node(data); return; &#125; Node p = tree; while (p != null) &#123; if (data &gt; p.data) &#123; if (p.right == null) &#123; p.right = new Node(data); return; &#125; p = p.right; &#125; else &#123; // data &lt; p.data if (p.left == null) &#123; p.left = new Node(data); return; &#125; p = p.left; &#125; &#125;&#125; 二叉查找树的删除操作二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了 。针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。 第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null。比如图中的删除节点 55。 第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点 13。 第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点 18。 123456789101112131415161718192021222324252627282930313233public void delete(int data) &#123; Node p = tree; // p 指向要删除的节点，初始化指向根节点 Node pp = null; // pp 记录的是 p 的父节点 while (p != null &amp;&amp; p.data != data) &#123; pp = p; if (data &gt; p.data) p = p.right; else p = p.left; &#125; if (p == null) return; // 没有找到 // 要删除的节点有两个子节点 if (p.left != null &amp;&amp; p.right != null) &#123; // 查找右子树中最小节点 Node minP = p.right; Node minPP = p; // minPP 表示 minP 的父节点 while (minP.left != null) &#123; minPP = minP; minP = minP.left; &#125; p.data = minP.data; // 将 minP 的数据替换到 p 中 p = minP; // 下面就变成了删除 minP 了 pp = minPP; &#125; // 删除节点是叶子节点或者仅有一个子节点 Node child; // p 的子节点 if (p.left != null) child = p.left; else if (p.right != null) child = p.right; else child = null; if (pp == null) tree = child; // 删除的是根节点 else if (pp.left == p) pp.left = child; else pp.right = child;&#125; 实际上，关于二叉查找树的删除操作，还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为“已删除”，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。而且，这种处理方法也并没有增加插入、查找操作代码实现的难度。 二叉查找树的其他操作除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。 二叉查找树除了支持上面几个操作之外，还有一个重要的特性，就是中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。 二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是 O(logn)。 不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于 log2n 的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到 O(n)。 支持重复数据的二叉查找树前面讲二叉查找树的时候，我们默认树中节点存储的都是数字。很多时候，在实际的软件开发中，我们在二叉查找树中存储的，是一个包含很多字段的对象。我们利用对象的某个字段作为键值（key）来构建二叉查找树。我们把对象中的其他字段叫作卫星数据。 前面我们讲的二叉查找树的操作，针对的都是不存在键值相同的情况。那如果存储的两个对象键值相同，这种情况该怎么处理呢？我这里有两种解决方法。 第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。 第二种方法比较不好理解，不过更加优雅。 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。 当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。 对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。 二叉查找树的时间复杂度分析实际上，二叉查找树的形态各式各样。比如这个图中，对于同一组数据，我们构造了三种二叉查找树。它们的查找、插入、删除操作的执行效率都是不一样的。图中第一种二叉查找树，根节点的左右子树极度不平衡，已经退化成了链表，所以查找的时间复杂度就变成了 O(n)。 现在来分析一个最理想的情况，二叉查找树是一棵完全二叉树（或满二叉树），不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是 O(height)。 树的高度就等于最大层数减一，为了方便计算，我们转换成层来表示。从图中可以看出，包含 n 个节点的完全二叉树中，第一层包含 1 个节点，第二层包含 2 个节点，第三层包含 4 个节点，依次类推，下面一层节点个数是上一层的 2 倍，第 K 层包含的节点个数就是 2^(K-1)。 不过，对于完全二叉树来说，最后一层的节点个数有点儿不遵守上面的规律了。它包含的节点个数在 1 个到 2^(L-1) 个之间（我们假设最大层数是 L）。如果我们把每一层的节点个数加起来就是总的节点个数 n。也就是说，如果节点的个数是 n，那么 n 满足这样一个关系： n >= 1+2+4+8+...+2^{(L-2)}+1\\\\ n","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构16-哈希算法的应用","date":"2020-12-26T02:49:49.000Z","path":"2020/12/26/01-数据结构/数据结构16-哈希算法的应用.html","text":"[toc] 什么是hash不管是“散列”还是“哈希”，这都是中文翻译的差别，英文其实就是“Hash”。所以，我们常听到有人把“散列表”叫作“哈希表”“Hash 表”，把“哈希算法”叫作“Hash 算法”或者“散列算法” 哈希算法的定义和原理非常简单，基本上一句话就可以概括了。将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。 一个优秀hash算法的要求： 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）； 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同； 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小； 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。 例如： 12MD5(\" 我今天讲哈希算法！\") = 425f0d5a917188d2c3c3dc85b5e4f2cbMD5(\" 我今天讲哈希算法 \") = a1fb91ac128e6aa37fe42c663971ac3d 应用一：安全加密说到哈希算法的应用，最先想到的应该就是安全加密。最常用于加密的哈希算法是MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法）和SHA（Secure Hash Algorithm，安全散列算法）。除了这两个之外，当然还有很多其他加密算法，比如DES（Data Encryption Standard，数据加密标准）、AES（Advanced Encryption Standard，高级加密标准）。 前面讲到的哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。 第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。所以我着重讲一下第二点。实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说呢？ 这里就基于组合数学中一个非常基础的理论，鸽巢原理（也叫抽屉原理）。这个原理本身很简单，它是说，如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个，换句话说就是，肯定有 2 只鸽子在 1 个鸽巢内。 有了鸽巢原理的铺垫之后，我们再来看，为什么哈希算法无法做到零冲突？ 哈希算法产生的哈希值的长度是固定且有限的。比如前面举的 MD5 的例子，哈希值是固定的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况。一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。 这两段字符串经过 MD5 哈希算法加密之后，产生的哈希值是相同的。 应用二：唯一标识如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那该如何搜索呢？ 可以给每一个图片取一个唯一标识，或者说信息摘要。比如，从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。 如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。 如果不存在，那就说明这个图片不在图库中；如果存在，再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。 应用三：数据校验BT 下载的原理是基于 P2P 协议的。我们从多个机器上并行下载一个 2GB 的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每块大约 20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。 但网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？ 具体的 BT 协议很复杂，校验方法也有很多，来理解下其中的一种思路。 通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。 应用四：散列函数散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，都可以通过开放寻址法或者链表法解决。 不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。 字典攻击如果用户信息被“脱库”，黑客虽然拿到是加密之后的密文，但可以通过“猜”的方式来破解密码，这是因为，有些用户的密码太简单。比如很多人习惯用 00000、123456 这样的简单数字组合做密码，很容易就被猜中。 那就需要维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。如果相同，基本上就可以认为，这个加密之后的密码对应的明文就是字典中的这个密码。（注意，这里说是的是“基本上可以认为”，因为根据前面的学习，哈希算法存在散列冲突，也有可能出现，尽管密文一样，但是明文并不一样的情况。） 针对字典攻击，可以引入一个盐（salt），跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。不过我这里想多说一句，我认为安全和攻击是一种博弈关系，不存在绝对的安全。所有的安全措施，只是增加攻击的成本而已。 应用五：负载均衡负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。 最直接的方法就是，维护一张映射关系表，这张表的内容是客户端 IP 地址或者会话 ID 与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端： 如果客户端很多，映射表可能会很大，比较浪费内存空间； 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大； 如果借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。 应用六：数据分片统计“搜索关键词”出现的次数假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？ 我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。 针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用 n 台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。 这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。 实际上，这里的处理过程也是 MapReduce 的基本设计思想。 快速判断图片是否在图库中如何快速判断图片是否在图库中？采用应用二：唯一标识，给每个图片取唯一标识（或者信息摘要），然后构建散列表。 假设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。 我们同样可以对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。 当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。 现在，我们来估算一下，给这 1 亿张图片构建散列表大约需要多少台机器。 散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设我们通过 MD5 来计算哈希值，那长度就是 128 比特，也就是 16 字节。文件路径长度的上限是 256 字节，我们可以假设平均长度是 128 字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用 8 字节。所以，散列表中每个数据单元就占用 152 字节（这里只是估算，并不准确）。 假设一台机器的内存大小为 2GB，散列表的装载因子为 0.75，那一台机器可以给大约 1000 万（2GB*0.75/152）张图片构建散列表。所以，如果要对 1 亿张图片构建索引，需要大约十几台机器。在工程中，这种估算还是很重要的，能让我们事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性。 实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。 应用七：分布式存储现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。 该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。 但是，如果数据增多，原来的 10 个机器已经无法承受了，我们就需要扩容了，比如扩到 11 个机器，这时候麻烦就来了。因为，这里并不是简单地加个机器就可以了。 原来的数据是通过与 10 来取模的。比如 13 这个数据，存储在编号为 3 这台机器上。但是新加了一台机器中，我们对数据按照 11 取模，原来 13 这个数据就被分配到 2 号这台机器上了。 因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。 所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了。 假设我们有 k 个机器，数据的哈希值的范围是 [0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。 一致性哈希算法的基本思想就是这么简单。除此之外，它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来。这里我就不展开讲了，如果感兴趣，你可以看下这个介绍。 除了我们上面讲到的分布式缓存，实际上，一致性哈希算法的应用非常广泛，在很多分布式存储系统中，都可以见到一致性哈希算法的影子。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构15-散列表","date":"2020-12-23T13:44:13.000Z","path":"2020/12/23/01-数据结构/数据结构15-散列表.html","text":"[toc] 散列思想散列表的英文叫“Hash Table”，我们平时也叫它“哈希表”或者“Hash 表”。 散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。 假如有 89 名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。编号用 6 位数字来表示。比如 051167，其中，前两位 05 表示年级，中间两位 11 表示班级，最后两位是 1 到 89。可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。这就是典型的散列思想。其中，参赛选手的编号我们叫作键（key）或者关键字。用它来标识一个选手。把参赛编号转化为数组下标的映射方法就叫作散列函数（或“Hash 函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash 值”“哈希值”）。 通过这个例子，可以总结出这样的规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当按照键值查询元素时，用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。 散列函数散列函数，顾名思义，它是一个函数。可以把它定义成hash(key)，其中 key 表示元素的键值，hash(key) 的值表示经过散列函数计算得到的散列值。 三点散列函数设计的基本要求： 散列函数计算得到的散列值是一个非负整数； 如果 key1 = key2，那 hash(key1) == hash(key2)； 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。 在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。 所以几乎无法找到一个完美的无冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，需要通过其他途径来解决。 如何设计散列函数？ 散列函数的设计不能太复杂，过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能 散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。 实际工作中，还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。 散列冲突再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。 比如，Java 中 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。 开放寻址法开放寻址法的核心思想是，如果出现了散列冲突，就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？比较经典的探测方法有：线性探测（Linear Probing）、二次探测（Quadratic probing）和双重散列（Double hashing）。 优点：开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。 缺点：用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。 线性探测： 往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。 在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。 对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。不能单纯地把要删除的元素设置为空。 在查找的时候，一旦通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。这个问题如何解决呢？ 可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。 线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。 二次探测： 所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2^，hash(key)+2^2^…… 双重散列： 所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。 不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，要尽可能保证散列表中有一定比例的空闲槽位，用装载因子（load factor）来表示空位的多少。 散列表的装载因子 = 填入表中的元素个数 / 散列表的长度 装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。 链表法链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。图中，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素都放到相同槽位对应的链表中。 优点：链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。 优点：链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。 实际上，对链表法稍加改造，可以实现一个更加高效的散列表。那就是，将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是 O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。 在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从 O(1) 急剧退化为 O(n)。 如果散列表中有 10 万个数据，退化后的散列表查询的效率就下降了 10 万倍。更直接点说，如果之前运行 100 次查询只需要 0.1 秒，那现在就需要 1 万秒。这样就有可能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就是散列表碰撞攻击的基本原理。 当插入的时候，只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。 那查找或删除操作的时间复杂度是多少呢？实际上，这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。 装载因子过大了怎么办？针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是 0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了 0.4。 针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。 如图，在原来的散列表中，21 这个元素原来存储在下标为 0 的位置，搬移到新的散列表中，存储在下标为 7 的位置。 插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。 实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果对空间消耗非常敏感，可以在装载因子小于某个值之后，启动动态缩容。当然，如果更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。 装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。 为了解决一次性扩容耗时过多的情况，可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，只申请新空间，但并不将老的数据搬移到新散列表中。 当有新数据要插入时，将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。 对于查询操作，为了兼容了新、老散列表中的数据，先从新散列表中查找，如果没有找到，再去老的散列表中查找。通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。 工业级散列表举例分析Java 中的 HashMap 这样一个工业级的散列表。 要求： 支持快速的查询、插入、删除操作； 内存占用合理，不能浪费过多的内存空间； 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。 设计一个合适的散列函数； 定义装载因子阈值，并且设计动态扩容策略； 选择合适的散列冲突解决方法。 初始大小HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 HashMap 的性能。 装载因子和动态扩容最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。 散列冲突解决方法HashMap 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能。 于是，在 JDK1.8 版本中，为了对 HashMap 做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。 散列函数散列函数的设计并不复杂，追求的是简单高效、分布均匀。 1234567891011121314151617int hash(Object key) &#123; int h = key.hashCode()； return (h ^ (h &gt;&gt;&gt; 16)) &amp; (capitity -1); //capicity 表示散列表的大小&#125;// 其中，hashCode() 返回的是 Java 对象的 hash code。比如 String 类型的对象的 hashCode() 就是下面这样：public int hashCode() &#123; int var1 = this.hash; if(var1 == 0 &amp;&amp; this.value.length &gt; 0) &#123; char[] var2 = this.value; for(int var3 = 0; var3 &lt; this.value.length; ++var3) &#123; var1 = 31 * var1 + var2[var3]; &#125; this.hash = var1; &#125; return var1;&#125; LRU 缓存淘汰算法缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 CPU 缓存、数据库缓存、浏览器缓存等等。 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）。 需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。 当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰算法的时间复杂很高，是 O(n)。 实际上，一个缓存（cache）系统主要包含下面这几个操作： 往缓存中添加一个数据； 从缓存中删除一个数据； 在缓存中查找一个数据。 这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是 O(n)。如果将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到 O(1)。 链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。因为散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。 如何查找一个数据。散列表中查找数据的时间复杂度接近 O(1)，所以通过散列表，可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。 如何删除一个数据。需要找到数据所在的结点，然后将结点删除。借助散列表，可以在 O(1) 时间复杂度里找到要删除的结点。因为链表是双向链表，双向链表可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要 O(1) 的时间复杂度。 如何添加一个数据。添加数据到缓存稍微有点麻烦，需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。 这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在 O(1) 的时间复杂度内完成。所以，这三个操作的时间复杂度都是 O(1)。至此，通过散列表和双向链表的组合使用，实现了一个高效的、支持 LRU 缓存淘汰算法的缓存系统原型。 散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那需要将散列表中的数据拷贝到数组中，然后排序，再遍历。 因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构14-跳表","date":"2020-11-27T09:37:28.000Z","path":"2020/11/27/01-数据结构/数据结构14-跳表.html","text":"[toc] 跳表（Skip list）二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，只需要对链表稍加改造，就可以支持类似“二分”的查找算法。把改造之后的数据结构叫作跳表（Skip list），这是一种各方面性能都比较优秀的动态数据结构，可以支持快速的插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。 对于一个单链表来讲，即便链表中存储的数据是有序的，如果要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是 O(n)。 对链表建立一级“索引”，每两个结点提取一个结点到上一级，把抽出来的那一级叫作索引或索引层。图中的 down 表示 down 指针，指向下一级结点。 如果现在要查找某个结点，比如 16。可以先在索引层遍历，当遍历到索引层中值为 13 的结点时，发现下一个结点是 17，那要查找的结点 16 肯定就在这两个结点之间。然后通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历 2 个结点，就可以找到值等于 16 的这个结点了。这样，原来如果要查找 16，需要遍历 10 个结点，现在只需要遍历 7 个结点。加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了 跟前面建立第一级索引的方式相似，在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在再来查找 16，只需要遍历 6 个结点了，需要遍历的结点数量又减少了。 从下图中可以看出，原来没有索引的时候，查找 62 需要遍历 62 个结点，现在只需要遍历 11 个结点，速度是不是提高了很多？所以，当链表的长度 n 比较大时，比如 1000、10000 的时候，在构建索引之后，查找效率的提升就会非常明显。 这种链表加多级索引的结构，就是跳表。 复杂度分析每两个结点会抽出一个结点作为上一级索引的结点，最后一层索引2个节点，索引有h层，那么n/(2^h)=2，$h=log_2n-1$，如果包含原始链表这一层，整个跳表的高度就是 log~2~n。如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*(h+1))=O(m*logn)，其中m=3，所以在跳表中查询任意数据的时间复杂度就是 O(logn)。 假设要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。 比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。如果将包含 n 个结点的单链表构造成跳表，我们需要额外再用接近 $\\frac{n}{2}+\\frac{n}{4}+\\frac{n}{8}+….+4+2=n-2$个结点的存储空间。如果每三个结点或五个结点，抽一个结点到上级索引，n/3+n/9+n/27+…+9+3+1=n/2。尽管空间复杂度还是 O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。 实际上，在软件开发中，不必太在意索引占用的额外空间。在讲数据结构和算法时，习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。 插入和删除对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，查找某个结点的的时间复杂度是 O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。 如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。 当不停地往跳表中插入数据时，如果不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。 作为一种动态数据结构，需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。 红黑树、AVL 树这样平衡二叉树是通过左右旋的方式保持左右子树的大小平衡，而跳表是通过随机函数来维护前面提到的“平衡性”。 当往跳表中插入数据的时候，可以选择同时将这个数据插入到部分索引层中。通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那就将这个结点添加到第一级到第 K 级这 K 级索引中。 随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构13-二分查找","date":"2020-11-27T07:41:26.000Z","path":"2020/11/27/01-数据结构/数据结构13-二分查找.html","text":"[toc] 二分查找（Binary Search）二分查找（Binary Search）算法，也叫折半查找算法。 二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。 假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。其中 n/2^k^=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度就是 O(k)。通过 n/2^k^=1，我们可以求得 k=log~2~n，所以时间复杂度就是 O(logn)。 对数时间复杂度是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 O(1) 的算法还要高效。 二分查找的递归与非递归实现最简单的情况就是有序数组中不存在重复元素，在其中用二分查找值等于给定值的数据。 1234567891011121314151617public int bsearch(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = (low + high) / 2; if (a[mid] == value) &#123; return mid; &#125; else if (a[mid] &lt; value) &#123; low = mid + 1; &#125; else &#123; high = mid - 1; &#125; &#125; return -1;&#125; 注意是 low&lt;=high，而不是 low&lt;high。 实际上，mid=(low+high)/2 这种写法是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以 2 操作转化成位运算 low+((high-low)&gt;&gt;1)。因为相比除法运算来说，计算机处理位运算要快得多。 low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。比如，当 high=3，low=3 时，如果 a[3] 不等于 value，就会导致一直循环不退出。 1234567891011121314151617// 二分查找的递归实现public int bsearch(int[] a, int n, int val) &#123; return bsearchInternally(a, 0, n - 1, val);&#125; private int bsearchInternally(int[] a, int low, int high, int value) &#123; if (low &gt; high) return -1; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] == value) &#123; return mid; &#125; else if (a[mid] &lt; value) &#123; return bsearchInternally(a, mid+1, high, value); &#125; else &#123; return bsearchInternally(a, low, mid-1, value); &#125;&#125; 首先，二分查找依赖的是顺序表结构，简单点说就是数组。 其次，二分查找针对的是有序数据。 再次，数据量太小不适合二分查找。不过，如果数据之间的比较操作非常耗时，不管数据量大小，都推荐使用二分查找。比如，数组中存储的都是长度超过 300 的字符串，如此长的两个字符串之间比对大小，就会非常耗时。需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。 最后，数据量太大也不适合二分查找。二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。 二分查找的变形问题 变体一：查找第一个值等于给定值的元素123456789101112131415161718192021222324252627282930313233public int bsearch(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt;= value) &#123; // ? high = mid - 1; &#125; else &#123; low = mid + 1; &#125; &#125; if (low &lt; n &amp;&amp; a[low]==value) return low; else return -1;&#125;public int bsearch(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt; value) &#123; high = mid - 1; &#125; else if (a[mid] &lt; value) &#123; low = mid + 1; &#125; else &#123; if ((mid == 0) || (a[mid - 1] != value)) return mid; else high = mid - 1; &#125; &#125; return -1;&#125; 变体二：查找最后一个值等于给定值的元素12345678910111213141516public int bsearch(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt; value) &#123; high = mid - 1; &#125; else if (a[mid] &lt; value) &#123; low = mid + 1; &#125; else &#123; if ((mid == n - 1) || (a[mid + 1] != value)) return mid; else low = mid + 1; &#125; &#125; return -1;&#125; 变体三：查找第一个大于等于给定值的元素1234567891011121314public int bsearch(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt;= value) &#123; if ((mid == 0) || (a[mid - 1] &lt; value)) return mid; else high = mid - 1; &#125; else &#123; low = mid + 1; &#125; &#125; return -1;&#125; 变体四：查找最后一个小于等于给定值的元素1234567891011121314public int bsearch7(int[] a, int n, int value) &#123; int low = 0; int high = n - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt; value) &#123; high = mid - 1; &#125; else &#123; if ((mid == n - 1) || (a[mid + 1] &gt; value)) return mid; else low = mid + 1; &#125; &#125; return -1;&#125;","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构12-排序之优化","date":"2020-11-26T13:51:30.000Z","path":"2020/11/26/01-数据结构/数据结构12-排序之优化.html","text":"[toc] 如何选择合适的排序算法？ 线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。 时间复杂度是 O(nlogn) 的排序算法不止一个，已经讲过的有归并排序、快速排序，后面讲堆的时候还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。 使用归并排序的情况其实并不多，因为归并排序并不是原地排序算法，空间复杂度是 O(n)。 如何优化快速排序？最坏情况下快速排序的时间复杂度是 O(n^2^)，出现的主要原因是因为分区点选的不够合理。 最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。 三数取中法从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。 随机法随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的 O(n2) 的情况，出现的可能性不大。 举例分析排序函数O(n^2^) 时间复杂度的算法并不一定比 O(nlogn) 的算法执行时间长 O(nlogn) 在没有省略低阶、系数、常数之前可能是 O(knlogn + c)，而且 k 和 c 有可能还是一个比较大的数。假设 k=1000，c=200，当我们对小规模数据（比如 n=100）排序时，n2的值实际上比 knlogn+c 还要小。 knlogn+c = 1000 100 log100 + 200 远大于 10000 n^2 = 100*100 = 10000 对于小规模数据的排序，O(n2) 的排序算法并不一定比 O(nlogn) 排序算法执行的时间长。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构10-线性排序(桶排序、计数排序、基数排序)","date":"2020-11-26T12:56:16.000Z","path":"2020/11/26/01-数据结构/数据结构11-排序之线性排序(桶排序、计数排序、基数排序).html","text":"[toc] 三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。 这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以重点的是掌握这些排序算法的适用场景。 桶排序（Bucket sort）核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。 如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k logk)。m 个桶排序的时间复杂度就是 O(m k logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(nlog(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。 实际上，桶排序对要排序数据的要求是非常苛刻的。 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。 计数排序（Counting sort）计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。 假设对50W个高考分数排序，考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以我们可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，我们将这 50 万考生划分到这 901 个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。 假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 A[8] 中，它们分别是：2，5，3，0，2，3，0，3。期望输出R[8]={0,0,2,2,3,3,3,5} 考生的成绩从 0 到 5 分，我们使用大小为 6 的数组 C[6] 表示桶，其中下标对应分数。不过，C[6] 内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到 C[6] 的值。 如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？ 对 C[6] 数组顺序求和，C[6] 存储的数据就变成C[6]={2,2,4,7,7,8}。C[k] 里存储==小于等于==分数 k 的考生个数。 从后到前依次扫描数组 A。比如，当扫描到 3 时，我们可以从数组 C 中取出下标为 3 的值 7，也就是说，到目前为止，包括自己在内，分数小于等于 3 的考生有 7 个，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 的元素就只剩下了 6 个了，所以相应的 C[3] 要减 1，变成 6。 以此类推，当我们扫描到第 2 个分数为 3 的考生的时候，就会把它放入数组 R 中的第 6 个元素的位置（也就是下标为 5 的位置）。当我们扫描完整个数组 A 后，数组 R 内的数据就是按照分数从小到大有序排列的了。 1234567891011121314151617181920212223242526272829303132333435363738394041// 计数排序，a 是数组，n 是数组大小。假设数组中存储的都是非负整数。public void countingSort(int[] a, int n) &#123; if (n &lt;= 1) return; // 查找数组中数据的范围 int max = a[0]; for (int i = 1; i &lt; n; ++i) &#123; if (max &lt; a[i]) &#123; max = a[i]; &#125; &#125; int[] c = new int[max + 1]; // 申请一个计数数组 c，下标大小 [0,max] for (int i = 0; i &lt;= max; ++i) &#123; c[i] = 0; &#125; // 计算每个元素的个数，放入 c 中 for (int i = 0; i &lt; n; ++i) &#123; c[a[i]]++; &#125; // 依次累加 for (int i = 1; i &lt;= max; ++i) &#123; c[i] = c[i-1] + c[i]; &#125; // 临时数组 r，存储排序之后的结果 int[] r = new int[n]; // 计算排序的关键步骤，有点难理解 for (int i = n - 1; i &gt;= 0; --i) &#123; int index = c[a[i]]-1; r[index] = a[i]; c[a[i]]--; &#125; // 将结果拷贝给 a 数组 for (int i = 0; i &lt; n; ++i) &#123; a[i] = r[i]; &#125;&#125; 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。 比如，还是拿考生这个例子。如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以 10，转化成整数，然后再放到 9010 个桶内。再比如，如果要排序的数据中有负数，数据的范围是 [-1000, 1000]，那我们就需要先对每个数据都加 1000，转化成非负整数。 基数排序（Radix sort）先按照最后一位来排序，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序 注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。因为如果是非稳定排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，那么低位的排序就完全没有意义了。 根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k*n)。当 k 不大的时候，所以基数排序的时间复杂度就近似于 O(n)。 实际上，有时候要排序的数据并不都是等长的，比如我们排序牛津字典中的 20 万个英文单词，最短的只有 1 个字母，最长的我特意去查了下，有 45 个字母，中文翻译是尘肺病。对于这种不等长的数据，基数排序还适用吗？实际上，我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”，因为根据ASCII 值，所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序。这样就可以继续用基数排序了。 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。 桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构10-排序之归并&快排","date":"2020-11-26T08:33:55.000Z","path":"2020/11/26/01-数据结构/数据结构10-排序之归并&快排.html","text":"[toc] 归并排序和快速排序都用到了分治思想，非常巧妙。这两种排序算法适合大规模的数据排序。 归并排序（Merge Sort）如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。 归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。 12345678910111213141516171819202122232425262728293031323334353637public static int[] sort(int[] a,int low,int high)&#123; int mid = (low+high)/2; if(low&lt;high)&#123; sort(a,low,mid); sort(a,mid+1,high); //左右归并 merge(a,low,mid,high); &#125; return a; &#125; public static void merge(int[] a, int low, int mid, int high) &#123; int[] temp = new int[high-low+1]; int i= low; int j = mid+1; int k=0; // 把较小的数先移到新数组中 while(i&lt;=mid &amp;&amp; j&lt;=high)&#123; if(a[i]&lt;a[j])&#123; temp[k++] = a[i++]; &#125;else&#123; temp[k++] = a[j++]; &#125; &#125; // 把左边剩余的数移入数组 while(i&lt;=mid)&#123; temp[k++] = a[i++]; &#125; // 把右边边剩余的数移入数组 while(j&lt;=high)&#123; temp[k++] = a[j++]; &#125; // 把新数组中的数覆盖nums数组 for(int x=0;x&lt;temp.length;x++)&#123; a[x+low] = temp[x]; &#125; &#125; 第一，归并排序是稳定的排序算法吗？ 归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。在合并的过程中，如果 A[p…q] 和 A[q+1…r] 之间有值相同的元素，那我们可以像伪代码中那样，先把 A[p…q] 中的元素放入 tmp 数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。 第二，归并排序的时间复杂度是多少？ 递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。 假设对 n 个元素进行归并排序需要的时间是 T(n)，那分解成两个子数组排序的时间都是 T(n/2)。我们知道，merge() 函数合并两个有序子数组的时间复杂度是 O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是： T(1) = C； n=1 时，只需要常量级的执行时间，所以表示为 C。 T(n) = 2*T(n/2) + n； n&gt;1 得到 T(n) = 2^k^T(n/2^k^)+k\\n。当 T(n/2^k^)=T(1) 时，也就是 n/2^k^=1，我们得到 k=log~2~n 。我们将 k 值代入上面的公式，得到 T(n)=C*n+n*log~2~n 。如果我们用大 O 标记法来表示的话，T(n) 就等于 O(nlogn)。所以归并排序的时间复杂度是 O(nlogn)。 ==归并排序的执行效率与要排序的原始数组的有序程度无关==，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。 第三，归并排序的空间复杂度是多少？ ==归并排序不是原地排序算法==。这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。空间复杂度是 O(n) 快速排序（Quicksort）快速排序（Quicksort）是对冒泡排序的一种改进。 如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。 根据分治、递归的处理思想，可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。 如果希望快排是原地排序算法，那它的空间复杂度得是 O(1)，那 partition() 分区函数就不能占用太多额外的内存空间，就需要在 A[p…r] 的原地完成分区操作。 1234567891011partition(A, p, r) &#123; pivot := A[r] i := p for j := p to r-1 do &#123; if A[j] &lt; pivot &#123; swap A[i] with A[j] i := i+1 &#125; &#125; swap A[i] with A[r] return i 因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列 6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个 6 的相对先后顺序就会改变。所以，快速排序并不是一个稳定的排序算法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.util.Arrays;public class QuickSort &#123; //三数取中法。取出不大不小的那个位置 public static int getPivotPos(int[] a,int low,int high) &#123; int mid=(low+high)/2; int pos=low; if(a[mid]&lt;a[low]) &#123; int temp=a[low]; a[low]=a[mid]; a[mid]=temp; &#125; if(a[high]&lt;a[low]) &#123; int temp=a[high]; a[high]=a[low]; a[low]=temp; &#125; if(a[high]&lt;a[mid]) &#123; int temp=a[high]; a[high]=a[mid]; a[mid]=temp; &#125; pos=mid; return pos; &#125; //划分，取出枢纽位置 public static int partition(int[] a,int low,int high) &#123; int pivotpos=getPivotPos(a,low,high); int pivot=a[pivotpos]; int temp=pivot; a[pivotpos]=a[low]; a[low]=temp; while(low&lt;high) &#123; while(low&lt;high&amp;&amp;a[high]&gt;=pivot) high--; a[low]=a[high]; while(low&lt;high&amp;&amp;a[low]&lt;=pivot) low++; a[high]=a[low]; &#125; a[low]=pivot; return low; &#125; //快排 public static void quickSort(int[] a,int low,int high) &#123; if(low&lt;high) &#123; int pivotpos=partition(a,low,high); quickSort(a,low,pivotpos-1); quickSort(a,pivotpos+1,high); &#125; &#125; //重载快排 public static void quickSort(int[] a) &#123; if(a.length==0) return; int low=0; int high=a.length-1; quickSort(a,low,high); &#125; public static void main(String[] args) &#123; int[] a= &#123;12,32,24,99,54,76,48&#125;; quickSort(a); System.out.println(Arrays.toString(a)); &#125;&#125; 快排是一种原地、不稳定的排序算法。快排的时间复杂度也是 O(nlogn)。举一个比较极端的例子。如果数组中的数据原来已经是有序的了，比如 1，3，5，6，8。如果每次选择最后一个元素作为 pivot，那每次分区得到的两个区间都是不均等的。需要进行大约 n 次分区操作，才能完成快排的整个过程。每次分区平均要扫描大约 n/2 个元素，这种情况下，快排的时间复杂度就从 O(nlogn) 退化成了 O(n2)。在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)。 区别 归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构09-排序之冒泡&插入&选择","date":"2020-11-26T07:48:56.000Z","path":"2020/11/26/01-数据结构/数据结构09-排序之冒泡&插入&选择.html","text":"[toc] 排序冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序. 展示动态排序网站：https://visualgo.net/zh 冒泡排序（Bubble Sort）冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。 实际上，刚讲的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。我这里还有另外一个例子，这里面给 6 个元素排序，只需要 4 次冒泡操作就可以了。 123456789101112131415161718// 冒泡排序，a 表示数组，n 表示数组大小public void bubbleSort(int[] a, int n) &#123; if (n &lt;= 1) return; for (int i = 0; i &lt; n; ++i) &#123; // 提前退出冒泡循环的标志位 boolean flag = false; for (int j = 0; j &lt; n - i - 1; ++j) &#123; if (a[j] &gt; a[j+1]) &#123; // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true; // 表示有数据交换 &#125; &#125; if (!flag) break; // 没有数据交换，提前退出 &#125;&#125; 第一，冒泡排序是原地排序算法吗？ 冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。 第二，冒泡排序是稳定的排序算法吗？ 在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。 第三，冒泡排序的时间复杂度是多少？ 最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以==最好情况时间复杂度是 O(n)==。 而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以==最坏情况时间复杂度为 O(n^2^)==。 对于包含 n 个数据的数组，这 n 个数据就有 n! 种排列方式。不同的排列方式，冒泡排序执行的时间肯定是不同的。比如我们前面举的那两个例子，其中一个要进行 6 次冒泡，而另一个只需要 4 次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我这里还有一种思路，通过“有序度”和“逆序度”这两个概念来进行分析。有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：有序元素对：a[i] &lt;= a[j], 如果 i &lt; j。同理，对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是n*(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作满有序度。逆序度的定义正好跟有序度相反（默认从小到大为有序）：逆序元素对：a[i] &gt; a[j], 如果 i &lt; j。逆序度 = 满有序度 - 有序度。排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的。对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 n*(n-1)/2 次交换。最好情况下，初始状态的有序度是 n*(n-1)/2，就不需要进行交换。我们可以取个中间值 n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。换句话说，平均情况下，需要 n*(n-1)/4 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n2)，所以==平均情况下的时间复杂度就是 O(n^2^)==。 这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。等我们讲到快排的时候，我还会再次用这种“不严格”的方法来分析平均时间复杂度。 插入排序（Insertion Sort）一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，只要遍历数组，找到数据应该插入的位置将其插入即可。 首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。 如图所示，要排序的数据是 4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。 插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。 对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。 123456789101112131415161718// 插入排序，a 表示数组，n 表示数组大小public void insertionSort(int[] a, int n) &#123; if (n &lt;= 1) return; for (int i = 1; i &lt; n; ++i) &#123; int value = a[i]; int j = i - 1; // 查找插入的位置 for (; j &gt;= 0; --j) &#123; if (a[j] &gt; value) &#123; a[j+1] = a[j]; // 数据移动 &#125; else &#123; break; &#125; &#125; a[j+1] = value; // 插入数据 &#125;&#125; 第一，插入排序是原地排序算法吗？ 从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。 第二，插入排序是\\**稳定**\\的排序算法吗？** 在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。 第三，插入排序的时间复杂度是多少？ 如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，==最好是时间复杂度为 O(n)==。注意，这里是从尾到头遍历已经有序的数据。 如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以==最坏情况时间复杂度为 O(n^2^)==。 还记得在数组中插入一个数据的平均时间复杂度是多少吗？没错，是 O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以==平均时间复杂度为 O(n^2^)==。 选择排序（Selection Sort）选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。 首先，选择排序空间复杂度为 O(1)，==是一种原地排序==算法。 选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度==都为 O(n^2^)==。 选择排序是一种==不稳定==的排序算法。比如 5，8，5，2，9 这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素 2，与第一个 5 交换位置，那第一个 5 和中间的 5 顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。 为什么插入排序要比冒泡排序更受欢迎呢？冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。 但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要 3 个赋值操作，而插入排序只需要 1 个： 1234567891011121314冒泡排序中数据的交换操作：if (a[j] &gt; a[j+1]) &#123; // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true;&#125; 插入排序中数据的移动操作：if (a[j] &gt; value) &#123; a[j+1] = a[j]; // 数据移动&#125; else &#123; break;&#125; 所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是 O(n^2^)，但是如果希望把性能优化做到极致，那肯定首选插入排序。插入排序的算法思路也有很大的优化空间，","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构08-递归","date":"2020-11-26T07:20:44.000Z","path":"2020/11/26/01-数据结构/数据结构08-递归.html","text":"[toc] 递归递归是一种应用非常广泛的算法（或者编程技巧）。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。 递归需要满足的三个条件： 一个问题的解可以分解为几个子问题的解 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件 写递归代码最关键的是写出递推公式，找到终止条件. 在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销， 举例假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？如果有 7 个台阶，你可以 2，2，2，1 这样子上去，也可以 1，2，1，1，2 这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？ 可以根据第一步的走法把所有走法分为两类，第一类是第一步走了 1 个台阶，另一类是第一步走了 2 个台阶。所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 加上先走 2 阶后，n-2 个台阶的走法。用公式表示就是：f(n) = f(n-1)+f(n-2) 来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法,所以 f(1)=1；有两个台阶时，只有两种走法，（一次走两步或者两次一步）所以 f(2)=2。 12345int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2);&#125; 计算机擅长做重复的事情，所以递归正和它的胃口。而我们人脑更喜欢平铺直叙的思维方式。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。人脑几乎没办法把整个“递”和“归”的过程一步一步都想清楚。 因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。 递归代码要警惕堆栈溢出函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。 可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如 1000）之后，我们就不继续往下再递归了，直接返回报错。 递归代码要警惕重复计算 为了避免重复计算，可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免。 12345678910111213public int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; // hasSolvedList 可以理解成一个 Map，key 是 n，value 是 f(n) if (hasSolvedList.containsKey(n)) &#123; return hasSovledList.get(n); &#125; int ret = f(n-1) + f(n-2); hasSovledList.put(n, ret); return ret;&#125; 递归代码改写为非递归代码对 f(x) =f(x-1)+1 这个递推公式： 1234567int f(int n) &#123; int ret = 1; for (int i = 2; i &lt;= n; ++i) &#123; ret = ret + 1; &#125; return ret;&#125; 针对，上面举例： 1234567891011121314int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; int ret = 0; int pre = 2; int prepre = 1; for (int i = 3; i &lt;= n; ++i) &#123; ret = pre + prepre; prepre = pre; pre = ret; &#125; return ret;&#125; 笼统地讲，所有的递归代码都可以改为这种迭代循环的非递归写法。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构07-队列","date":"2020-11-25T09:36:21.000Z","path":"2020/11/25/01-数据结构/数据结构07-队列.html","text":"[toc] 队列队列跟栈一样，也是一种操作受限的线性表数据结构。先进者先出 队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。 实现用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102// 用数组实现的队列public class ArrayQueue &#123; // 数组：items，数组大小：n private String[] items; private int n = 0; // head 表示队头下标，tail 表示队尾下标 private int head = 0; private int tail = 0; // 申请一个大小为 capacity 的数组 public ArrayQueue(int capacity) &#123; items = new String[capacity]; n = capacity; &#125; // 入队操作，将 item 放入队尾 public boolean enqueue(String item) &#123; // tail == n 表示队列末尾没有空间了 if (tail == n) &#123; // tail ==n &amp;&amp; head==0，表示整个队列都占满了 if (head == 0) return false; // 数据搬移,从[head, tail]到[0, tail-head],把后面的移到前面 for (int i = head; i &lt; tail; ++i) &#123; items[i-head] = items[i]; &#125; // 搬移完之后重新更新 head 和 tail tail -= head; head = 0; &#125; items[tail] = item; ++tail; return true; &#125; // 出队 public String dequeue() &#123; // 如果 head == tail 表示队列为空 if (head == tail) return null; String ret = items[head]; ++head; return ret; &#125;&#125;/** * 基于链表实现的队列 */public class QueueBasedOnLinkedList &#123; // 队列的队首和队尾 private Node head = null; private Node tail = null; // 入队 public void enqueue(String value) &#123; if (tail == null) &#123; Node newNode = new Node(value, null); head = newNode; tail = newNode; &#125; else &#123; tail.next = new Node(value, null); tail = tail.next; &#125; &#125; // 出队 public String dequeue() &#123; if (head == null) return null; String value = head.data; head = head.next; if (head == null) &#123; tail = null; &#125; return value; &#125; public void printAll() &#123; Node p = head; while (p != null) &#123; System.out.print(p.data + \" \"); p = p.next; &#125; System.out.println(); &#125; private static class Node &#123; private String data; private Node next; public Node(String data, Node next) &#123; this.data = data; this.next = next; &#125; public String getData() &#123; return data; &#125; &#125;&#125; 循环队列用数组来实现队列的时候，在 tail==n 时，会有数据搬移操作，这样入队操作性能就会受到影响。循环队列可以解决。 图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后 tail 加 1 更新为 1。所以，在 a，b 依次入队之后，循环队列中的元素就变成了下面的样子： 通过这样的方法，我们成功避免了数据搬移操作。看起来不难理解，但是循环队列的代码实现难度要比前面讲的非循环队列难多了。要想写出没有 bug 的循环队列的实现代码，最关键的是，确定好队空和队满的判定条件。 队列为空的判断条件仍然是 head == tail。 当队列满时，(tail+1)%n=head。图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。 1234567891011121314151617181920212223242526272829303132public class CircularQueue &#123; // 数组：items，数组大小：n private String[] items; private int n = 0; // head 表示队头下标，tail 表示队尾下标 private int head = 0; private int tail = 0; // 申请一个大小为 capacity 的数组 public CircularQueue(int capacity) &#123; items = new String[capacity]; n = capacity; &#125; // 入队 public boolean enqueue(String item) &#123; // 队列满了 if ((tail + 1) % n == head) return false; items[tail] = item; tail = (tail + 1) % n; return true; &#125; // 出队 public String dequeue() &#123; // 如果 head == tail 表示队列为空 if (head == tail) return null; String ret = items[head]; head = (head + 1) % n; return ret; &#125;&#125; 阻塞队列和并发队列阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。 在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？ 线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因.","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构06-栈","date":"2020-11-25T09:07:34.000Z","path":"2020/11/25/01-数据结构/数据结构06-栈.html","text":"[toc] 栈的概述栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。后进者先出，先进者后出 事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。 当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。 栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，叫作顺序栈，用链表实现的栈，叫作链式栈。 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// 基于数组实现的顺序栈public class ArrayStack &#123; private String[] items; // 数组 private int count; // 栈中元素个数 private int n; // 栈的大小 // 初始化数组，申请一个大小为 n 的数组空间 public ArrayStack(int n) &#123; this.items = new String[n]; this.n = n; this.count = 0; &#125; // 入栈操作 public boolean push(String item) &#123; // 数组空间不够了，直接返回 false，入栈失败。 if (count == n) return false; // 将 item 放到下标为 count 的位置，并且 count 加一 items[count] = item; ++count; return true; &#125; // 出栈操作 public String pop() &#123; // 栈为空，则直接返回 null if (count == 0) return null; // 返回下标为 count-1 的数组元素，并且栈中元素个数 count 减一 String tmp = items[count-1]; --count; return tmp; &#125;&#125;/** * 基于链表实现的栈。 * Author: Zheng */public class StackBasedOnLinkedList &#123; private Node top = null; public void push(int value) &#123; Node newNode = new Node(value, null); // 判断是否栈空 if (top == null) &#123; top = newNode; &#125; else &#123; newNode.next = top; top = newNode; &#125; &#125; /** * 用-1表示栈中没有数据。 */ public int pop() &#123; if (top == null) return -1; int value = top.data; top = top.next; return value; &#125; public void printAll() &#123; Node p = top; while (p != null) &#123; System.out.print(p.data + \" \"); p = p.next; &#125; System.out.println(); &#125; private static class Node &#123; private int data; private Node next; public Node(int data, Node next) &#123; this.data = data; this.next = next; &#125; public int getData() &#123; return data; &#125; &#125;&#125; 支持动态扩容的顺序栈要实现一个支持动态扩容的栈，只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，就申请一个更大的数组，将原来的数据搬移到新数组中。 实际上，支持动态扩容的顺序栈，平时开发中并不常用到。主要练习一下前面的复杂度分析方法。 对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。 对于入栈操作来说，当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。 为了分析的方便，需要事先做一些假设和定义： 栈空间不够时，重新申请一个是原来大小两倍的数组； 为了简化分析，假设只有入栈操作没有出栈操作； 定义不涉及内存搬移的入栈操作为 simple-push 操作(不需要扩容时的入栈操作)，时间复杂度为 O(1)。 如果当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存，并且做 K 个数据的搬移操作，然后再入栈。但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push 操作就可以完成。 这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分情况下，入栈操作的时间复杂度 O 都是 O(1)，只有在个别时刻才会退化为 O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。 栈的应用函数调用栈操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。 123456789101112131415int main() &#123; int a = 1; int ret = 0; int res = 0; ret = add(3, 5); res = a + ret; printf(\"%d\", res); reuturn 0;&#125; int add(int x, int y) &#123; int sum = 0; sum = x + y; return sum;&#125; main() 函数调用了 add() 函数，获取计算结果，并且与临时变量 a 相加，最后打印 res 的值。 表达式求值为了方便解释，将算术表达式简化为只包含加减乘除四则运算，比如：3+5*8-6。对于这个四则运算，人脑可以很快求解出答案，但是对于计算机来说，理解这个表达式本身就是个挺难的事儿。 实际上，编译器就是通过两个栈来实现的。其中一个保存==操作数==的栈，另一个是保存==运算符==的栈。从左向右遍历表达式，当遇到数字，就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。 括号匹配假设表达式中只包含三种括号，圆括号 ()、方括号 [] 和花括号{}，并且它们可以任意嵌套。比如，{[{}]}或 [{()}([])] 等都为合法格式，而{[}()] 或 [({)] 为不合法的格式。现在给有一个包含三种括号的表达式字符串，如何检查它是否合法呢？ 用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。 当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构05-链表","date":"2020-11-24T08:34:47.000Z","path":"2020/11/24/01-数据结构/数据结构05-链表.html","text":"[toc] 链表介绍从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。 而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。 介绍三种最常见的链表结构，分别是：单链表、双向链表和循环链表。 单链表为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针 next。 其中有两个结点是比较特殊的，它们分别是头结点和尾结点。头结点用来记录链表的基地址；尾结点不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。 在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移（假设需要保证有序），所以时间复杂度是 O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点。 针对链表的插入和删除操作，只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。 链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点，需要 O(n) 的时间复杂度。 循环链表循环链表是一种特殊的单链表。循环链表的尾结点指针是指向链表的头结点。 和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。 双向链表双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。 双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。 从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。 双向循环链表 删除操作分析从链表中删除一个数据无外乎这两种情况： 删除结点中“值等于某个给定值”的结点；查找+删除操作=O(n+1)=O(n) 删除给定指针指向的结点。 对于第二种情况，已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-&gt;next=q，说明 p 是 q 的前驱结点。 但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了！ 同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。 对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。 在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛。 用空间换时间的设计思想。当内存空间充足的时候，如果更追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。 链表 VS 数组性能比较 在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。 数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。 开发技巧 快慢指针找链表等分点：慢指针一次走一步，快指针一次两步，可找到中点。快指针三步，可找到1/3点，以此类推 警惕指针丢失和内存泄漏： 在结点 a 和相邻的结点 b 之间插入结点 x，假设当前指针 p 指向结点 a。如果我们将代码实现变成下面这个样子，就会发生指针丢失和内存泄露。 12p-&gt;next = x; // 将 p 的 next 指针指向 x 结点；x-&gt;next = p-&gt;next; // 将 x 的结点的 next 指针指向 b 结点 利用哨兵简化实现难度 重点留意边界条件处理 如果链表为空时，代码是否能正常工作？ 如果链表只包含一个结点时，代码是否能正常工作？ 如果链表只包含两个结点时，代码是否能正常工作？ 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？ 链表实现增删改查、返回长度、反转、排序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244/** * @author niumt * @create 2020-11-24 下午 07:38 * @aa 增删改查、返回长度、反转、排序。 */public class sinListNode&lt;T extends Comparable&gt; implements nodeMethods&lt;T&gt; &#123; // 泛型类T，需实现compareTo接口，排序会用到比较 // 内部类表示节点 private class node &#123; private T value; private node next = null; public node() &#123; &#125; public node(T value) &#123; this.value = value; &#125; public node(T value, node next) &#123; this.value = value; this.next = next; &#125; @Override public String toString() &#123; return \"\" + value; &#125; public T getValue() &#123; return value; &#125; public void setValue(T value) &#123; this.value = value; &#125; public node getNext() &#123; return next; &#125; public void setNext(node next) &#123; this.next = next; &#125; &#125; private node head = null; @Override public void add(T v) &#123; if (v == null) return; node next = new node(v); if (head == null) &#123; head = next; return; &#125; node tem = head; while (tem.getNext() != null) &#123; tem = tem.getNext(); &#125; tem.setNext(next); &#125; // index从0开始 public void add(T v, int index) &#123; if ((v == null) || index &gt; length()) return; node pre = head; for (int i = 0; i &lt; index - 1; i++) &#123; pre = pre.getNext(); &#125; pre.setNext(new node(v, pre.getNext())); &#125; @Override public int find(T v) &#123; if (head == null) return -1; int index = 0; node tem = head; while (tem!=null) &#123; if (tem.getValue().compareTo(v)==0)&#123; return index; &#125; tem = tem.getNext(); index++; &#125; if (tem == null) return -1; return index; &#125; @Override public void change(T v, int index) &#123; if ((v == null) || (head == null) || (index &gt; length()-1)) return; node tem = head; for (int i = 0; i &lt; index; i++) &#123; tem = tem.getNext(); &#125; tem.setValue(v); &#125; @Override public void delete(T v) &#123; if ((v == null) || (head == null)) return; if (head.getValue().compareTo(v)==0)&#123; head = head.getNext(); return; &#125; node pre = head; node cur = head.getNext(); while (cur!=null) &#123; if (cur.getValue().compareTo(v)==0)&#123; pre.setNext(cur.getNext()); break; &#125; pre = cur; cur = cur.getNext(); &#125; if (cur == null) return; &#125; @Override public void deleteByIndex(int index) &#123; if ((head == null) || (index &gt; length())) return; if (index &gt; (length() - 1)) return; if (index == 0) &#123; head = head.getNext(); return; &#125; int i = 0; node pre = head; for (i = 0; i &lt; index - 1; i++) &#123; pre = pre.getNext(); &#125; pre.setNext(pre.getNext().getNext()); &#125; @Override public int length() &#123; if (head == null) return 0; int l = 0; node n = head; while (n != null) &#123; n = n.getNext(); l++; &#125; return l; &#125; @Override public void printList() &#123; if (head == null) &#123; return; &#125; StringBuilder info = new StringBuilder(); node n = head; while (n != null) &#123; info.append(n.toString()).append(\"\\t\"); n = n.getNext(); &#125; System.out.println(info); &#125; @Override public void reverseList() &#123; /*两种方法：正向遍历，反向递归(未写)*/ if (length()&lt;=1)&#123; return; &#125; // node newHead = null; // node cur = head; // while (cur!=null)&#123; // node n = new node(cur.getValue()); // n.setNext(newHead); // newHead = n; // cur = cur.getNext(); // &#125; // head = newHead; node pre = head; node cur = pre.getNext(); pre.setNext(null); node tem; while (cur!=null)&#123; tem = cur.getNext(); cur.setNext(pre); pre = cur; cur = tem; &#125; head = pre; &#125; // 冒泡排序 @Override public void orderList() &#123; if (head == null) return; node cur = null; node pre = null; node next = null; int l = length(); for (int i = 0; i &lt; l - 1; i++) &#123; int j=0; cur = head; pre = head; while (cur.getNext() != null) &#123; if (j==l-1-i)&#123; break; &#125; next = cur.getNext(); int result = cur.getValue().compareTo(next.getValue()); if (result &gt; 0) &#123; cur.setNext(next.getNext()); next.setNext(cur); if (cur == pre) &#123; head = next; pre = head; &#125; else &#123; pre.setNext(next); pre = next; &#125; &#125; else &#123; pre = cur; cur = cur.getNext(); &#125; // System.out.print(\"\"+i+\",\"+j+\"\\t\"); // printList(); j++; &#125; &#125; &#125;&#125;","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构04-数组","date":"2020-11-24T08:14:20.000Z","path":"2020/11/24/01-数据结构/数据结构04-数组.html","text":"[toc] 数组概念数组（Array）是一种==线性表==数据结构。它用一组==连续的内存空间==，来存储一组具有==相同类型==的数据。 线性表（Linear List）：顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。 非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。 连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。 低效的“插入”和“删除”如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。 如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数组插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。 如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。这不就是 JVM 标记清除垃圾回收算法的核心思想吗？ 警惕数组的访问越界问题123456789int main(int argc, char* argv[])&#123; int i = 0; int arr[3] = &#123;0&#125;; for(; i&lt;=3; i++)&#123; arr[i] = 0; printf(\"hello world\\n\"); &#125; return 0;&#125; 在 C 语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。根据我们前面讲的数组寻址公式，a[3] 也会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量 i 的内存地址，那么 a[3]=0 就相当于 i=0，所以就会导致代码无限循环。 数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。 总结：数组和ArrayList Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。 要表示多维数组时，用数组往往会更加直观。比如 Object array[][] 对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。 为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式： a[k]_{address} = base_{address} + k * TypeSize但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为： a[k]_{address} = base_{address} + (k-1) * TypeSize从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。 数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"HBase00-安装","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/00-环境/06-HBase安装.html","text":"启动Zookeeper 启动Hadoop：hdfs和yarn HBase的解压 tar -zxvf HBase-1.3.1-bin.tar.gz -C /opt/module 修改HBase的配置文件 HBase-env.sh export JAVA_HOME=/opt/module/jdk1.6.0_144 export HBASE_MANAGES_ZK=false HBase-site.xml 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181, hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.10/zkData&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers: hadoop102 hadoop103 hadoop104 软连接hadoop配置文件到HBase 12[atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/HBase/conf/core-site.xml[atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/HBase/conf/hdfs-site.xml 分发HBase到其他节点 HBase服务的启动 启动方式1 12bin/HBase-daemon.sh start masterbin/HBase-daemon.sh start regionserver 提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。 修复提示： a、同步时间服务 b、属性：hbase.master.maxclockskew设置更大的值 12345&lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;Time difference of regionserver from master&lt;/description&gt; &lt;/property&gt; 启动方式2 bin/start-HBase.sh bin/stop-HBase.sh 查看HBase页面 启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：http://hadoop102:16010","tags":[{"name":"HBase","slug":"HBase","permalink":"https://mtcai.github.io/tags/HBase/"}]},{"title":"Flume00-安装","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume00-安装.html","text":"安装地址 Flume 官网地址：http://flume.apache.org/ 文档查看地址：http://flume.apache.org/FlumeUserGuide.html 下载地址：http://archive.apache.org/dist/flume/ 安装部署 解压 apache flume 1.7.0 bin.tar.gz 到 /opt/ 目录下 将 flume/conf 下的 flume env.sh.template 文件修改为 flume env.sh ，并 配置 flumeenv.sh 文件 123[atguigu@hadoop102 conf]$ mv flume env.sh.template flume env.sh[atguigu@hadoop102 conf]$ vi flume env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144","tags":[]},{"title":"Flume01-概述","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume01-概述.html","text":"定义Flume 是Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。 Flume最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写入到HDFS。 基础架构 AgentAgent是一个 JVM 进程，它以事件的形式将数据从源头送至目的。 Agent主要有 3 个部分组成， Source 、 Channel 、 Sink 。 SourceSource是负责接收数据到 Flume Agent 的组件。 Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、 thrift、 exec、 jms、 spooling directory、 netcat、 sequence generator、 syslog、 http、 legacy。 SinkSink不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent 。 Sink组件目的地包括 hdfs、 logger、avro、 thrift、 ipc、 file、 HBase、 solr、自定义。 ChannelChannel是位于 Source 和 Sink 之间的缓冲区。因此， Channel 允许 Source 和 Sink 运作在不同的速率上。 Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个Sink 的读取操作。 Flume自带两种 Channel：Memory Channel 和 File Channel 以及 Kafka Channel。 Memory Channel是内存中的队列。 Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。 FileChannel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。 Event传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。Event 由 Header 和 Body 两部分组成， Header 用来存放该 event 的一些属性，为 K-V 结构，Body 用来存放该条数据，形式为字节数组。","tags":[]},{"title":"Flume02-案例","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume02-案例.html","text":"Flume入门案例监控端口数据官方案例案例需求：使用Flume 监听一个端口， 收集该端口数据 ，并打印到控制台。 实现步骤： 安装 netcat 工具 sudo yum install -y nc 判断44444 端口是否被占用 sudo netstat -tunlp | grep 44444 创建Flume Agent 配置文件flume-netcat-logger.conf 在flume-netcat-logger.conf 文件中添加如下内容: 1234567891011121314151617'Name the components on this agent'a1.sources = r1a1.sinks = k1a1.channels = c1'Describe/configure the source'a1.sources.r1.type = netcata1.sources.r1.bind = localhost 表示a1监听的主机a1.sources.r1.port = 44444 表示a1监听的端口号'Describe the sink'a1.sinks.k1.type = logger'Use a channel which buffers events in memory'a1.channels.c1.type = memorya1.channels.c1.capacity = 1000 表示a1的channel总容量1000个eventa1.channels.c1.transactionCapacity = 100 表示a1的channel传输时收集到了100条event后再去提交事务'Bind the source and sink to the channel'a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 先开启flume 监听端口 12345678bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console 或bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console --conf/-c：表示配置文件存储在conf/目录--name/-n：表示给agent 起名为a1--conf-file/-f：flume 本次启动读取的配置文件是在job 文件夹下的flume-telnet.conf文件。-Dflume.root.logger=INFO,console ：-D 表示flume 运行时动态修改flume.root.logger 参数属性值，并将控制台日志打印级别设置为INFO 级别。日志级别包括:log、info、warn、error。 使用netcat 工具向本机的44444 端口发送内容 123[atguigu@hadoop102 ~]$ nc localhost 44444helloatguigu 在Flume 监听页面观察接收数据情况 实时监控单个追加文件案例需求：实时监控Hive 日志，并上传到HDFS 中 实现步骤: Flume 要想将数据输出到HDFS，须持有Hadoop 相关jar 包 1234567commons-configuration-1.6.jar、hadoop-auth-2.7.2.jar、hadoop-common-2.7.2.jar、hadoop-hdfs-2.7.2.jar、commons-io-2.4.jar、htrace-core-3.1.0-incubating.jar拷贝到/opt/module/flume/lib 文件夹下。 创建flume-file-hdfs.conf 文件 123456789101112131415161718192021222324252627' Name the components on this agenta2.sources = r2a2.sinks = k2a2.channels = c2' Describe/configure the sourcea2.source s.r2.type = execa2.sources.r2.command = tail -F /opt/module/hive/logs/hive.loga2.sources.r2.shell = /bin/bash -c' Describe the sinka2.sinks.k2.type = hdfsa2.sinks.k2.hdfs.path = hdfs://hadoop102:9000/flume/%Y%m%d/%Ha2.sinks.k2.hdfs.filePrefix = logs- '上传文件的前缀a2.sinks.k2.hdfs.round = true '是否按照时间滚动文件夹a2.sinks.k2.hdfs.roundValue = 1 '多少时间单位创建一个新的文件夹a2.sinks.k2.hdfs.roundUnit = hour '重新定义时间单位a2.sinks.k2.hdfs.useLocalTimeStamp = true '是否使用本地时间戳a2.sinks.k2.hdfs.batchSize = 1000 '积攒多少个 Event 才 flush 到 HDFS 一次a2.sinks.k2.hdfs.fileType = DataStream '设置文件类型，可支持压缩a2.sinks.k2.hdfs.rollInterval = 30 '多久生成一个新的文件a2.sinks.k2.hdfs.rollSize = 134217700 '设置每个文件的滚动大小,略小于块的大小a2.sinks.k2.hdfs.rollCount = 0 '文件的滚动与 Event 数量无关a2.channels.c2.type = memory ' Use a channel which buffers events in memorya2.channels.c2.capacity = 1000a2.channels.c2.transactionCapacity = 100' Bind the source and sink to the channela2.sources.r2.channels = c2a2.sinks.k2.channel = c2 运行Flume bin/flume-ng agent —conf conf/ —name a2 —conf-file job/flume-file-hdfs.conf 开启Hadoop 和Hive 并操作Hive 产生日志 在HDFS 上查看文件。 实时监控目录下多个新文件案例需求：使用Flume 监听整个目录的文件，并上传至HDFS 创建配置文件flume-dir-hdfs.conf 123456789101112131415161718192021222324252627282930a3.sources = r3a3.sinks = k3a3.channels = c3'Describe/configure the sourcea3.sources.r3.type = spooldira3.sources.r3.spoolDir = /opt/module/flume/uploada3.sources.r3.fileSuffix = .COMPLETEDa3.sources.r3.fileHeader = truea3.sources.r3.ignorePattern = ([^ ]*\\.tmp) '忽略所有以.tmp 结尾的文件，不上传'Describe the sinka3.sinks.k3.type = hdfsa3.sinks.k3.hdfs.path =hdfs://hadoop102:9000/flume/upload/%Y%m%d/%Ha3.sinks.k3.hdfs.filePrefix = upload- '上传文件的前缀a3.sinks.k3.hdfs.round = true '是否按照时间滚动文件夹a3.sinks.k3.hdfs.roundValue = 1 '多少时间单位创建一个新的文件夹a3.sinks.k3.hdfs.roundUnit = hour '重新定义时间单位a3.sinks.k3.hdfs.useLocalTimeStamp = true '是否使用本地时间戳a3.sinks.k3.hdfs.batchSize = 100 '积攒多少个Event 才flush 到HDFS 一次a3.sinks.k3.hdfs.fileType = DataStream '设置文件类型，可支持压缩a3.sinks.k3.hdfs.rollInterval = 60 '多久生成一个新的文件a3.sinks.k3.hdfs.rollSize = 134217700 '设置每个文件的滚动大小大概是128Ma3.sinks.k3.hdfs.rollCount = 0 '文件的滚动与Event 数量无关'Use a channel which buffers events in memorya3.channels.c3.type = memorya3.channels.c3.capacity = 1000a3.channels.c3.transactionCapacity = 100'Bind the source and sink to the channela3.sources.r3.channels = c3a3.sinks.k3.channel = c3 启动监控文件夹命令 bin/flume-ng agent —conf conf/ —name a3 —conf-file job/flume-dir-hdfs.conf 说明：在使用Spooling Directory Source 时，不要在监控目录中创建并持续修改文件，上传完成的文件会以.COMPLETED 结尾，被监控文件夹每500 毫秒扫描一次文件变动 向upload 文件夹中添加文件 查看HDFS 上的数据 实时监控目录下的多个追加文件Exec source 适用于监控一个实时追加的文件，但不能保证数据不丢失；Spooldir Source 能够保证数据不丢失，且能够实现断点续传，但延迟较高，不能实时监控；而Taildir Source 既能够实现断点续传，又可以保证数据不丢失，还能够进行实时监控。 案例需求：使用Flume 监听整个目录的实时追加文件，并上传至HDFS 创建配置文件flume-taildir-hdfs.conf 12345678910111213141516171819202122232425262728a3.sources = r3a3.sinks = k3a3.channels = c3'Describe/configure the sourcea3.sources.r3.type = TAILDIRa3.sources.r3.positionFile = /opt/flume/tail_dir.jsona3.sources.r3.filegroups = f1a3.sources.r3.filegroups.f1 = /opt/module/flume/files/file.*'Describe the sinka3.sinks.k3.type = hdfsa3.sinks.k3.hdfs.path =hdfs://hadoop102:9000/flume/upload/%Y%m%d/%Ha3.sinks.k3.hdfs.filePrefix = upload- '上传文件的前缀'a3.sinks.k3.hdfs.round = true '是否按照时间滚动文件夹'a3.sinks.k3.hdfs.roundValue = 1 '多少时间单位创建一个新的文件夹a3.sinks.k3.hdfs.roundUnit = hour '重新定义时间单位'a3.sinks.k3.hdfs.useLocalTimeStamp = true ' 是否使用本地时间戳'a3.sinks.k3.hdfs.batchSize = 100 '积攒多少个 Event 才 flush 到 HDFS 一次'a3.sinks.k3.hdfs.fileType = DataStream '设置文件类型，可支持压缩'a3.sinks.k3.hdfs.rollInterval = 60 '多久生成一个新的文件'a3.sinks.k3.hdfs.rollSize = 134217700 '设置每个文件的滚动大小大概是 128M'a3.sinks.k3.hdfs.rollCount = 0 '文件的滚动与 Event 数量无关''Use a channel which buffers events in memorya3.channels.c3.type = memorya3.channels.c3.capacity = 1000a3.channels.c3.transactionCapacity = 100'Bind the source and sink to the channela3.sources.r3. channels = c3a3.sinks.k3.channel = c3 启动监控文件夹命令 bin/flume-ng agent —conf conf/ —name a3 —conf-file job/flume-taildir-hdfs.conf 向files 文件夹中追加内容 echo hello &gt;&gt; file1.txt 查看HDFS 上的数据 Taildir 说明： Taildir Source 维护了一个json 格式的position File，其会定期的往position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。PositionFile 的格式如下： 12&#123;\"inode\":2496272,\"pos\":12,\"file\":\"/opt/module/flume/files/file1.txt\"&#125;&#123;\"inode\":2496275,\"pos\":12,\"file\":\"/opt/module/flume/files/file2.txt\"&#125; 注：Linux 中储存文件元数据的区域就叫做inode，每个inode 都有一个号码，操作系统用inode 号码来识别不同的文件，Unix/Linux 系统内部不使用文件名，而使用inode 号码来识别文件。 Flume 企业开发案例复制和多路复用案例需求：使用Flume-1 监控文件变动，Flume-1 将变动内容传递给Flume-2，Flume-2 负责存储到HDFS。同时Flume-1 将变动内容传递给Flume-3，Flume-3 负责输出到Local FileSystem。 在/opt/module/datas/目录下创建flume3 文件夹 创建flume-file-flume.conf 1234567891011121314151617181920212223242526272829'Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2'将数据流复制给所有 channela1.sources.r1.selector.type = replicating'Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /opt/module/hive/logs/hive.loga1.sources.r1.shell = /bin/bash -c'Describe the sink'sink 端的 avro 是一个数据发送者a1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop102a1.sinks.k1.port = 4141a1.sinks.k2.type = avroa1.sinks.k2.hostname = hadoop102a1.sinks.k2.port = 4142'Describe the channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100'Bi nd the source and sink to the channela1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2 创建 flume-flume-hdfs.conf 1234567891011121314151617181920212223242526272829'Name the components on this agenta2.sources = r1a2.sinks = k1a2.channels = c1'Describe/configure the source'source 端的 avro 是一个数据接收服务a2.sources.r1.type = avroa2.sources.r1.bind = hadoop102a2.sources.r1.port = 4141'Describe the sinka2.sinks.k1.type = hdfsa2.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume2/%Y%m%d/%Ha2.sinks.k1.hdfs.filePrefix = flume2 '上传文件的前缀a2.sinks.k1.hdfs.round = true'是否按照时间滚动文件夹a2.sinks.k1.hdfs.roundValue = 1 '多少时间单位创建一个新的文件夹a2.sinks.k1.hdfs.roundUnit = hour '重新定义时间单位a2.sinks.k1.hdfs.useLocalTimeStamp = true '是否使用 本地时间戳a2.sinks.k1.hdfs.batchSize = 100 '积攒多少个 Event 才 flush 到 HDFS 一次a2.sinks.k1.hdfs.fileType = DataStream '设置文件类型，可支持压缩a2.sinks.k1.hdfs.rollInterval = 600 '多久生成一个新的文件a2.sinks.k1.hdfs.rollSize = 134217700 '设置每个文件的滚动大小大概是 128Ma2.sinks.k1.hdfs.rollCount = 0 '文件的滚动与 Event 数量无关'Describe the channela2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100'Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 创建 flume-flume-dir.conf 12345678910111213141516171819'Name the components on this agenta3.sources = r1a3.sinks = k1a3.channels = c2'Describe/configure the sourcea3.sources.r1.type = avroa3.sources.r1.bind = hadoop102a3.sources.r1.port = 4142'Describe the sink'提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。a3.sinks.k1.type = file_rolla3.sinks.k1.sink.directory = /opt/module/data/flume3'Describe the channela3.channels.c2.type = memorya3.cha nnels.c2.capacity = 1000a3.channels.c2.transactionCapacity = 100'Bind the source and sink to the channela3.sources.r1.channels = c2a3.sinks.k1.channel = c2 执行配置文件 分别启动对应的 flume 进程 flume flume dir flume flume h dfs flume file flume : [atguigu@hadoop102 flume]$ bin/flume-ng agent —conf conf/ —namea3 —conf-file job/group1/flume-flume-dir.conf [atguigu@hadoop102 flume]$ bin/flume-ng agent —conf conf/ —namea2 —conf-file job/group1/flume-flume-hdfs.conf [atguigu@hadoop102 flume]$ bin/flume-ng agent —conf conf/ —namea1 —conf-file job/group1/flume-file-flume.conf 启动 Hadoop 和 Hive 检查 /opt/module/datas/flume3 目录中数据和 HDFS 上数据 负载均衡和故障转移使用Flume 1 监控一个端口，其 sink 组中的 sink 分别对接 Flume2 和Flume3 ，采用FailoverSinkProcessor ，实现故障转移的功能。 创建flume-netcat-flume.conf 1234567891011121314151617181920212223242526272829'Name the components on this agenta1.sources = r1a1.channels = c1a1.sinkgroups = g1a1.sinks = k1 k2'Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sinkgroups.g1.processor.type = failovera1.sinkgroups.g1.processor.priority.k1 = 5a1.sinkgroups.g1.processor.priority.k2 = 10a1.sinkgroups.g1.processor.maxpenalty = 10000'Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop102a1.sinks.k1.port = 4141a1.sinks.k2.type = avroa1.sinks.k2.hostname = hadoop102a1.sinks.k2.port = 4142'Describe the channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100'Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinkgroups.g1.sinks = k1 k2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c1 创 建 flume-flume-console1.conf 1234567891011121314151617'Name the components on this agenta2.sources = r1a2.sinks = k1a2.channels = c1'Describe/configure the sourcea2.sources.r1.type = avroa2.sources.r1.bind = hadoop102a2.sources.r1.port = 4141'Describe the sinka2.sinks.k1.type = logger'Describe the channela2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100'Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 flume-flume-console2.conf 1234567891011121314151617'Name the components on this agenta3.sources = r1a3.sinks = k1a3.channels = c2'Describe/configure the sourcea3.sources.r1.type = avroa3.sources.r1.bind = hadoop102a3.sources.r1.port = 4142'escribe the sinka3.sinks.k1.type = logger'escribe the channela3.channels.c2.type = memorya3.channels.c2.capacity = 1000a3.channels.c2.transactionCapacity = 100'ind the source and sink to the channela3.sources.r1.channels = c2a3.sinks.k1.channel = c2 分别开启对应配置文件：flume-flume-console2.conf, flume-flume-console1.conf, flume-netcat-flume.conf。 123[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf 使用 net cat 工具向本机的 44444 端口发送内容 查看 F lume 2 及 F lume 3 的控制台打印日志 将 Flume2 kill ，观察 Flume 3 的控制台打印情况。 注：使用jps -ml 查看 Flume 进程。 聚合案例需求：hadoop102 上的 Flume1 监控文件 opt/module /data group.log；hadoop103 上的 Flume2 监控某一个端口的数据流，Flume1 与 Flume2 将数据发送给 hadoop104 上的 Flume3, Flume3 将最终数据打印到控制台。 分发Flume 创建flume1-logger-flume.conf 12345678910111213141516171819'Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1'Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /opt/module/group.loga1.sources.r1.shell = /bin/bash -c'Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop104a1.sinks.k1.port = 4141'Describe the channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100'Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 创建 flume2-netcat-flume.conf 12345678910111213141516171819'Name the components on this agenta2.sources = r1a2.sinks = k1a2.channels = c1'Describe/configure the sourcea2.sources.r1.type = netcata2.sources.r1.bind = hadoop103a2.sources.r1.port = 44444'Describe the sinka2.sinks.k1.type = avroa2.sinks.k1.hostname = hadoop104a2.sinks.k1.port = 4141'Use a channel which buffers events in memorya2.channels.c1.type = memorya2.channels.c1.capa city = 1000a2.channels.c1.transactionCapacity = 100'Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 创建 flume3-flume-logger.conf 123456789101112131415161718'Name the components on this agenta3.sources = r1a3.sinks = k1a3.channels = c1'Describe/configure the sourcea3.sources.r1.type = avroa3.sources.r1.bind = hadoop10 4a3.sources.r1.port = 4141'Describe the sink'Describe the sinka3.sinks.k1.type = logger'Describe the channela3.channels.c1.type = memorya3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100'Bind the source and sink to the channela3.sources.r1.channels = c1a3.sinks.k1.channel = c1 执行配置文件 123[atguigu@hadoop104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume1-logger-flume.conf[atguigu@hadoop103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume2-netcat-flume.conf 在 hadoop 103 上向 opt/module 目录下的 group .log 追加内容 在 hadoop 102 上向 44444 端口发送 数据 检查 hadoop 104 上数据","tags":[]},{"title":"Flume04-面试题","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume04-面试题.html","text":"如何实现Flume 数据传输的监控的使用第三方框架Ganglia 实时监控Flume。 Flume 的Source，Sink，Channel 的作用？你们Source 是什么类型？作用： Source 组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy Channel 组件对采集到的数据进行缓存，可以存放在Memory 或File 中。 Sink 组件是用于把数据发送到目的地的组件，目的地包括HDFS、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。 我公司采用的Source 类型为 监控后台日志：exec 监控后台产生日志的端口：netcat Flume 的Channel Selectors Flume参数调优Source增加Source 个（使用 Tair Dir Source 时可增加 FileGroups 个数）可以增大 Source 的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个 Source 以保证 Source 有足够的能力获取到新产生的数据。 batchSize参数决定 Source 一次批量运输到 Channel 的 event 条数，适当调大这个参数可以提高 Source 搬运 Event 到 Channel 时的性能。 Channeltype选择 memory 时 Channel 的性能最好，但是如果 Flume 进程意外挂掉可能会丢失数据。 type 选择 file 时 Channel 的容错性更好，但是性能上会比 memory channel 差。 使用file Channel 时 dataDirs 配置多个不同盘下的目录可以提高性能。 Capacity参数决定 Channel 可容纳最大的 event 条数。 transactionCapacity 参数决定每次 Source 往 channel 里面写的最大 event 条数和每次 Sink 从 channel 里面读的最大 event条数。 transactionCapacity 需要大于 Source 和 Sink 的 batchSize 参数。 Sink增加Sink 的个数可以增加 Sink 消费 event 的能力。 Sink 也不是越多越好够用就行，过多的 Sink 会占用系统资源，造成系统资源不必要的浪费。 batchSize参数决定 Sink 一次批量从 Channel 读取的 event 条数，适当调大这个参数可以提高 Sink 从 Channel 搬出 event 的性能。 Flume的事务机制Flume的事务机制（类似数据库的事务机制）： Flume 使用两个独立的事务分别负责从Soucrce 到 Channel ，以及从 Channel 到 Sink 的事件传递。比如 spooling directory source为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到 Channel 且提交成功，那么 Soucrce 就将该文件标记为完成。同理，事务以类似的方式处理从 Channel 到 Sink 的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到 Channel 中，等待重新传递。 Flume采集数据会丢失吗 ?根据Flume 的架构原理， Flume 是不可能丢失数据的，其内部有完善的事务机制，Source 到 Channel 是事务性的， Channel 到 Sink 是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是 Channel 采用 memory C hannel agent 宕机导致数据丢失，或者 Channel 存储数据已满，导致 Source 不再写入，未写入的数据丢失。 Flume不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由 Sink 发出，但是没有接收到响应， Sink 会再次发送数据，此时可能会导致数据的重复 。","tags":[]},{"title":"Flume03-原理","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume03-原理.html","text":"Flume事务 Agent 内部原理 ChannelSelector ChannelSelector 的作用就是选出Event 将要被发往哪个Channel。其共有两种类型，分别是Replicating（复制）和Multiplexing（多路复用）。 ReplicatingSelector会将同一个 Event 发往所有的 Channel， Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel。 SinkProcessor SinkProcessor共有三种类型，分别是 DefaultSinkProcessor 、LoadBalancingSinkProcessor 和 FailoverSinkProcessor。 DefaultSinkProcessor 对应的是单个的 Sink，LoadBalancingSinkProcessor 和FailoverSinkProcessor 对应的是 Sink Group，LoadBalancingSinkProcessor 可以实现负载均衡的功能， FailoverSinkProcessor 可以 实现故障转移的功能。 拓扑结构 串联 这种模式是将多个flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量 flume 数量 过多不仅会影响传输速率，而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统。 复制和多路复用图 Flume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel 中，或者将不同数据分发到不同的 channel 中， sink 可以选择传送到不同的目的地。 负载均衡和故障转移 Flume 支持使用将多个 sink 逻辑上分到一个 sink 组， sink 组配合不同的 SinkProcessor可以实现负载均衡和错误恢复的功能。 聚合 这种模式是我们最常见的，也非常实用，日常web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个flume 采集日志，传送到一个集中收集日志的flume，再由此flume 上传到hdfs、hive、hbase 等，进行日志分析。 自定义Interceptor在实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要发送到不同的分析系统。此时会用到 Flume 拓扑结构中的 Multiplexing 结构， Multiplexing的原理是，根据 event 中 Header 的某个 key 的值，将不同的 event 发送到不同的 Channel中，所以我们需要自定义一个Interceptor，为不同类型的event 的Header 中的key 赋予不同的值。 在该案例中，我们以端口数据模拟日志，以数字（单个）和字母（单个）模拟不同类型的日志，我们需要自定义interceptor 区分数字和字母，将其分别发往不同的分析系统（Channel）。 实现步骤: 创建一个maven 项目，并引入以下依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt;&lt;/dependency&gt; 定义CustomInterceptor 类并实现Interceptor 接口 123456789101112131415161718192021222324252627282930313233343536package com.atguigu.flume.interceptor;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.interceptor.Interceptor;import java.util.List;public class CustomInterceptor implements Interceptor &#123; @Override public void initialize() &#123; &#125; @Override public Event intercept(Event event) &#123; byte[] body = event.getBody(); if (body[0] &lt; 'z' &amp;&amp; body[0] &gt; 'a') &#123; event.getHeaders().put(\"type\", \"letter\"); &#125; else if (body[0] &gt; '0' &amp;&amp; body[0] &lt; '9') event.getHeaders().put(\"type\", \"number\"); return event; &#125; @Override public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123; for (Event event : events) &#123; intercept(event); &#125; return events; &#125; @Override public void close() &#123;&#125; public static class Builder implements Interceptor.Builder &#123; @Override public Interceptor build() &#123; return new CustomInterceptor(); &#125; @Override public void configure(Context context) &#123;&#125; &#125;&#125; 编辑 flume 配置文件 为hadoop102 上的 Flume1 配置 1 个 netcat source, 1 个 sink group, 2 个 avro sink并配置相应的 ChannelSelector 和 interceptor。 123456789101112131415161718192021222324252627282930313233'Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2'Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.CustomInterceptor$Buildera1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = typea1.sources.r1.selector.mapping.letter = c1a1.sourc es.r1.selector.mapping.number = c2# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop103a1.sinks.k1.port = 4141a1.sinks.k2.type=avroa1.sinks.k2.hostname = hadoop104a1.sinks.k2.port = 4242'Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100'Use a channel which buffers events in memorya1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100'Bind the source and sink to the channela1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2 为 hadoop 103 上的 F lume2 配置一个 avro source 和一个 logger sink 。 12345678910111213141516a1.sources = r1a1.sinks = k1a1.channels = c1 a1.sources.r1.type = avroa1.sources.r1.bind = hadoop103a1.sources.r1.port = 4141 a1.sinks.k1.type = logger a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 a1.sinks.k1.channel = c1a1.sources.r1.channels = c1 为hadoop 10 4 上的 Flume3 配置一个 avro source 和一个 logger sink 。 12345678910111213141516a1.sources = r1a1.sinks = k1a1.channels = c1 a1.sources.r1.type = avroa1.sources.r1.bind = hadoop104a1.sources.r1.port = 4242 a1.sinks.k1.type = logger a1.channels.c1.type = memorya1.channels.c1.capacity = 10 00a1.channels.c1.transactionCapacity = 100 a1.sinks.k1.channel = c1a1.sources.r1.channels = c1 分别在 hadoop102 hadoop103 hadoop104 上启动 flume 进程 ，注意先后顺序。 在 hadoop102 使用 netcat 向 localhost 444 44 发送字母和数字 观察 hadoop103 和 hadoop104 打印的日志 。 自定义SourceSource是负责接收数据到 Flume Agent 的组件。 Source 组件可以处理各种类型、各种格式的日志数据 包括 avro、 thrift、 exec、 jms、 spooling directory、 netcat、 sequence generator、 syslog、 http、 legacy。官方提供的 source 类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些 source。 MySource 需要继承 AbstractSource 类并实现 Configurable 和 PollableSource 接口。 实现相应方法： getBackOffSleepIncrement暂不用 getMaxBackOffSleepInterval 暂不用 configure(Context context初始化 context （读取配置文件内容) process()获取数据封装成 event 并写入 channel ，这个方法将被循环调用。使用场景：读取MySQL 数据或者其他文件系统。 需求：使用flume 接收数据，并给每条数据添加前缀，输出到控制台。前缀可从 flume 配置文件中配置。 导入pom 依赖 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写 MySource 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import org.apache.flume.Context;import org.apache.flume.EventDeliveryException;import org.apache.flume.PollableSource;import org.apache.flume.conf.Configurable;import org.apache.flume.event.SimpleEvent;import org.apache.flume.source.AbstractSource;import java.util.HashMap; public class MySource extends AbstractSource implements Configurable, PollableSource &#123; //定义配置文件将来要读取的字段 private Long delay; private String field; //初始化配置信息 @Override public void configure(Context context) &#123; delay = context.getLong(\"delay\"); field = context.getString(\"field\", \"Hello!\"); &#125; @Override public Status process() throws EventDeliveryException &#123; try &#123; //创建事件头信息 HashMap&lt;String, String&gt; hearderMap = new HashMap&lt;&gt;(); //创建事件 SimpleEvent event = new SimpleEvent(); //循环封装事件 for (int i = 0; i &lt; 5; i++) &#123; //给事件设置头信息 event.setHeaders(hearderMap); //给事件设置内容 event.setBody((field + i).getBytes()); //将事件写入 channel getChannelProcessor().processEvent(event); Thread.sleep(delay); &#125; &#125; catch (E xception e) &#123; e.printStackTrace(); return Status.BACKOFF; &#125; return Status.READY; &#125; @Override public long getBackOffSleepIncrement() &#123; return 0; &#125; @Override public long getMaxBackOffS leepInterval() &#123; return 0; &#125;&#125; 打包，将写好的代码打包，并放到flume 的 lib 目录（ opt/module/flume ）下。 配置文件 1234567891011121314151617'Name the components on this agent'a1.sources = r1a1.sinks = k1a1.channels = c1'Describe/configure the source'a1.sources.r1.type = com.atguigu.MySourcea1.sources.r1.delay = 1000a1.sources.r1.field = atguigu'Describe the sink'a1.sinks.k1.type = logger'Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transac tionCapacity = 100'Bind the source and sink to the channel'a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 开启任务 [atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -fjob/my-source.conf -n a1 -Dflume.root.logger=INFO,console 结果展示 自定义SinkSink不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent 。 Sink是完全事务性的。在从 Channel 批量删除数据之前，每个 Sink 用 Channel 启动一个事务。批量事件一旦成功写出到存储系统或下一个 Flume Agent Sink 就利用 Channel 提交事务。 事务一旦被提交，该 Channel 从自己的内部缓冲区删除事件。 Sink 组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的Sink 类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Sink。 MySink 需要继承AbstractSink 类并实现Configurable 接口。 实现相应方法： configure(Context context)//初始化 context（读取配置文件内容） process() //从Channel 读取获取数据（event），这个方法将被循环调用。 使用场景：读取Channel 数据写入MySQL 或者其他文件系统 需求：使用flume 接收数据，并在Sink 端给每条数据添加前缀和后缀，输出到控制台。前后缀可在flume 任务配置文件中配置。 编码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.sink.AbstractSink;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class MySink extends AbstractSink implements Configurable &#123; //创建 Logger 对象 private static final Logger LOG = LoggerFactory.getLogger(AbstractSink.class); private String prefix; private String suffix; @Override public Status process() throws EventDeliveryException &#123; //声明返回值状态信息 Status status; //获取当前 Sink 绑定的 Channel Channel ch = getChannel(); //获取事务 Transaction txn = ch.getTransaction(); //声明事件 Event event; //开启事务 txn.begin(); //读取 Channel 中的事件，直到读取到事件结束循环 while (true) &#123; event = ch.take(); if (event != null) &#123; break; &#125; &#125; try &#123; //处理事件（打印） LOG.info(prefix + new String(event.getBody()) + suffix); //事务提交 txn.commit(); status = Status.READY; &#125; catch (Exception e)&#123; //遇到异常，事务回滚 txn.rollback(); status = Status.BACKOFF; &#125; finally&#123; //关闭事务 txn.close(); &#125; return status; &#125; @Override public void configure(Context context) &#123; //读取配置文件内容，有默认值 prefix = context.getString(\"prefix\", \"hello:\"); //读取配置文件内容，无默认值 suffix = context.getString(\"suffix\"); &#125;&#125; 打包 将写好的代码打包，并放到flume 的 lib 目录（ （/opt/module/flume ）下。 配置文件 12345678910111213141516171819'Name the components on this agent'a1.sources = r1a1.sinks = k1a1.channels = c1'Describe/configure the source'a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444'Describe the sink'a1.sinks.k1.type = com.atguigu.MySink'a1.sinks.k1.prefix ='a1.sinks.k1.suffix = :atguigu'Use a channel which buffers events in memory'a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100'Bind the source and sink to the channel'a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 开启任务 123456[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console[atguigu@hadoop102 ~]$ nc localhost 44444helloOKatguiguOK 结果展示 Flume数据流监控Ganglia的安装与部署 安装 httpd 服务与 php [atguigu@hadoop102 flume]$ sudo yum y install httpd php 安装其他依赖 sudo yum y install rrdtool perl rrdtoolrrdtool devel [atguigu@hadoop102 flume]$ sudo yum y install apr devel 安装 ganglia sudo rpm Uvhhttp://dl.fedoraproject.org/pub/epel/6/x86_64/epel release 68.noarch.rpm sudo yum y install ganglia gmetad sudo yum y install ganglia web sudo yum y install ganglia gmond Ganglia由 gmond、 gmetad 和 gweb 三部分组成。 gmond (Ganglia Monitoring Daemon) 是一种轻量级服务，安装在每台需要收集指标数据的节点主 机上。使用 gmond ，你可以很容易收集很多系统指标数据，如 CPU 、内存、磁盘、网络和活跃进程的数据等。 gmetad (Ganglia Meta Daemon)整合所有信息，并将其以 RRD 格式存储至磁盘的服务。 gweb (Ganglia Web Ganglia) 可视化工具， gweb 是一种利用浏览器显示 gmetad 所存储数据的 PHP 前端。在 Web 界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。 修改配置文件 /etc/httpd/conf.d/ganglia.conf 12345678910# Ganglia monitoring system php web frontendAlias /ganglia /usr/share/ganglia&lt;Location/ ganglia&gt; Order deny,allow #Deny from all Allow from all ##这一行 # Allow from 127.0.0.1 # Allow from ::1 # Allow from .example.com&lt;/Location&gt; 修改配置文件 /etc/ganglia/gmetad.conf data_source “hadoop102” 192.168. 9 .102 修改配置文件 /etc/ganglia/gmond.conf 12345678910111213141516171819202122232425262728cluster &#123; name = \"hadoop102\" owner = \"unspecified\" latlong = \"unspecified\" url = \"unspecified\"&#125;udp_send_channel &#123; #bind_hostname = yes # Highly recommended, soon to be default. # This option tells gmond to use a source address # that resolves to the machine's hostname. Without # this, the metrics may appear to come from any # interface and the DNS names associated with # those IPs will be used to create the # mcast_join = 239.2.11.71 host = 192.168.9.102 port = 8649 ttl = 1&#125;udp_recv_channel &#123; # mcast_join = 239.2.11.71 port = 8649 bind = 192.168. 9 .102 retry_bind = true # Size of the UDP buffer. If you are handling lots of metrics you really # should bump it up to e.g. 10MB or even # buffer = 10485760&#125; 修改配置文件 /etc/selinux/config 12345678910111213# This file controls the state of SELinux on the# SELINUX= can take one of these three# enforcing SELinux security policy is enforced.# permissive SELinux prints warnings instead of enforcing.# disabled No SE Linux policy is loaded.SELINUX=disabled ### 这一行# SELINUXTYPE= can take one of these two# targeted Targeted processes are protected,# mls Multi Level Security protection.SELINUXTYPE=targeted'提示selinux 本次生效关闭必须重启，如果此时不想重启，可以临时生效之：'[atguigu@hadoop102 flume]$ sudo setenforce 0 启动 ganglia 123[atguigu@hadoop102 flume]$ sudo service httpd start[atguigu@hadoop102 flume]$ sudo service gmetad start[atguigu@hadoop102 flume]$ sudo service gmond start 打开网页浏览 ganglia 页面 http://192.168.9.102/ganglia 提示：如果完成以上操作依然出现权限不足错误，请修改 /var/lib/ganglia 目录的权限： [atguigu@hadoop102 flume]$ sudo chmod R 777 /var/lib/ganglia 操作 Flume测试监控 修改 /opt/module/flume/conf 目录下的 flume env.sh 配置： 1234JAVA_OPTS=\" Dflume.monitoring.type=ganglia-Dflume.monitoring.hosts= 192.168.9.102:8649-Xms100m-Xmx200m\" 启动 Flume 任务 1[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume netcat logger.conf -Dflume.root.logger==INFO,console -Dflume.monitoring.type=ganglia -Dflume.monitorin g.hosts= 192.168.9.102:8649 发送数据观察 ganglia 监测图 [atguigu@hadoop102 flume]$ nc localhost 44444 字段（图表名称） 字段含义 EventPut AttemptCount source 尝试写入 channel 的事件总数量 EventPutSuccessCount 成功写入 channel 且提交的事件总数量 EventTakeAttemptCount sink 尝试从 channel 拉取事件的总数量 E ventTakeSuccessCount sink sink 成功读取的事件的总数量 StartTime channel 启动的时间（毫秒） StopTime channel 停止的时间（毫秒） ChannelSize 目前 channel 中事件的总数量 ChannelFillPercentage channel 占用的百分比 ChannelCapacity channel 的容量","tags":[]},{"title":"Kafka00-安装","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka00-安装.html","text":"1.解压缩安装包 [atguigu@hadoop102 software]$ tar -zxvf kafka_2.11 0.11.0.0.tgz -C/opt/module 2.创建logs文件夹: kafka根目录下 3.修改配置文件 1234567891011121314151617181920212223242526vim /config/server.properties'broker的全局唯一编号，不能重复'broker.id=0'删除 topic 功能使能'delete.topic.enable=true'处理网络请求的线程数量'num.network.threads=3'用来处理磁盘 IO 的现成数量'num.io.threads=8'发送套接字的缓冲区大小'socket.send.buffer.bytes=102400'接收套接字的缓冲区大小'socket.receive.buffer.bytes=102400'请求套接字的缓冲区大小'socket.request.max.bytes=104857600'kafka 运行日志存放的路径'log.dirs=/opt/module/kafka/logs'topic 在当前 broker 上的分区个数'num.partitions=1'用来恢复和清理 data 下数据的线程数量'num.recovery.threads.per.data.dir=1'segment文件保留的最长时间，超时将被删除'log.retention.hours=168'配置连接 Zookeeper 集群地址'zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181 4.配置环境变量 KAFKA_HOMEexport KAFKA_HOME=/opt/module/kafka export PATH=\\$PATH:\\$KAFKA_HOME/bin 5.其他机器修改broker.id: broker.id不得重复 6.启动 [atguigu@hadoop104 kafka]$ bin/kafka-server-start.sh -daemonconfig/server.properties 7.关闭 [atguigu@hadoop102 kafka]$ bin/kafka-server-stop.sh stop","tags":[]},{"title":"Kafka01-概述&shell操作","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka01-概述&shell操作.html","text":"概述Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 消息队列MQ：异步处理 使用消息队列的好处： 解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 缓冲 有助于控制和优化数据流经过系统的速度解决生产消息和消费消息的处理速度不一致 的情况。 灵活性 &amp; 峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪 费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 消息队列的 两种模式: 点对点模式 (一对一 ，消费者主动拉取数据，消息收到后消息清除) 消息生产者生产消息发送到 Queue中 然后消息消费者从 Queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布 /订阅模式 (一对多 ，消费者消费数据之后不会清除消息) 消息生产者（发布）将消息发布到 topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic的消息会被所有订阅者消费。 Kafka基础架构 Producer ：消息生产者，就是向kafka broker 发消息的客户端； Consumer ：消息消费者，向kafka broker 取消息的客户端； Consumer Group （CG）：消费者组，由多个consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者； Broker ：一台kafka 服务器就是一个broker。一个集群由多个broker 组成。一个broker可以容纳多个topic； Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic； Partition：为了实现扩展性，一个非常大的topic 可以分布到多个broker（即服务器）上，一个topic 可以分为多个partition，每个partition 是一个有序的队列； Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka提供了副本机制，一个 topic的每个分区都有若干个副本，一个 leader和若干个 follower； leader 每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader； follower 每个分区多个副本中的“从”，实时从 leader中同步数据，保持和 leader数据的同步。 leader发生故障时，某个 follower会成为新的 follower。 shell操作1234567891011121314151617181920212223'查看当前服务器中的所有 topic'[atguigu@hadoop102 kafka]$ bin/kafka topics.sh --zookeeper hadoop102:2181 --list'创建 topic'bin/kafka topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first'删除 topic'[atguigu@hadoop102 kafka]$ bin/kafka topics.sh --zookeeper hadoop102:2181 --delete --topic first'需要 server.properties中设置 delete.topic.enable=true否则只是标记删除。''发送消息'[atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first'消费消息'[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first'查看某个Topic 的详情'[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first'修改分区数'[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6","tags":[]},{"title":"Kafka02-结构原理","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka02-架构原理.html","text":"Kafka工作流程与文件存储 Kafka 中消息是以topic 进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。 topic 是逻辑上的概念，而partition 是物理上的概念，每个partition 对应于一个log 文件，该log 文件中存储的就是producer 生产的数据。Producer 生产的数据会被不断追加到该log 文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。 由于生产者生产的消息会不断追加到log 文件末尾，为防止log 文件过大导致数据定位效率低下，Kafka 采取了分片和索引机制，将每个partition 分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称+分区序号。例如，first 这个topic 有三个分区，则其对应的文件夹为first-0,first-1,first-2。 12345600000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.log index 和log 文件以当前segment 的第一条消息的offset 命名。下图为index 文件和log文件的结构示意图。 “.index”文件存储大量的 索引信息 ，“.log”文件存储大量的数据 ，索引文件中的元数据指向对应数据文件中 message的物理偏移地址 生产者分区策略 分区的原因 方便在集群中扩展 ，每个 Partition可以通过调整以适应它所在的机器，而一个 topic又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了； 可以提高并发 ，因为可以以 Partition为单位读写了。 分区的原则 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 数据可靠性保证为保证 producer 发送的数据，能可靠的发送到指定的 topic, topic 的每个 partition 收到producer发送的数据后， 都 需要向 producer 发送 ack (acknowledgement确认收到)，如果producer收到 ack 就会进行下一轮的发送，否 则重新发送数据。 副本数据同步策略 方案 优点 缺点 半数以上完成同步，就发送ack 延迟低 选举新的leader 时，容忍n 台节点的故障，需要2n+1 个副本 全部完成同步，才发送ack 选举新的leader 时，容忍n 台节点的故障，需要n+1 个副本 延迟高 Kafka 选择了第二种方案，原因如下： 同样为了容忍n 台节点的故障，第一种方案需要2n+1 个副本，而第二种方案只需要n+1个副本，而Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。 虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka 的影响较小。 ISR采用第二种方案之后，设想以下情景：leader 收到数据，所有follower 都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader 进行同步，那leader 就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？ Leader 维护了一个动态的in-sync replica set (ISR)，意为和leader 保持同步的follower 集合。当ISR 中的follower 完成数据的同步之后，leader 就会给follower 发送ack。如果follower 长时间未向leader 同步数据， 则该follower 将被踢出ISR ， 该时间阈值由replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从ISR 中选举新的leader。 ack 应答机制对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR 中的follower 全部接收成功。所以Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。 acks=0：producer 不等待broker 的ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当broker 故障时有可能丢失数据； aks=1：producer 等待broker 的ack，partition 的leader 落盘成功后返回ack，如果在follower同步成功之前leader 故障，那么将会丢失数据; acks=-1（all）：producer 等待broker 的ack，partition 的leader 和follower 全部落盘成功后才返回ack。但是如果在follower 同步完成后，broker 发送ack 之前，leader 发生故障，那么会造成数据重复。 故障处理细节LEO：指的是每个副本最大的offset； HW：指的是消费者能见到的最大的offset，ISR 队列中最小的LEO。 follower 故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入ISR 了。 leader 故障 leader 发生故障之后，会从ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性, 其余的 follower会先将各自的 log文件 高于 HW的部分截掉 ，然后从新的 leader同步数据。 注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 Exactly Once语义将服务器的ACK级别设置为 -1，可以保证 Producer到 Server之间不会丢失数据，即 At Least Once语义 。相对的，将服务器 ACK级别设置为 0，可以保证生产者每条消息只会被发送一次，即 At Most Once语义。 At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的， At Least Once可以保证数据不重复，但是不能保证数据不丢失。 但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once语义。 在 0.11版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 0.11版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer不论向 Server发送多少次重复数据， Server端都只会持久化一条。幂等性结合 At Least Once语义，就构成了 Kafka的 Exactly Once语义。 \\text{At Least Once} + \\text{幂等性} = \\text{Exactly Once}要启用幂等性，只需要将Producer的参数中 enable.idompotence设置为 true即可。 Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer在初始化的时候会被分配一个 PID，发往同一 Partition的消息会附带 Sequence Number。而Broker端会对 做缓存，当具有相同主键的消息提交时， Broker只会持久化一条。 但是PID重启就会变化，同时不同的 Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。 消费者消费方式consumer采用 pull（拉模式从 broker中读取数据）。 push（推模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker决定的）。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer来不及处理消息， 典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。pull 模式不足之处是，如果kafka 没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer 会等待一段时间之后再返回，这段时长即为timeout。 分区分配策略一个consumer group 中有多个consumer，一个 topic 有多个partition，所以必然会涉及到partition 的分配问题，即确定那个partition 由哪个consumer 来消费。Kafka 有两种分配策略，一是RoundRobin，一是Range。 RoundRobin： Range： offset 的维护由于consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以consumer 需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。 Kafka 0.9 版本之前，consumer 默认将offset 保存在Zookeeper 中，从0.9 版本开始，consumer 默认将offset 保存在Kafka 一个内置的topic 中，该topic 为__consumer_offsets。 修改配置文件consumer.properties exclude.internal.topics=false 读取offset 0.11.0.0 之前版本: bin/kafka-console-consumer.sh —topic __consumer_offsets—zookeeper hadoop102 :2181 —formatter“kafka.coordinator.GroupMetadataManager \\\\$OffsetsMessageFormatter” —consumer.config config/consumer.properties —from-beginning 0.11.0.0之后版 本 (含 ): bin/kafka console consumer.sh —topic __consumer_offsets—zookeeper hadoop102 :2181 —formatter“kafka.coordinator.group.GroupMetadataManager \\\\$OffsetsMessageFormatter” —consumer.config config/consumer.properties —from-beginning 高效写数据顺序写磁盘： Kafka的 producer生产数据，要写入到 log文件中，写的过程是一直追加到文件末端，为顺序写 。 官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与 磁盘的机械机构有关，顺序写之所以快，是因为其省去了 大量 磁头寻址的时间 。 零复制技术： zk的作用Kafka 集群中有一个broker 会被选举为Controller，负责管理集群broker 的上下线，所有topic 的分区副本分配和leader 选举等工作。 Controller 的管理工作都是依赖于Zookeeper 的。 以下为partition 的leader 选举过程： Kafka事务Kafka 从0.11 版本开始引入了事务支持。事务可以保证Kafka 在Exactly Once 语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 Producer事务 为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将 Producer获得的 PID和 Transaction ID绑定。这样当 Producer重启后就可以通过正在进行的 Transaction ID获得原来的 PID。 为了管理Transaction Kafka引入了一个新的组件 Transaction Coordinator。 Producer就是通过和 Transaction Coordinator交互获得 Transaction ID对应的任务状态。 Transaction Coordinator还负责将事务所有写入 Kafka的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 Consumer事务 上述事务机制主要是从Producer方面考虑，对于 Consumer而言，事务的保证就会相对较弱，尤其时无法保证 Commit的信息被精确消费。这是由于 Consumer可以通过 offset访问任意信息，而且不同的 Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。","tags":[]},{"title":"Kafka01-概述&shell操作","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka04-监控.html","text":"概述Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 消息队列MQ：异步处理 使用消息队列的好处： 解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 缓冲 有助于控制和优化数据流经过系统的速度解决生产消息和消费消息的处理速度不一致 的情况。 灵活性 &amp; 峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪 费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 消息队列的 两种模式: 点对点模式 (一对一 ，消费者主动拉取数据，消息收到后消息清除) 消息生产者生产消息发送到 Queue中 然后消息消费者从 Queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布 /订阅模式 (一对多 ，消费者消费数据之后不会清除消息) 消息生产者（发布）将消息发布到 topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic的消息会被所有订阅者消费。 Kafka基础架构 Producer ：消息生产者，就是向kafka broker 发消息的客户端； Consumer ：消息消费者，向kafka broker 取消息的客户端； Consumer Group （CG）：消费者组，由多个consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者； Broker ：一台kafka 服务器就是一个broker。一个集群由多个broker 组成。一个broker可以容纳多个topic； Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic； Partition：为了实现扩展性，一个非常大的topic 可以分布到多个broker（即服务器）上，一个topic 可以分为多个partition，每个partition 是一个有序的队列； Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka提供了副本机制，一个 topic的每个分区都有若干个副本，一个 leader和若干个 follower； leader 每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader； follower 每个分区多个副本中的“从”，实时从 leader中同步数据，保持和 leader数据的同步。 leader发生故障时，某个 follower会成为新的 follower。 shell操作1234567891011121314151617181920212223'查看当前服务器中的所有 topic'[atguigu@hadoop102 kafka]$ bin/kafka topics.sh --zookeeper hadoop102:2181 --list'创建 topic'bin/kafka topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first'删除 topic'[atguigu@hadoop102 kafka]$ bin/kafka topics.sh --zookeeper hadoop102:2181 --delete --topic first'需要 server.properties中设置 delete.topic.enable=true否则只是标记删除。''发送消息'[atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first'消费消息'[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first'查看某个Topic 的详情'[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first'修改分区数'[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6","tags":[]},{"title":"Kafka03-API","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka03-API.html","text":"Producer API消息发送流程Kafka的 Producer发送消息采用的是 异步发送 的方式。在消息发送的过程中，涉及到了两个线程 main线程和 Sender线程 ，以及 一个线程共享变量 RecordAccumulator。main线程将消息发送给 RecordAccumulator Sender线程不断从 RecordAccumulator中拉取消息发送到 Kafka broker。 batch.size：只有数据积累到batch.size 之后，sender 才会发送数据。 linger.ms：如果数据迟迟未达到batch.size，sender 等待linger.time 之后就会发送数据。 异步发送API导入依赖: 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt;&lt;/dependency&gt; 编写代码: KafkaProducer类：需要创建一个生产者对象，用来发送数据 ProducerConfig类：获取所需的一系列配置参数 ProducerRecord类：每条数据都要封装成一个ProducerRecord 对象 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465'不带回调函数的API:'package com.atguigu.kafka;import org.apache.kafka.clients.producer.*;import java.util.Properties;import java.util.concurrent.ExecutionException;public class CustomProducer &#123; public static void main(String[] args) throws ExecutionException,InterruptedException &#123; Properties props = new Properties(); //kafka 集群， broker list props.put(\"bootstrap.servers\", \"hadoop102:9092\") props.put(\"acks\", \"all\"); //重试次数 props.put(\"retries\", 1); //批次大小 props.put(\"batch.size\", 16384); //等待时间 props.put(\"linger.ms\", 1); //RecordAccumulator 缓冲区大小 props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer &lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) &#123; producer.send(new ProducerRecord&lt;String, String&gt;(\"first\", Integer.toString(i), Integer.toString(i))); &#125; producer.close(); &#125;&#125;'带回调函数的 API'\"回调函数会在 producer收到 ack时调用，为异步调用， 该方法有两个参数，分别是RecordMetadata 和 Exception ，如果 Exception 为 null ，说明消息发送成功，如果Exception 不为 null ，说明消息发送失败。\"\"注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。\"package com.atguigu.kafka;import org.apache.kafka.clients.producer.*;import java.util.Properties;import java.util.concurrent.ExecutionException;public class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop102:9092\");//kafka 集群， broker list props.put(\"acks\", \"all\"); props.put(\"retries\", 1);// 重试次数 props.put(\"batch.size\", 16384);// 批次大小 props.put(\"linger.ms\", 1);// 等待时间 props.put(\"buffer.memory\", 33554432);//RecordAccumulator 缓冲区大小 props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) &#123; producer.send(new ProducerRecord&lt;String, String&gt;(\"first\", Integer.toString(i), Integer.toString(i)), new Callback() &#123; // 回调函数， 该方法会在 Producer 收到 ack 时调用，为异步调用 @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if (exception == null) &#123; System.out.println(\"s uccess --&gt;\" + metadata.offset()); &#125; else&#123; exception.printStackTrace(); &#125; &#125; &#125;); &#125; producer.close(); &#125;&#125; 同步发送 API同步发送的意思就是，一条消息发送之后，会阻塞当前线程， 直至返回 ack。 由于send方法返回的是一个 Future对象，根据 Futrue对象 的特点，我们也可以实现 同步发送的效果 ，只需在调用 Future对象的 get方发即可。 123456789101112131415161718192021222324package com.atguigu.kafka;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;import java.util.concurrent.ExecutionException;public class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop102:9092\");//kafka 集群， broker list props.put(\"acks\", \"all\"); props.put(\"retries\", 1);// 重试次数 props.put(\"batch.size\", 16384);// 批次大小 props.put(\"linger.ms\", 1);// 等待时间 props.put(\"buffer.memory\", 33554432);//RecordAccumulator 缓冲区大小 props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) &#123; producer.send(new ProducerRecord&lt;String, String&gt;(\"first\", Integer.toString(i), Integer.toString(i))).get();// .get() &#125; &#125;&#125; Consumer APIConsumer消费数据时的可靠性是很容易保证的，因为数据在 Kafka中是持久化的，故不用担心数据丢失问题。 由于consumer在消费过程中可能会出现断电宕机等故障， consumer恢复后，需要从故障前的位置的继续消费，所以 consumer需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。 所以offset的维护是 Consumer消费数据是必须考虑的问题。 自动提交 offset导入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt;&lt;/dependency&gt; KafkaConsumer类： 需要创建一个 消费 者对象，用来 消费 数据 ConsumerConfig类： 获取所需的一系列配置参数 ConsuemrRecord类： 每条数据都要封装成一个 ConsumerRecord对象 enable.auto.commit 是否开启自动提交 offset功能 auto.commit.interval.ms 自动提交 offset的时间间隔 123456789101112131415161718192021222324252627//以下为自动提交offset的代码package com.atguigu.kafka;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;public class Custom Consumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"true\"); props.put(\"auto.commit.interval.ms\", \"1000\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerR ecord&lt;String, String&gt; record : records)&#123; System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); &#125; &#125; &#125;&#125; 手动提交 offset虽然自动提交offset十分简介便利，但由于其是基于时间提交的 开发人员难以把握offset提交的时机。因此 Kafka还提供了手动提交 offset的 API。 手动提交offset的方法有两种：分别是 commitSync（同步提交） 和 commitAsync（异步提交） 。两者的相同点是，都会将本次 poll的一批数据最高的偏移量提交 ；不同点是commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（ 由不可控因素导致，也会出现提交失败 ）；而 commitAsync则没有失败重试机制，故有可能提交失败。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// 由于同步提交 offset有失败重试机制，故更加可靠 ，以下为同步提交 offset的示例。package com.atguigu.kafka.consumer;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;public class CustomC omsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); //Kafka 集群 props.put(\"bootstrap.servers\", \"hadoop102:9092\"); // 消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"false\");// 关闭自动提交 offset props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDecerializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDecerializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\"));// 消费者订阅主题 while (true) &#123; //消费者拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(\"offset = %d, key = %s, value= %s%n\", record.offset(), record.key(), value()); &#125; // 同步提交，当前线程会阻塞 直到 offset 提交成功 consumer.commitSync(); &#125; &#125;&#125;// 异步提交 offset// 虽然同步提交 offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此 吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交 offset的方式。package com.atguigu.kafka.consumer;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;public class CustomC omsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); //Kafka 集群 props.put(\"bootstrap.servers\", \"hadoop102:9092\"); // 消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"false\");// 关闭自动提交 offset props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDecerializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDecerializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\"));// 消费者订阅主题 while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);// 消费者拉取数据 for (ConsumerRecord&lt;String, String&gt; record : records)&#123; System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); &#125; //异步提交 consumer.commitAsync(new OffsetCommitCallback()&#123; @Override public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123; if (exception != null) &#123; System.err.println(\"Commit failed for\" + offsets); &#125; &#125; &#125;); &#125; &#125;&#125; 数据漏消费和重复消费分析: 无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交 offset后消费，有可能造成数据的漏消费；而先消费后提交 offset，有可能会造成数据的重复消费。 自定义存储 offsetKafka 0.9版本之前， offset存储在 zookeeper 0.9版本 及 之后，默认将 offset存储在 Kafka的一个内置的 topic中。除此之外， Kafka还可以选择自定义存储 offset。 offset的维护是相当繁琐的， 因为需要考虑到 消费者的 Rebalace。当有新的消费者加入消费者组、已有的消费者推出消费者组 或者所订阅的主题的分区发生变化 ，就会触发到分区的重新分配，重新分配的过程叫做 Rebalance。 消费者发生Rebalance之后，每个消费者消费的分区就会发生变 化。 因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的offset位置继续消费。 要实现自定义存储offset，需要借助 ConsumerRebalanceListener 以 下为 示例代码 ，其中提交和获取 offset的方法，需要根据所选的 offset存储系统自行实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.atguigu.kafka.consumer;import org.apache.kafka.clients.consumer.*;import org.apache.kafka.common.TopicPartition;import java.util.*;public class CustomCo nsumer &#123; private static Map&lt;TopicPartition, Long&gt; currentOffset = new HashMap&lt;&gt;(); public static void main(String[] args) &#123; //创建配置信息 Properties props = new Properties(); //Kafka 集群 props.put(\"bootstrap.servers\", \"hadoop102:9092\"); //消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(\"group.id\", \"test\"); //关闭自动提交 offset props.put(\"enable.auto.commit\", \"false\"); //Key 和 Value 的反序列化类 props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserilizer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserilizer\"); // 创建一个消费者 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // 消费者订阅 主题 consumer.subscribe(Arrays.asList(\"first\"), new ConsumerRebalanceListener() &#123; // 该方法会在 Rebalance 之前调用 @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; commitOffset(currentOffset); &#125; // 该方法会在 Rebalance 之后调用 @Override public void onPartitionsAssigned(Collection&lt; TopicPartition&gt; partitions) &#123; currentOffset.clear(); for (TopicPartition partition : partitions) &#123; consumer.seek(partition, getOffset(partition));//定位到最近提交的 offset 位置继续消费 &#125; &#125; &#125;); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);// 消费者拉取数据 for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(\"offset = %d, key = %s, value = %s%,\" record.offset(), record.key(), record.value()); currentOffset.put(new TopicPartition(record.topic(), record.partition()), record.offset()); &#125; commitOffset(currentOffset);// 异步提交 &#125; &#125; // 获取某分区的最新 offset private static long getOffset(TopicPartition partition) &#123; return 0; &#125; // 提交该消费者所有分区的 offset private static void commitOffset(Map&lt;TopicPartition, Long&gt; currentOffset) &#123;&#125;&#125; 自定义 Interceptor拦截器原理Producer拦截器 (interceptor)是在 Kafka 0.10版本被引入的，主要用于实现 clients端的定制化控制逻辑。 对于producer而言， interceptor使得用户在消息发送前以及 producer回调逻辑前有机会对消息做一些定制化需求，比如 修改消息 等。同时， producer允许用户指定多个 interceptor按序作用于同一条消息从而形成一个拦截链 (interceptor chain)。 Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其 定义的方法包括： configure(configs) 获取配置 信息 和 初始化数据时调用 onSend(ProducerRecord) 该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。 Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的 topic和分区， 否则会影响目标分区的计算 。 onAcknowledgement(RecordMetadata,Exception) 该方法会在消息从 RecordAccumulator成功 发送到 Kafka Broker之后，或者在发送过程中失败时调用。 并且通常都是在 producer回调逻辑触发之前。 onAcknowledgement运行在producer的 IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢 producer的消息发送效率 close 关闭interceptor，主要用于执行一些资源清理工作 如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外 倘若指定了多个 interceptor，则 producer将按照指定顺序调用它们 ，并仅仅是捕获每个 interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。 拦截器案例实现一个简单的双interceptor组成的拦截链。第一个 interceptor会在消息发送前将时间戳信息加到消息 value的最前部；第二个 interceptor会在消息发送后更新成功发送消息数或失败发送消息数。 增加时间戳拦截器 12345678910111213141516171819package com.atguigu.kafka.interceptor;import java.util.Map;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apac he.kafka.clients.producer.RecordMetadata;public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123; @Override public void configure(Map&lt;String, ?&gt; configs) &#123;&#125; @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; // 创建一个新的 record ，把时间戳写入消息体的最前部 return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(), System.currentTimeMillis() + \",\" + record.value().toString()); &#125; @Over ride public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;&#125; @Override public void close() &#123;&#125;&#125; 统计发送消息成功和发送失败消息数 ，并在 producer关闭时打印这两个计数器 12345678910111213141516171819202122232425262728293031package com.atguigu.kafka.interceptor;import java.util.Map;import org.apache.kafka. clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;&#123; private int errorCounter = 0; private int successCounter = 0; @Override public void configure(Map&lt;String, ?&gt; configs) &#123;&#125; @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; return record; &#125; @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; // 统计成功和失败的次数 if (exception == null) &#123; successCounter++; &#125; else&#123; errorCounter++; &#125; &#125; @Override public void cl ose() &#123; // 保存结果 System.out.println(\"Successful sent: \" + successCounter); System.out.println(\"Failed sent: \" + errorCounter); &#125;&#125; producer主程序 1234567891011121314151617181920212223242526272829303132333435363738package com.atguigu.kafka.interceptor;import java.util.ArrayList;import java.util.List;import java.util.Properties;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.Produc erRecord;public class InterceptorProducer &#123; public static void main(String[] args) throws Exception &#123; // 1 设置配置信息 Properties props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); props.put(\"retr ies\", 3); props.put(\"batch.size\", 16384); props.put(\"linger.ms\", 1); props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 2 构建拦截链 List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add(\"com.atguigu.kafka.interceptor.TimeInterceptor\"); interceptors.add(\"com.atguigu.kafka.interceptor.CounterInterce ptor\"); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); String topic = \"first\"; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); // 3 发送消息 for (int i = 0; i &lt; 10; i++) &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, \"message\" + i); producer.send(record); &#125; // 4 一定要关闭 producer ，这样才会调用 interceptor 的 close 方法 producer.close(); &#125;&#125; 测试 在 kafka上启动消费者 然后运行 客户端 java程序。 [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh—bootstrap-server hadoop102: 9092 —from-beginning —topicfirst 1501904047034,message0 1501904047225,message1 1501904047230,message2 1501904047234,message3 1501904047236,message4 1501904047240,message5 1501904047243,message6 1501904047246,message7 1501904047249,message8 1501904047252,mes sage 9","tags":[]},{"title":"Kafka05-Flume对接Kafka","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka05-Flume对接Kafka.html","text":"配置 flume(flume-kafka.conf) 1234567891011121314151617181920212223242526'define'a1.sources = r1a1.sinks = k1a1.channels = c1' source'a1.sources.r1.type = execa1.sources.r1.command = tail -F -c +0 /opt/module/data/flume.loga1.sources.r1.shell = /bin/bash -c'sink'a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:90 92,hadoop104:9092a1.sinks.k1.kafka.topic = firsta1.sinks.k1.kafka.flumeBatchSize = 20a1.sinks.k1.kafka.producer.acks = 1a1.sinks.k1.kafka.producer.linger.ms = 1'channel'a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100'bind'a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 flume $ bin/flume-ng agent -c conf/ -n a1 -f jobs/flume kafka.conf 向 /opt/module/data/flume.log里追加数据，查看 kafka消费者消费情况 $ echo hello &gt;&gt;&gt; /opt/module/data/flume.log","tags":[]},{"title":"Kafka06-Kafka面试题","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka06-面试题.html","text":"Kafka中的 ISR(InSyncRepli)、 OSR(OutSyncRepli)、 AR(AllRepli)代表什么？ ISR：与 leader保持同步的follower集合 AR：分区的所有副本 Kafka中的 HW、 LEO等分别代表什么？ LEO：没个副本的最后条消息的offset HW：一个分区中所有副本最小的offset Kafka中是怎么体现消息顺序性的？ 每个分区内，每条消息都有一个offset，故只能保证分区内有序。 Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？ 拦截器 -&gt; 序列化器 -&gt; 分区器 Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？ “消费组中的消费者个数如果超过 topic的分区，那么就会有消费者消费不到数据”这句话是否正确？ 正确 消费者提交消费位移时提交的是当前消费到的最新消息的 offset还是 offset+1 offset+1 有哪些情形会造成重复消费？ 那些情景会造成消息漏消费？ 先提交offset，后消费，有可能造成数据的重复 当你使用kafka-topics.sh 创建（删除）了一个topic 之后，Kafka 背后会执行什么逻辑？ 1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点,如：/brokers/topics/first 2）触发Controller的监听程序 3）kafka Controller 负责topic的创建工作，并更新metadata cache topic 的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？ 可以增加 bin/kafka-topics.sh —zookeeper localhost:2181/kafka —alter —topic topic-config —partitions 3 topic 的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？ 不可以减少，现有的分区数据难以处理。 Kafka 有内部的topic 吗？如果有是什么？有什么所用？ __consumer_offsets,保存消费者offset Kafka 分区分配的概念？ 一个topic多个分区，一个消费者组多个消费者，故需要将分区分配个消费者(roundrobin、range) 简述Kafka 的日志目录结构？ 每个分区对应一个文件夹，文件夹的命名为topic-0，topic-1，内部为.log和.index文件 如果我指定了一个offset，Kafka Controller 怎么查找到对应的消息？ 聊一聊Kafka Controller 的作用？ 负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作 Kafka 中有那些地方需要选举？这些地方的选举策略又有哪些？ partition leader（ISR），controller（先到先得） 失效副本是指什么？有那些应对措施？ 不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入 Kafka 的哪些设计让它有如此高的性能？ 分区，顺序写磁盘，0-copy","tags":[]},{"title":"HBase00-安装","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase00-安装.html","text":"启动Zookeeper 启动Hadoop：hdfs和yarn HBase的解压 tar -zxvf HBase-1.3.1-bin.tar.gz -C /opt/module 修改HBase的配置文件 HBase-env.sh export JAVA_HOME=/opt/module/jdk1.6.0_144 export HBASE_MANAGES_ZK=false HBase-site.xml 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181, hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.10/zkData&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers: hadoop102 hadoop103 hadoop104 软连接hadoop配置文件到HBase 12[atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/HBase/conf/core-site.xml[atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/HBase/conf/hdfs-site.xml 分发HBase到其他节点 HBase服务的启动 启动方式1 12bin/HBase-daemon.sh start masterbin/HBase-daemon.sh start regionserver 提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。 修复提示： a、同步时间服务 b、属性：hbase.master.maxclockskew设置更大的值 12345&lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;Time difference of regionserver from master&lt;/description&gt; &lt;/property&gt; 启动方式2 bin/start-HBase.sh bin/stop-HBase.sh 查看HBase页面 启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：http://hadoop102:16010","tags":[]},{"title":"HBase01-概述","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase01-概述.html","text":"NoSQL简介关系型数据库的查询瓶颈: 当用户表的数据达到几千万甚至几亿级别的时候，对单条数据的检索将花费数秒甚至达到分钟级别。实际情况更复杂，查询的操作速度将会受到以下两个因素的影响： ①高并发的更新(插入、修改、删除)操作。大中型网站的并发操作一般能达到几十乃至几百并发，此时单条数据查询的延时将轻而易举地达到分钟级别。 ②多表关联后的复杂查询，以及频繁的group by或者order by操作，此时，性能下降较为明显。 分摊读写压力的有效方式是将单个关系型数据库扩展为分布式数据库。但是，随之而来的问题则是很难保证原子性。没有了原子性，事务也无从谈起，关系型数据库也就没有了存在的意义。 CAP定理: 20世纪90年代初期Berkerly大学有位Eric Brewer教授提出了一个CAP理论。全称是Consistency Availability and Partition tolerance。 Consistency（强一致性）：数据更新操作的一致性，所有数据变动都是同步的。 Availability（高可用性）：良好的响应性能。 Partition tolerance（高分区容错性）：可靠性。 Brewer教授给出的定理是：任何分布式系统只可同时满足二点，没法三者兼顾。 Brewer教授给出的忠告是：架构师不要将精力浪费在如何设计能满足三者的完美分布式系统，而是应该进行取舍。所以专家们始终没有办法构建出一个既有完美原子性又兼具高性能的分布式数据库。 NoSQL: 些数据库在实现性能的同时会牺牲一部分一致性，即数据在更新时，不会立刻同步，而是经过了一段时间才达到一致性。这个特性也称之为最终一致性！例如你发了一条朋友圈，你的一部分朋友立马看到了这条信息，而另一部分朋友可能要等到1分钟之后才能刷出这条消息。虽然有延时，但是对于这样一个社交的场景，这个延时是可以容忍的。而如果使用传统关系型数据库，可能这些即时通信软件就早已崩溃！ NoSQL数据库最初指不使用SQL标准的数据库，现在泛指非关系型数据库。NoSQL一词最早出现于1998年，是Carlo Strozzi开发的一个轻量、开源、不提供SQL功能的数据库。 现在NoSQL被普遍理解理解为“Not Only SQL”，意为不仅仅是SQL。NoSQL和传统的关系型数据库在很多场景下是相辅相成的，谁也不能完全替代谁。 HBase简介2006年Google技术人员Fay Chang发布了一篇文章Bigtable: ADistributed Storage System for Structured Data。该文章向世人介绍了一种分布式的数据库，这种数据库可以在局部几台服务器崩溃的情况下继续提供高性能的服务。 HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。 Hbase面向列存储，构建于Hadoop之上，类似于Google的BigTable，提供对10亿级别表数据的快速随机实时读写！ HBase的特点 海量存储 HBase适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到百毫秒内返回数据。这与HBase的极易扩展性息息相关。正式因为HBase良好的扩展性，才为海量数据的存储提供了便利。 列式存储 这里的列式存储其实说的是列族存储，HBase是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。 极易扩展 HBase的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。 通过横向添加RegionSever的机器，进行水平扩展，提升HBase上层的处理能力，提升Hbsae服务更多Region的能力。 高并发 由于目前大部分使用HBase的架构，都是采用的廉价PC，因此单个IO的延迟其实并不小，一般在几十到上百ms之间。这里说的高并发，主要是在并发的情况下，HBase的单个IO延迟下降并不多。能获得高并发、低延迟的服务。 稀疏 稀疏主要是针对HBase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。 Hbase的优缺点优点： ① HDFS有高容错，高扩展的特点，而Hbase基于HDFS实现数据的存储，因此Hbase拥有与生俱来的超强的扩展性和吞吐量。 ② HBase采用的是Key/Value的存储方式，这意味着，即便面临海量数据的增长，也几乎不会导致查询性能下降。 ③ HBase是一个列式数据库，相对于于传统的行式数据库而言。当你的单张表字段很多的时候，可以将相同的列(以regin为单位)存在到不同的服务实例上，分散负载压力。 缺点： ① 架构设计复杂，且使用HDFS作为分布式存储，因此只是存储少量数据，它也不会很快。在大数据量时，它慢的不会很明显！ ② Hbase不支持表的关联操作，因此数据分析是HBase的弱项。常见的 group by或order by只能通过编写MapReduce来实现！ ③ Hbase部分支持了ACID 适合场景：单表超千万，上亿，且高并发！ 不适合场景：主要需求是数据分析，比如做报表。数据量规模不大，对实时性要求高！ HBase数据模型逻辑结构逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构（K-V）来看，HBase更像是一个multi-dimensional map。 物理存储结构 数据模型 Name Space 命名空间，类似于关系型数据库的database概念，每个命名空间下有多个表。HBase两个自带的命名空间，分别是 hbase和 default，hbase中存放的是 HBase内置的表，default表是用户默认使用的命名空间。 一个表可以自由选择是否有命名空间，如果创建表的时候加上了命名空间后，这个表名字以&lt;Namespace&gt;:&lt;Table&gt;作为区分！ Table 类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，数据属性，比如超时时间（TTL），压缩算法（COMPRESSION）等，都在列族的定义中定义，不需要声明具体的列。 这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。 Row HBase表中的每行数据都由一个RowKey和多个Column（列）组成。一个行包含了多个列，这些列通过列族来分类,行中的数据所属列族只能从该表所定义的列族中选取,不能定义这个表中不存在的列族，否则报错NoSuchColumnFamilyException。 RowKey Rowkey由用户指定的一串不重复的字符串定义，是一行的唯一标识！数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。 如果使用了之前已经定义的RowKey，那么会将之前的数据更新掉！ Column Family 列族是多个列的集合。一个列族可以动态地灵活定义多个列。表的相关属性大部分都定义在列族上，同一个表里的不同列族可以有完全不同的属性配置，但是同一个列族内的所有列都会有相同的属性。 列族存在的意义是HBase会把相同列族的列尽量放在同一台机器上，所以说，如果想让某几个列被放到一起，你就给他们定义相同的列族。 官方建议一张表的列族定义的越少越好，列族太多会极大程度地降低数据库性能，且目前版本Hbase的架构，容易出BUG。 Column Qualifier Hbase中的列是可以随意定义的，一个行中的列不限名字、不限数量，只限定列族。因此列必须依赖于列族存在！列的名称前必须带着其所属的列族！例如info：name，info：age。 因为HBase中的列全部都是灵活的，可以随便定义的，因此创建表的时候并不需要指定列！列只有在你插入第一条数据的时候才会生成。其他行有没有当前行相同的列是不确定，只有在扫描数据的时候才能得知！ TimeStamp 用于标识数据的不同版本（version）。时间戳默认由系统指定，也可以由用户显式指定。 在读取单元格的数据时，版本号可以省略，如果不指定，Hbase默认会获取最后一个版本的数据返回！ Cell 一个列中可以存储多个版本的数据。而每个版本就称为一个单元格（Cell）。 Cell由{rowkey, column Family：column Qualifier, time Stamp}确定。 Cell中的数据是没有类型的，全部是字节码形式存贮。 Region Region由一个表的若干行组成！在Region中行的排序按照行键（rowkey）字典排序。 Region不能跨RegionSever，且当数据量大的时候，HBase会拆分Region。 Region由RegionServer进程管理。HBase在进行负载均衡的时候，一个Region有可能会从当前RegionServer移动到其他RegionServer上。 Region是基于HDFS的，它的所有数据存取操作都是调用了HDFS的客户端接口来实现的。 HBase基本架构 Region Server RegionServer是一个服务，负责多个Region的管理。其实现类为HRegionServer，主要作用如下: 对于数据的操作：get, put, delete； 对于Region的操作：splitRegion、compactRegion。 客户端从ZooKeeper获取RegionServer的地址，从而调用相应的服务，获取数据。 Master Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下： 对于表的操作：create, delete, alter，这些操作可能需要跨多个ReginServer，因此需要Master来进行协调！ 对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。 即使Master进程宕机，集群依然可以执行数据的读写，只是不能进行表的创建和修改等操作！当然Master也不能宕机太久，有很多必要的操作，比如创建表、修改列族配置，以及更重要的分割和合并都需要它的操作。 Zookeeper RegionServer非常依赖ZooKeeper服务，ZooKeeper管理了HBase所有RegionServer的信息，包括具体的数据段存放在哪个RegionServer上。 客户端每次与HBase连接，其实都是先与ZooKeeper通信，查询出哪个RegionServer需要连接，然后再连接RegionServer。Zookeeper中记录了读取数据所需要的元数据表 hbase:meata,因此关闭Zookeeper后，客户端是无法实现读操作的！ HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。 HDFS HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。","tags":[]},{"title":"HBase02-HBase-shell操作","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase02-HBase-shell操作.html","text":"使用hbase shell可以进入一个shell命令行界面：bin/HBase shell 集群的操作12345678910111213141516171819202122232425262728293031'查看集群状态'使用status可以查看集群状态，默认为summary，可以选择‘simple’和‘detailed’来查看详情hbase(main):011:0&gt; status1 active master, 0 backup masters, 3 servers, 0 dead, 0.6667 average load'查看版本'hbase(main):002:0&gt; version1.3.1, r930b9a55528fe45d8edce7af42fef2d35e77677a, Thu Apr 6 19:36:54 PDT 2017'查看操作用户及组信息'hbase(main):003:0&gt; whoamiatguigu (auth:SIMPLE)groups: atguigu'查看表操作信息'hbase(main):004:0&gt; table_helpHelp for table-reference commands.'查看帮助信息'hbase(main):005:0&gt; helpHBase Shell, version 1.3.1, r930b9a55528fe45d8edce7af42fef2d35e77677a, Thu Apr 6 19:36:54 PDT 2017Type 'help \"COMMAND\"', (e.g. 'help \"get\"' -- the quotes are necessary) for help on a specific command.Commands are grouped. Type 'help \"COMMAND_GROUP\"', (e.g. 'help \"general\"') for help on a command group.'查看具体命令的帮助''注意引号是必须的！'hbase(main):006:0&gt; help 'get'Get row or cell contents; pass table name, row, and optionallya dictionary of column(s), timestamp, timerange and versions. Examples: hbase&gt; get 'ns1:t1', 'r1' 表的操作 list 1234hbase(main):008:0&gt; listTABLE0 row(s) in 0.0410 seconds'list后可以使用*等通配符来进行表的过滤！' create 创建表时，需要指定表名和列族名，而且至少需要指定一个列族，没有列族的表是没有任何意义的。 创建表时，还可以指定表的属性，表的属性需要指定在列族上！ 格式： ​ create ‘表名’, { NAME =&gt; ‘列族名1’, 属性名 =&gt; 属性值}, {NAME =&gt; ‘列族名2’, 属性名 =&gt; 属性值}, … 如果你只需要创建列族，而不需要定义列族属性，那么可以采用以下快捷写法： ​ create’表名’,’列族名1’ ,’列族名2’, … 1HBase(main):002:0&gt; create 'student','info' desc 12hbase(main):003:0&gt; describe 'person'hbase(main):004:0&gt; desc 'person' disable 停用表后，可以防止在对表做一些维护时，客户端依然可以持续写入数据到表。一般在删除表前，必须停用表。 在对表中的列族进行修改时，也需要停用表. disable_all ‘正则表达式’ 可以使用正则来匹配表名。 is_disabled 可以用来判断表是否被停用。 12hbase(main):005:0&gt; disable 'person'0 row(s) in 2.4250 seconds enable 和停用表类似。enable ‘表名’用来启用表，is_enabled ‘表名’用来判断一个表是否被启用。 enable_all ‘正则表达式’可以通过正则来过滤表，启用复合条件的表。 exists 123hbase(main):008:0&gt; exists 'person'Table person does exist0 row(s) in 0.0210 seconds count 123hbase(main):012:0&gt; count 'person'1 row(s) in 0.0240 seconds=&gt; 1 drop 删除表前，需要先disable表，否则会报错。ERROR: Table xxx is enabled. Disable it first. 1hbase(main):011:0&gt; drop 'person' truncate 12345hbase(main):013:0&gt; truncate 'person'Truncating 'person' table (it may take a while): - Disabling table... - Truncating table...0 row(s) in 4.0010 seconds get_split 1234hbase(main):015:0&gt; get_splits 'person'Total number of splits = 1=&gt; [] 获取表所对应的Region个数。每个表在一开始只有一个region，之后记录增多后，region会被自动拆分。 alter alter命令可以修改表的属性，通常是修改某个列族的属性。 ​ alter ‘表名’， ‘delete’ =&gt; ‘列族名’ 12hbase(main):050:0&gt; alter 'myns:t1',&#123;NAME =&gt; 'info',VERSIONS =&gt; '5'&#125;hbase&gt; alter 'ns1:t1', 'delete' =&gt; 'f1' 数据的操作 scan scan命令可以按照 rowkey 的字典顺序来遍历指定的表的数据。 scan ‘表名’：默认当前表的所有列族。 scan ‘表名’,{COLUMNS=&gt; [‘列族:列名’],…} ： 遍历表的指定列 scan ‘表名’, { STARTROW =&gt; ‘起始行键’, ENDROW =&gt; ‘结束行键’ }：指定 rowkey 范围。如果不指定，则会从表的开头一直显示到表的结尾。区间为左闭右开。 scan ‘表名’, { LIMIT =&gt;行数量}：指定返回的行的数量 scan ‘表名’, {VERSIONS =&gt; 版本数}：返回 cell 的多个版本 scan ‘表名’, { TIMERANGE =&gt; [最小时间戳,最大时间戳]}：指定时间戳范围 ​ 注意：此区间是一个左闭右开的区间，因此返回的结果包含最小时间戳的记录，但是不包含最大时间戳记录 scan ‘表名’, { RAW =&gt; true, VERSIONS =&gt;版本数} ​ 显示原始单元格记录，在Hbase中，被删掉的记录在HBase被删除掉的记录并不会立即从磁盘上清除，而是先被打上墓碑标记，然后等待下次major compaction的时候再被删除掉。注意RAW参数必须和VERSIONS一起使用，但是不能和COLUMNS参数一起使用。 scan ‘表名’, { FILTER =&gt; “过滤器”} and|or { FILTER =&gt; “过滤器”}: 使用过滤器扫描 123HBase(main):008:0&gt; scan 'student'HBase(main):009:0&gt; scan 'student',&#123;STARTROW =&gt; '1001', STOPROW =&gt; '1001'&#125;HBase(main):010:0&gt; scan 'student',&#123;STARTROW =&gt; '1001'&#125; put put可以新增记录还可以为记录设置属性。 put ‘表名’, ‘行键’, ‘列名’, ‘值’ put ‘表名’, ‘行键’, ‘列名’, ‘值’,时间戳 put ‘表名’, ‘行键’, ‘列名’, ‘值’, { ‘属性名’ =&gt; ‘属性值’} put ‘表名’, ‘行键’, ‘列名’, ‘值’,时间戳, { ‘属性名’ =&gt;’属性值’} 123456HBase(main):012:0&gt; put 'student','1001','info:name','Nick'HBase(main):003:0&gt; put 'student','1001','info:sex','male'HBase(main):004:0&gt; put 'student','1001','info:age','18'HBase(main):005:0&gt; put 'student','1002','info:name','Janna'HBase(main):006:0&gt; put 'student','1002','info:sex','female'HBase(main):007:0&gt; put 'student','1002','info:age','20' get get支持scan所支持的大部分属性，如COLUMNS，TIMERANGE，VERSIONS，FILTER 12HBase(main):014:0&gt; get 'student','1001'HBase(main):015:0&gt; get 'student','1001','info:name' delete 1234'删除某rowkey的全部数据：'HBase(main):016:0&gt; deleteall 'student','1001''删除某rowkey的某一列数据：'HBase(main):017:0&gt; delete 'student','1002','info:sex'","tags":[]},{"title":"HBase03-HBase进阶","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase03-HBase进阶.html","text":"RegionServer 架构 StoreFile 保存实际数据的物理文件，StoreFile以Hfile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。 MemStore 写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。 WAL 由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 每间隔hbase.regionserver.optionallogflushinterval(默认1s)， HBase会把操作从内存写入WAL。 一个RegionServer上的所有Region共享一个WAL实例。 WAL的检查间隔由hbase.regionserver.logroll.period定义，默认值为1小时。检查的内容是把当前WAL中的操作跟实际持久化到HDFS上的操作比较，看哪些操作已经被持久化了，被持久化的操作就会被移动到.oldlogs文件夹内（这个文件夹也是在HDFS上的）。一个WAL实例包含有多个WAL文件。WAL文件的最大数量通过hbase.regionserver.maxlogs（默认是32）参数来定义。 BlockCache 读缓存，每次查询出的数据会缓存在BlockCache中，方便下次查询。 写流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。 与目标Region Server进行通讯； 将数据顺序写入（追加）到WAL； 将数据写入对应的MemStore，数据会在MemStore进行排序； 向客户端发送ack； 等达到MemStore的刷写时机后，将数据刷写到HFile。 读流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。 与目标Region Server进行通讯； 分别在Block Cache（读缓存），MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。 将查询到的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。 将合并后的最终结果返回给客户端。 MemStore Flush MemStore存在的意义是在写入HDFS前，将其中的数据整理有序。 MemStore刷写时机： 当某个memstore的大小达到了hbase.hregion.memstore.flush.size（默认值128M），其所在region的所有memstore都会刷写。 当memstore的大小达到了 hbase.hregion.memstore.flush.size（默认值128M） * hbase.hregion.memstore.block.multiplier（默认值4）时，会阻止继续往该memstore写数据。 当region server中memstore的总大小达到 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值0.95），region会按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下。 当region server中memstore的总大小达到 java_heapsize* hbase.regionserver.global.memstore.size（默认值0.4） 时，会阻止继续往所有的memstore写数据。 到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置hbase.regionserver.optionalcacheflushinterval（默认1小时）。 当WAL文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log以下（该属性名已经废弃，现无需手动设置，最大值为32） StoreFile Compaction由于Hbase依赖HDFS存储，HDFS只支持追加写。所以，当新增一个单元格的时候，HBase在HDFS上新增一条数据。当修改一个单元格的时候，HBase在HDFS又新增一条数据，只是版本号比之前那个大（或者自定义）。当删除一个单元格的时候，HBase还是新增一条数据！只是这条数据没有value，类型为DELETE，也称为墓碑标记（Tombstone） HBase每间隔一段时间都会进行一次合并（Compaction），合并的对象为HFile文件。合并分为两种 minor compaction和major compaction。 在HBase进行major compaction的时候，它会把多个HFile合并成1个HFile，在这个过程中，一旦检测到有被打上墓碑标记的记录，在合并的过程中就忽略这条记录。这样在新产生的HFile中，就没有这条记录了，自然也就相当于被真正地删除了。 由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。 Compaction分为两种，分别是Minor Compaction和Major Compaction。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据。 Region Split默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。 Region Split时机： 当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize，该Region就会进行拆分（0.94版本之前）。 0.94版本之后的切分策略 默认使用IncreasingToUpperBoundRegionSplitPolicy策略切分region，getSizeToCheck()是检查region的大小以判断是否满足切割切割条件 12345678910protected long getSizeToCheck(final int tableRegionsCount) &#123; // safety check for 100 to avoid numerical overflow in extreme cases return tableRegionsCount == 0 || tableRegionsCount &gt; 100 ? getDesiredMaxFileSize() : Math.min(getDesiredMaxFileSize(), initialSize * tableRegionsCount * tableRegionsCount * tableRegionsCount); &#125;// tableRegionsCount：为当前Region Server中属于该Table的region的个数。// getDesiredMaxFileSize() 这个值是hbase.hregion.max.filesize参数值，默认为10GB。// initialSize的初始化比较复杂，由多个参数决定。 1234567891011121314151617181920212223242526272829@Overrideprotected void configureForRegion(HRegion region) &#123; super.configureForRegion(region); Configuration conf = getConf(); //默认hbase.increasing.policy.initial.size 没有在配置文件中指定 initialSize = conf.getLong(\"hbase.increasing.policy.initial.size\", -1); if (initialSize &gt; 0) &#123; return; &#125; // 获取用户表中自定义的memstoreFlushSize大小，默认也为128M HTableDescriptor desc = region.getTableDesc(); if (desc != null) &#123; initialSize = 2 * desc.getMemStoreFlushSize(); &#125; // 判断用户指定的memstoreFlushSize是否合法，如果不合法，则为hbase.hregion.memstore.flush.size，默认为128. if (initialSize &lt;= 0) &#123; initialSize = 2 * conf.getLong(HConstants.HREGION_MEMSTORE_FLUSH_SIZE, HTableDescriptor.DEFAULT_MEMSTORE_FLUSH_SIZE); &#125;&#125;/*具体的切分策略为tableRegionsCount在0和100之间，则为 initialSize（默认为2*128） * tableRegionsCount^3,例如：第一次split：1^3 * 256 = 256MB 第二次split：2^3 * 256 = 2048MB 第三次split：3^3 * 256 = 6912MB 第四次split：4^3 * 256 = 16384MB &gt; 10GB，因此取较小的值10GB 后面每次split的size都是10GB了。tableRegionsCount超过100个，则超过10GB才会切分region。*/ hbase.regionserver.region.split.policy：","tags":[]},{"title":"HBase04-API","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase04-HBase-API.html","text":"pom.xml中添加依赖12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt; 在对HBase执行增删改查时，只需要引入hbase-client模块即可. 运行MR操作hbase时，需要引入hbase-server。拷贝hdfs-site.xml文件到客户端的类路径下！ 获取Configuration对象Connection代表对集群的连接对象，封装了与实际服务器的低级别单独连接以及与zookeeper的连接。 Connection可以通过ConnectionFactory类实例化。 Connection的生命周期由调用者管理，使用完毕后需要执行close()以释放资源。 Connection是线程安全的，多个Table和Admin可以共用同一个Connection对象。因此一个客户端只需要实例化一个连接即可。 反之，Table和Admin不是线程安全的！因此不建议并缓存或池化这两种对象。 1234567public static Configuration conf;static&#123; //使用HBaseConfiguration的单例方法实例化 conf = HBaseConfiguration.create(); conf.set(\"HBase.zookeeper.quorum\", \"192.166.9.102\"); conf.set(\"HBase.zookeeper.property.clientPort\", \"2181\");&#125; 表是否存在Admin为HBase的管理类，可以通过Connection.getAdmin() 获取实例，且在使用完成后调用close()关闭。 Admin可用于创建，删除，列出，启用和禁用以及以其他方式修改表，以及执行其他管理操作。 1234567public static boolean isTableExist(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; //在HBase中管理、访问表需要先创建HBaseAdmin对象 //Connection connection = ConnectionFactory.createConnection(conf); //HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); HBaseAdmin admin = new HBaseAdmin(conf); return admin.tableExists(tableName);&#125; 创建表123456789101112131415161718public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; HBaseAdmin admin = new HBaseAdmin(conf); //判断表是否存在 if(isTableExist(tableName))&#123; System.out.println(\"表\" + tableName + \"已存在\"); //System.exit(0); &#125;else&#123; //创建表属性对象,表名需要转字节 HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName)); //创建多个列族 for(String cf : columnFamily)&#123; descriptor.addFamily(new HColumnDescriptor(cf)); &#125; //根据对表的配置，创建表 admin.createTable(descriptor); System.out.println(\"表\" + tableName + \"创建成功！\"); &#125;&#125; 删除表1234567891011public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; HBaseAdmin admin = new HBaseAdmin(conf); if(isTableExist(tableName))&#123; admin.disableTable(tableName); admin.deleteTable(tableName); System.out.println(\"表\" + tableName + \"删除成功！\"); &#125;else&#123; System.out.println(\"表\" + tableName + \"不存在！\"); &#125;&#125; 向表中插入数据1234567891011public static void addRowData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException&#123; //创建HTable对象 HTable hTable = new HTable(conf, tableName); //向表中插入数据 Put put = new Put(Bytes.toBytes(rowKey)); //向Put对象中组装数据 put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); hTable.put(put); hTable.close(); System.out.println(\"插入数据成功\");&#125; 删除多行数据12345678910public static void deleteMultiRow(String tableName, String... rows) throws IOException&#123; HTable hTable = new HTable(conf, tableName); List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); for(String row : rows)&#123; Delete delete = new Delete(Bytes.toBytes(row)); deleteList.add(delete); &#125; hTable.delete(deleteList); hTable.close();&#125; 获取所有数据123456789101112131415161718public static void getAllRows(String tableName) throws IOException&#123; HTable hTable = new HTable(conf, tableName); //得到用于扫描region的对象 Scan scan = new Scan(); //使用HTable得到resultcanner实现类的对象 ResultScanner resultScanner = hTable.getScanner(scan); for(Result result : resultScanner)&#123; Cell[] cells = result.rawCells(); for(Cell cell : cells)&#123; //得到rowkey System.out.println(\"行键:\" + Bytes.toString(CellUtil.cloneRow(cell))); //得到列族 System.out.println(\"列族\" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\"列:\" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\"值:\" + Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125;&#125; 获取某一行数据1234567891011121314public static void getRow(String tableName, String rowKey) throws IOException&#123; HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); //get.setMaxVersions();显示所有版本 //get.setTimeStamp();显示指定时间戳的版本 Result result = table.get(get); for(Cell cell : result.rawCells())&#123; System.out.println(\"行键:\" + Bytes.toString(result.getRow())); System.out.println(\"列族\" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\"列:\" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\"值:\" + Bytes.toString(CellUtil.cloneValue(cell))); System.out.println(\"时间戳:\" + cell.getTimestamp()); &#125;&#125; 获取某一行指定“列族:列”的数据123456789101112public static void getRowQualifier(String tableName, String rowKey, String family, String qualifier) throws IOException&#123; HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier)); Result result = table.get(get); for(Cell cell : result.rawCells())&#123; System.out.println(\"行键:\" + Bytes.toString(result.getRow())); System.out.println(\"列族\" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\"列:\" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\"值:\" + Bytes.toString(CellUtil.cloneValue(cell))); &#125;&#125;","tags":[]},{"title":"HBase05-MR","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase05-HBase-MR.html","text":"MapReduce统计的需要：我们知道HBase的数据都是分布式存储在RegionServer上的，所以对于类似传统关系型数据库的group by操作，扫描器是无能为力的，只有当所有结果都返回到客户端的时候，才能进行统计。这样做一是慢，二是会产生很大的网络开销，所以使用MapReduce在服务器端就进行统计是比较好的方案。 性能的需要：说白了就是“快”！如果遇到较复杂的场景，在扫描器上添加多个过滤器后，扫描的性能很低；或者当数据量很大的时候扫描器也会执行得很慢，原因是扫描器和过滤器内部实现的机制很复杂，虽然使用者调用简单，但是服务器端的性能就不敢保证了 通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。 官方HBase-MapReduce 查看HBase的MapReduce任务的执行 bin/HBase mapredcp 环境变量的导入 让Hadoop加载Hbase的jar包，最简单的就是把HBase的jar包复制到Hadoop的lib里面，或者把HBase的包地址写到Hadoop的环境变量里面 临时生效，执行环境变量的导入（在命令行执行下述操作） $ export HBASE_HOME=/opt/module/HBase-1.3.1 $ export HADOOP_HOME=/opt/module/hadoop-2.7.2 $ export HADOOP_CLASSPATH=\\${HBASE_HOME}/bin/hbase mapredcp 永久生效：在/etc/profile配置 export HBASE_HOME=/opt/module/HBase-1.3.1 export HADOOP_HOME=/opt/module/hadoop-2.7.2 并在hadoop-env.sh中配置：（注意：在for循环之后配） export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/HBase/lib/* 运行官方的MapReduce任务 案例一：统计Student表中有多少行数据 $ /opt/module/hadoop-2.7.2/bin/yarn jar lib/HBase-server-1.3.1.jar rowcounter student 案例二：使用MapReduce将本地数据导入到HBase 123456789101112131415161718191) 在本地创建一个tsv格式的文件：fruit.tsv1001 Apple Red1002 Pear Yellow1003 Pineapple Yellow2) 创建HBase表HBase(main):001:0&gt; create 'fruit','info'3) 在HDFS中创建input_fruit文件夹并上传fruit.tsv文件$ /opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/$ /opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/4）执行MapReduce到HBase的fruit表中$ /opt/module/hadoop-2.7.2/bin/yarn jar lib/HBase-server-1.3.1.jar importtsv \\-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \\hdfs://hadoop102:9000/input_fruit5）使用scan命令查看导入后的结果HBase(main):001:0&gt; scan ‘fruit’ 自定义HBase-MapReduce1目标：将fruit表中的一部分数据，通过MR迁入到fruit_mr表中 构建ReadFruitMapper类，用于读取fruit表中的数据 构建WriteFruitMRReducer类，用于将读取到的fruit表中的数据写入到fruit_mr表中 构建Fruit2FruitMRRunner extends Configured implements Tool用于组装运行Job任务 主函数中调用运行该Job任务 打包运行任务 12345678910111213141516171819202122232425262728293031323334import java.io.IOException;import org.apache.hadoop.HBase.Cell;import org.apache.hadoop.HBase.CellUtil;import org.apache.hadoop.HBase.client.Put;import org.apache.hadoop.HBase.client.Result;import org.apache.hadoop.HBase.io.ImmutableBytesWritable;import org.apache.hadoop.HBase.mapreduce.TableMapper;import org.apache.hadoop.HBase.util.Bytes;public class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; //将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。 Put put = new Put(key.get()); //遍历添加column行 for(Cell cell: value.rawCells())&#123; //添加/克隆列族:info if(\"info\".equals( Bytes.toString(CellUtil.cloneFamily(cell))))&#123; //添加/克隆列：name if(\"name\".equals( Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; //将该列cell加入到put对象中 put.add(cell); //添加/克隆列:color &#125;else if(\"color\".equals( Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; //向该列cell加入到put对象中 put.add(cell); &#125; &#125; &#125; //将从fruit读取到的每行数据写入到context中作为map的输出 context.write(key, put); &#125;&#125; 12345678910111213141516import java.io.IOException;import org.apache.hadoop.HBase.client.Put;import org.apache.hadoop.HBase.io.ImmutableBytesWritable;import org.apache.hadoop.HBase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; //读出来的每一行数据写入到fruit_mr表中 for(Put put: values)&#123; context.write(NullWritable.get(), put); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233//组装Job public int run(String[] args) throws Exception &#123; //得到Configuration Configuration conf = this.getConf(); //创建Job任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Fruit2FruitMRRunner.class); //配置Job Scan scan = new Scan(); scan.setCacheBlocks(false); scan.setCaching(500); //设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本 TableMapReduceUtil.initTableMapperJob( \"fruit\", //数据源的表名 scan, //scan扫描控制器 ReadFruitMapper.class,//设置Mapper类 ImmutableBytesWritable.class,//设置Mapper输出key类型 Put.class,//设置Mapper输出value值类型 job//设置给哪个JOB ); //设置Reducer TableMapReduceUtil.initTableReducerJob(\"fruit_mr\", WriteFruitMRReducer.class, job); //设置Reduce数量，最少1个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess)&#123; throw new IOException(\"Job running with error\"); &#125; return isSuccess ? 0 : 1; &#125; 12345public static void main( String[] args ) throws Exception&#123; Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args); System.exit(status);&#125; 1$ /opt/module/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/HBase-0.0.1-SNAPSHOT.jar com.z.HBase.mr1.Fruit2FruitMRRunner 提示：运行任务前，如果待数据导入的表不存在，则需要提前创建。 提示：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin） 自定义HBase-MapReduce2目标：实现将HDFS中的数据写入到HBase表中 构建ReadFruitFromHDFSMapper于读取HDFS中的文件数据 构建WriteFruitMRFromTxtReducer类 创建Txt2FruitRunner组装Job 调用执行Job 打包运行 1234567891011121314151617181920212223242526272829303132333435import java.io.IOException;import org.apache.hadoop.HBase.client.Put;import org.apache.hadoop.HBase.io.ImmutableBytesWritable;import org.apache.hadoop.HBase.util.Bytes;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //从HDFS中读取的数据 String lineValue = value.toString(); //读取出来的每行数据使用\\t进行分割，存于String数组 String[] values = lineValue.split(\"\\t\"); //根据数据中值的含义取值 String rowKey = values[0]; String name = values[1]; String color = values[2]; //初始化rowKey ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey)); //初始化put对象 Put put = new Put(Bytes.toBytes(rowKey)); //参数分别:列族、列、值 put.add(Bytes.toBytes(\"info\"), Bytes.toBytes(\"name\"), Bytes.toBytes(name)); put.add(Bytes.toBytes(\"info\"), Bytes.toBytes(\"color\"), Bytes.toBytes(color)); context.write(rowKeyWritable, put); &#125;&#125; 123456789101112131415import java.io.IOException;import org.apache.hadoop.HBase.client.Put;import org.apache.hadoop.HBase.io.ImmutableBytesWritable;import org.apache.hadoop.HBase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; //读出来的每一行数据写入到fruit_hdfs表中 for(Put put: values)&#123; context.write(NullWritable.get(), put); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728public int run(String[] args) throws Exception &#123; //得到Configuration Configuration conf = this.getConf(); //创建Job任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Txt2FruitRunner.class); Path inPath = new Path(\"hdfs://hadoop102:9000/input_fruit/fruit.tsv\"); FileInputFormat.addInputPath(job, inPath); //设置Mapper job.setMapperClass(ReadFruitFromHDFSMapper.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setMapOutputValueClass(Put.class); //设置Reducer TableMapReduceUtil.initTableReducerJob(\"fruit_mr\", WriteFruitMRFromTxtReducer.class, job); //设置Reduce数量，最少1个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess)&#123; throw new IOException(\"Job running with error\"); &#125; return isSuccess ? 0 : 1;&#125; 12345public static void main(String[] args) throws Exception &#123; Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Txt2FruitRunner(), args); System.exit(status);&#125; 1$ /opt/module/hadoop-2.7.2/bin/yarn jar HBase-0.0.1-SNAPSHOT.jar com.atguigu.HBase.mr2.Txt2FruitRunner","tags":[]},{"title":"HBase06-与Hive集成","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase06-与Hive集成.html","text":"HBase与Hive的对比1.Hive 数据仓库：Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。 用于数据分析、清洗：Hive适用于离线的数据分析和清洗，延迟较高。 基于HDFS、MapReduce：Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。 2.HBase 数据库：是一种面向列存储的非关系型数据库。 用于存储结构化和非结构化的数据：适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。 基于HDFS：数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。 延迟较低，接入在线业务使用：面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。 HBase与Hive集成使用尖叫提示：HBase与Hive的集成在最新的两个版本中无法兼容。所以，只能含着泪勇敢的重新编译：hive-HBase-handler-1.2.2.jar！！ 因为我们后续可能会在操作Hive的同时对HBase也会产生影响，所以Hive需要持有操作HBase的Jar，那么接下来拷贝Hive所依赖的Jar包（或者使用软连接的形式）。 1234567891011export HBASE_HOME=/opt/module/HBaseexport HIVE_HOME=/opt/module/hiveln -s $HBASE_HOME/lib/HBase-common-1.3.1.jar $HIVE_HOME/lib/HBase-common-1.3.1.jarln -s $HBASE_HOME/lib/HBase-server-1.3.1.jar $HIVE_HOME/lib/HBase-server-1.3.1.jarln -s $HBASE_HOME/lib/HBase-client-1.3.1.jar $HIVE_HOME/lib/HBase-client-1.3.1.jarln -s $HBASE_HOME/lib/HBase-protocol-1.3.1.jar $HIVE_HOME/lib/HBase-protocol-1.3.1.jarln -s $HBASE_HOME/lib/HBase-it-1.3.1.jar $HIVE_HOME/lib/HBase-it-1.3.1.jarln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jarln -s $HBASE_HOME/lib/HBase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/HBase-hadoop2-compat-1.3.1.jarln -s $HBASE_HOME/lib/HBase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/HBase-hadoop-compat-1.3.1.jar 同时在hive-site.xml中修改zookeeper的属性，如下: 12345678910&lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102,hadoop103,hadoop104&lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt; 案例一: 建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。 1.在Hive中创建表同时关联HBase: 123456789101112CREATE TABLE hive_HBase_emp_table(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno\")TBLPROPERTIES (\"hbase.table.name\" = \"hbase_emp_table\"); ​ 提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表 2.在Hive中创建临时中间表，用于load文件中的数据 ​ 提示：不能将数据直接load进Hive所关联HBase的那张表中 12345678910CREATE TABLE emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)row format delimited fields terminated by '\\t'; 3.向Hive中间表中load数据 ​ hive&gt; load data local inpath ‘/home/admin/softwares/data/emp.txt’ into table emp; 4.通过insert命令将中间表中的数据导入到Hive关联HBase的那张表中 ​ hive&gt; insert into table hive_hbase_emp_table select * from emp; 5.查看Hive以及关联的HBase表中是否已经成功的同步插入了数据 ​ hive&gt; select * from hive_HBase_emp_table; ​ HBase&gt; scan ‘HBase_emp_table’ 案例二: 目标：在HBase中已经存储了某一张表HBase_emp_table，然后在Hive中创建一个外部表来关联HBase中的HBase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。 注：该案例2紧跟案例1的脚步，所以完成此案例前，请先完成案例1。 1.在Hive中创建外部表 1234567891011121314CREATE EXTERNAL TABLE relevance_HBase_emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno\") TBLPROPERTIES (\"hbase.table.name\" = \"HBase_emp_table\"); 2.关联后就可以使用Hive函数进行一些分析操作了 hive (default)&gt; select * from relevance_HBase_emp;","tags":[]},{"title":"HBase07-HBase优化","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase07-HBase优化.html","text":"高可用在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。 关闭HBase集群： bin/stop-HBase.sh 在conf目录下创建backup-masters文件： touch conf/backup-masters 在backup-masters文件中配置高可用HMaster节点：echo hadoop103 &gt; conf/backup-masters 分发conf 打开页面测试查看：http://hadooo102:16010 预分区每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。 手动设定预分区 1HBase&gt; create 'staff1','info','partition1',SPLITS =&gt; ['1000','2000','3000','4000'] 生成16进制序列预分区 1create 'staff2','info','partition2',&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit'&#125; 按照文件中设置的规则预分区 1234567创建splits.txt文件内容如下：aaaabbbbccccddddcreate 'staff3','partition3',SPLITS_FILE =&gt; 'splits.txt' 使用JavaAPI创建预分区 12345678//自定义算法，产生一系列Hash散列值存储在二维数组中byte[][] splitKeys = 某个散列值函数//创建HBaseAdmin实例HBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create());//创建HTableDescriptor实例HTableDescriptor tableDesc = new HTableDescriptor(tableName);//通过HTableDescriptor实例和散列值二维数组创建带有预分区的HBase表hAdmin.createTable(tableDesc, splitKeys); RowKey设计一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案. 生成随机数、hash、散列值 原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7 原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd 原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913 在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。 字符串反转 20170524000001转成10000042507102 20170524000002转成20000042507102 这样也可以在一定程度上散列逐步put进来的数据。 字符串拼接 20170524000001_a12e 20170524000001_93i7 内存优化HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。 基础优化 允许在HDFS的文件中追加内容 hdfs-site.xml、HBase-site.xml 属性：dfs.support.append 解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。 优化DataNode允许的最大文件打开数 hdfs-site.xml 属性：dfs.datanode.max.transfer.threads 解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096 优化延迟高的数据操作的等待时间 hdfs-site.xml 属性：dfs.image.transfer.timeout 解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。 优化数据的写入效率 mapred-site.xml 属性： mapreduce.map.output.compress mapreduce.map.output.compress.codec 解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。 设置RPC监听数量 HBase-site.xml 属性：HBase.regionserver.handler.count 解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 优化HStore文件大小 HBase-site.xml 属性：HBase.hregion.max.filesize 解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。 优化HBase客户端缓存 HBase-site.xml 属性：HBase.client.write.buffer 解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。 指定scan.next扫描HBase所获取的行数 HBase-site.xml 属性：HBase.client.scanner.caching 解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。 flush、compact、split机制 当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。 涉及属性： 即：128M就是Memstore的默认阈值 HBase.hregion.memstore.flush.size：134217728 即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。 HBase.regionserver.global.memstore.upperLimit：0.4 HBase.regionserver.global.memstore.lowerLimit：0.38 即：当MemStore使用内存总量达到HBase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit","tags":[]},{"title":"HBase08-扩展","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase08-扩展.html","text":"布隆过滤器在日常生活中，包括在设计计算机软件时，我们经常要判断一个元素是否在一个集合中。比如在字处理软件中，需要检查一个英语单词是否拼写正确（也就是要判断它是否在已知的字典中）；在 FBI，一个嫌疑人的名字是否已经在嫌疑名单上；在网络爬虫里，一个网址是否被访问过等等。最直接的方法就是将集合中全部的元素存在计算机中，遇到一个新元素时，将它和集合中的元素直接比较即可。一般来讲，计算机中的集合是用哈希表（hash table）来存储的。它的好处是快速准确，缺点是费存储空间。当集合比较小时，这个问题不显著，但是当集合巨大时，哈希表存储效率低的问题就显现出来了。比如说，一个像 Yahoo,Hotmail 和 Gmai 那样的公众电子邮件（email）提供商，总是需要过滤来自发送垃圾邮件的人（spamer）的垃圾邮件。一个办法就是记录下那些发垃圾邮件的 email 地址。由于那些发送者不停地在注册新的地址，全世界少说也有几十亿个发垃圾邮件的地址，将他们都存起来则需要大量的网络服务器。如果用哈希表，每存储一亿个 email 地址， 就需要 1.6GB 的内存（用哈希表实现的具体办法是将每一个 email 地址对应成一个八字节的信息指纹googlechinablog.com/2006/08/blog-post.html，然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email 地址需要占用十六个字节。一亿个地址大约要 1.6GB， 即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB 的内存。除非是超级计算机，一般服务器是无法存储的。 布隆过滤器只需要哈希表 1/8 到 1/4 的大小就能解决同样的问题。 Bloom Filter是一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并能判断一个元素是否属于这个集合。Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。 下面我们具体来看Bloom Filter是如何用位数组表示集合的。初始状态时，Bloom Filter是一个包含m位的位数组，每一位都置为0，如下图所示。 为了表达S={x1, x2,…,xn}这样一个n个元素的集合，Bloom Filter使用k个相互独立的哈希函数（Hash Function），它们分别将集合中的每个元素映射到{1,…,m}的范围中。对任意一个元素x，第i个哈希函数映射的位置hi(x)就会被置为1（1≤i≤k）。注意，如果一个位置多次被置为1，那么只有第一次会起作用，后面几次将没有任何效果。如图9-6所示，k=3，且有两个哈希函数选中同一个位置（从左边数第五位）。 在判断y是否属于这个集合时，我们对y应用k次哈希函数，如果所有hi(y)的位置都是1（1≤i≤k），那么我们就认为y是集合中的元素，否则就认为y不是集合中的元素。如图9-7所示y1就不是集合中的元素。y2或者属于这个集合，或者刚好是一个false positive。 为了add一个元素，用k个hash function将它hash得到bloom filter中k个bit位，将这k个bit位置1。 · 为了query一个元素，即判断它是否在集合中，用k个hash function将它hash得到k个bit位。若这k bits全为1，则此元素在集合中；若其中任一位不为1，则此元素比不在集合中（因为如果在，则在add时已经把对应的k个bits位置为1）。 · 不允许remove元素，因为那样的话会把相应的k个bits位置为0，而其中很有可能有其他元素对应的位。因此remove会引入false negative，这是绝对不被允许的。 布隆过滤器决不会漏掉任何一个在黑名单中的可疑地址。但是，它有一条不足之处，也就是它有极小的可能将一个不在黑名单中的电子邮件地址判定为在黑名单中，因为有可能某个好的邮件地址正巧对应一个八个都被设置成一的二进制位。好在这种可能性很小，我们把它称为误识概率。 布隆过滤器的好处在于快速，省空间，但是有一定的误识别率，常见的补救办法是在建立一个小的白名单，存储那些可能个别误判的邮件地址。 布隆过滤器具体算法高级内容，如错误率估计，最优哈希函数个数计算，位数组大小计算，请参见http://blog.csdn.net/jiaomeng/article/details/1495500。","tags":[]},{"title":"Hive安装","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/00-环境/04-Hive.html","text":"Hive安装 解压apache-hive-1.2.1-bin.tar.gz 修改/opt/module/hive/conf 目录下的hive-env.sh.template 名称为hive-env.sh 配置hive-env.sh 文件 12345配置HADOOP_HOME 路径 ：export HADOOP_HOME=/opt/module/hadoop-2.7.2 配置HIVE_CONF_DIR 路径 ：export HIVE_CONF_DIR=/opt/module/hive/conf 启动hdfs和yarn：sbin/start-dfs.sh、sbin/start-yarn.sh 在 HDFS 上创建/tmp 和/user/hive/warehouse 两个目录并修改他们的同组权限可写。(可不操作，系统会自动创建) 12345[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse Hive基本操作 1234567891011121314151617181920# 启动hive[atguigu@hadoop102 hive]$ bin/hive # 查看数据库hive&gt; show databases; # 打开默认数据库hive&gt; use default; # 显示default 数据库中的表hive&gt; show tables; # 创建一张表hive&gt; create table student(id int, name string); # 显示数据库中有几张表hive&gt; show tables; # 查看表的结构hive&gt; desc student; # 向表中插入数据hive&gt; insert into student values(1000,\"ss\");# 查询表中数据hive&gt; select * from student;# 退出hivehive&gt; quit; 本地文件导入Hive将本地/opt/module/data/student.txt 这个目录下的数据导入到 hive 的 student(id int, name string)表中。 12345678910111213141516171819# student.txt 注意以tab键间隔1001 zhangshan 1002 lishi 1003 zhaoliu # 启动hive# 创建student 表, 并声明文件分隔符’\\t’hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'; # 加载/opt/module/data/student.txt 文件到 student 数据库表中。# 这个命令其实就是HDFS的put操作。新建相同格式的txt放到对应的HDFS目录下，同样可以查询到数据hive&gt; load data local inpath '/opt/module/data/student.txt' into table student;hive&gt; load data inpath \"/HDFS/path\" into table stu; #这个是从HDFS上mv文件。# Hive 查询结果hive&gt; select * from student; OK 1001 zhangshan 1002 lishi 1003 zhaoliu Time taken: 0.266 seconds, Fetched: 3 row(s) 注意：Metastore 默认存储在自带的derby 数据库中，不能同时打开多个hive窗口，但mysql支持。 MySQL安装安装MySQL 查看mysql 是否安装，如果安装了，卸载mysql 12rpm -qa|grep mysql # 查询rpm -e --nodeps mysql-libs-XXXXX.x86_64 # 卸载 安装mysql 服务端：rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm 查看产生的随机密码：cat /root/.mysql_secret 查看mysql 状态：service mysql status 启动mysql服务：service mysql start 安装MySql 客户端：rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm 链接mysql：mysql -uroot -pOEXaQuS8IWkG19Xs 修改密码：mysql&gt;SET PASSWORD=PASSWORD(‘000000’); 退出：mysql&gt;exit; MySql 中user 表中主机配置配置只要是root 用户+密码，在任何主机上都能登录MySQL 数据库。 12345678910111213[root@hadoop102 mysql-libs]# mysql -uroot -p000000mysql&gt;use mysql;mysql&gt;show tables; mysql&gt;select User, Host, Password from user;mysql&gt;update user set host='%' where host='localhost'; mysql&gt;delete from user where Host='hadoop102'; mysql&gt;delete from user where Host='127.0.0.1'; mysql&gt;delete from user where Host='::1'; mysql&gt;flush privileges; mysql&gt;quit; 配置Metastore 到MySql 驱动拷贝：cp /opt/software/mysql-libs/mysql-connector-java-5.1.27/mysql-c onnector-java-5.1.27-bin.jar /opt/module/hive/lib/ 配置 hive-site.xml 先创建 hive-site.xml ```xml &lt;?xml version=”1.0”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseI fNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;000000&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 1234567891011121314151617183. 配置完毕后，如果启动 hive 异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop 集群） 4. 打开多个窗口，分别启动 hive：bin&#x2F;hive5. 启动 hive 后，回到 MySQL 窗口查看数据库，显示增加了 metastore 数据库 &#96;&#96;&#96;bash mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | metastore | | mysql | | performance_schema | | test | +--------------------+ Hive JDBC访问 启动hiveserver2 服务：[atguigu@hadoop102 hive]$ bin/hiveserver2 启动beeline 123[atguigu@hadoop102 hive]$ bin/beeline Beeline version 1.2.1 by Apache Hive beeline&gt; 连接hiveserver2 1234567891011121314beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车） Connecting to jdbc:hive2://hadoop102:10000 Enter username for jdbc:hive2://hadoop102:10000: atguigu（回车） Enter password for jdbc:hive2://hadoop102:10000: （直接回车） Connected to: Apache Hive (version 1.2.1) Driver: Hive JDBC (version 1.2.1) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://hadoop102:10000&gt; show databases; +----------------+--+ | database_name | +----------------+--+ | default || hive db2 |+----------------+--+ Hive命令123456789101112[atguigu@hadoop102 hive]$ bin/hive -help usage: hive-d,--define &lt;key=value&gt; Variable subsitution to apply to hive. commands. e.g. -d A=B or --define A=B--database &lt;databasename&gt; Specify the database to use-e &lt;quoted-query-string&gt; SQL from command line-f &lt;filename&gt; SQL from files-H,--help Print help information--hiveconf &lt;property=value&gt; Use value for given property--hivevar &lt;key=value&gt; Variable subsitution to apply to hive. commands. e.g. --hivevar A=B-i &lt;filename&gt; Initialization SQL file-S,--silent Silent mode in interactive shell-v,--verbose Verbose mode (echo executed SQL to the console) [atguigu@hadoop102 hive]$ bin/hive -e “select id from student;” [atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql 在 hive cli 命令窗口中如何查看 hdfs 文件系统：hive&gt; dfs -ls /; 在 hive cli 命令窗口中如何查看本地文件系统：hive&gt; ! ls /opt/module/datas; 查看在 hive 中输入的所有历史命令：入到当前用户的根目录/root 或/home/atguigu，查看. hivehistory 文件 Hive 常见属性配置Hive 数据仓库位置配置Default 数据仓库的最原始位置是在hdfs 上的：/user/hive/warehouse 路径下。 在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 数据库，直接在数据仓库目录下创建一个文件夹。 在hive-site.xml 文件中添加以下内容： 12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 查询后信息显示配置在 hive-site.xml 文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。 123456789&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; Hive 运行日志信息配置Hive 的log 默认存放在/tmp/atguigu/hive.log 目录下（当前用户名下） 修改 hive 的 log 存放日志到/opt/module/hive/logs 修改/opt/module/hive/conf/hive-log4j.properties.template 文件名称为hive-log4j.properties 在hive-log4j.properties 文件中修改log 存放位置：hive.log.dir=/opt/module/hive/logs 参数配置方式查看当前所有的配置信息：hive&gt;set; 参数的配置三种方式： 配置文件方式 默认配置文件：hive-default.xml 用户自定义配置文件：hive-site.xml 注意：用户自定义配置会覆盖默认配置。另外，Hive 也会读入Hadoop 的配置，因为Hive 是作为Hadoop 的客户端启动的，Hive 的配置会覆盖Hadoop 的配置。配置文件的设定对本机启动的所有Hive 进程都有效。 命令行参数方式（hive外） 启动Hive 时，可以在命令行添加 -hiveconf param=value 来设定参数。 注意：仅对本次 hive 启动有效 如：bin/hive -hiveconf mapred.reduce.tasks=10; 查看参数设置：hive (default)&gt; set mapred.reduce.tasks; 参数声明方式（hive内） 可以在HQL 中使用SET 关键字设定参数。注意：仅对本次 hive 启动有效。 例如： hive (default)&gt; set mapred.reduce.tasks=100; 查看参数设置 hive (default)&gt; set mapred.reduce.tasks; 上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。 集成Tez引擎Tez是一个Hive的运行引擎，性能优于MR。 用Hive直接编写MR程序，假设有四个有依赖关系的MR作业，上图中，绿色是Reduce Task，云状表示写屏蔽，需要将中间结果持久化写到HDFS。 Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。 安装Tez 将apache-tez-0.9.1-bin.tar.gz上传到HDFS的/tez目录下 本地解压缩apache-tez-0.9.1-bin.tar.gz 在Hive的/opt/module/hive/conf下面创建一个tez-site.xml文件 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;$&#123;fs.defaultFS&#125;/tez/apache-tez-0.9.1-bin.tar.gz&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.use.cluster.hadoop-libs&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.history.logging.service.class&lt;/name&gt; &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在hive-env.sh文件中添加tez环境变量配置和依赖包环境变量配置 1234567891011121314151617# Set HADOOP_HOME to point to a specific hadoop install directoryexport HADOOP_HOME=/opt/module/hadoop-2.7.2# Hive Configuration Directory can be controlled by:export HIVE_CONF_DIR=/opt/module/hive/conf# Folder containing extra libraries required for hive compilation/execution can be controlled by:export TEZ_HOME=/opt/module/tez-0.9.1 #是你的tez的解压目录export TEZ_JARS=\"\"for jar in `ls $TEZ_HOME |grep jar`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jardonefor jar in `ls $TEZ_HOME/lib`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jardoneexport HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS 在hive-site.xml文件中添加如下配置，更改hive计算引擎 1234&lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;tez&lt;/value&gt;&lt;/property&gt; 测试 启动Hive: bin/hive 创建表 hive (default)&gt; create table student( id int, name string); 向表中插入数据 hive (default)&gt; insert into student values(1,”zhangsan”); 如果没有报错就表示成功了 hive (default)&gt; select * from student; 1 zhangsan 注意事项 运行Tez时检查到用过多内存而被NodeManager杀死进程问题 Caused by: org.apache.tez.dag.api.SessionNotRunning: TezSession has already shutdown. Application application_1546781144082_0005 failed 2 times due to AM Container for appattempt_1546781144082_0005_000002 exited with exitCode: -103 For more detailed output, check application tracking page:http://hadoop103:8088/cluster/app/application_1546781144082_0005Then, click on links to logs of each attempt. Diagnostics: Container [pid=11116,containerID=container_1546781144082_0005_02_000001] is running beyond virtual memory limits. Current usage: 216.3 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container. 这种问题是从机上运行的Container试图使用过多的内存，而被NodeManager kill掉了。 [摘录] The NodeManager is killing your container. It sounds like you are trying to use hadoop streaming which is running as a child process of the map-reduce task. The NodeManager monitors the entire process tree of the task and if it eats up more memory than the maximum set in mapreduce.map.memory.mb or mapreduce.reduce.memory.mb respectively, we would expect the Nodemanager to kill the task, otherwise your task is stealing memory belonging to other containers, which you don’t want. 解决方法： 关掉虚拟内存检查，修改yarn-site.xml， ​ yarn.nodemanager.vmem-check-enabled ​ false 修改后一定要分发，并重新启动hadoop集群","tags":[]},{"title":"Zookeeper概述","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper01-概述.html","text":"概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 特点 Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。 集群中只要有半数以上(不包括一半)节点存活，Zookeeper集群就能正常服务。 全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 更新请求顺序进行，来自同一个Client的更新请求按其发送顺序依次执行。 数据更新原子性，一次数据更新要么成功，要么失败。 实时性，在一定时间范围内，Client能读到最新数据。 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。 应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper安装","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper00-安装.html","text":"官网首页：https://zookeeper.apache.org/ 本地模式安装部署安装 安装JDK 解压Zookeeper安装包 [atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 修改zoo.cfg配置、创建zkData目录 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件，修改dataDir路径： dataDir=/opt/module/zookeeper-3.4.10/zkData 在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹 mkdir zkData 操作Zookeeper 启动Zookeeper：bin/zkServer.sh start 查看进程是否启动 123[atguigu@hadoop102 zookeeper-3.4.10]$ jps4020 Jps4001 QuorumPeerMain 查看状态 1234[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: standalone 启动客户端：bin/zkCli.sh 退出客户端：[zk: localhost:2181(CONNECTED) 0] quit 停止Zookeeper：bin/zkServer.sh stop 配置参数解读Zookeeper中的配置文件zoo.cfg中参数含义解读如下： tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) initLimit =10：Leader 和 Fllower初始通信时限 集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 syncLimit =5：LF同步通信时限 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。 dataDir：数据文件目录+数据持久化路径 主要用于保存Zookeeper中的数据。 clientPort =2181：客户端连接端口 监听客户端连接的端口。 分布式安装部署在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper 解压安装1234# 解压Zookeeper安装包到/opt/module/目录下[atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/# 同步/opt/module/zookeeper-3.4.10目录内容到hadoop103、hadoop104[atguigu@hadoop102 module]$ xsync zookeeper-3.4.10/ 配置服务器编号 在/opt/module/zookeeper-3.4.10/这个目录下创建zkData：mkdir -p zkData 在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件：touch myid 注：添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 在myid文件中添加与server对应的编号：2 可以不从0开始，但是需要是唯一编号！ 拷贝配置好的zookeeper到其他机器上：[atguigu@hadoop102 zkData]$ xsync myid 并分别在hadoop102、hadoop103上修改myid文件中内容为3、4 修改zoo.cfg配置 重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg 修改zoo.cfg配置 12345dataDir=/opt/module/zookeeper-3.4.10/zkData#######################cluster##########################server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888 同步zoo.cfg配置文件 配置参数解读：server.A=B:C:D。 A是一个数字，表示这个是第几号服务器；集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群操作分别启动Zookeeper123[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start[atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start[atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start 查看状态123456789101112[atguigu@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[atguigu@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader[atguigu@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper内部原理","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper02-内部原理.html","text":"选举机制相关概念 Serverid：服务器ID 比如有三台服务器，编号分别是1,2,3。 编号越大在选择算法中的权重越大。 Zxid：数据ID 服务器中存放的最大数据ID. 值越大说明数据越新，在选举算法中数据越新权重越大。 Epoch：逻辑时钟 或者叫投票的次数，同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加，然后与接收到的其它服务器返回的投票信息中的数值相比，根据不同的值做出不同的判断。 Server状态：选举状态 LOOKING，竞选状态。 FOLLOWING，随从状态，同步leader状态，参与投票。 OBSERVING，观察状态,同步leader状态，不参与投票。 LEADING，领导者状态。 选举算法 leader的选择机制，zookeeper提供了三种方式： LeaderElection AuthFastLeaderElection FastLeaderElection （最新默认） 默认的算法是FastLeaderElection。 半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。 Zookeeper虽然在配置文件中并没有指定 Master和 Slave。但是，Zookeeper工作时，是有一个节点为 Leader，其他则为 Followe。Leader是通过内部的选举机制临时产生的。 判断规则 (rules judging)：保存的 zxid最大值和 leader Serverid来进行判断的。先看数据 zxid,数据 zxid大者胜出；其次再判断 leader Serverid，leader Serverid大者胜出；然后再将自身最新的选举结果(也就是上面提到的三种数据（leader Serverid，Zxid，Epoch）广播给其他server 选举消息内容 在每轮投票完成后，需要将投票信息发送给集群中的所有服务器，它包含如下内容。 服务器ID 数据ID 逻辑时钟 选举状态 选举流程一、首先开始选举阶段，每个Server读取自身的zxid。 二、发送投票信息 2.1 首先，每个Server第一轮都会投票给自己。 2.2 投票信息包含 ：所选举leader的Serverid、Zxid、Epoch。Epoch会随着选举轮数的增加而递增。 三、接收投票信息 3.1 如果服务器B接收到服务器A的数据（服务器A处于选举状态(LOOKING 状态) ​ a. 首先，判断逻辑时钟值： 如果发送过来的逻辑时钟 Epoch 大于目前的逻辑时钟。首先，更新本逻辑时钟 Epoch，同时清空本轮收到的来自其他 server的选举数据。然后，根据判断规则 判断是否需要更新当前自己的选举 leader Serverid。 如果发送过来的逻辑时钟 Epoch 小于目前的逻辑时钟。说明对方 server 在一个相对较早的 Epoch 中，忽略该 server 的投票，无需修改投票，并将本机的三种数据（leader Serverid，Zxid，Epoch）发送过去就行。 如果发送过来的逻辑时钟 Epoch 等于目前的逻辑时钟。再根据上述判断规则 rules judging 来选举 leader ，然后再将自身最新的选举结果（leader Serverid，Zxid，Epoch）广播给其他server。 ​ b. 其次，判断服务器是不是已经收集到了所有服务器的选举状态：若是，根据选举结果设置自己的角色(FOLLOWING还是LEADER)，退出选举过程就是了。 最后，若没有收到没有收集到所有服务器的选举状态：也可以判断一下根据以上过程之后最新的选举leader是不是得到了超过半数以上服务器的支持,如果是,那么尝试在200ms内接收一下数据,如果没有新的数据到来,说明大家都已经默认了这个结果,同样也设置角色退出选举过程。 3.2 如果所接收服务器A处在其它状态（FOLLOWING或者LEADING）。 ​ a. 逻辑时钟Epoch等于目前的逻辑时钟，将该数据保存到recvset。此时Server已经处于LEADING状态，说明此时这个server已经投票选出结果。若此时这个接收服务器宣称自己是leader, 那么将判断是不是有半数以上的服务器选举它，如果是则设置选举状态退出选举过程。 ​ b. 否则这是一条与当前逻辑时钟不符合的消息，那么说明在另一个选举过程中已经有了选举结果，于是将该选举结果加入到outofelection集合中，再根据outofelection来判断是否可以结束选举,如果可以也是保存逻辑时钟，设置选举状态，退出选举过程。 假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。 服务器1启动，此时只有它一台服务器启动了，它投自己一票，它发出去的报文没有任何响应，且票数未过半，所以它的选举状态一直是LOOKING状态。 服务器2启动，它也投自己一票，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。 服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的Leader。 服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接受当小弟的命了。 服务器5启动，同4一样当小弟。 节点类型 持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除 短暂（Ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久化目录节点：客户端与Zookeeper断开连接后，该节点依旧存在 持久化顺序编号目录节点：客户端与Zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 临时目录节点：客户端与Zookeeper断开连接后，该节点被删除 临时顺序编号目录节点：客户端与Zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 说明：创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护。 注意：在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 Stat结构体 czxid-创建节点的事务zxid 每次修改 ZooKeeper 状态都会收到一个 zxid 形式的时间戳，也就是 ZooKeeper 事务ID。 事务 ID是 ZooKeeper 中所有修改总的次序。每个修改都有唯一的 zxid，如果 zxid1小于zxid2，那么zxid1在zxid2之前发生。 ctime - znode被创建的毫秒数(从1970年开始) mzxid - znode最后更新的事务zxid mtime - znode最后修改的毫秒数(从1970年开始) pZxid-znode最后更新的子节点zxid cversion - znode子节点变化号，znode子节点修改次数 dataversion - znode数据变化号 aclVersion - znode访问控制列表的变化号 ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。 dataLength- znode的数据长度 numChildren - znode子节点数量 监听器原理 监听原理详解 首先要有一个main()线程 在main线程中创建Zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 通过connect线程将注册的监听事件发送给Zookeeper。 在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。 Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。 listener线程内部调用了process()方法。 常见的监听 监听节点数据的变化：get path [watch] 监听子节点增减的变化：ls path [watch] 写数据流程","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper-shell操作","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper03-Shell操作.html","text":"基本语法 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建; -s 含有序列; -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 启动客户端[atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh 显示所有操作命令[zk: localhost:2181(CONNECTED) 1] help 查看当前znode中所包含的内容12[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper] 查看当前节点详细数据12345678910111213[zk: localhost:2181(CONNECTED) 1] ls2 /[zookeeper]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x0cversion = -1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1 分别创建2个普通节点1234[zk: localhost:2181(CONNECTED) 3] create /sanguo \"jinlian\"Created /sanguo[zk: localhost:2181(CONNECTED) 4] create /sanguo/shuguo \"liubei\"Created /sanguo/shuguo 获得节点的值123456789101112131415161718192021222324252627[zk: localhost:2181(CONNECTED) 5] get /sanguojinliancZxid = 0x100000003ctime = Wed Aug 29 00:03:23 CST 2018mZxid = 0x100000003mtime = Wed Aug 29 00:03:23 CST 2018pZxid = 0x100000004cversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 1[zk: localhost:2181(CONNECTED) 6][zk: localhost:2181(CONNECTED) 6] get /sanguo/shuguoliubeicZxid = 0x100000004ctime = Wed Aug 29 00:04:35 CST 2018mZxid = 0x100000004mtime = Wed Aug 29 00:04:35 CST 2018pZxid = 0x100000004cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 0 创建短暂节点1234567891011[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo \"zhouyu\"Created /sanguo/wuguo# 在当前客户端是能查看到的[zk: localhost:2181(CONNECTED) 3] ls /sanguo [wuguo, shuguo]# 退出当前客户端然后再重启客户端[zk: localhost:2181(CONNECTED) 12] quit[atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh# 再次查看根目录下短暂节点已经删除[zk: localhost:2181(CONNECTED) 0] ls /sanguo[shuguo] 创建带序号的节点1234567891011# 先创建一个普通的根节点/sanguo/weiguo[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo \"caocao\"Created /sanguo/weiguo# 创建带序号的节点[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/xiaoqiao \"jinlian\"Created /sanguo/weiguo/xiaoqiao0000000000[zk: localhost:2181(CONNECTED) 3] create -s /sanguo/weiguo/daqiao \"jinlian\"Created /sanguo/weiguo/daqiao0000000001[zk: localhost:2181(CONNECTED) 4] create -s /sanguo/weiguo/diaocan \"jinlian\"Created /sanguo/weiguo/diaocan0000000002# 如果原来没有序号节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推。 修改节点数据值[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo “simayi” 节点的值变化监听12345678# 在hadoop104主机上注册监听/sanguo节点数据变化# 注意：watch仅能监听一次，节点第二次修改不会有提示，注册一次，监听一次[zk: localhost:2181(CONNECTED) 8] get /sanguo watch# 在hadoop103主机上修改/sanguo节点的数据[zk: localhost:2181(CONNECTED) 1] set /sanguo \"xisi\"# 观察hadoop104主机收到数据变化的监听WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/sanguo 节点的子节点变化监听（路径变化）12345678910# 在hadoop104主机上注册监听/sanguo节点的子节点变化# 注意：watch仅能监听一次，节点第二次修改不会有提示，注册一次，监听一次[zk: localhost:2181(CONNECTED) 1] ls /sanguo watch[aa0000000001, server101]# 在hadoop103主机/sanguo节点上创建子节点[zk: localhost:2181(CONNECTED) 2] create /sanguo/jin \"simayi\"Created /sanguo/jin# 观察hadoop104主机收到子节点变化的监听WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/sanguo 删除节点[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin 递归删除节点[zk: localhost:2181(CONNECTED) 15] rmr /sanguo/shuguo 查看节点状态123456789101112[zk: localhost:2181(CONNECTED) 17] stat /sanguocZxid = 0x100000003ctime = Wed Aug 29 00:03:23 CST 2018mZxid = 0x100000011mtime = Wed Aug 29 00:21:23 CST 2018pZxid = 0x100000014cversion = 9dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 1","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper实战","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper04-实战.html","text":"API应用 创建一个Maven工程 添加pom文件 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加log4j.properties文件到项目根目录 需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。 12345678log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 创建ZooKeeper客户端 12345678910111213141516171819202122private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\";private static int sessionTimeout = 2000;private ZooKeeper zkClient = null; @Beforepublic void init() throws Exception &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（用户的业务逻辑） System.out.println(event.getType() + \"--\" + event.getPath()); // 再次启动监听 try &#123; zkClient.getChildren(\"/\", true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;);&#125; 创建子节点 1234567// 创建子节点@Testpublic void create() throws Exception &#123; // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型 String nodeCreated = zkClient.create(\"/atguigu\", \"jinlian\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);&#125; 获取子节点并监听节点变化 12345678910// 获取子节点@Testpublic void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(child); &#125; // 延时阻塞 Thread.sleep(Long.MAX_VALUE);&#125; 判断Znode是否存在 123456// 判断znode是否存在@Testpublic void exist() throws Exception &#123; Stat stat = zkClient.exists(\"/eclipse\", false); System.out.println(stat == null ? \"not exist\" : \"exist\");&#125; 监听服务器节点动态上下线案例某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。 服务器端向Zookeeper注册代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.ZooDefs.Ids;public class DistributeServer &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException&#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); &#125; // 注册服务器 public void registServer(String hostname) throws Exception&#123; String create = zk.create(parentNode + \"/server\", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +\" is online \"+ create); &#125; // 业务功能 public void business(String hostname) throws Exception&#123; System.out.println(hostname+\" is working ...\"); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 1获取zk连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 2 利用zk连接注册服务器信息 server.registServer(args[0]); // 3 启动业务功能 server.business(args[0]); &#125;&#125; 客户端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 再次启动监听 try &#123; getServerList(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // 获取服务器列表信息 public void getServerList() throws Exception &#123; // 1获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); // 2存储服务器信息列表 ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); // 3遍历所有节点，获取节点中的主机名称信息 for (String child : children) &#123; byte[] data = zk.getData(parentNode + \"/\" + child, false, null); servers.add(new String(data)); &#125; // 4打印服务器列表信息 System.out.println(servers); &#125; // 业务功能 public void business() throws Exception&#123; System.out.println(\"client is working ...\"); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 1获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 2获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 3业务进程启动 client.business(); &#125;&#125; 企业面试真题 请简述ZooKeeper的选举机制 ZooKeeper的监听原理是什么？ ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？ （1）部署方式单机模式、集群模式 （2）角色：Leader和Follower （3）集群最少需要机器数：3 ZooKeeper的常用命令 ls create get delete set…","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Hive概述","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive01-概述.html","text":"什么是HiveHive：由Facebook 开源用于解决海量结构化日志的数据统计。 Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。 本质是：将HQL 转化成MapReduce 程序 Hive 处理的数据存储在HDFS Hive 分析数据底层的默认实现是MapReduce 执行程序运行在Yarn 上 优缺点优点: 操作接口采用类SQL 语法，提供快速开发的能力（简单、容易上手） 避免了去写MapReduce，减少开发人员的学习成本。 Hive 的执行延迟比较高，因此Hive 常用于数据分析，对实时性要求不高的场合。 Hive 优势在于处理大数据，对于处理小数据没有优势，因为Hive 的执行延迟比较高。 Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 缺点: Hive 的HQL 表达能力有限 迭代式算法无法表达 数据挖掘方面不擅长 Hive 的效率比较低 Hive 自动生成的MapReduce 作业，通常情况下不够智能化 Hive 调优比较困难，粒度较粗 结构原理 用户接口：Client CLI（hive shell）、JDBC/ODBC(java 访问hive)、WEBUI（浏览器访问hive） 元数据：Metastore 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的 derby 数据库中，推荐使用MySQL 存储Metastore Hadoop 使用HDFS 进行存储，使用MapReduce 进行计算。 驱动器：Driver 解析器（SQL Parser）：将SQL 字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误。 编译器（Physical Plan）：将AST编译生成逻辑执行计划。 优化器（Query Optimizer）：对逻辑执行计划进行优化。 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive 来说，就是MR/Spark。 Hive 通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。 Hive和数据库的比较由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本节将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。 优势：易于扩展机器数量；可处理大规模数据 劣势： 修改数据耗时，读多写少； 不适合在线查询； 延时相对高； 相同：SQL语法； 不同： 存储在HDFS（本地文件系统）； 无索引（根据列建立索引），由于MR并行，对大量数据查询缺陷不明显； 执行引擎MR（有自己的的执行引擎） 查询语言由于SQL 被广泛的应用在数据仓库中，因此，专门针对Hive 的特性设计了类SQL 的查询语言HQL。熟悉SQL 开发的开发者可以很方便的使用Hive 进行开发。 数据存储位置Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。 数据更新由于Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的， 因此可以使用 INSERT INTO … VALUES 添加数据， 使用 UPDATE … SET 修改数据。 索引Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。 执行Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。 执行延迟Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive 的并行计算显然能体现出优势。 可扩展性由于Hive 是建立在Hadoop 之上的，因此Hive 的可扩展性是和Hadoop 的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009 年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有100 台左右。 数据规模由于Hive 建立在集群上并可以利用MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。","tags":[]},{"title":"Hive安装","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive02-安装.html","text":"Hive安装 解压apache-hive-1.2.1-bin.tar.gz 修改/opt/module/hive/conf 目录下的hive-env.sh.template 名称为hive-env.sh 配置hive-env.sh 文件 12345配置HADOOP_HOME 路径 ：export HADOOP_HOME=/opt/module/hadoop-2.7.2 配置HIVE_CONF_DIR 路径 ：export HIVE_CONF_DIR=/opt/module/hive/conf 启动hdfs和yarn：sbin/start-dfs.sh、sbin/start-yarn.sh 在 HDFS 上创建/tmp 和/user/hive/warehouse 两个目录并修改他们的同组权限可写。(可不操作，系统会自动创建) 12345[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse Hive基本操作 1234567891011121314151617181920# 启动hive[atguigu@hadoop102 hive]$ bin/hive # 查看数据库hive&gt; show databases; # 打开默认数据库hive&gt; use default; # 显示default 数据库中的表hive&gt; show tables; # 创建一张表hive&gt; create table student(id int, name string); # 显示数据库中有几张表hive&gt; show tables; # 查看表的结构hive&gt; desc student; # 向表中插入数据hive&gt; insert into student values(1000,\"ss\");# 查询表中数据hive&gt; select * from student;# 退出hivehive&gt; quit; 本地文件导入Hive将本地/opt/module/data/student.txt 这个目录下的数据导入到 hive 的 student(id int, name string)表中。 12345678910111213141516171819# student.txt 注意以tab键间隔1001 zhangshan 1002 lishi 1003 zhaoliu # 启动hive# 创建student 表, 并声明文件分隔符’\\t’hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'; # 加载/opt/module/data/student.txt 文件到 student 数据库表中。# 这个命令其实就是HDFS的put操作。新建相同格式的txt放到对应的HDFS目录下，同样可以查询到数据hive&gt; load data local inpath '/opt/module/data/student.txt' into table student;hive&gt; load data inpath \"/HDFS/path\" into table stu; #这个是从HDFS上mv文件。# Hive 查询结果hive&gt; select * from student; OK 1001 zhangshan 1002 lishi 1003 zhaoliu Time taken: 0.266 seconds, Fetched: 3 row(s) 注意：Metastore 默认存储在自带的derby 数据库中，不能同时打开多个hive窗口，但mysql支持。 MySQL安装安装MySQL 查看mysql 是否安装，如果安装了，卸载mysql 12rpm -qa|grep mysql # 查询rpm -e --nodeps mysql-libs-XXXXX.x86_64 # 卸载 安装mysql 服务端：rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm 查看产生的随机密码：cat /root/.mysql_secret 查看mysql 状态：service mysql status 启动mysql服务：service mysql start 安装MySql 客户端：rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm 链接mysql：mysql -uroot -pOEXaQuS8IWkG19Xs 修改密码：mysql&gt;SET PASSWORD=PASSWORD(‘000000’); 退出：mysql&gt;exit; MySql 中user 表中主机配置配置只要是root 用户+密码，在任何主机上都能登录MySQL 数据库。 12345678910111213[root@hadoop102 mysql-libs]# mysql -uroot -p000000mysql&gt;use mysql;mysql&gt;show tables; mysql&gt;select User, Host, Password from user;mysql&gt;update user set host='%' where host='localhost'; mysql&gt;delete from user where Host='hadoop102'; mysql&gt;delete from user where Host='127.0.0.1'; mysql&gt;delete from user where Host='::1'; mysql&gt;flush privileges; mysql&gt;quit; 配置Metastore 到MySql 驱动拷贝：cp /opt/software/mysql-libs/mysql-connector-java-5.1.27/mysql-c onnector-java-5.1.27-bin.jar /opt/module/hive/lib/ 配置 hive-site.xml 先创建 hive-site.xml ```xml &lt;?xml version=”1.0”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseI fNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;000000&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 1234567891011121314151617183. 配置完毕后，如果启动 hive 异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop 集群） 4. 打开多个窗口，分别启动 hive：bin&#x2F;hive5. 启动 hive 后，回到 MySQL 窗口查看数据库，显示增加了 metastore 数据库 &#96;&#96;&#96;bash mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | metastore | | mysql | | performance_schema | | test | +--------------------+ Hive JDBC访问 启动hiveserver2 服务：[atguigu@hadoop102 hive]$ bin/hiveserver2 启动beeline 123[atguigu@hadoop102 hive]$ bin/beeline Beeline version 1.2.1 by Apache Hive beeline&gt; 连接hiveserver2 1234567891011121314beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车） Connecting to jdbc:hive2://hadoop102:10000 Enter username for jdbc:hive2://hadoop102:10000: atguigu（回车） Enter password for jdbc:hive2://hadoop102:10000: （直接回车） Connected to: Apache Hive (version 1.2.1) Driver: Hive JDBC (version 1.2.1) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://hadoop102:10000&gt; show databases; +----------------+--+ | database_name | +----------------+--+ | default || hive db2 |+----------------+--+ Hive命令123456789101112[atguigu@hadoop102 hive]$ bin/hive -help usage: hive-d,--define &lt;key=value&gt; Variable subsitution to apply to hive. commands. e.g. -d A=B or --define A=B--database &lt;databasename&gt; Specify the database to use-e &lt;quoted-query-string&gt; SQL from command line-f &lt;filename&gt; SQL from files-H,--help Print help information--hiveconf &lt;property=value&gt; Use value for given property--hivevar &lt;key=value&gt; Variable subsitution to apply to hive. commands. e.g. --hivevar A=B-i &lt;filename&gt; Initialization SQL file-S,--silent Silent mode in interactive shell-v,--verbose Verbose mode (echo executed SQL to the console) [atguigu@hadoop102 hive]$ bin/hive -e “select id from student;” [atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql 在 hive cli 命令窗口中如何查看 hdfs 文件系统：hive&gt; dfs -ls /; 在 hive cli 命令窗口中如何查看本地文件系统：hive&gt; ! ls /opt/module/datas; 查看在 hive 中输入的所有历史命令：入到当前用户的根目录/root 或/home/atguigu，查看. hivehistory 文件 Hive 常见属性配置Hive 数据仓库位置配置Default 数据仓库的最原始位置是在hdfs 上的：/user/hive/warehouse 路径下。 在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 数据库，直接在数据仓库目录下创建一个文件夹。 在hive-site.xml 文件中添加以下内容： 12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 查询后信息显示配置在 hive-site.xml 文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。 123456789&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; Hive 运行日志信息配置Hive 的log 默认存放在/tmp/atguigu/hive.log 目录下（当前用户名下） 修改 hive 的 log 存放日志到/opt/module/hive/logs 修改/opt/module/hive/conf/hive-log4j.properties.template 文件名称为hive-log4j.properties 在hive-log4j.properties 文件中修改log 存放位置：hive.log.dir=/opt/module/hive/logs 参数配置方式查看当前所有的配置信息：hive&gt;set; 参数的配置三种方式： 配置文件方式 默认配置文件：hive-default.xml 用户自定义配置文件：hive-site.xml 注意：用户自定义配置会覆盖默认配置。另外，Hive 也会读入Hadoop 的配置，因为Hive 是作为Hadoop 的客户端启动的，Hive 的配置会覆盖Hadoop 的配置。配置文件的设定对本机启动的所有Hive 进程都有效。 命令行参数方式（hive外） 启动Hive 时，可以在命令行添加 -hiveconf param=value 来设定参数。 注意：仅对本次 hive 启动有效 如：bin/hive -hiveconf mapred.reduce.tasks=10; 查看参数设置：hive (default)&gt; set mapred.reduce.tasks; 参数声明方式（hive内） 可以在HQL 中使用SET 关键字设定参数。注意：仅对本次 hive 启动有效。 例如： hive (default)&gt; set mapred.reduce.tasks=100; 查看参数设置 hive (default)&gt; set mapred.reduce.tasks; 上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。 集成Tez引擎Tez是一个Hive的运行引擎，性能优于MR。 用Hive直接编写MR程序，假设有四个有依赖关系的MR作业，上图中，绿色是Reduce Task，云状表示写屏蔽，需要将中间结果持久化写到HDFS。 Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。 安装Tez 将apache-tez-0.9.1-bin.tar.gz上传到HDFS的/tez目录下 本地解压缩apache-tez-0.9.1-bin.tar.gz 在Hive的/opt/module/hive/conf下面创建一个tez-site.xml文件 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;$&#123;fs.defaultFS&#125;/tez/apache-tez-0.9.1-bin.tar.gz&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.use.cluster.hadoop-libs&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.history.logging.service.class&lt;/name&gt; &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在hive-env.sh文件中添加tez环境变量配置和依赖包环境变量配置 1234567891011121314151617# Set HADOOP_HOME to point to a specific hadoop install directoryexport HADOOP_HOME=/opt/module/hadoop-2.7.2# Hive Configuration Directory can be controlled by:export HIVE_CONF_DIR=/opt/module/hive/conf# Folder containing extra libraries required for hive compilation/execution can be controlled by:export TEZ_HOME=/opt/module/tez-0.9.1 #是你的tez的解压目录export TEZ_JARS=\"\"for jar in `ls $TEZ_HOME |grep jar`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jardonefor jar in `ls $TEZ_HOME/lib`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jardoneexport HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS 在hive-site.xml文件中添加如下配置，更改hive计算引擎 1234&lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;tez&lt;/value&gt;&lt;/property&gt; 测试 启动Hive: bin/hive 创建表 hive (default)&gt; create table student( id int, name string); 向表中插入数据 hive (default)&gt; insert into student values(1,”zhangsan”); 如果没有报错就表示成功了 hive (default)&gt; select * from student; 1 zhangsan 注意事项 运行Tez时检查到用过多内存而被NodeManager杀死进程问题 Caused by: org.apache.tez.dag.api.SessionNotRunning: TezSession has already shutdown. Application application_1546781144082_0005 failed 2 times due to AM Container for appattempt_1546781144082_0005_000002 exited with exitCode: -103 For more detailed output, check application tracking page:http://hadoop103:8088/cluster/app/application_1546781144082_0005Then, click on links to logs of each attempt. Diagnostics: Container [pid=11116,containerID=container_1546781144082_0005_02_000001] is running beyond virtual memory limits. Current usage: 216.3 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container. 这种问题是从机上运行的Container试图使用过多的内存，而被NodeManager kill掉了。 [摘录] The NodeManager is killing your container. It sounds like you are trying to use hadoop streaming which is running as a child process of the map-reduce task. The NodeManager monitors the entire process tree of the task and if it eats up more memory than the maximum set in mapreduce.map.memory.mb or mapreduce.reduce.memory.mb respectively, we would expect the Nodemanager to kill the task, otherwise your task is stealing memory belonging to other containers, which you don’t want. 解决方法： 关掉虚拟内存检查，修改yarn-site.xml， ​ yarn.nodemanager.vmem-check-enabled ​ false 修改后一定要分发，并重新启动hadoop集群","tags":[]},{"title":"Hive数据类型","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive03-数据类型.html","text":"基本数据类型 Hive 数据类型 Java 数据类型 长度 例子 TINYINT byte 1byte 有符号整数 20 SMALINT short 2byte 有符号整数 20 INT int 4byte 有符号整数 20 BIGINT long 8byte 有符号整数 20 BOOLEAN boolean 布尔类型，true 或者 false TRUE、FALSE FLOAT float 单精度浮点数 3.14 DOUBLE double 双精度浮点数 3.14 STRING string 字符系列。可以指定字符集。可以使用单引号或者双引号。 ‘now is the time’ “for all good men” TIMESTAMP 时间类型 BINARY 字节数组 对于Hive 的String 类型相当于数据库的varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。 集合数据类型 数据类型 描述 语法示例 STRUCT 和c 语言中的 struct 类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT {first STRING, last STRING},那么第1 个元素可以通过字段.first 来引用。 struct() MAP MAP 是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是 MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素 map() ARRAY 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第 2 个元素可以通过数组名[1]进行引用。 Array() Hive 有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY 和MAP 与Java 中的Array 和Map 类似，而STRUCT 与C 语言中的Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。 案例： 123456789101112&#123; \"name\": \"songsong\", \"friends\": [\"bingbing\" , \"lili\"] , //列表 Array, \"children\": &#123; //键值 Map, \"xiao song\": 18 , \"xiaoxiao song\": 19 &#125; \"address\": &#123; //结构 Struct, \"street\": \"hui long guan\" , \"city\": \"beijing\" &#125;&#125; 基于上述数据结构，我们在Hive 里创建对应的表，并导入数据。 创建本地测试文件 test.txt 12songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 注意：MAP，STRUCT 和ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。 Hive 上创建测试表 test: 123456789create table test( name string, friends array&lt;string&gt;, children map&lt;string, int&gt;, address struct&lt;street:string, city:string&gt; ) row format delimited fields terminated by ',' # -- 列分隔符collection items terminated by '_' # --MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)map keys terminated by ':' # -- MAP 中的key 与value 的分隔符 lines terminated by '\\n'; # -- 行分隔符 导入文本数据到测试表: hive (default)&gt; load data local inpath “/opt/module/datas/test.txt” into table test; 访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT 的访问方式： 12345hive (default)&gt; select friends[1],children['xiao song'],address.city from test where name=\"songsong\";OK_c0 _c1 citylili 18 beijingTime taken: 0.076 seconds, Fetched: 1 row(s) 类型转化Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式 使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如， 某表达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使 用 CAST 操作。 隐式类型转换规则 任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成INT，INT 可以转换成BIGINT。 所有整数类型、FLOAT 和STRING 类型都可以隐式地转换成DOUBLE TINYINT、SMALLINT、INT 都可以转换为 FLOAT。 BOOLEAN 类型不可以转换为任何其它的类型。 使用 CAST 操作显示进行数据类型转换例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数 1； 如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。 算数运算符 运算符 描述 A+B A 和B 相加 A-B A 减去B A*B A 和B 相乘 A/B A 除以B A%B A 对B 取余 A&amp;B A 和B 按位取与 A \\ B A 和B 按位取或 A^B A 和B 按位取异或 ~A A 按位取反 比较运算符 操作符 支持的数据类型 描述 A=B 基本数据类型 如果A 等于B 则返回TRUE，反之返回FALSE A&lt;=&gt;B 基本数据类型 如果A 和B 都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL 则结果为NULL A&lt;&gt;B,A!=B 基本数据类型 A 或者B 为NULL 则返回NULL；如果A 不等于B，则返回TRUE，反之返回FALSE A&lt;B 基本数据类型 A 或者B 为NULL，则返回NULL；如果A 小于B，则返回TRUE，反之返回FALSE A&lt;=B 基本数据类型 A 或者B 为NULL，则返回NULL；如果A 小于等于B，则返回TRUE，反之返回FALSE A&gt;B 基本数据类型 A 或者B 为NULL，则返回NULL；如果A 大于B，则返回TRUE，反之返回FALSE A&gt;=B 基本数据类型 A 或者B 为NULL，则返回NULL；如果A 大于等于B，则返回TRUE，反之返回FALSE A [NOT] BETWEEN B AND C 基本数据类型 如果A，B 或者C 任一为NULL，则结果为NULL。如果A 的值大于等于B 而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT 关键字则可达到相反的效果。 A IS NULL 基本数据类型 如果A 等于NULL，则返回TRUE，反之返回FALSE A IS NOT NULL 基本数据类型 如果A 不等于NULL，则返回TRUE，反之返回FALSE IN(数值1, 数值2) 基本数据类型 使用 IN 运算显示列表中的值 A [NOT] LIKE B 基本数据类型 B 是一个SQL 下的简单正则表达式，如果A 与其匹配的话，则返回TRUE；反之返回FALSE。B 的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A 必须以字母’x’结尾，而‘%x%’表示A 包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT 关键字则可达到相反的效果。 A RLIKE B, A REGEXP B 基本数据类型 B 是一个正则表达式，如果A 与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK 中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A 相匹配，而不是只需与其字符串匹配。 逻辑运算符 操作符 含义 AND 逻辑与 OR 逻辑或 NOT 逻辑否","tags":[]},{"title":"Hive-DDL","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive04-DDL.html","text":"创建数据库1234567hive&gt; create database if not exists db_hive; -- 默认存储路径是/user/hive/warehouse/*.db。hive&gt; create database db_hive2 location '/db_hive2.db';hive&gt; create database db_hive2 location '/hive/db';-- 指定数据库在HDFS 上存放的位置-- 路径和数据库名可以不一致 查询数据库12345678910111213141516-- 显示数据库hive&gt; show databases; -- 过滤显示查询的数据库hive&gt; show databases like 'db_hive*'; -- 显示数据库信息hive&gt; desc database db_hive; OKdb_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbatguiguUSER-- 显示数据库详细信息hive&gt; desc database extended db_hive; OKdb_hivehdfs://hadoop102:9000/user/hive/warehouse/db_hive.dbatguiguUSER 修改数据库用户可以使用ALTER DATABASE 命令为某个数据库的DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。只能修改自定义的属性，其他属性不能修改。 1234567hive&gt; alter database hive set dbproperties('createtime'='20170830');hive&gt; desc database extended db_hive;db_name comment location owner_name owner_type parametersdb_hivehdfs://hadoop102:8020/user/hive/warehouse/db_hive.dbatguigu USER &#123;createtime=20170830&#125; 删除数据库123456-- 删除空数据库hive&gt; drop database db_hive2; -- 如果删除的数据库不存在，最好采用if exists 判断数据库是否存在hive&gt; drop database if exists db_hive2;-- 如果数据库不为空，可以采用cascade 命令，强制删除hive&gt; drop database db_hive cascade; 创建表123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)][CLUSTERED BY (col_name, col_name, ...)[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS][ROW FORMAT row_format][STORED AS file_format][LOCATION hdfs_path] CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 COMMENT：为表和列添加注释。 PARTITIONED BY 创建分区表 CLUSTERED BY 创建分桶表 SORTED BY 不常用 ROW FORMAT 123DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 用户在建表的时候可以自定义SerDe 或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive 通过SerDe确定表的具体的列的数据。 SerDe 是Serialize/Deserilize 的简称，目的是用于序列化和反序列化。 STORED AS 指定存储文件类型 常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件） 如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 LOCATION ：指定表在HDFS 上的存储位置。 LIKE 允许用户复制现有的表结构，但是不复制数据 管理表(内部表)默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir (如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive 也会删除这个表中数据。管理表不适合和其他工具共享数据。 12345678910111213141516-- 普通创建表create table if not exists student2(id int, name string)row format delimited fields terminated by '\\t'stored as textfilelocation '/user/hive/warehouse/student2';-- 根据查询结果创建表（查询的结果会添加到新创建的表中）create table if not exists student3 as select id, name from student;-- 根据已经存在的表结构创建表create table if not exists student4 like student;-- 查询表的类型hive (default)&gt; desc formatted student2;Table Type: MANAGED_TABLE 外部表 (元数据+表)因为表是外部表，所以Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。删除后再建立相同的表（即恢复元数据），不导入数据，依然能查到原来的数据。 每天将收集到的网站日志定期流入HDFS 文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT 进入内部表。 12345678910create external table if not exists default.dept(deptno int,dname string,loc int )row format delimited fields terminated by '\\t';load data local inpath '/opt/module/data/dept.txt' into table default.dept;hive (default)&gt; desc formatted dept;Table Type: EXTERNAL_TABLE 管理表与外部表的互相转换12345-- 修改内部表student2 为外部表alter table student2 set tblproperties('EXTERNAL'='TRUE'); # 必须大写-- 修改外部表student2 为内部表alter table student2 set tblproperties('EXTERNAL'='FALSE'); 分区表分区表实际上就是对应一个HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 123456789101112131415-- 需要根据日期对日志进行管理/user/hive/warehouse/log_partition/20170702/20170702.log/user/hive/warehouse/log_partition/20170703/20170703.log/user/hive/warehouse/log_partition/20170704/20170704.log-- 创建分区表hive (default)&gt; create table dept_partition(deptno int, dname string, loc string )partitioned by (month string)row format delimited fields terminated by '\\t';-- 加载数据到分区表中hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201709');hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201708');hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201707’); 12345678910111213141516171819202122232425262728293031323334353637383940-- 查询分区表中数据hive (default)&gt; select * from dept_partition where month='201709';-- 多分区联合查询hive (default)&gt; select * from dept_partition where month='201709'unionselect * from dept_partition where month='201708'unionselect * from dept_partition where month='201707';_u3.deptno _u3.dname _u3.loc _u3.month10 ACCOUNTING NEW YORK 20170710 ACCOUNTING NEW YORK 20170810 ACCOUNTING NEW YORK 20170920 RESEARCH DALLAS 20170720 RESEARCH DALLAS 20170820 RESEARCH DALLAS 20170930 SALES CHICAGO 20170730 SALES CHICAGO 20170830 SALES CHICAGO 20170940 OPERATIONS BOSTON 20170740 OPERATIONS BOSTON 20170840 OPERATIONS BOSTON 201709-- 增加分区hive (default)&gt; alter table dept_partition add partition(month='201706') ;hive (default)&gt; alter table dept_partition add partition(month='201705') partition(month='201704');-- 删除分区hive (default)&gt; alter table dept_partition drop partition (month='201704');hive (default)&gt; alter table dept_partition drop partition (month='201705'), partition (month='201706');-- 查看分区表有多少分区hive&gt; show partitions dept_partition;-- 查看分区表结构hive&gt; desc formatted dept_partition;-- # Partition Information-- # col_name data_type commentmonth string 1234567891011-- 创建二级分区表hive (default)&gt; create table dept_partition2(deptno int, dname string, loc string )partitioned by (month string, day string)row format delimited fields terminated by '\\t';-- 加载数据到二级分区表中hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition2 partition(month='201709', day='13');-- 查询分区数据hive (default)&gt; select * from dept_partition2 where month='201709' and day='13'; 把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式： 上传数据后修复 123456789-- 上传数据hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; -- 查询数据（查询不到刚上传的数据）hive (default)&gt; select * from dept_partition2 where month='201709' and day='12'; -- 执行修复命令hive&gt; msck repair table dept_partition2; 上传数据后添加分区 123456-- 上传数据hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11;hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=11; -- 执行添加分区hive (default)&gt; alter table dept_partition2 add partition(month='201709', day='11'); 创建文件夹后load 数据到分区 12345-- 创建目录hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10; -- 上传数据hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table dept_partition2 partition(month='201709',day='10'); 修改表123456789101112131415-- 重命名ALTER TABLE table_name RENAME TO new_table_name-- 增加/修改/替换列信息-- 更新列，可以更改列名和列的数据类型，即使不修改类型也要写上ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]-- 增加和替换列-- ADD 是代表新增一字段，字段位置在所有列后面(partition 列前)，REPLACE 则是表示替换表中所有字段ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)-- alter table xixi replace colums(id int, name string).-- 把所有字段替换为两个字段，相当于至保留前两个字段，修改了元数据，但没有修改HDFS上的文件。如果多加列，数据会显示为null。HDFS上文件没有添加相应字段的数据hive (default)&gt; alter table dept_partition add columns(deptdesc string); # 添加列hive (default)&gt; alter table dept_partition change column deptdesc desc int; # 更新列hive (default)&gt; alter table dept_partition replace columns(deptno string, dname string, loc string); # 替换列 删除表hive (default)&gt; drop table dept_partition;","tags":[]},{"title":"Hive-DML","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive05-DML.html","text":"数据导入向表中装载数据（Load）1hive&gt; load data [local] inpath '/opt/module/datas/student.txt' [overwrite] | into table student [partition (partcol1=val1,…)]; load data:表示加载数据 local:表示从本地加载数据到hive 表；否则从HDFS 加载数据到hive 表 inpath:表示加载数据的路径 overwrite:表示覆盖表中已有数据，否则表示追加 into table:表示加载到哪张表 student:表示具体的表 partition:表示上传到指定分区 12345678910hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by '\\t';-- 加载本地文件到hivehive (default)&gt; load data local inpath '/opt/module/datas/student.txt' into table default.student;-- 加载HDFS 文件到hive 中hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;hive (default)&gt; load data inpath '/user/atguigu/hive/student.txt' into table default.student;-- 加载数据覆盖表中已有的数据hive (default)&gt; load data inpath '/user/atguigu/hive/student.txt' overwrite into table default.student; 通过查询语句向表中插入数据（Insert）12345678910111213141516hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by '\\t';-- 基本插入数据hive (default)&gt; insert into table student partition(month='201709') values(1,'wangwu');-- 基本模式插入（根据单张表查询结果）hive (default)&gt; insert overwrite table student partition(month='201708') select id, name from student where month='201709';-- 多插入模式（根据多张表查询结果）hive (default)&gt; from student insert overwrite table student partition(month='201707') select id, name where month='201709'insert overwrite table student partition(month='201706') select id, name where month='201709'; 查询语句中创建表并加载数据（As Select）根据查询结果创建表（查询的结果会添加到新创建的表中） 1create table if not exists student3 as select id, name from student; 创建表时通过Location 指定加载数据路径12345678910-- 创建表，并指定在hdfs 上的位置hive (default)&gt; create table if not exists student5(id int, name string)row format delimited fields terminated by '\\t'location '/user/hive/warehouse/student5';-- 上传数据到hdfs 上hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5;-- 查询数据hive (default)&gt; select * from student5; Import 数据到指定Hive 表中注意：先用export 导出后，再将数据导入。 可以直接导入不存在的表 1hive (default)&gt; import table student2 partition(month='201709') from '/user/hive/warehouse/export/student'; 数据导出Insert 导出12345678910111213-- 将查询的结果导出到本地，local directory 这个代表本地文件hive (default)&gt; insert overwrite local directory '/opt/module/datas/export/student'select * from student;-- 将查询的结果格式化导出到本地hive(default)&gt;insert overwrite local directory '/opt/module/datas/export/student1'ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'select * from student;-- 将查询的结果导出到HDFS 上(没有local)hive (default)&gt; insert overwrite directory '/user/atguigu/student2' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' select * from student; Hadoop 命令导出到本地1hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/datas/export/student3.txt; Hive Shell 命令导出基本语法：（hive -f/-e 执行语句或者脚本&gt; file） -e 执行语句 -f 执行sql脚本 1[atguigu@hadoop102 hive]$ bin/hive -e 'select * from default.student;' &gt; /opt/module/datas/export/student4.txt; Export 导出到HDFS 上1(defahiveult)&gt; export table default.student to '/user/hive/warehouse/export/student'; 清除表中数据注意：Truncate 只能删除管理表，不能删除外部表中数据。只清除数据，不清除元数据 hive (default)&gt; truncate table student;","tags":[]},{"title":"Hive07-函数","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive07-函数.html","text":"系统内置函数查看系统自带的函数：hive&gt; show functions; 显示自带的函数的用法：hive&gt; desc function upper; 详细显示自带的函数的用法：hive&gt; desc function extended upper; 自定义函数 UDF（User-Defined-Function）：一进一出 UDAF（User-Defined Aggregation Function）：聚集函数，多进一出，类似于：count/max/min UDTF（User-Defined Table-Generating Functions）：一进多出，如 lateral view explore() 官方文档地址：https://cwiki.apache.org/confluence/display/Hive/HivePlugins 自定义UDF 函数UDF(user defined functions) 用于处理单行数据，并生成单个数据行。 编程步骤 继承org.apache.hadoop.hive.ql.UDF 需要实现evaluate 函数；evaluate 函数支持重载；回调方法，函数名不可修改 在hive 的命令行窗口创建函数 添加 jar：add jar linux_jar_path 创建 function：create [temporary] function [dbname.]function_name AS class_name; 在hive 的命令行窗口删除函数 Drop [temporary] function [if exists] [dbname.]function_name; UDF 必须要有返回类型，可以返回null，但是返回类型不能为void 实操： 创建一个Maven 工程Hive； 导入依赖 12345678&lt;dependencies&gt;&lt;!--https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建自定义UDF类 12345678910package com.atguigu.hive;import org.apache.hadoop.hive.ql.exec.UDF;public class Lower extends UDF &#123; public String evaluate (String s) &#123; if (s == null) &#123; return null; &#125; return s.toLowerCase(); &#125;&#125; 打成jar 包上传到服务器/opt/module/datas/udf.jar 将jar 包添加到hive 的classpath：hive (default)&gt; add jar /opt/module/datas/udf.jar; 创建临时函数与开发好的java class 关联：hive (default)&gt; create temporary function mylower as“com.atguigu.hive.Lower”; 在hql 中使用自定义的函数：hive (default)&gt; select ename, mylower(ename) lowername from emp; 自定义UDTF 函数UDTF(user defined Table functions) 用于处理单行数据，并生成多个数据行。 用户定义聚集函数（user-defined aggregate function ）， UDAF 自定义一个UDTF 实现将一个任意分割符的字符串切割成独立的单词 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.atguigu.udtf;import org.apache.hadoop.hive.ql.exec.UDFArgumentException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;import java.util.ArrayList;import java.util.List;public class MyUDTF extends GenericUDTF &#123; private ArrayList&lt;String&gt; outList = new ArrayList&lt;&gt;(); @Override public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException &#123; //1.定义输出数据的列名和类型 List&lt;String&gt; fieldNames = new ArrayList&lt;&gt;(); List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;&gt;(); //2.添加输出数据的列名和类型 fieldNames.add(\"lineToWord\"); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); &#125; @Override public void process(Object[] args) throws HiveException &#123; //1.获取原始数据 String arg = args[0].toString(); //2.获取数据传入的第二个参数，此处为分隔符 String splitKey = args[1].toString(); //3.将原始数据按照传入的分隔符进行切分 String[] fields = arg.split(splitKey); //4.遍历切分后的结果，并写出 for (String field : fields) &#123; //集合为复用的，首先清空集合 outList.clear(); //将每一个单词添加至集合 outList.add(field); //将集合内容写出 forward(outList); &#125; &#125; @Override public void close() throws HiveException &#123; &#125;&#125; 打成jar 包上传到服务器/opt/module/data/udtf.jar 将jar 包添加到hive 的classpath 下：hive (default)&gt; add jar /opt/module/data/udtf.jar; 创建临时函数与开发好的java class 关联：hive (default)&gt; create temporary function myudtf as “com.atguigu.hive.MyUDTF”; 在hql 中使用自定义的函数：hive (default)&gt; select myudtf(line, “,”) word from words;","tags":[]},{"title":"Hive-查询","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive06-查询.html","text":"相关指南：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select 123456789SELECT [ALL | DISTINCT] select_expr, select_expr, ...FROM table_reference[WHERE where_condition][GROUP BY col_list][ORDER BY col_list][CLUSTER BY col_list| [DISTRIBUTE BY col_list] [SORT BY col_list]][LIMIT number] 基本查询 SQL 语言大小写不敏感。 SQL 可以写在一行或者多行 关键字不能被缩写也不能分行 各子句一般要分行写 使用缩进提高语句的可读性 列别名：便于计算、紧跟列名，也可以在列名和别名之间加入关键字‘AS’ 1234567891011121314hive (default)&gt; select * from emp;hive (default)&gt; select empno, ename from emp;hive (default)&gt; select ename AS name, deptno dn from emp;-- 求总行数（count）、求工资的最大值（max）、求工资的最小值（min）-- 求工资的总和（sum）、求工资的平均值（avg）hive (default)&gt; select count(*) cnt from emp;hive (default)&gt; select max(sal) max_sal from emp;hive (default)&gt; select min(sal) min_sal from emp;hive (default)&gt; select sum(sal) sum_sal from emp;hive (default)&gt; select avg(sal) avg_sal from emp;-- 典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数hive (default)&gt; select * from emp limit 5; where语句&amp;Like 使用WHERE 子句，将不满足条件的行过滤掉 WHERE 子句紧随FROM 子句 1234567891011121314151617hive (default)&gt; select * from emp where sal =5000;hive (default)&gt; select * from emp where sal between 500 and 1000; -- between and 的区间不能反hive (default)&gt; select * from emp where comm is null;hive (default)&gt; select * from emp where sal IN (1500, 5000);hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30;hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;hive (default)&gt; select * from emp where deptno not IN(30, 20);-- Like 和RLike使用LIKE 运算选择类似的值选择条件可以包含字符或数字: % 代表零个或多个字符(任意个字符) _ 代表一个字符RLIKE 子句是Hive 中这个功能的一个扩展，其可以通过Java 的正则表达式这个更强大的语言来指定匹配条件hive (default)&gt; select * from emp where sal LIKE '2%'; # 查找以2 开头薪水的员工信息hive (default)&gt; select * from emp where sal LIKE '_2%'; # 查找第二个数值为2 的薪水的员工信息hive (default)&gt; select * from emp where sal RLIKE '[2]'; # 查找薪水中含有2 的员工信息 分组（Group By &amp; Having）12345678910111213141516171819202122232425-- Group ByGROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno; # 计算emp 表每个部门的平均工资hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group byt.deptno, t.job; # 计算emp 每个部门中每个岗位的最高薪水-- Having 语句having 与 where 不同点: 1. where 针对表中的列发挥作用，查询数据；having 针对查询结果中的列发挥作用，筛选数据。 2. where 后面不能写聚合函数，而having 后面可以使用聚合函数。 3. having 只用于 group by 分组统计语句。hive (default)&gt; select deptno, avg(sal) from emp group by deptno; # 求每个部门的平均工资hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000; # 求每个部门的平均薪水大于2000 的部门 join语句Hive 支持通常的SQL JOIN 语句，但是只支持等值连接，不支持非等值连接。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455-- 等值Joinhive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept don e.deptno = d.deptno;-- 内连接, 只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;-- 左外连接, JOIN 操作符左边表中符合WHERE 子句的所有记录将会被返回。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;-- 右外连接, JOIN 操作符右边表中符合WHERE 子句的所有记录将会被返回。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;-- 满外连接, 将会返回所有表中符合WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL 值替代。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;-- 多表连接注意：连接 n 个表，至少需要 n-1 个连接条件。例如：连接三个表，至少需要两个连接条件。hive (default)&gt;SELECT e.ename, d.deptno, l.loc_nameFROM emp eJOIN dept dON d.deptno = e.deptnoJOIN location lON d.loc = l.loc;大多数情况下，Hive 会对每对JOIN 连接对象启动一个MapReduce 任务。本例中会首 先启动一个MapReduce job 对表e 和表d 进行连接操作，然后会再启动一个MapReduce job 将第一个MapReduce job 的输出和表l进行连接操作。注意：为什么不是表d 和表l 先进行连接操作呢？这是因为Hive 总是按照从左到右的顺序执行的。-- 笛卡尔积笛卡尔集会在下面条件下产生 1. 省略连接条件 2. 连接条件无效 3. 所有表中的所有行互相连接hive (default)&gt; select empno, dname from emp, dept;-- 连接谓词中不支持orhive (default)&gt; select e.empno, e.ename, d.deptno&gt; from emp e&gt; join dept d&gt; on e.deptno=d.deptno or e.ename=d.dname;FAILED: SemanticException [Error 10019]: Line 10:3 OR notsupported in JOIN currently 'dname' 排序全局排序（Order By）Order By：全局排序，一个Reducer, 无论set几个reducer，order by都使用一个reducer ASC（ascend）: 升序（默认） DESC（descend）: 降序 ORDER BY 子句在 SELECT 语句的结尾 1234hive (default)&gt; select * from emp order by sal; # 查询员工信息按工资升序排列hive (default)&gt; select * from emp order by sal desc; # 查询员工信息按工资降序排列hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal; # 按照部门和工资升序排序 每个MapReduce 内部排序（Sort By）Sort By：每个Reducer 内部进行排序，对全局结果集来说不是排序。 123456789101112-- 设置reduce 个数hive (default)&gt; set mapreduce.job.reduces=3;-- 查看设置reduce 个数hive (default)&gt; set mapreduce.job.reduces;-- 根据部门编号降序查看员工信息hive (default)&gt; select * from emp sort by empno desc;-- 将查询结果导入到文件中（按照部门编号降序排序）hive (default)&gt; insert overwrite local directory '/opt/module/datas/sortby-result'select * from emp sort by deptno desc; 分区排序（Distribute By）Distribute By：类似MR 中partition，进行分区，结合sort by 使用。 注意，Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。 对于distribute by 进行测试，一定要分配多reduce 进行处理，否则无法看到distribute by的效果。 12345-- 先按照部门编号分区，再按照员工编号降序排序。hive (default)&gt; set mapreduce.job.reduces=3;hive (default)&gt; insert overwrite local directory '/opt/module/datas/distribute-result' select * from emp distribute by deptno sort by empno desc; Cluster By当distribute by 和sorts by 字段相同时，可以使用cluster by 方式 cluster by 除了具有distribute by 的功能外还兼具sort by 的功能。但是排序只能是升序排序，不能指定排序规则为ASC 或者DESC。 1234-- 两种写法等价hive (default)&gt; select * from emp cluster by deptno;hive (default)&gt; select * from emp distribute by deptno sort by deptno;-- 注意：按照部门编号分区，不一定就是固定死的数值，可以是20 号和30 号部门分到一个分区里面去。 分桶及抽样查询分区针对的是数据的存储路径；分桶针对的是数据文件。 分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。 分桶是将数据集分解成更容易管理的若干部分的另一个技术。 12345678910111213141516-- 设置属性hive (default)&gt; set hive.enforce.bucketing=true;hive (default)&gt; set mapreduce.job.reduces=-1;-- 创建分桶表create table stu_buck(id int, name string)clustered by(id) into 4 bucketsrow format delimited fields terminated by '\\t';-- 查看表结构hive (default)&gt; desc formatted stu_buck;Num Buckets: 4-- 创建分桶表时，数据通过子查询的方式导入insert into table stu_buckselect id, name from stu; 1234567891011121314151617181920-- 查询分桶的数据hive (default)&gt; select * from stu_buck;OKstu_buck.id stu_buck.name1004 ss41008 ss81012 ss121016 ss161001 ss11005 ss51009 ss91013 ss131002 ss21006 ss61010 ss101014 ss141003 ss31007 ss71011 ss111015 ss15 分桶抽样查询 对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive 可以通过对表进行抽样来满足这个需求. 123456789-- 查询表stu_buck 中的数据hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id);-- 注：tablesample 是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。-- y 必须是 table 总 bucket 数的倍数或者因子。hive 根据 y 的大小，决定抽样的比例。-- 例如，table 总共分了4 份，当y=2 时，抽取(4/2=)2 个bucket 的数据，当y=8 时，抽取(4/8=)1/2个bucket 的数据.-- 如果抽取 1/2 个桶，则返回相应桶的1/2的数据，也不会抽取第二个桶。-- x 表示从哪个bucket 开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。-- 例如，table 总bucket 数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2 个bucket 的数据，抽取第1(x)个和第3(x+y)个bucket 的数据。-- 注意：x 的值必须小于等于y 的值. 其他查询函数空字段赋值NVL：给值为NULL 的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果string1 为NULL，则NVL 函数返回replace_with 的值，否则返回string1 的值，如果两个参数都为NULL ，则返回NULL。 select nvl(comm,-1) from emp; 时间类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455-- date_format:格式化时间hive (default)&gt; select date_format('2019-06-29','yyyy-MM-dd');OK_c02019-06-29只能是\"-\"，\"/\"不能识别，但可以使用函数替换\"/\"，regexp_replace('2019/07/03', '/', '-'); // 2019-07-03-- date_add:时间跟天数相加hive (default)&gt; select date_add('2019-06-29',5);OK_c02019-07-04hive (default)&gt; select date_add('2019-06-29',-5);OK_c02019-06-24-- date_sub:时间跟天数相减hive (default)&gt; select date_sub('2019-06-29',5);OK_c02019-06-24hive (default)&gt; select date_sub('2019-06-29 12:12:12',5);OK_c02019-06-24hive (default)&gt; select date_sub('2019-06-29',-5);OK_c02019-07-04-- datediff:两个时间相减hive (default)&gt; select datediff('2019-06-29','2019-06-24');OK_c05hive (default)&gt; select datediff('2019-06-24','2019-06-29');OK_c0-5hive (default)&gt; select datediff('2019-06-24 12:12:12','2019-06-29');OK_c0-5hive (default)&gt; select datediff('2019-06-24 12:12:12','2019-06-29 13:13:13');OK_c0-5 CASE WHEN数据准备： name dept_id sex 悟空 A 男 大海 A 男 宋宋 B 男 凤姐 A 女 婷姐 B 女 婷婷 B 女 123456789101112131415161718192021222324-- 需求：求出不同部门男女各多少人，结果如下A 2 1B 1 2-- 创建hive 表并导入数据create table emp_sex(name string,dept_id string,sex string)row format delimited fields terminated by \"\\t\";load data local inpath '/opt/module/data/emp_sex.txt' into table emp_sex;-- 按需求查询数据select dept_id,sum(case sex when '男' then 1 else 0 end) male_count,sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id;-- case when 可以替换为sum(if(sex='男',1,0)) male_count,sum(if(sex='女',1,0)) famale_count 行转列CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串; CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数是剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间; COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array 类型字段。 数据准备： name constellation blood_type 孙悟空 白羊座 A 大海 射手座 A 宋宋 白羊座 B 猪八戒 白羊座 A 凤姐 射手座 A 12345678910111213141516171819202122232425-- 需求：把星座和血型一样的人归类到一起。预期结果如下射手座,A 大海|凤姐白羊座,A 孙悟空|猪八戒白羊座,B 宋宋-- 创建hive 表并导入数据create table person_info(name string,constellation string,blood_type string)row format delimited fields terminated by \"\\t\";load data local inpath \"/opt/module/data/person_info.txt\" into table person_info;-- 按需求查询数据selectt1.base,concat_ws('|', collect_set(t1.name)) namefrom (select name, concat(constellation, \",\", blood_type) base from person_info) t1group by t1.base; 列转行EXPLODE(col)：将hive 一列中复杂的array 或者map 结构拆分成多行。 LATERAL VIEW：用法：LATERAL VIEW udtf(expression) tableAlias(表名) AS columnAlias(udtf的列别名)。解释：用于和split, explode 等UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合 数据准备： movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 《Lie to me》 悬疑,警匪,动作,心理,剧情 《战狼2》 战争,动作,灾难 12345678910111213141516171819202122232425262728-- 将电影分类中的数组数据展开。预期结果如下：《疑犯追踪》 悬疑《疑犯追踪》 动作《疑犯追踪》 科幻《疑犯追踪》 剧情《Lie to me》 悬疑《Lie to me》 警匪《Lie to me》 动作《Lie to me》 心理《Lie to me》 剧情《战狼2》 战争《战狼2》 动作《战狼2》 灾难-- 创建hive 表并导入数据create table movie_info( movie string, category array&lt;string&gt;)row format delimited fields terminated by \"\\t\"collection items terminated by \",\";load data local inpath \"/opt/module/datas/movie.txt\" into table movie_info;-- 按需求查询数据select movie, category_namefrom movie_info lateral view explode(category) table_tmp as category_name; 窗口函数OVER()：指定分析函数工作的数据窗口大小(理解为当前行可以看到的视野)，这个数据窗口大小可能会随着行的变化而变化； CURRENT ROW：当前行； n PRECEDING：往前n 行数据； n FOLLOWING：往后n 行数据； UNBOUNDED：起点 UNBOUNDED PRECEDING 表示从前面的起点 UNBOUNDED FOLLOWING 表示到后面的终点； LAG(col,n)：往前第n 行数据； LEAD(col,n)：往后第n 行数据 NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1 开始，对于每一行，NTILE 返回此行所属的组的编号。注意：n 必须为int 类型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354name，orderdate，costjack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94-- 查询在2017 年4 月份购买过的顾客及总人数select name,count(*) over ()from businesswhere substring(orderdate,1,7) = '2017-04'group by name;-- 查询顾客的购买明细及月购买总额select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;-- 上述的场景,要将cost 按照日期进行累加select name,orderdate,cost, sum(cost) over() as sample1,-- 所有行相加 sum(cost) over(partition by name) as sample2,-- 按name 分组，组内数据相加 sum(cost) over(distribute by name sort by orderdate) as sample33， sum(cost) over(partition by name order by orderdate) as sample3, -- 按name 分组，组内数据累加 sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,-- 和sample3 一样,由起点到当前行的聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, -- 当前行和前面一行做聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,-- 当前行和前边一行及后面一行 sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 -- 当前行及后面所有行from business;-- 查看顾客上次的购买时间select name,orderdate,cost,lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate ) as time1, lag(orderdate,2) over (partition by name order by orderdate) as time2from business;-- 查询前20%时间的订单信息select * from ( select name,orderdate,cost, ntile(5) over(order by orderdate) sorted from business) twhere sorted = 1; Rank RANK() 排序相同时会重复，总数不会变 DENSE_RANK() 排序相同时会重复，总数会减少 ROW_NUMBER() 会根据顺序计算 1234567891011121314151617181920212223select name, subject, score, rank() over(partition by subject order by score desc) rp, dense_rank() over(partition by subject order by score desc) drp, row_number() over(partition by subject order by score desc) rmpfrom score;name subject score rp drp rmp孙悟空 数学 95 1 1 1宋宋 数学 86 2 2 2婷婷 数学 85 3 3 3大海 数学 56 4 4 4宋宋 英语 84 1 1 1大海 英语 84 1 1 2婷婷 英语 78 3 2 3孙悟空 英语 68 4 3 4大海 语文 94 1 1 1孙悟空 语文 87 2 2 2婷婷 语文 65 3 3 3宋宋 语文 64 4 4 4","tags":[]},{"title":"Hive09-调优","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive09-企业级调优.html","text":"Fetch 抓取Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用MapReduce 计算。例如：SELECT * FROM employees;在这种情况下，Hive 可以简单地读取employee 对应的存储目录下的文件，然后输出查询结果到控制台。 在hive-default.xml.template 文件中hive.fetch.task.conversion 默认是more，老版本hive默认是minimal，该属性修改为more 以后，在全局查找、字段查找、limit 查找等都不走mapreduce。 123456789101112&lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;more&lt;/value&gt; &lt;description&gt; Expects one of [none, minimal, more]. Some select queries can be converted to single FETCH task minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins. 0. none : disable hive.fetch.task.conversion 1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns) &lt;/description&gt;&lt;/property&gt; 本地模式大多数的Hadoop Job 是需要Hadoop 提供的完整的可扩展性来处理大数据集的。不过，有时Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。 用户可以通过设置hive.exec.mode.local.auto 的值为true，来让Hive 在适当的时候自动启动这个优化。 12345set hive.exec.mode.local.auto=true; //开启本地mr//设置local mr 的最大输入数据量，当输入数据量小于这个值时采用local mr 的方式，默认为134217728，即128Mset hive.exec.mode.local.auto.inputbytes.max=50000000;//设置local mr 的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4set hive.exec.mode.local.auto.input.files.max=10; 表的优化小表、大表Join将key 相对分散，并且数据量小的表放在join 的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join 让小的维度表（1000 条以下的记录条数）先进内存。在map 端完成reduce。 实际测试发现：新版的hive 已经对小表JOIN 大表和大表JOIN 小表进行了优化。小表放在左边和右边已经没有明显区别。 大表 Join 大表空KEY 过滤有时join 超时是因为某些key 对应的数据太多，而相同key 对应的数据都会发送到相同的reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key 对应的数据是异常数据，我们需要在SQL 语句中进行过滤。 insert overwrite table jointableselect n.* from (select * from nullidtable where id is not null )n left join ori o on n.id = o.id; 空key 转换有时虽然某个key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join 的结果中，此时我们可以表a 中key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer 上 设置5 个 reduce 个数：set mapreduce.job.reduces = 5; JOIN 两张表：insert overwrite table jointableselect n.* from nullidtable n left join ori b on n.id = b.id; 出现了数据倾斜，某些reducer 的资源消耗远大于其他reducer。 随机分布空null 值：insert overwrite table jointableselect n.* from nullidtable n full join ori o oncase when n.id is null then concat(‘hive’, rand()) else n.id end= o.id; 消除了数据倾斜，负载均衡reducer 的资源消耗。 MapJoin如果不指定MapJoin 或者不符合MapJoin 的条件，那么Hive 解析器会将Join 操作转换成Common Join，即：在Reduce 阶段完成join。容易发生数据倾斜。可以用MapJoin 把小表全部加载到内存在map 端进行join，避免reducer 处理。 开启MapJoin 参数设置; 设置自动选择MapJoin：set hive.auto.convert.join = true; 默认为true 大表小表的阈值设置（默认25M 一下认为是小表）：set hive.mapjoin.smalltable.filesize=25000000; MapJoin 工作机制： Group By默认情况下，Map 阶段同一Key 数据分发给一个reduce，当一个key 数据过大时就倾斜了。 并不是所有的聚合操作都需要在Reduce 端完成，很多聚合操作都可以先在Map 端进行部分聚合，最后在Reduce 端得出最终结果。 是否在Map 端进行聚合，默认为True：hive.map.aggr = true 在Map 端进行聚合操作的条目数目：hive.groupby.mapaggr.checkinterval = 100000 有数据倾斜的时候进行负载均衡（默认是false）：hive.groupby.skewindata = true 当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job 中，Map 的输出结果会随机分布到Reduce 中，每个Reduce 做部分聚合操作，并输出结果，这样处理的结果是==相同的Group By Key 有可能被分发到不同的Reduce 中==，从而达到负载均衡的目的；第二个MR Job 再根据预处理的数据结果按照Group By Key 分布到Reduce 中（这个过程可以保证相同的Group By Key 被分布到同一个Reduce 中），最后完成最终的聚合操作。 Count(Distinct) 去重统计数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT 操作需要用一个Reduce Task 来完成，这一个Reduce 需要处理的数据量太大，就会导致整个Job 很难完成，一般COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换 123select count(distinct id) from bigtable;select count(id) from (select id from bigtable group by id) a;'虽然会多用一个Job 来完成，但在数据量大的情况下，这个绝对是值得的' 笛卡尔积尽量避免笛卡尔积，join 的时候不加on 条件，或者无效的on 条件，Hive 只能使用1个reducer 来完成笛卡尔积。 行列过滤列处理：在SELECT 中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。 行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where 后面，那么就会先全表关联，之后再过滤。 12345678-- 测试先关联两张表，再用where 条件过滤hive (default)&gt; select o.id from bigtable bjoin ori o on o.id = b.idwhere o.id &lt;= 10;-- 通过子查询后，再关联表select b.id from bigtable bjoin (select id from ori where id &lt;= 10 ) o on b.id = o.id; 动态分区调整关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive 的动态分区，需要进行相应的配置。 开启动态分区功能（默认true，开启） hive.exec.dynamic.partition=true 设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。） hive.exec.dynamic.partition.mode=nonstrict 在所有执行MR 的节点上，最大一共可以创建多少个动态分区 hive.exec.max.dynamic.partitions=1000 在每个执行MR 的节点上，最大可以创建多少个动态分区 hive.exec.max.dynamic.partitions.pernode=100 整个MR Job 中，最大可以创建多少个HDFS 文件 hive.exec.max.created.files=100000 当有空分区生成时，是否抛出异常。一般不需要设置 hive.error.on.empty.partition=false 123insert overwrite table ori_partitioned_target partition (p_time)select id, time, uid, keyword, url_rank, click_num, click_url, p_time from ori_partitioned;'select插入的最后一个字段就是分区字段' MR 优化设置 Map 数 通常情况下，作业会通过input 的目录产生一个或者多个map 任务。主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。 map 数不是越多越好。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map 任务来完成，而一个map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map 数是受限的。 比如有一个127m 的文件，正常会用一个map 去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map 处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。 针对上面的问题 2 和 3，我们需要采取两种方式来解决：即减少 map 数和增加 map 数； 小文件进行合并在map 执行前合并小文件，减少map 数：CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。 set hive.input.format =org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 复杂文件增加 Map 数当input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加Map数，来使得每个map 处理的数据量减少，从而提高任务的执行效率。 增加map 的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M 公式，调整maxSize 最大值。让maxSize 最大值低于blocksize 就可以增加map 的个数。 合理设置 Reduce 数 调整reduce 个数方法 123456789'在hadoop 的mapred-default.xml 文件中修改'set mapreduce.job.reduces = 15; '每个Reduce 处理的数据量默认是256MB'hive.exec.reducers.bytes.per.reducer=256000000'每个任务最大的reduce 数，默认为1009'hive.exec.reducers.max=1009'计算reducer 数的公式'N=min(参数2，总输入数据量/参数1) reduce 个数并不是越多越好 123过多的启动和初始化reduce 也会消耗时间和资源；另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；在设置reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce 数；使单个reduce 任务处理数据量大小要合适； 并行执行Hive 会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce 阶段、抽样阶段、合并阶段、limit 阶段。或者Hive 执行过程中可能需要的其他阶段。默认情况下，Hive 一次只会执行一个阶段。不过，某个特定的job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job 的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job 可能就越快完成。 通过设置参数hive.exec.parallel 值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job 中并行阶段增多，那么集群利用率就会增加。当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。 12set hive.exec.parallel=true; //打开任务并行执行set hive.exec.parallel.thread.number=16; //同一个sql 允许最大并行度，默认为8。 严格模式Hive 提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。 通过设置属性hive.mapred.mode 值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode 值为strict，开启严格模式可以禁止3 种类型的查询。 1234567891011121314&lt;property&gt; &lt;name&gt;hive.mapred.mode&lt;/name&gt; &lt;value&gt;strict&lt;/value&gt; &lt;description&gt; The mode in which the Hive operations are being performed. In strict mode, some risky queries are not allowed to run. They include: Cartesian Product. No partition being picked up for a query. Comparing bigints and strings. Comparing bigints and doubles. Orderby without limit. &lt;/description&gt;&lt;/property&gt; 对于分区表，除非where 语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表. 对于使用了order by 语句的查询，要求必须使用limit 语句。因为order by 为了执行排序过程会将所有的结果数据分发到同一个Reducer 中进行处理，强制要求用户增加这个LIMIT 语句可以防止Reducer 额外执行很长一段时间。 限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN 查询的时候不使用ON 语句而是使用where 语句，这样关系数据库的执行优化器就可以高效地将WHERE 语句转化成那个ON 语句。不幸的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。 JVM 重用JVM 重用是Hadoop 调优参数的内容，其对Hive 的性能具有非常大的影响，特别是==对于很难避免小文件的场景或task 特别多的场景，这类场景大多数执行时间都很短==。 Hadoop 的默认配置通常是使用派生JVM 来执行map 和Reduce 任务的。这时JVM 的启动过程可能会造成相当大的开销，尤其是执行的job 包含有成百上千task 任务的情况。JVM重用可以使得JVM 实例在同一个job 中重新使用N 次。N 的值可以在Hadoop 的 mapred-site.xml 文件中进行配置。通常在10-20 之间，具体多少需要根据具体业务场景测试得出。 12345&lt;property&gt; &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;description&gt;How many tasks to run per jvm. If set to -1, there is no limit.&lt;/description&gt;&lt;/property&gt; 这个功能的缺点是，开启JVM 重用将一直占用使用到的task 插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job 中有某几个reduce task 执行的时间要比其他Reduce task 消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task 都结束了才会释放。 推测执行在分布式集群环境下，因为程序Bug（包括Hadoop 本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop 采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。 设置开启推测执行参数：Hadoop 的mapred-site.xml 文件中进行配置 12345678910&lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; 不过hive 本身也提供了配置项来控制reduce-side 的推测执行： 12345&lt;property&gt; &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;/description&gt;&lt;/property&gt; 关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map 或者Reduce task 的话，那么启动推测执行造成的浪费是非常巨大大。 执行计划（Explain）语法：EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query 12345hive (default)&gt; explain select * from emp;hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by deptno;'查看详细执行计划'hive (default)&gt; explain extended select * from emp;hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp group by deptno;","tags":[]},{"title":"Hive08-压缩和存储","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive08-压缩和存储.html","text":"Hadoop 源码编译支持Snappy 压缩 hadoop-2.7.2-src.tar.gz jdk-8u144-linux-x64.tar.gz snappy-1.1.3.tar.gz apache-maven-3.0.5-bin.tar.gz protobuf-2.5.0.tar.gz 准备编译环境 12345yum install svnyum install autoconf automake libtool cmakeyum install ncurses-develyum install openssl-develyum install gcc* 编译安装snappy 123456cd snappy-1.1.3/./configuremakemake install'查看snappy 库文件ls -lh /usr/local/lib |grep snappy 编译安装protobuf 123456cd protobuf-2.5.0/./configuremakemake install'查看protobuf 版本以测试是否安装成功protoc --version 编译hadoop native 12cd hadoop-2.7.2-src/mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy 执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/hadoop-2.7.2.tar.gz 即为新生成的支持snappy 压缩的二进制安装包。 Hadoop 压缩配置MR 支持的压缩编码 压缩格式 工具 算法 文件扩展名 是否可切分 DEFAULT 无 DEFAULT .deflate 否 Gzip gzip DEFAULT .gz 否 bzip2 bzip2 bzip2 .bz2 是 LZO lzop LZO .lzo 是 Snappy 无 Snappy .snappy 否 为了支持多种压缩/解压缩算法，Hadoop 引入了编码/解码器 压缩格式 对应的编码/解码器 DEFAULT org.apache.hadoop.io.compress.DefaultCodec Gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较： 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 Gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s 压缩参数配置core-site.xml 参数 默认值 阶段 建议 io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec 输入压缩 Hadoop 使用文件扩展名判断是否支持某种编解码器 mapred-site.xml 参数 默认值 阶段 建议 mapreduce.map.output.compress false mapper 输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.DefaultCodec mapper 输出 使用LZO、LZ4 或snappy 编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress false reducer 输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec org.apache.hadoop.io.compress. DefaultCodec reducer 输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type RECORD reducer 输出 SequenceFile 输出使用的压缩类型：NONE 和BLOCK 开启Map 输出阶段压缩开启map 输出阶段压缩可以减少job 中map 和Reduce task 间数据传输量。具体配置如下： 12345678'开启hive 中间传输数据压缩功能hive (default)&gt;set hive.exec.compress.intermediate=true;'开启mapreduce 中map 输出压缩功能hive (default)&gt;set mapreduce.map.output.compress=true;'设置mapreduce 中map 输出数据的压缩方式hive (default)&gt;set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec;'执行查询语句hive (default)&gt; select count(ename) name from emp; 开启Reduce 输出阶段压缩当Hive 将输出写入到表中时， 输出内容同样可以进行压缩。属性hive.exec.compress.output 控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。 12345678910'开启hive 最终输出数据压缩功能hive (default)&gt;set hive.exec.compress.output=true;'开启mapreduce 最终输出数据压缩hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;'设置mapreduce 最终数据输出压缩方式hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;'设置mapreduce 最终数据输出压缩为块压缩hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;'测试一下输出结果是否是压缩文件hive (default)&gt; insert overwrite local directory '/opt/module/datas/distribute-result' select * from emp distribute by deptno sort by empno desc; 文件存储格式Hive 支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。图中左边为逻辑表，右边第一个为行式存储，第二个为列式存储。 列式存储和行式存储 TEXTFILE 和SEQUENCEFILE 的存储格式都是基于行存储的； ORC 和PARQUET 是基于列式存储的。 行存储的特点 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。 列存储的特点 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。 TextFile 格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2 使用，但使用Gzip 这种方式，hive 不会对数据进行切分，从而无法对数据进行并行操作。 Orc 格式Orc (Optimized Row Columnar)是Hive 0.11 版里引入的新的存储格式。 每个Orc 文件由1 个或多个stripe 组成，每个stripe 250MB 大小，这个Stripe 实际相当于RowGroup 概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe 里有三部分组成，分别是Index Data，Row Data，Stripe Footer： Index Data：一个轻量级的index，默认是每隔1W 行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data 中的offset。 Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream 来存储。 Stripe Footer：存的是各个Stream 的类型，长度等信息。 每个文件有一个File Footer，这里面存的是每个Stripe 的行数，每个Column 的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek 到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe 信息，再读各个Stripe，即从后往前读。 Parquet 格式Parquet 是面向分析型业务的列式存储格式，由Twitter 和Cloudera 合作开发，2015 年5月从Apache 的孵化器里毕业成为Apache 顶级项目。 Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet 格式文件是自解析的。 通常情况下，在存储Parquet 数据的时候会按照Block 大小设置行组的大小，由于一般情况下每一个Mapper 任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper 任务处理，增大任务执行并行度。 上图展示了一个Parquet 文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet 文件，Footer length 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema 信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet 中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet 中还不支持索引页。 存储和压缩结合修改Hadoop 集群具有Snappy 压缩方式123456789101112131415161718192021222324252627282930313233343536'查看hadoop checknative 命令使用'[atguigu@hadoop104 hadoop-2.7.2]$ hadoop checknative [-a|-h] check native hadoop and compression libraries availability'查看hadoop 支持的压缩方式'[atguigu@hadoop104 hadoop-2.7.2]$ hadoop checknative17/12/24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version17/12/24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib libraryNative library checking:hadoop: true/opt/module/hadoop-2.7.2/lib/native/libhadoop.sozlib: true /lib64/libz.so.1snappy: falselz4: true revision:99bzip2: false'将编译好的支持Snappy 压缩的hadoop-2.7.2.tar.gz 包导入到hadoop102 的 /opt/software 中''进入到/opt/software/hadoop-2.7.2/lib/native 路径可以看到支持Snappy 压缩的动态链接库'libsnappy.alibsnappy.lalibsnappy.so -&gt; libsnappy.so.1.3.0libsnappy.so.1 -&gt; libsnappy.so.1.3.0libsnappy.so.1.3.0'拷贝/opt/software/hadoop-2.7.2/lib/native 里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native 路径上''分发集群''再次查看hadoop 支持的压缩类型'[atguigu@hadoop102 hadoop-2.7.2]$ hadoop checknative17/12/24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will usepure-Java version17/12/24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib libraryNative library checking:hadoop: true/opt/module/hadoop-2.7.2/lib/native/libhadoop.sozlib: true /lib64/libz.so.1snappy: true/opt/module/hadoop-2.7.2/lib/native/libsnappy.so.1lz4: true revision:99bzip2: false'重新启动hadoop 集群和hive' 测试存储和压缩在实际的项目开发当中，hive 表的数据存储格式一般选择：orc 或parquet。压缩方式一般选择snappy，lzo。 123456789101112131415'创建一个SNAPPY 压缩的ORC 存储方式:create table log_orc_snappy(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orc tblproperties (\"orc.compress\"=\"SNAPPY\");'插入数据''查看插入后数据'hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/; ORC 存储方式的压缩： Key Default Notes orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries (must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)","tags":[]},{"title":"Hive10-实战","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive10-实战.html","text":"需求描述统计硅谷影音视频网站的常规指标，各种TopN 指标： 统计视频观看数Top10 统计视频类别热度Top10 统计视频观看数Top20 所属类别以及类别包含的Top20 的视频个数 统计视频观看数Top50 所关联视频的所属类别Rank 统计每个类别中的视频热度Top10 统计每个类别中视频流量Top10 统计上传视频最多的用户Top10 以及他们上传的观看次数在前20 视频 统计每个类别视频观看数Top10 数据结构视频表 字段 备注 详细描述 video id 视频唯一id 11 位字符串 uploader 视频上传者 上传视频的用户名String age 视频年龄 视频在平台上的整数天 category 视频类别 上传视频指定的视频分类 length 视频长度 整形数字标识的视频长度 views 观看次数 视频被浏览的次数 rate 视频评分 满分5 分 ratings 流量 视频的流量，整型数字 conments 评论数 一个视频的整数评论数 related ids 相关视频id 相关视频的id，最多20 个 用户表 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int 答案 统计视频观看数Top10 123456789101112131415select videoId, viewsfrom gulivideo_orcorder by views desclimit 10; dMH0bHeiRNg 425134170XxI-hvPRRA 202824641dmVU08zVpA 16087899RB-wUgnyGv0 15712924QjA5faZF1A8 15256922-_CSo1gOd48 1319983349IDp76kjPw 11970018tYnn51C3X_w 11823701pv5zWaTEVkI 11672017D2kJZOfq7zk 11184051 统计视频类别热度Top10 1234567select category_name, count(*) category_countfrom (select videoId, category_name from gulivideo_orc lateral view explode(category) tmp_category as category_name) t1group by category_nameorder by category_count desclimit 10; 统计视频观看数Top20 所属类别以及类别包含的Top20 的视频个数 12345678910select category_name, count(*) category_countfrom (select videoId, category_name from (select videoId, views, category from gulivideo_orc order by views desc limit 20) t1 lateral view explode(category) tmp_category as category_name) t2group by category_nameorder by category_count desc; 统计视频观看数Top50 所关联视频的所属类别Rank 1234567891011121314151617select category_name, count(*) category_countfrom (select explode(category) category_name from (select category from (select related_id from (select relatedId, views from gulivideo_orc order by views desc limit 50) t1 lateral view explode(relatedId) tmp_related as related_id group by related_id) t2 join gulivideo_orc orc on t2.related_id=orc.videoId) t3 ) t4 group by category_name order by category_count desc; 统计每个类别中的视频热度Top10 12345678910select video.videoId, video.viewsfrom (select uploader, videosfrom gulivideo_user_orcorder by videos desclimit 10) t1join gulivideo_orc videoon t1.uploader=video.uploaderorder by views desclimit 20; 统计每个类别中视频流量Top10 统计上传视频最多的用户Top10 以及他们上传的观看次数在前20 视频 统计每个类别视频观看数Top10 1234567891011121.给每一种类别根据视频观看数添加rank值(倒序)select categoryId, videoId, views, rank() over(partition by categoryId order by views desc) rkfrom gulivideo_category; 2.过滤前十select categoryId, videoId, viewsfrom (select categoryId, videoId, views, rank() over(partition by categoryId order by views desc) rk from gulivideo_category) t1where rk&lt;=10;","tags":[]},{"title":"数据结构02-复杂度分析(下)","date":"2020-06-03T13:40:14.000Z","path":"2020/06/03/01-数据结构/数据结构03-复杂度分析(下).html","text":"复杂度分析(下)浅析最好、最坏、平均、均摊时间复杂度今天会继续讲四个复杂度分析方面的知识点，最好情况时间复杂度（best case time complexity）、最坏情况时间复杂度（worst case time complexity）、平均情况时间复杂度（average case time complexity）、均摊时间复杂度（amortized time complexity）。如果这几个概念你都能掌握，那对你来说，复杂度分析这部分内容就没什么大问题了。 最好、最坏情况时间复杂度用上节教你的分析技巧，自己先试着分析一下这段代码的时间复杂度。 123456789// n 表示数组 array 的长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for (; i &lt; n; ++i) &#123; if (array[i] == x) pos = i; &#125; return pos;&#125; 应该可以看出来，这段代码要实现的功能是，在一个无序的数组（array）中，查找变量 x 出现的位置。如果没有找到，就返回 -1。按照上节课讲的分析方法，这段代码的复杂度是 O(n)，其中，n 代表数组的长度。 在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。但是，这段代码写得不够高效。我们可以这样优化一下这段查找代码。 123456789101112// n 表示数组 array 的长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for (; i &lt; n; ++i) &#123; if (array[i] == x) &#123; pos = i; break; &#125; &#125; return pos;&#125; 这个时候，问题就来了。我们优化完之后，这段代码的时间复杂度还是 O(n) 吗？很显然，咱们上一节讲的分析方法，解决不了这个问题。 因为，要查找的变量 x 可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量 x，那就不需要继续遍历剩下的 n-1 个数据了，那时间复杂度就是 O(1)。但如果数组中不存在变量 x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了 O(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。 为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度。 顾名思义，最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。就像我们刚刚讲到的，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。 同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。就像刚举的那个例子，如果数组中没有要查找的变量 x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。 平均情况时间复杂度最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，需要引入另一个概念：平均情况时间复杂度，后面简称为平均时间复杂度。 平均时间复杂度又该怎么分析呢？还是借助刚才查找变量 x 的例子来解释。 要查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0～n-1 位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即： 时间复杂度的大 O 标记法中，可以省略掉系数、低阶、常量，所以，把刚刚这个公式简化之后，得到的平均时间复杂度就是 O(n)。 这个结论虽然是正确的，但是计算过程稍微有点儿问题。究竟是什么问题呢？刚讲的这 n+1 种情况，出现的概率并不是一样的。 要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。 因此，前面的推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样： 这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。 引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。 你可能会说，平均时间复杂度分析好复杂啊，还要涉及概率论的知识。实际上，在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。像上一节课举的那些例子那样，很多时候，使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，才会使用这三种复杂度表示法来区分。 均摊时间复杂度均摊时间复杂度，听起来跟平均时间复杂度有点儿像。对于初学者来说，这两个概念确实非常容易弄混。我前面说了，大部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。 123456789101112131415161718// array 表示一个长度为 n 的数组// 代码中的 array.length 就等于 nint[] array = new int[n];int count = 0;void insert(int val) &#123; if (count == array.length) &#123; int sum = 0; for (int i = 0; i &lt; array.length; ++i) &#123; sum = sum + array[i]; &#125; array[0] = sum; count = 1; &#125; array[count] = val; ++count;&#125; 这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的 count == array.length 时，我们用 for 循环遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。 那这段代码的时间复杂度是多少呢？可以先用我们刚讲到的三种时间复杂度的分析方法来分析一下： 最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置就可以了，所以最好情况时间复杂度为 O(1)。 最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 O(n)。 那平均时间复杂度是多少呢？答案是 O(1)。我们还是可以通过前面讲的概率论的方法来分析。假设数组的长度是 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂度是 O(1)。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 O(n)。而且，这 n+1 种情况发生的概率一样，都是 1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是： 至此为止，前面的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是这个例子里的平均复杂度分析其实并不需要这么复杂，不需要引入概率论的知识。这是为什么呢？我们先来对比一下这个 insert() 的例子和前面那个 find() 的例子，你就会发现这两者有很大差别。 首先，find() 函数在极端情况下，复杂度才为 O(1)。但 insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()第一个区别于 find() 的地方我们再来看第二个不同的地方。对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。 所以，针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。 针对这种特殊的场景，我们引入了一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。 那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？ 我们还是继续看在数组中插入数据的这个例子。每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。这就是均摊分析的大致思路。你都理解了吗？ 均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。为了方便你理解、记忆，我这里简单总结一下它们的应用场景。如果你遇到了，知道是怎么回事儿就行了。 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。 尽管很多数据结构和算法书籍都花了很大力气来区分平均时间复杂度和均摊时间复杂度，但其实我个人认为，均摊时间复杂度就是一种特殊的平均时间复杂度，我们没必要花太多精力去区分它们。你最应该掌握的是它的分析方法，摊还分析。至于分析出来的结果是叫平均还是叫均摊，这只是个说法，并不重要。 内容小结今天我们学习了几个复杂度分析相关的概念，分别有：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。之所以引入这几个复杂度概念，是因为，同一段代码，在不同输入的情况下，复杂度量级有可能是不一样的。 在引入这几个概念之后，我们可以更加全面地表示一段代码的执行效率。而且，这几个概念理解起来都不难。最好、最坏情况下的时间复杂度分析起来比较简单，但平均、均摊两个复杂度分析相对比较复杂。如果你觉得理解得还不是很深入，不用担心，在后续具体的数据结构和算法学习中，我们可以继续慢慢实践！ 12345678910111213141516171819202122// 全局变量，大小为 10 的数组 array，长度 len，下标 i。int array[] = new int[10]; int len = 10;int i = 0; // 往数组中添加一个元素void add(int element) &#123; if (i &gt;= len) &#123; // 数组空间不够了 // 重新申请一个 2 倍大小的数组空间 int new_array[] = new int[len*2]; // 把原来 array 数组中的数据依次 copy 到 new_array for (int j = 0; j &lt; len; ++j) &#123; new_array[j] = array[j]; &#125; // new_array 复制给 array，array 现在大小就是 2 倍 len 了 array = new_array; len = 2 * len; &#125; // 将 element 放到下标为 i 的位置，下标 i 加一 array[i] = element; ++i;&#125; 答：最小是O(1)，最大是O(n)，平均和分摊都是O(1),","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构01-大纲","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/01-数据结构/数据结构01-大纲.html","text":"以下两个表来自：https://www.bigocheatsheet.com/","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"时间规划","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/00-时间计划.html","text":"","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"IDEA配置","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/IDEA配置.html","text":"使用内存设置 安装目录下：idea64.exe.vmoptions 鼠标滚轮更改字体大小 鼠标悬浮提示 自动导包 显示行号和方法间的分隔符 忽略大小写提示 设置取消单行显示tabs的操作 注释颜色 设置超过import个数，改为* 文件头 文件格式-UTF-8 自动编译 模板 第一种不能修改或者添加模板，第二种可以。","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"学习路线","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java01-概述.html","text":"学习路线 Java知识图解 Java技术体系平台 Java SE (Java Standard Edition) 标准版 支持面向桌面级应用（如Windows 下的应用程序）的 Java 平台，提供了完整的 Java 核心 API ，此版本以前称为 J2SE Java EE(Java Enterprise Edition) 企业版 是为开发企业环境下的应用程序提供的一套解决方案。该技术体系中包含的技术如Servlet 、 Jsp 等，主要针对于 Web 应用程序开发。版本以前称为 J2EE Java ME(Java Micro Edition) 小型版支持 支持Java 程序运行在移动终端（手机、 PDA ）上的平台，对 Java API 有所精简，并加入了针对移动终端的支持，此版本以前称为 J2ME Java Card 支持一些Java 小程序（ Applets ）运行在小内存设备（如智能卡）上的平台 JDK 、 JRE 、 JVM 关系 • JDK = JRE + 开发工具集（例如 Javac 编译工具 等） • JRE = JVM + Java SE 标准类库","tags":[]},{"title":"MySQL安装","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL00-安装.html","text":"MySQL安装安装文件：mysql-5.5.15-winx64.msi 接下来，需要注意：1）Typical 是经典安装，包含服务端和自带的客户端；2）Custom 是自定义安装；3）Complete 是完全安装。这里选择自定义安装。同时修改安装路径。 然后期间会出现mysql的广告，点击下一步即可。然后会继续安装，安装成功后，保证下面是勾选状态（默认也是勾选的），到这里仅是安装好了服务，还没配置。 如果取消了勾选，或配置时中途退出，也可以在安装目录下重新运行配置程序。 D:\\Program Files\\MySQL\\MySQL Server 5.5\\bin\\MySQLInstanceConfig.exe 配置界面有两个选项：1）Detailed XX 是精确配置；2）Standard XXX是标准配置。这里使用精确配置。然后选择服务类型，从上到下依次是开发机、服务器和专用服务器，占用的内存也依次递增。一般选择开发机即可。 接下来选择数据库类型，1）多功能型数据库；2）事务型数据库；3）非事务型数据库。存储引擎有事务性和非事务性，多功能型数据库在两种存储引擎速度都比较快，事务型数据库在事务型引擎较快，非事务型数据库在非事务型引擎速度较快。一般选择多功能型数据库。下一个界面直接下一步。 接下来配置数据库并发连接数：1）策略式，支持20个连接；2）在线式，允许500个连接；3）自定义，自己设定连接数。一般选择第一个即可。然后是配置端口号，开发中一般需要修改，防止别人恶意攻击。学习使用可先不改。 接下来选择字符集：使用第三个，然后下拉选择utf8。下一个界面中起一个服务名，其中绿线是开机自启，红线是添加环境变量。 接下来，设置root账户密码，同时勾选允许远程连接。然后点击“Execute”执行，等待完成，如下图。 图像化界面安装文件：SQLyog-10.0.0-0.exe 激活码： Name: any key: dd987f34-f358-4894-bd0f-21f3f04be9c1 一路下一步即可。然后新建一个连接，输入刚才设置的密码，连接。 连接成功就完成全部配置。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"DQL语言","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL02-DQL.html","text":"DQL 基础查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182SELECT 要查询的东西 (FROM 表名);# 通过select查询完的结果 ，是一个虚拟的表格，不是真实存在# 要查询的东西 可以是常量值、可以是表达式、可以是字段、可以是函数USE myemployees;# 1.查询表中的单个字段SELECT last_name FROM employees;# 2.查询表中的多个字段SELECT last_name,salary,email FROM employees;# 3.查询表中的所有字段#方式一：SELECT &#96;employee_id&#96;, &#96;first_name&#96;, &#96;last_name&#96;, &#96;phone_number&#96;, &#96;last_name&#96;, &#96;job_id&#96;, &#96;phone_number&#96;, &#96;job_id&#96;, &#96;salary&#96;, &#96;commission_pct&#96;, &#96;manager_id&#96;, &#96;department_id&#96;, &#96;hiredate&#96; FROM employees;# &#96;xxx&#96;: &#96;是着重号，用于区分字段和关键字#方式二： SELECT * FROM employees;# 4.查询常量值SELECT 100;SELECT &#39;john&#39;; # 5.查询表达式SELECT 100%98; # 6.查询函数SELECT VERSION();# 7.起别名# ①便于理解# ②如果要查询的字段有重名的情况，使用别名可以区分开来#方式一：使用asSELECT 100%98 AS 结果;SELECT last_name AS 姓,first_name AS 名 FROM employees;#方式二：使用空格SELECT last_name 姓,first_name 名 FROM employees;# 别名有特殊符号或关键字，则需把别名加上双引号# 案例：查询salary，显示结果为 out putSELECT salary AS &quot;out put&quot; FROM employees;# 8.去重#案例：查询员工表中涉及到的所有的部门编号SELECT DISTINCT department_id FROM employees;# 9. + 号的作用&#x2F;*java中的+号：①运算符，两个操作数都为数值型②连接符，只要有一个操作数为字符串mysql中的+号：仅仅只有一个功能：运算符select 100+90;（190） 两个操作数都为数值型，则做加法运算select &#39;123&#39;+90;（113） 只要其中一方为字符型，试图将字符型数值转换成数值型，如果转换成功，则继续做数值的加法运算select &#39;john+90;（90） 如果转换失败，则将字符型数值转换成0select null+10;（null） 只要其中一方为null，则结果肯定为null*&#x2F;#案例：查询员工名和姓连接成一个字段，并显示为 姓名SELECT CONCAT(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;) AS 结果;SELECT CONCAT(last_name,first_name) AS 姓名FROM employees; 条件查询123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# 语法：select 要查询的字段|表达式|常量值|函数from 表where 条件;# 分类：&#x2F;*一、条件表达式示例：salary&gt;10000条件运算符： &gt; &lt; &gt;&#x3D; &lt;&#x3D; &#x3D; !&#x3D; &lt;&gt;*&#x2F;#案例：查询部门编号不等于90号的员工名和部门编号SELECT last_name, department_idFROM employeesWHERE department_id&lt;&gt;90;&#x2F;*二、逻辑表达式示例：salary&gt;10000 &amp;&amp; salary&lt;20000逻辑运算符： and（&amp;&amp;）:两个条件如果同时成立，结果为true，否则为false or(||)：两个条件只要有一个成立，结果为true，否则为false not(!)：如果条件成立，则not后为false，否则为true*&#x2F;#案例：查询部门编号不是在90到110之间，或者工资高于15000的员工信息SELECT *FROM employeesWHERE NOT(department_id&gt;&#x3D;90 AND department_id&lt;&#x3D;110) OR salary&gt;15000;&#x2F;*三、模糊查询示例：last_name like &#39;a%&#39;通配符： % : 任意多个字符,包含0个字符； _ : 任意单个字符；运算符： like: 一般和通配符搭配使用。 between XXX and XXX: 包含临界值;两个临界值不要调换顺序. in: in列表的值类型必须一致或兼容;in列表中不支持通配符. is null: &#x3D;或&lt;&gt;不能用于判断null值 is not null*&#x2F;#案例：查询员工名中第二个字符为_的员工名SELECT last_nameFROM employeesWHERE last_name LIKE &#39;_$_%&#39; ESCAPE &#39;$&#39;; # 指定$为转义字符，默认转义字符为&quot;\\&quot;#案例：查询员工编号在100到120之间的员工信息SELECT *FROM employeesWHERE employee_id BETWEEN 120 AND 100;#案例：查询员工的工种编号是 IT_PROG、AD_VP、AD_PRES中的一个员工名和工种编号SELECT last_name, job_idFROM employeesWHERE job_id IN( &#39;IT_PROT&#39; ,&#39;AD_VP&#39;,&#39;AD_PRES&#39;);#案例：查询没有奖金的员工名和奖金率SELECT last_name, commission_pctFROM employeesWHERE commission_pct IS NULL; &#x2F;*四、安全等于 &lt;&#x3D;&gt;*&#x2F;#案例：查询没有奖金的员工名和奖金率SELECT last_name, commission_pctFROM employeesWHERE commission_pct &lt;&#x3D;&gt;NULL; #案例：查询工资为12000的员工信息SELECT last_name, salaryFROM employeesWHERE salary &lt;&#x3D;&gt; 12000; # IS NULL 与 &lt;&#x3D;&gt;# IS NULL:仅仅可以判断NULL值，可读性较高，建议使用# &lt;&#x3D;&gt; :既可以判断NULL值，又可以判断普通的数值，可读性较低SELECT ISNULL(commission_pct) FROM employees; 面试题 问：select * from employees; 和 select * from employees where com mission_pct like ‘%%’ and last_name like ‘%%’;结果是否一样？为什么？ 如果判断字段没有null，结果一致；若字段中含有null，则结果不一样！ 排序查询12345678910111213141516171819202122232425262728293031323334353637383940# 语法：select 要查询的东西 # 执行次序：3from 表 # 执行次序：1where 条件 # 执行次序：2order by 排序的字段|表达式|函数|别名 【asc|desc】 # 执行次序：4&#x2F;*1、asc代表的是升序，可以省略，desc代表的是降序2、order by子句可以支持 单个字段、别名、表达式、函数、多个字段3、order by子句在查询语句的最后面，除了limit子句*&#x2F;# 按单个字段排序SELECT * FROM employees ORDER BY salary DESC;# 添加筛选条件再排序# 案例：查询部门编号&gt;&#x3D;90的员工信息，并按员工编号降序SELECT *FROM employeesWHERE department_id&gt;&#x3D;90ORDER BY employee_id DESC;# 按表达式排序# 案例：查询员工信息 按年薪降序SELECT *,salary*12*(1+IFNULL(commission_pct,0))FROM employeesORDER BY salary*12*(1+IFNULL(commission_pct,0)) DESC;# 按函数排序# 案例：查询员工名，并且按名字的长度降序SELECT LENGTH(last_name),last_name FROM employeesORDER BY LENGTH(last_name) DESC;# 按多个字段排序# 案例：查询员工信息，要求先按工资降序，再按employee_id升序SELECT *FROM employeesORDER BY salary DESC,employee_id ASC; 常见函数字符函数 concat 拼接 substr 截取子串 upper 转换成大写 lower 转换成小写 trim 去前后指定的空格和字符 ltrim 去左边空格 rtrim 去右边空格 replace 替换 lpad 左填充 rpad 右填充 instr 返回子串第一次出现的索引，如果找不到就返回0 length 获取字节个数 ```mysqlSHOW VARIABLES LIKE ‘%char%’; # 可以查看编码utf-8 ####### 注意：索引从1开始截取从 指定索引处 后面 所有字符SELECT SUBSTR(‘李莫愁爱上了陆展元’,7) out_put; # 陆展元 截取从 指定索引处 指定字符 长度 的字符SELECT SUBSTR(‘李莫愁爱上了陆展元’,1,3) out_put; # 李莫愁 SELECT LENGTH(TRIM(‘ 张翠山 ‘)) AS out_put; # 9SELECT TRIM(‘aa’ FROM ‘aaaaaaaaa张aaaaaaaaaaaa翠山aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa’) AS out_put; # 只能去除两边的，中间不去 lpad 用指定的字符实现左填充指定长度，第二个参数：填充后的长度 SELECT LPAD(‘殷素素’,2,’*’) AS out_put; # 殷素 rpad 用指定的字符实现右填充指定长度 SELECT RPAD(‘殷素素’,12,’ab’) AS out_put; 12345678910111213141516#### 数学函数- round 四舍五入- rand 随机数- floor 向下取整 - ceil 向上取整 - mod 取余 - runcate 截断- &#96;&#96;&#96;mysql #round 四舍五入 SELECT ROUND(-1.55); SELECT ROUND(1.567,2); # 第二个参数：有效数字 #truncate 截断 SELECT TRUNCATE(1.69999,1); # 小数位数，直接截断 日期函数 now 当前系统日期+时间 curdate 当前系统日期 curtime 当前系统时间 str_to_date 将字符转换成日期 date_format 将日期转换成字符 year 返回当前年 month 返回当前月 day 返回当前天 hour 返回当前小时 minute 返回当前分钟 second 返回当前秒 datediff 返回两个日期的差 monthname 以英文形式返回月份 ```mysql 可以获取指定的部分，年、月、日、小时、分钟、秒SELECT YEAR(NOW()) 年;SELECT YEAR(‘1998-1-1’) 年; SELECT YEAR(hiredate) 年 FROM employees; SELECT MONTH(NOW()) 月;SELECT MONTHNAME(NOW()) 月; str_to_date 将字符通过指定的格式转换成日期SELECT STR_TO_DATE(‘1998-3-2’,’%Y-%c-%d’) AS out_put; 查询入职日期为1992-4-3的员工信息SELECT FROM employees WHERE hiredate = ‘1992-4-3’;SELECT FROM employees WHERE hiredate = STR_TO_DATE(‘4-3 1992’,’%c-%d %Y’); date_format 将日期转换成字符SELECT DATE_FORMAT(NOW(),’%y年%m月%d日’) AS out_put; 查询有奖金的员工名和入职日期(xx月/xx日 xx年)SELECT last_name,DATE_FORMAT(hiredate,’%m月/%d日 %y年’) 入职日期FROM employeesWHERE commission_pct IS NOT NULL; 查询员工表中的最大入职时间和最小入职时间的相差天数 （DIFFRENCE）SELECT MAX(hiredate) 最大,MIN(hiredate) 最小,(MAX(hiredate)-MIN(hiredate))/1000/3600/24 DIFFRENCEFROM employees; SELECT DATEDIFF(MAX(hiredate),MIN(hiredate)) DIFFRENCEFROM employees; SELECT DATEDIFF(‘1995-2-7’,’1995-2-6’); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;img src&#x3D;&quot;http:&#x2F;&#x2F;typora-nmt.oss-cn-qingdao.aliyuncs.com&#x2F;img&#x2F;java&#x2F;image-20201101140050631.png&quot; alt&#x3D;&quot;image-20201101140050631&quot; style&#x3D;&quot;zoom: 70%;&quot; &#x2F;&gt;#### 流程控制函数- if(条件表达时， 表达式1， 表达式2) 处理双分支- case语句 处理多分支 - 情况1：处理等值判断 - 情况2：处理条件判断 &#96;&#96;&#96;mysqlSELECT IF(10&lt;5,&#39;大&#39;,&#39;小&#39;);SELECT last_name,commission_pct,IF(commission_pct IS NULL,&#39;没奖金，呵呵&#39;,&#39;有奖金，嘻嘻&#39;) 备注FROM employees;&#x2F;*java中switch(变量或表达式)&#123; case 常量1：语句1;break; ... default:语句n;break;&#125;mysql中：case 要判断的字段或表达式when 常量1 then 要显示的值1或语句1;when 常量2 then 要显示的值2或语句2;else 要显示的值n或语句n;end*&#x2F;&#x2F;*案例：查询员工的工资，要求部门号&#x3D;30，显示的工资为1.1倍部门号&#x3D;40，显示的工资为1.2倍部门号&#x3D;50，显示的工资为1.3倍其他部门，显示的工资为原工资*&#x2F;SELECT salary 原始工资,department_id,CASE department_idWHEN 30 THEN salary\\*1.1WHEN 40 THEN salary\\*1.2WHEN 50 THEN salary\\*1.3ELSE salaryEND AS 新工资FROM employees;#案例：查询员工的工资的情况如果工资&gt;20000,显示A级别如果工资&gt;15000,显示B级别如果工资&gt;10000，显示C级别否则，显示D级别SELECT salary,CASE WHEN salary&gt;20000 THEN &#39;A&#39;WHEN salary&gt;15000 THEN &#39;B&#39;WHEN salary&gt;10000 THEN &#39;C&#39;ELSE &#39;D&#39;END AS 工资级别FROM employees; 其他函数 version 版本 database 当前库 user 当前连接用户 password(‘字符’) 返回该字符的加密形式 md5(‘字符’) 返回该字符的md5加密形式 分组函数 sum 求和 max 最大值 min 最小值 avg 平均值 count 计数 特点： 1、以上五个分组函数都忽略null值，除了count(*) 2、sum和avg一般用于处理数值型；max、min、count可以处理任何数据类型 3、都可以搭配distinct使用，用于统计去重后的结果 4、count的参数可以支持：字段、*、常量值； count(*) 和count(1)用来结果集的行数；count(字段)。 效率：MYISAM存储引擎下 ，COUNT(*)的效率高INNODB存储引擎下，COUNT(*)和COUNT(1)的效率差不多，比COUNT(字段)要高一些 5、和分组函数一同查询的字段要求是group by后的字段 6、和分组函数一同查询的字段有限制： SELECT AVG(salary),employee_id FROM employees;# “employee_id”无意义 谓词函数 LIKE BETWEEN IS NULL、IS NOT NULL IN EXISTS （链接） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# LIKE谓词 – 用于字符串的部分一致查询。部分一致大体可以分为前方一致、中间一致和后方一致三种类型。SELECT *FROM samplelikeWHERE strcol LIKE &#39;ddd%&#39;; # # %是代表“零个或多个任意字符串”的特殊符号，SELECT *FROM samplelikeWHERE strcol LIKE &#39;abc__&#39;; # _下划线匹配任意 1 个字符# BETWEEN谓词 – 用于范围查询SELECT product_name, sale_priceFROM productWHERE sale_price BETWEEN 100 AND 1000; # BETWEEN 的特点就是结果中会包含 100 和 1000 这两个临界值，也就是闭区间。如果不想让结果中包含临界值，那就必须使用 &lt; 和 &gt;。SELECT product_name, sale_priceFROM productWHERE sale_price &gt; 100AND sale_price &lt; 1000;# IS NULL、 IS NOT NULL – 用于判断是否为NULL# 为了选取出某些值为 NULL 的列的数据，不能使用 &#x3D;，而只能使用特定的谓词IS NULL。SELECT product_name, purchase_priceFROM productWHERE purchase_price IS NULL;SELECT product_name, purchase_priceFROM productWHERE purchase_price IS NOT NULL;# IN谓词 – OR的简便用法# 多个查询条件取并集时可以选择使用or语句。SELECT product_name, purchase_priceFROM productWHERE purchase_price &#x3D; 320OR purchase_price &#x3D; 500OR purchase_price &#x3D; 5000;SELECT product_name, purchase_priceFROM productWHERE purchase_price IN (320, 500, 5000); &#x2F; WHERE purchase_price NOT IN (320, 500, 5000); # 需要注意的是，在使用IN 和 NOT IN 时是无法选取出NULL数据的。实际结果也是如此，上述两组结果中都不包含进货单价为 NULL 的叉子和圆珠笔。 NULL 只能使用 IS NULL 和 IS NOT NULL 来进行判断。# EXIST谓词的使用方法。谓词的作用就是 “判断是否存在满足某种条件的记录”，判断子查询得到的结果集是否是一个空集，如果不是，则返回 True，如果是，则返回 False。SELECT product_name, sale_price FROM product AS p WHERE EXISTS (SELECT * FROM shopproduct AS sp WHERE sp.shop_id &#x3D; &#39;000C&#39; AND sp.product_id &#x3D; p.product_id); 分组查询123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#x2F;*语法：select 查询列表from 表【where 筛选条件】group by 分组的字段【having 分组后的筛选】【order by 排序的字段】;特点：1、和分组函数一同查询的字段必须是group by后出现的字段2、筛选分为两类：分组前筛选和分组后筛选 针对的表 位置 连接的关键字分组前筛选 原始表 group by前 where分组后筛选 group by后的结果集 group by后 having问题1：分组函数做筛选能不能放在where后面答：不能问题2：where——group by——having一般来讲，能用分组前筛选的，尽量使用分组前筛选，提高效率3、分组可以按单个字段也可以按多个字段(多个字段用逗号隔开没有顺序要求)4、可以搭配着排序使用，放在最后*&#x2F;#查询每个工种的员工平均工资SELECT AVG(salary),job_idFROM employeesGROUP BY job_id;#查询有奖金的每个领导手下员工的平均工资SELECT AVG(salary),manager_idFROM employeesWHERE commission_pct IS NOT NULLGROUP BY manager_id;#分组后筛选#案例：查询哪个部门的员工个数&gt;5SELECT COUNT(*),department_idFROM employeesGROUP BY department_idHAVING COUNT(*)&gt;5;#领导编号&gt;102的每个领导手下的最低工资大于5000的领导编号和最低工资SELECT manager_id,MIN(salary)FROM employeesWHERE manager_id&gt;102GROUP BY manager_idHAVING MIN(salary)&gt;5000;#5.按多个字段分组#查询每个工种每个部门的最低工资,并按最低工资降序SELECT MIN(salary),job_id,department_idFROM employeesGROUP BY department_id,job_idORDER BY MIN(salary) DESC; 连接查询123456789101112131415161718192021&#x2F;*含义：又称多表查询，当查询的字段来自于多个表时，就会用到连接查询笛卡尔乘积现象：表1 有m行，表2有n行，结果&#x3D;m*n行发生原因：没有有效的连接条件如何避免：添加有效的连接条件分类： 按年代分类： sql92标准:仅仅支持内连接 sql99标准【推荐】：支持内连接+外连接（左外和右外）+交叉连接 按功能分类： 内连接： 等值连接 非等值连接 自连接 外连接： 左外连接 右外连接 全外连接（mysql不支持） 交叉连接*&#x2F; sql92标准12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#一、sql92标准#1、等值连接&#x2F;*① 多表等值连接的结果为多表的交集部分② n表连接，至少需要n-1个连接条件③ 多表的顺序没有要求④ 一般需要为表起别名⑤ 可以搭配前面介绍的所有子句使用，比如排序、分组、筛选语法格式：select 查询列表from 表1，表2where 等值条件【and 筛选条件】【group by 分组字段】【having 分组后的筛选】【order by 排序字段】*&#x2F;#查询员工名和对应的部门名SELECT last_name,department_nameFROM employees,departmentsWHERE employees.&#96;department_id&#96;&#x3D;departments.&#96;department_id&#96;;&#x2F;* ①提高语句的简洁度 ②区分多个重名的字段注意：如果为表起了别名，则查询的字段就不能使用原来的表名去限定*&#x2F;#查询员工名、工种号、工种名SELECT e.last_name,e.job_id,j.job_titleFROM employees e,jobs jWHERE e.&#96;job_id&#96;&#x3D;j.&#96;job_id&#96;;#案例：查询有奖金的员工名、部门名SELECT last_name,department_name,commission_pctFROM employees e,departments dWHERE e.&#96;department_id&#96;&#x3D;d.&#96;department_id&#96;AND e.&#96;commission_pct&#96; IS NOT NULL;#查询有奖金的每个部门的部门名和部门的领导编号和该部门的最低工资SELECT department_name,d.&#96;manager_id&#96;,MIN(salary)FROM departments d,employees eWHERE d.&#96;department_id&#96;&#x3D;e.&#96;department_id&#96;AND commission_pct IS NOT NULLGROUP BY department_name,d.&#96;manager_id&#96;;#三表连接：查询员工名、部门名和所在的城市SELECT last_name,department_name,cityFROM employees e,departments d,locations lWHERE e.&#96;department_id&#96;&#x3D;d.&#96;department_id&#96;AND d.&#96;location_id&#96;&#x3D;l.&#96;location_id&#96;AND city LIKE &#39;s%&#39;ORDER BY department_name DESC;#2、非等值连接#查询员工的工资和工资级别SELECT salary,grade_levelFROM employees e,job_grades gWHERE salary BETWEEN g.&#96;lowest_sal&#96; AND g.&#96;highest_sal&#96;AND g.&#96;grade_level&#96;&#x3D;&#39;A&#39;;#3、自连接#案例：查询 员工名和上级的名称SELECT e.employee_id,e.last_name,m.employee_id,m.last_nameFROM employees e,employees mWHERE e.&#96;manager_id&#96;&#x3D;m.&#96;employee_id&#96;; sql99语法1234567891011121314151617181920#二、sql99语法&#x2F;*语法： select 查询列表 from 表1 别名 【连接类型】 join 表2 别名 on 连接条件 【where 筛选条件】 【group by 分组】 【having 筛选条件】 【order by 排序列表】 分类：(连接类型)内连接（★）：inner外连接 左外(★):left 【outer】 右外(★)：right 【outer】 全外：full【outer】交叉连接：cross *&#x2F; 内连接： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&#x2F;*语法：select 查询列表from 表1 别名inner join 表2 别名on 连接条件;分类： 等值 非等值 自连接 自然连结(NATURAL JOIN)特点：1 添加排序、分组、筛选2 inner可以省略3 筛选条件放在where后面，连接条件放在on后面，提高分离性，便于阅读4 inner join连接和sql92语法中的等值连接效果是一样的，都是查询多表的交集5 SELECT 子句中的列最好按照 表名.列名 的格式来使用.*&#x2F;#等值连接#查询员工名、部门名SELECT last_name,department_nameFROM departments dINNER JOIN employees eON e.&#96;department_id&#96; &#x3D; d.&#96;department_id&#96;;#查询员工名、部门名、工种名，并按部门名降序（添加三表连接）SELECT last_name,department_name,job_titleFROM employees eINNER JOIN departments d ON e.&#96;department_id&#96;&#x3D;d.&#96;department_id&#96;INNER JOIN jobs j ON e.&#96;job_id&#96; &#x3D; j.&#96;job_id&#96; ORDER BY department_name DESC;#非等值连接#查询员工的工资级别SELECT salary,grade_levelFROM employees eINNER JOIN job_grades gON e.&#96;salary&#96; BETWEEN g.&#96;lowest_sal&#96; AND g.&#96;highest_sal&#96;;#查询工资级别的个数&gt;20的个数，并且按工资级别降序SELECT COUNT(*),grade_levelFROM employees eINNER JOIN job_grades gON e.&#96;salary&#96; BETWEEN g.&#96;lowest_sal&#96; AND g.&#96;highest_sal&#96;GROUP BY grade_levelHAVING COUNT(*)&gt;20ORDER BY grade_level DESC;#自连接#查询员工的名字、上级的名字SELECT e.last_name,m.last_nameFROM employees eJOIN employees mON e.&#96;manager_id&#96;&#x3D; m.&#96;employee_id&#96;;#查询姓名中包含字符k的员工的名字、上级的名字SELECT e.last_name,m.last_nameFROM employees eJOIN employees mON e.&#96;manager_id&#96;&#x3D; m.&#96;employee_id&#96;WHERE e.&#96;last_name&#96; LIKE &#39;%k%&#39;;# 自然连结并不是区别于内连结和外连结的第三种连结, 它其实是内连结的一种特例–当两个表进行自然连结时, 会按照两个表中都包含的列名来进行等值内连结, 此时无需使用 ON 来指定连接条件.# 把两个表的公共列(这里是 product_id, 可以有多个公共列)放在第一列, 然后按照两个表的顺序和表中列的顺序, 将两个表中的其他列都罗列出来。SELECT * FROM shopproduct NATURAL JOIN Product 外连接 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#x2F;*应用场景：用于查询一个表中有，另一个表没有的记录特点：1、外连接的查询结果为主表中的所有记录 如果从表中有和它匹配的，则显示匹配的值 如果从表中没有和它匹配的，则显示null 外连接查询结果&#x3D;内连接结果+主表中有而从表没有的记录2、左外连接，left join左边的是主表 右外连接，right join右边的是主表3、左外和右外交换两个表的顺序，可以实现同样的效果 4、全外连接&#x3D;内连接的结果+表1中有但表2没有的+表2中有但表1没有的*&#x2F;#引入：查询男朋友 不在男神表的的女神名#左外连接SELECT b.*,bo.*FROM boys boLEFT OUTER JOIN beauty bON b.&#96;boyfriend_id&#96; &#x3D; bo.&#96;id&#96;WHERE bo.&#96;id&#96; IS NULL; #查询哪个部门没有员工#左外SELECT d.*,e.employee_idFROM departments dLEFT OUTER JOIN employees eON d.&#96;department_id&#96; &#x3D; e.&#96;department_id&#96;WHERE e.&#96;employee_id&#96; IS NULL;#右外SELECT d.*,e.employee_idFROM employees eRIGHT OUTER JOIN departments dON d.&#96;department_id&#96; &#x3D; e.&#96;department_id&#96;WHERE e.&#96;employee_id&#96; IS NULL;#全外USE girls;SELECT b.*,bo.*FROM beauty bFULL OUTER JOIN boys boON b.&#96;boyfriend_id&#96; &#x3D; bo.id;#交叉连接，实现效果和笛卡尔乘积一致SELECT b.*,bo.*FROM beauty bCROSS JOIN boys bo; #sql92 和 sql99&#x2F;*功能：sql99支持的较多可读性：sql99实现连接条件和筛选条件的分离，可读性较高*&#x2F; 结合 WHERE 子句使用左连结 使用外连结从ShopProduct表和Product表中找出那些在某个商店库存少于50的商品及对应的商店.希望得到如下结果. 注意高压锅和圆珠笔两种商品在所有商店都无货, 所以也应该包括在内。按照”结合WHERE子句使用内连结”的思路, 我们很自然会写出如下代码 12345678910SELECT P.product_id ,P.product_name ,P.sale_price ,SP.shop_id ,SP.shop_name ,SP.quantity FROM Product AS P LEFT OUTER JOIN ShopProduct AS SP ON SP.product_id &#x3D; P.product_id WHERE quantity&lt; 50 然而不幸的是, 得到的却是如下的结果: null值不能比较！ 123456789101112SELECT P.product_id ,P.product_name ,P.sale_price ,SP.shop_id ,SP.shop_name ,SP.quantity FROM Product AS P LEFT OUTER JOIN-- 先筛选quantity&lt;50的商品 (SELECT * FROM ShopProduct WHERE quantity &lt; 50 ) AS SP ON SP.product_id &#x3D; P.product_id 子查询含义： 一条查询语句中又嵌套了另一条完整的select语句，其中被嵌套的select语句，称为子查询或内查询，在外面的查询语句，称为主查询或外查询。 分类： 按子查询出现的位置： select后面：仅仅支持标量子查询 from后面：支持表子查询 where或having后面：标量子查询（单行）、列子查询（多行）、行子查询 exists后面（相关子查询）：表子查询 按结果集的行列数不同： 标量子查询（结果集只有一行一列，单行子查询） 列子查询（结果集只有一列多行，多行子查询） 行子查询（结果集有一行多列，多列多行） 表子查询（结果集一般为多行多列） 特点： 子查询都放在小括号内 子查询一般放在条件的右侧 子查询可以放在from后面、select后面、where后面、having后面、exists后面，但一般放在条件的右侧 子查询优先于主查询执行，主查询使用了子查询的执行结果 子查询搭配： 单行子查询 (标量子查询) 结果集只有一行 一般搭配单行操作符使用：&gt; &lt; = &lt;&gt; &gt;= &lt;= 非法使用子查询的情况：a、子查询的结果为一组值b、子查询的结果为空 多行子查询 (列子查询) 结果集有多行 一般搭配多行操作符使用：any、all、in、not inin： 属于子查询结果中的任意一个就行 any和all往往可以用其他查询代替 子查询就是将用来定义视图的 SELECT 语句直接用于 FROM 子句当中。其中AS studentSum可以看作是子查询的名称，而且由于子查询是一次性的，所以子查询不会像视图那样保存在存储介质中， 而是在 SELECT 语句执行之后就消失了。 标量子查询1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#案例 谁的工资比 Abel 高?SELECT *FROM employeesWHERE salary&gt;( SELECT salary FROM employees WHERE last_name &#x3D; &#39;Abel&#39;);#案例 返回job_id与141号员工相同，salary比143号员工多的员工 姓名，job_id 和工资SELECT last_name,job_id,salaryFROM employeesWHERE job_id &#x3D; ( SELECT job_id FROM employees WHERE employee_id &#x3D; 141) AND salary&gt;( SELECT salary FROM employees WHERE employee_id &#x3D; 143);#案例 查询最低工资大于50号部门最低工资的部门id和其最低工资SELECT MIN(salary),department_idFROM employeesGROUP BY department_idHAVING MIN(salary)&gt;( SELECT MIN(salary) FROM employees WHERE department_id &#x3D; 50);#案例 查询每个部门的员工个数SELECT d.*,( SELECT COUNT(*) FROM employees e WHERE e.department_id &#x3D; d.&#96;department_id&#96; ) 个数 FROM departments d; #案例 查询员工号&#x3D;102的部门名&#x2F;*select后面，仅仅支持标量子查询*&#x2F;SELECT ( SELECT department_name,e.department_id FROM departments d INNER JOIN employees e ON d.department_id&#x3D;e.department_id WHERE e.employee_id&#x3D;102) 部门名; 列子查询（多行子查询）12345678#案例 返回location_id是1400或1700的部门中的所有员工姓名SELECT last_nameFROM employeesWHERE department_id &lt;&gt;ALL( SELECT DISTINCT department_id FROM departments WHERE location_id IN(1400,1700)); 行子查询（结果集一行多列或多行多列）123456789101112131415161718#案例 查询员工编号最小并且工资最高的员工信息#方法1SELECT *FROM employeesWHERE employee_id&#x3D;( SELECT MIN(employee_id) FROM employees)AND salary&#x3D;( SELECT MAX(salary) FROM employees);#方法2SELECT * FROM employeesWHERE (employee_id,salary)&#x3D;( SELECT MIN(employee_id),MAX(salary) FROM employees); 表子查询1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&#x2F;*from后面将子查询结果充当一张表，要求必须起别名*&#x2F;#案例：查询每个部门的平均工资的工资等级SELECT ag_dep.*,g.&#96;grade_level&#96;FROM ( SELECT AVG(salary) ag,department_id FROM employees GROUP BY department_id) ag_depINNER JOIN job_grades gON ag_dep.ag BETWEEN lowest_sal AND highest_sal;#######################################exists关键字：语法：exists(完整的查询语句)结果：1或0########################################案例 查询有员工的部门名#inSELECT department_nameFROM departments dWHERE d.&#96;department_id&#96; IN( SELECT department_id FROM employees);#existsSELECT department_nameFROM departments dWHERE EXISTS( SELECT * FROM employees e WHERE d.&#96;department_id&#96;&#x3D;e.&#96;department_id&#96;);#案例 查询没有女朋友的男神信息#inSELECT bo.*FROM boys boWHERE bo.id NOT IN( SELECT boyfriend_id FROM beauty);#existsSELECT bo.*FROM boys boWHERE NOT EXISTS( SELECT boyfriend_id FROM beauty b WHERE bo.&#96;id&#96;&#x3D;b.&#96;boyfriend_id&#96;); 关联子查询关联子查询就是通过一些标志将内外两层的查询连接起来起到过滤数据的目的. 123456SELECT product_type, product_name, sale_price FROM product AS p1 WHERE sale_price &gt; (SELECT AVG(sale_price) FROM product AS p2 WHERE p1.product_type &#x3D; p2.product_type GROUP BY product_type); 关联子查询博客 首先执行不带WHERE的主查询 从主查询的product _type先取第一个值=‘衣服’，通过WHERE P1.product_type = P2.product_type传入子查询， 从子查询得到的结果AVG(sale_price)=2500，返回主查询 将子查询结果再与主查询结合执行完整的SQL语句 product _type取第二个值，得到整个语句的第二结果，依次类推 分页查询123456789101112131415161718192021222324252627282930313233343536373839&#x2F;*应用场景：当要显示的数据，一页显示不全，需要分页提交sql请求语法： select 查询列表 from 表 【join type join 表2 on 连接条件 where 筛选条件 group by 分组字段 having 分组后的筛选 order by 排序的字段】 limit 【offset,】size; offset要显示条目的起始索引（起始索引从0开始） size 要显示的条目个数特点： ①limit语句放在查询语句的最后 ②公式 要显示的页数 page，每页的条目数size select 查询列表 from 表 limit (page-1)*size,size; size&#x3D;10 page 1 0 2 10 3 20*&#x2F;#案例 查询第11条——第25条SELECT * FROM employees LIMIT 10,15;#案例 有奖金的员工信息，并且工资较高的前10名显示出来SELECT * FROM employees WHERE commission_pct IS NOT NULL ORDER BY salary DESC LIMIT 10; 联合查询1234567891011121314151617181920212223242526272829&#x2F;*union：将多条查询语句的结果合并成一个结果语法： select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】 select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】 select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】 ..... select 字段|常量|表达式|函数 【from 表】 【where 条件】应用场景：要查询的结果来自于多个表，且多个表没有直接的连接关系，但查询的信息一致时。对于同一个表的两个不同的筛选结果集, 使用 UNION 对两个结果集取并集, 和把两个子查询的筛选条件用 OR 谓词连接, 会得到相同的结果, 但倘若要将两个不同的表中的结果合并在一起, 就不得不使用 UNION 了.而且, 即便是对于同一张表, 有时也会出于查询效率方面的因素来使用 UNION.特点：★1、要求多条查询语句的查询列数是一致的！2、要求多条查询语句的查询的每一列的类型和顺序最好一致3、union关键字默认去重，如果使用union all 可以包含重复项*&#x2F;# 引入的案例：查询部门编号&gt;90或邮箱包含a的员工信息SELECT * FROM employees WHERE email LIKE &#39;%a%&#39;UNIONSELECT * FROM employees WHERE department_id&gt;90;#案例 查询中国用户中男性的信息以及外国用户中年男性的用户信息SELECT id,cname FROM t_ca WHERE csex&#x3D;&#39;男&#39;UNION ALLSELECT t_id,tname FROM t_ua WHERE tGender&#x3D;&#39;male&#39;;","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"DML语言","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL03-DML.html","text":"DML语言数据操作语言： 插入：insert 修改：update 删除：delete 插入经典的插入 12345678910111213141516171819202122232425262728&#x2F;*语法：insert into 表名(列名,...) values(值1,...);*&#x2F;#1.插入的值的类型要与列的类型一致或兼容INSERT INTO beauty(id,NAME,sex,borndate,phone,photo,boyfriend_id)VALUES(13,&#39;唐艺昕&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2);#2.不可以为null的列必须插入值。可以为null的列如何插入值？#方式一：INSERT INTO beauty(id,NAME,sex,borndate,phone,photo,boyfriend_id)VALUES(13,&#39;唐艺昕&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2);#方式二：INSERT INTO beauty(id,NAME,sex,phone)VALUES(15,&#39;娜扎&#39;,&#39;女&#39;,&#39;1388888888&#39;);#3.列的顺序是否可以调换，对应即可INSERT INTO beauty(NAME,sex,id,phone)VALUES(&#39;蒋欣&#39;,&#39;女&#39;,16,&#39;110&#39;);#4.列数和值的个数必须一致INSERT INTO beauty(NAME,sex,id,phone)VALUES(&#39;关晓彤&#39;,&#39;女&#39;,17,&#39;110&#39;);#5.可以省略列名，默认所有列，而且列的顺序和表中列的顺序一致INSERT INTO beautyVALUES(18,&#39;张飞&#39;,&#39;男&#39;,NULL,&#39;119&#39;,NULL,NULL); 方式二：插入 1234567&#x2F;*语法：insert into 表名set 列名&#x3D;值,列名&#x3D;值,...*&#x2F;INSERT INTO beautySET id&#x3D;19,NAME&#x3D;&#39;刘涛&#39;,phone&#x3D;&#39;999&#39;; 两种方式比较： 12345678910111213#1、方式一支持插入多行,方式二不支持INSERT INTO beautyVALUES(23,&#39;唐艺昕1&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2),(24,&#39;唐艺昕2&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2),(25,&#39;唐艺昕3&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2);#2、方式一支持子查询，方式二不支持INSERT INTO beauty(id,NAME,phone)SELECT 26,&#39;宋茜&#39;,&#39;11809866&#39;; # 将查询后的结果插入到表中INSERT INTO beauty(id,NAME,phone)SELECT id,boyname,&#39;1234567&#39;FROM boys WHERE id&lt;3; 修改12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#x2F;*1.修改单表的记录★语法：update 表名set 列&#x3D;新值,列&#x3D;新值,...where 筛选条件;2.修改多表的记录【补充】语法：sql92语法：update 表1 别名,表2 别名set 列&#x3D;值,...where 连接条件and 筛选条件;sql99语法：update 表1 别名inner|left|right join 表2 别名on 连接条件set 列&#x3D;值,...where 筛选条件;*&#x2F;#1.修改单表的记录#案例 修改beauty表中姓唐的女神的电话为13899888899UPDATE beauty SET phone &#x3D; &#39;13899888899&#39;WHERE NAME LIKE &#39;唐%&#39;;#案例 修改boys表中id好为2的名称为张飞，魅力值 10UPDATE boys SET boyname&#x3D;&#39;张飞&#39;,usercp&#x3D;10WHERE id&#x3D;2;#2.修改多表的记录#案例 修改张无忌的女朋友的手机号为114UPDATE boys boINNER JOIN beauty b ON bo.&#96;id&#96;&#x3D;b.&#96;boyfriend_id&#96;SET b.&#96;phone&#96;&#x3D;&#39;119&#39;,bo.&#96;userCP&#96;&#x3D;1000WHERE bo.&#96;boyName&#96;&#x3D;&#39;张无忌&#39;;#案例 修改没有男朋友的女神的男朋友编号都为2号UPDATE boys boRIGHT JOIN beauty b ON bo.&#96;id&#96;&#x3D;b.&#96;boyfriend_id&#96;SET b.&#96;boyfriend_id&#96;&#x3D;2WHERE bo.&#96;id&#96; IS NULL; 删除1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&#x2F;*方式一：delete语法：1、单表的删除【★】delete from 表名 where 筛选条件2、多表的删除【补充】2.1 sql92语法：delete 表1的别名,表2的别名from 表1 别名,表2 别名where 连接条件and 筛选条件;2.2 sql99语法：delete 表1的别名,表2的别名from 表1 别名inner|left|right join 表2 别名 on 连接条件where 筛选条件;方式二：truncate语法：truncate table 表名; # 删除整个表*&#x2F;#方式一：delete#1.单表的删除#案例：删除手机号以9结尾的女神信息DELETE FROM beauty WHERE phone LIKE &#39;%9&#39;;#2.多表的删除#案例：删除张无忌的女朋友的信息DELETE bFROM beauty bINNER JOIN boys bo ON b.&#96;boyfriend_id&#96; &#x3D; bo.&#96;id&#96;WHERE bo.&#96;boyName&#96;&#x3D;&#39;张无忌&#39;;#案例：删除黄晓明的信息以及他女朋友的信息DELETE b,boFROM beauty bINNER JOIN boys bo ON b.&#96;boyfriend_id&#96;&#x3D;bo.&#96;id&#96;WHERE bo.&#96;boyName&#96;&#x3D;&#39;黄晓明&#39;;#方式二：truncate语句#案例：将魅力值&gt;100的男神信息删除TRUNCATE TABLE boys ;# delete 和 truncate【面试题★】&#x2F;*1.delete 可以加where 条件，truncate不能加2.truncate删除，效率高一丢丢3.假如要删除的表中有自增长列， 如果用delete删除后，再插入数据，自增长列的值从断点开始， 而truncate删除后，再插入数据，自增长列的值从1开始。4.truncate删除没有返回值，delete删除有返回值5.truncate删除不能回滚，delete删除可以回滚. 相比drop&#96;&#96;&#x2F;&#96;&#96;delete，truncate用来清除数据时，速度最快。*&#x2F;DELETE FROM boys;TRUNCATE TABLE boys;INSERT INTO boys (boyname,usercp)VALUES(&#39;张飞&#39;,100),(&#39;刘备&#39;,100),(&#39;关云长&#39;,100);","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"MySQL概述","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL01-概述.html","text":"数据库的相关概念数据库的好处： 持久化数据到本地 可以实现结构化查询，方便管理 数据库相关概念 DB (database)：数据库，保存一组有组织的数据的容器 DBMS (Database Management System)：数据库管理系统，又称为数据库软件（产品），用于管理DB中的数据 DBMS分为两类： 基于共享文件系统的DBMS （Access） 客户机——服务器的DBMS（MySQL、Oracle、SqlServer） SQL (Structure Query Language)：结构化查询语言，用于和DBMS通信的语言 SQL的优点： 不是某个特定数据库供应商专有的语言，几乎所有DBMS都支持SQL 简单易学 虽然简单，但实际上是一种强有力的语言，灵活使用其语言元素，可以进行非常复杂和高级的数据库操作。 数据库存储数据的特点 将数据放到表中，表再放到库中 一个数据库中可以有多个表，每个表都有一个的名字，用来标识自己。表名具有唯一性。 表具有一些特性，这些特性定义了数据在表中如何存储，类似java中 “类”的设计。 表由列组成，我们也称为字段。所有表都是由一个或多个列组成的，每一列类似java 中的”属性” 表中的数据是按行存储的，每一行类似于java中的“对象”。 初识MySQL配置文件路径：C:\\Program Files\\MySQL\\MySQL Server 5.5\\my.ini [mysql] ：客户端配置 [mysqld]：服务端配置 端口号：port=3306 安装目录：basedir=”C:/Program Files/MySQL/MySQL Server 5.5/“ 文件目录：datadir=”C:/ProgramData/MySQL/MySQL Server 5.5/Data/“ 字符集：character-set-server=utf8 存储引擎：default-storage-engine=INNODB 最大连接数：max_connections=100 启动、终止、登录、退出启动1：右击计算机—管理—服务—启动或停止MySQL服务 启动2：net start mysql服务名 停止：net stop mysql服务名 登录：mysql –h 主机名 -P 端口 –u用户名 –p密码 (-p和密码不能有空格) 退出：exit，Ctrl+C MySQL使用语法规范 不区分大小写，但建议关键字大写，表名、列名小写； 每句话用 ; 或 \\g 结尾； 各子句一般分行写； 关键字不能缩写也不能分行； 用缩进提高语句的可读性。 单行注释：#注释文字 单行注释：— 注释文字(—后有空格) 多行注释：/ 注释文字 / mysql常用命令 1234567891011121314151617181920show databases; # 查看mysql中有哪些个数据库use 数据库名称; # 打开指定的库create database 数据库名; # 新建一个数据库create table 表名( 列名 列类型, 列名 列类型， ...); # 创建一个表 show tables; # 查看指定的数据库中有哪些数据表show tables from 数据库名称; # 查看其它库的所有表desc 表名; # 查看表的结构drop table 表名; # 删除表select database(); # 查看在哪个库select * from 表名; # 查看有哪些数据select version(); # 登录到mysql服务端，查看服务器的版本mysql --version 或 mysql --V # 没有登录到mysql服务端，查看服务器的版本 SQL的语言分类DML（Data Manipulation Language)：数据操纵语句，用于添加、删除、修改、查询数据库记录，并检查数据完整性 DDL（Data Definition Language)：数据定义语句，用于库和表的创建、修改、删除。 DCL（Data Control Language)：数据控制语句，用于定义用户的访问权限和安全级别。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"DDL语言","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL04-DDL.html","text":"DDL语言库和表的管理 一、库的管理：创建、修改、删除 二、表的管理：创建、修改、删除 创建： create；修改： alter；删除： drop。 库的管理 123456789101112131415#1、库的创建&#x2F;*语法：create database [if not exists] 库名;*&#x2F;#案例：创建库BooksCREATE DATABASE IF NOT EXISTS books ;#2、库的修改RENAME DATABASE books TO 新库名;#更改库的字符集ALTER DATABASE books CHARACTER SET gbk;#3、库的删除DROP DATABASE IF EXISTS books; 表的管理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#1.表的创建 ★&#x2F;*语法：create table 表名( 列名 列的类型【(长度) 约束】, 列名 列的类型【(长度) 约束】, 列名 列的类型【(长度) 约束】, ... 列名 列的类型【(长度) 约束】)*&#x2F;#案例：创建表BookCREATE TABLE book( id INT,#编号 bName VARCHAR(20),#图书名 price DOUBLE,#价格 authorId INT,#作者编号 publishDate DATETIME#出版日期);#案例：创建表authorCREATE TABLE IF NOT EXISTS author( id INT, au_name VARCHAR(20), nation VARCHAR(10))#2.表的修改&#x2F;*语法alter table 表名 add|drop|modify|change column 列名 【列类型 约束】;*&#x2F;#①修改列名ALTER TABLE book CHANGE COLUMN publishdate pubDate DATETIME;#②修改列的类型或约束ALTER TABLE book MODIFY COLUMN pubdate TIMESTAMP;#③添加新列ALTER TABLE author ADD COLUMN annual DOUBLE; #④删除列ALTER TABLE book_author DROP COLUMN annual;#⑤修改表名ALTER TABLE author RENAME TO book_author;#3.表的删除DROP TABLE IF EXISTS book_author;SHOW TABLES;#通用的写法：DROP DATABASE IF EXISTS 旧库名;CREATE DATABASE 新库名;DROP TABLE IF EXISTS 旧表名;CREATE TABLE 表名();#4.表的复制INSERT INTO author VALUES(1,&#39;村上春树&#39;,&#39;日本&#39;),(2,&#39;莫言&#39;,&#39;中国&#39;),(3,&#39;冯唐&#39;,&#39;中国&#39;),(4,&#39;金庸&#39;,&#39;中国&#39;);#1.仅仅复制表的结构，没有数据CREATE TABLE copy LIKE author;#2.复制表的结构+数据CREATE TABLE copy2 SELECT * FROM author;#只复制部分数据CREATE TABLE copy3SELECT id,au_nameFROM author WHERE nation&#x3D;&#39;中国&#39;;#仅仅复制某些字段，不复制数据CREATE TABLE copy4 SELECT id,au_nameFROM authorWHERE 0; MySQL数据类型 数值型： 整型 小数：定点数、浮点数 字符型： 较短的文本：char、varchar 较长的文本：text、blob（较长的二进制数据） 日期型： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 一、整型&#x2F;*分类：tinyint、smallint、mediumint、int&#x2F;integer、bigint 1 2 3 4 8特点：① 如果不设置无符号还是有符号，默认是有符号，如果想设置无符号，需要添加unsigned关键字② 如果插入的数值超出了整型的范围,会报out of range异常，然后会插入临界值③ 如果不设置长度，会有默认的长度，长度代表了显示的默认宽度，如果不够会用0在左边填充，但必须搭配zerofill使用，强制为无符号整型！如果显示长度大于该值，但不会截断。*&#x2F;# 1.如何设置无符号和有符号DROP TABLE IF EXISTS tab_int;CREATE TABLE tab_int( t1 INT(7) ZEROFILL, t2 INT(7) ZEROFILL );DESC tab_int;INSERT INTO tab_int VALUES(-123456);INSERT INTO tab_int VALUES(-123456,-123456);INSERT INTO tab_int VALUES(2147483648,4294967296);INSERT INTO tab_int VALUES(123,123);SELECT * FROM tab_int;# 二、小数&#x2F;*分类：1.浮点型 float(M,D) double(M,D)2.定点型 dec(M，D)或decimal(M,D)特点：① M：整数部位+小数部位 D：小数部位 如果超过范围，则插入临界值② M和D都可以省略 如果是decimal，则M默认为10，D默认为0 如果是float和double，则会根据插入的数值的精度来决定精度③ 定点型的精确度较高，如果要求插入数值的精度较高如货币运算等则考虑使用*&#x2F;#测试M和DDROP TABLE tab_float;CREATE TABLE tab_float( f1 FLOAT, f2 DOUBLE, f3 DECIMAL);SELECT * FROM tab_float;DESC tab_float;INSERT INTO tab_float VALUES(123.4523,123.4523,123.4523);INSERT INTO tab_float VALUES(123.456,123.456,123.456);INSERT INTO tab_float VALUES(123.4,123.4,123.4);INSERT INTO tab_float VALUES(1523.4,1523.4,1523.4);#原则：&#x2F;*所选择的类型越简单越好，能保存数值的类型越小越好*&#x2F; 123456789101112131415161718192021222324252627282930313233343536#三、字符型&#x2F;*较短的文本： char varchar其他： binary和varbinary 用于保存较短的二进制 enum 用于保存枚举 set 用于保存集合较长的文本： text blob(较大的二进制)特点： 写法 M的意思 特点 空间的耗费 效率char char(M) 最大的字符数，可以省略，默认为1 固定长度的字符 比较耗费 高varchar varchar(M) 最大的字符数，不可以省略 可变长度的字符 比较节省 低*&#x2F;CREATE TABLE tab_char( c1 ENUM(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;));INSERT INTO tab_char VALUES(&#39;a&#39;);INSERT INTO tab_char VALUES(&#39;b&#39;);INSERT INTO tab_char VALUES(&#39;c&#39;);INSERT INTO tab_char VALUES(&#39;m&#39;); # 失败,内容为空INSERT INTO tab_char VALUES(&#39;A&#39;);SELECT * FROM tab_set;CREATE TABLE tab_set( s1 SET(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;));INSERT INTO tab_set VALUES(&#39;a&#39;);INSERT INTO tab_set VALUES(&#39;A,B&#39;);INSERT INTO tab_set VALUES(&#39;a,c,d&#39;); 12345678910111213141516171819202122# 四、日期型&#x2F;*分类： date 只保存日期 time 只保存时间 year 只保存年 datetime 保存日期+时间 timestamp 保存日期+时间特点： 字节 范围 时区等的影响datetime 8 1000——9999 不受timestamp 4 1970-2038 受*&#x2F;CREATE TABLE tab_date( t1 DATETIME, t2 TIMESTAMP # 当前时区时间);INSERT INTO tab_date VALUES(NOW(),NOW());SELECT * FROM tab_date;SHOW VARIABLES LIKE &#39;time_zone&#39;;SET time_zone&#x3D;&#39;+9:00&#39;; 常见约束含义：一种限制，用于限制表中的数据，为了保证表中的数据的准确和可靠性 分类：六大约束 NOT NULL：非空，用于保证该字段的值不能为空，比如姓名、学号等 DEFAULT：默认，用于保证该字段有默认值，比如性别 PRIMARY KEY：主键，用于保证该字段的值具有唯一性，并且非空，比如学号、员工编号等 UNIQUE：唯一，用于保证该字段的值具有唯一性，可以为空，比如座位号 CHECK：检查约束【mysql中不支持】，比如年龄、性别 FOREIGN KEY：外键，用于限制两个表的关系，用于保证该字段的值必须来自于主表的关联列的值。在从表添加外键约束，用于引用主表中某列的值，比如学生表的专业编号，员工表的部门编号，员工表的工种编号。 添加约束的时机：1.创建表时；2.修改表时 约束的添加分类： 列级约束：六大约束语法上都支持，但外键约束没有效果 表级约束：除了非空、默认，其他的都支持 ```mysqlCREATE TABLE 表名( 字段名 字段类型 列级约束, 字段名 字段类型, 表级约束 ); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133**主键和唯一的对比：**&gt; ​ 保证唯一性 是否允许为空 一个表中可以有多少个 是否允许组合&gt;&gt; 主键 √ × 至多有1个 √，但不推荐&gt;&gt; 唯一 √ √ 可以有多个 √，但不推荐&gt;外键：1. 要求在从表设置外键关系2. 从表的外键列的类型和主表的关联列的类型要求一致或兼容，名称无要求3. &#x3D;&#x3D;主表的关联列必须是一个key（一般是主键或唯一）&#x3D;&#x3D;4. 插入数据时，先插入主表，再插入从表；删除数据时，先删除从表，再删除主表&#96;&#96;&#96;mysql#一、创建表时添加约束#1.添加列级约束&#x2F;*语法：直接在字段名和类型后面追加 约束类型即可。只支持：默认、非空、主键、唯一*&#x2F;CREATE TABLE major( id INT PRIMARY KEY, majorName VARCHAR(20));USE students;DROP TABLE stuinfo;CREATE TABLE stuinfo( id INT PRIMARY KEY,#主键 stuName VARCHAR(20) NOT NULL UNIQUE,#非空 gender CHAR(1) CHECK(gender&#x3D;&#39;男&#39; OR gender &#x3D;&#39;女&#39;),#检查 seat INT UNIQUE,#唯一 age INT DEFAULT 18,#默认约束 majorId INT REFERENCES major(id)#外键);#查看stuinfo中的所有索引，包括主键、外键、唯一SHOW INDEX FROM stuinfo;#2.添加表级约束&#x2F;*语法：在各个字段的最下面 【constraint 约束名】 约束类型(字段名) *&#x2F;DROP TABLE IF EXISTS stuinfo;CREATE TABLE stuinfo( id INT, stuname VARCHAR(20), gender CHAR(1), seat INT, age INT, majorid INT, CONSTRAINT pk PRIMARY KEY(id),#主键 CONSTRAINT uq UNIQUE(seat),#唯一键 CONSTRAINT ck CHECK(gender &#x3D;&#39;男&#39; OR gender &#x3D; &#39;女&#39;),#检查 CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id)#外键);SHOW INDEX FROM stuinfo;# 通用的写法：★CREATE TABLE IF NOT EXISTS stuinfo( id INT PRIMARY KEY, stuname VARCHAR(20) NOT NULL, sex CHAR(1), age INT DEFAULT 18, seat INT UNIQUE, majorid INT, CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id));# 二、修改表时添加约束&#x2F;*1、添加列级约束alter table 表名 modify column 字段名 字段类型 新约束;2、添加表级约束alter table 表名 add 【constraint 约束名】 约束类型(字段名) 【外键的引用】;*&#x2F;DROP TABLE IF EXISTS stuinfo;CREATE TABLE stuinfo( id INT, stuname VARCHAR(20), gender CHAR(1), seat INT, age INT, majorid INT)DESC stuinfo;#1.添加非空约束ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NOT NULL;#2.添加默认约束ALTER TABLE stuinfo MODIFY COLUMN age INT DEFAULT 18;#3.添加主键#①列级约束ALTER TABLE stuinfo MODIFY COLUMN id INT PRIMARY KEY;#②表级约束ALTER TABLE stuinfo ADD PRIMARY KEY(id);#4.添加唯一#①列级约束ALTER TABLE stuinfo MODIFY COLUMN seat INT UNIQUE;#②表级约束ALTER TABLE stuinfo ADD UNIQUE(seat);#5.添加外键ALTER TABLE stuinfo ADD CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id); #三、修改表时删除约束#1.删除非空约束ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NULL;#2.删除默认约束ALTER TABLE stuinfo MODIFY COLUMN age INT ;#3.删除主键ALTER TABLE stuinfo DROP PRIMARY KEY;#4.删除唯一ALTER TABLE stuinfo DROP INDEX seat;#5.删除外键ALTER TABLE stuinfo DROP FOREIGN KEY fk_stuinfo_major;SHOW INDEX FROM stuinfo; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 二、修改表时添加约束&#x2F;*1、添加列级约束alter table 表名 modify column 字段名 字段类型 新约束;2、添加表级约束alter table 表名 add 【constraint 约束名】 约束类型(字段名) 【外键的引用】;*&#x2F;DROP TABLE IF EXISTS stuinfo;CREATE TABLE stuinfo( id INT, stuname VARCHAR(20), gender CHAR(1), seat INT, age INT, majorid INT)DESC stuinfo;#1.添加非空约束ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NOT NULL;#2.添加默认约束ALTER TABLE stuinfo MODIFY COLUMN age INT DEFAULT 18;#3.添加主键#①列级约束写法ALTER TABLE stuinfo MODIFY COLUMN id INT PRIMARY KEY;#②表级约束写法ALTER TABLE stuinfo ADD PRIMARY KEY(id);#4.添加唯一#①列级约束ALTER TABLE stuinfo MODIFY COLUMN seat INT UNIQUE;#②表级约束ALTER TABLE stuinfo ADD UNIQUE(seat);#5.添加外键ALTER TABLE stuinfo ADD CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id); #三、修改表时删除约束#1.删除非空约束ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NULL;#2.删除默认约束ALTER TABLE stuinfo MODIFY COLUMN age INT ;#3.删除主键ALTER TABLE stuinfo DROP PRIMARY KEY;#4.删除唯一ALTER TABLE stuinfo DROP INDEX seat;#5.删除外键ALTER TABLE stuinfo DROP FOREIGN KEY fk_stuinfo_major;SHOW INDEX FROM stuinfo;&#x2F;* 位置 支持的约束类型 是否可以起约束名列级约束： 列的后面 语法都支持，但外键没有效果 不可以表级约束： 所有列的下面 默认和非空不支持，其他支持 可以（主键没有效果）*&#x2F; 标识列又称为自增长列，含义：可以不用手动的插入值，系统提供默认的序列值。 特点： 标识列必须和主键搭配吗？不一定，但要求是一个key 一个表可以有几个标识列？至多一个！ 标识列的类型只能是数值型 标识列可以通过 SET auto_increment_increment=3, 设置步长。可以通过手动插入第一个值，设置起始值. 12345678910111213141516# 创建表时设置标识列DROP TABLE IF EXISTS tab_identity;CREATE TABLE tab_identity( id INT , NAME FLOAT UNIQUE AUTO_INCREMENT, seat INT );TRUNCATE TABLE tab_identity;INSERT INTO tab_identity(id,NAME) VALUES(NULL,&#39;john&#39;);INSERT INTO tab_identity(NAME) VALUES(&#39;lucy&#39;);SELECT * FROM tab_identity;SHOW VARIABLES LIKE &#39;%auto_increment%&#39;;SET auto_increment_increment&#x3D;3;","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"视图&存储过程&流程控制","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL06-视图&存储过程&流程控制.html","text":"视图视图：MySQL从5.0.1版本开始提供视图功能。一种虚拟存在的表，行和列的数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的，只保存了sql逻辑，不保存查询结果。 《sql础教程第2版》用一句话非常凝练的概括了视图与表的区别—“==是否保存了实际的数据==”。所以视图并不是数据库真实存储的数据表，它可以看作是一个窗口，通过这个窗口我们可以看到数据库表中真实存在的数据。所以我们要区别视图和数据表的本质，即视图是基于真实表的一张虚拟的表，其数据来源均建立在真实表的基础上。 那既然已经有数据表了，为什么还需要视图呢？主要有以下几点原因： 通过定义视图可以将频繁使用的SELECT语句保存以提高效率。 通过定义视图可以使用户看到的数据更加清晰。 通过定义视图可以不对外公开数据表全部字段，增强数据的保密性。 通过定义视图可以降低数据的冗余。 需要注意的是视图名在数据库中需要是唯一的，不能与其他视图和表重名。视图不仅可以基于真实表，我们也可以在视图的基础上继续创建视图。 创建视图123456789101112131415161718192021222324252627282930313233343536373839404142434445&#x2F;*语法：create view 视图名as查询语句;*&#x2F;USE myemployees;#查询姓名中包含a字符的员工名、部门名和工种信息#①创建CREATE VIEW myv1ASSELECT last_name,department_name,job_titleFROM employees eJOIN departments d ON e.department_id &#x3D; d.department_idJOIN jobs j ON j.job_id &#x3D; e.job_id;#②使用SELECT * FROM myv1 WHERE last_name LIKE &#39;%a%&#39;;#查询各部门的平均工资级别#①创建视图查看每个部门的平均工资CREATE VIEW myv2ASSELECT AVG(salary) ag,department_idFROM employeesGROUP BY department_id;#②使用SELECT myv2.&#96;ag&#96;,g.grade_levelFROM myv2JOIN job_grades gON myv2.&#96;ag&#96; BETWEEN g.&#96;lowest_sal&#96; AND g.&#96;highest_sal&#96;;#查询平均工资最低的部门信息SELECT * FROM myv2 ORDER BY ag LIMIT 1;#查询平均工资最低的部门名和工资CREATE VIEW myv3ASSELECT * FROM myv2 ORDER BY ag LIMIT 1;SELECT d.*,m.agFROM myv3 mJOIN departments dON m.&#96;department_id&#96;&#x3D;d.&#96;department_id&#96;; 视图修改、删除、查看123456789101112131415161718192021222324252627282930313233343536#方式一：&#x2F;*create or replace view 视图名as查询语句;*&#x2F;SELECT * FROM myv3 CREATE OR REPLACE VIEW myv3ASSELECT AVG(salary),job_idFROM employeesGROUP BY job_id;#方式二：&#x2F;*语法：alter view 视图名as 查询语句;*&#x2F;ALTER VIEW myv3ASSELECT * FROM employees;# 删除视图&#x2F;*语法：drop view 视图名,视图名,...;*&#x2F;DROP VIEW emp_v1,emp_v2,myv3;# 查看视图DESC myv3;SHOW CREATE VIEW myv3; 视图的更新因为视图是一个虚拟表，所以对视图的操作就是对底层基础表的操作，所以在修改时只有满足底层基本表的定义才能成功修改。 对于一个视图来说，如果包含以下结构的任意一种都是不可以被更新的： 聚合函数 SUM()、MIN()、MAX()、COUNT() 等。 DISTINCT 关键字。 GROUP BY 子句。 HAVING 子句。 UNION 或 UNION ALL 运算符。 FROM 子句中包含多个表。 1234567891011121314151617181920212223CREATE OR REPLACE VIEW myv1ASSELECT last_name,emailFROM employees;#插入、会影响原始表INSERT INTO myv1 VALUES(&#39;张飞&#39;,&#39;zf@qq.com&#39;);#修改、会影响原始表UPDATE myv1 SET last_name &#x3D; &#39;张无忌&#39; WHERE last_name&#x3D;&#39;张飞&#39;;#删除、会影响原始表DELETE FROM myv1 WHERE last_name &#x3D; &#39;张无忌&#39;;&#x2F;*视图的可更新性和视图中查询的定义有关系，以下类型的视图是不能更新的。• 包含以下关键字的sql语句：分组函数、distinct、group by、having、union或者union all• 常量视图• Select中包含子查询• join• from一个不能更新的视图• where子句的子查询引用了from子句中的表*&#x2F; ​ 创建语法的关键字 是否实际占用物理空间 使用 视图 create view 只是保存了sql逻辑 增删改查，只是一般不能增删改 表 create table 保存了数据 增删改查 变量、存储过程和函数变量系统变量：全局变量、会话变量 自定义变量：用户变量、局部变量 系统变量：说明：变量由系统定义，不是用户定义，属于服务器层面；注意：全局变量需要添加global关键字，会话变量需要添加session关键字，如果不写，==默认会话级别==；全局变量作用域：针对于所有会话（连接）有效，但不能跨重启；会话变量作用域：针对于当前会话（连接）有效。 1、查看所有系统变量show global|【session】variables; 2、查看满足条件的部分系统变量show global|【session】 variables like ‘%char%’; 3、查看指定的系统变量的值select @@global|【session】系统变量名; 4、为某个系统变量赋值；方式一：set global|【session】系统变量名=值；方式二：set @@global|【session】系统变量名=值; 自定义变量：说明：变量由用户自定义，而不是系统提供的；使用步骤：1、声明2、赋值3、使用（查看、比较、运算等）； 用户变量：用户变量作用域：针对于当前会话（连接）有效，作用域同于会话变量。 赋值操作符：=或 := ①声明并初始化: SET @变量名=值;SET @变量名:=值;SELECT @变量名:=值; ②赋值（更新变量的值） 方式一：SET @变量名=值; SET @变量名:=值; SELECT @变量名:=值; 方式二：SELECT 字段 INTO @变量名 FROM 表; ③使用（查看变量的值）：SELECT @变量名; 局部变量：作用域：仅仅在定义它的begin end块中有效，应用==在 begin end中的第一句话== ①声明：DECLARE 变量名 类型； DECLARE 变量名 类型 【DEFAULT 值】; ②赋值（更新变量的值） 方式一：SET 局部变量名=值;SET 局部变量名:=值; SELECT 局部变量名:=值; 方式二：SELECT 字段 INTO 具备变量名 FROM 表; ③使用（查看变量的值）SELECT 局部变量名; 用户变量和局部变量的对比： ​ 作用域 定义位置 语法 用户变量 当前会话 会话的任何地方 加@符号，不用指定类型 局部变量 定义它的BEGIN END中 BEGIN END的第一句话 一般不用加@,需要指定类型 存储过程存储过程和函数：类似于java中的方法 含义：一组预先编译好的SQL语句的集合，理解成批处理语句 提高代码的重用性 简化操作 减少了编译次数并且减少了和数据库服务器的连接次数，提高了效率 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#创建语法CREATE PROCEDURE 存储过程名(参数列表)BEGIN 存储过程体（一组合法的SQL语句）END&#x2F;*参数列表包含三部分参数模式 参数名 参数类型参数模式：in：该参数可以作为输入，也就是该参数需要调用方传入值out：该参数可以作为输出，也就是该参数可以作为返回值inout：该参数既可以作为输入又可以作为输出，也就是该参数既需要传入值，又可以返回值举例：in stuname varchar(20)如果存储过程体仅仅只有一句话，begin end可以省略存储过程体中的每条sql语句的结尾要求必须加分号。存储过程的结尾可以使用 delimiter 重新设置语法：delimiter 结束标记案例：delimiter $*&#x2F;#调用语法CALL 存储过程名(实参列表);#案例 创建存储过程实现，用户是否登录成功CREATE PROCEDURE myp4(IN username VARCHAR(20),IN PASSWORD VARCHAR(20))BEGIN DECLARE result INT DEFAULT 0;#声明并初始化 SELECT COUNT(*) INTO result#赋值 FROM admin WHERE admin.username &#x3D; username AND admin.password &#x3D; PASSWORD; SELECT IF(result&gt;0,&#39;成功&#39;,&#39;失败&#39;);#使用END $#调用CALL myp3(&#39;张飞&#39;,&#39;8888&#39;)$#案例 根据输入的女神名，返回对应的男神名和魅力值CREATE PROCEDURE myp7(IN beautyName VARCHAR(20),OUT boyName VARCHAR(20),OUT usercp INT) BEGIN SELECT boys.boyname, boys.usercp INTO boyname,usercp FROM boys RIGHT JOIN beauty b ON b.boyfriend_id &#x3D; boys.id WHERE b.name&#x3D;beautyName ;END $#调用CALL myp7(&#39;小昭&#39;,@name,@cp)$SELECT @name,@cp$#存储过程的删除，不能修改存储过程#语法：drop procedure 存储过程名，一次只能删除一个DROP PROCEDURE p1;DROP PROCEDURE p2,p3;#×#存储过程的查看DESC myp2;×SHOW CREATE PROCEDURE myp2; 函数含义：一组预先编译好的SQL语句的集合，理解成批处理语句 提高代码的重用性 简化操作 减少了编译次数并且减少了和数据库服务器的连接次数，提高了效率 区别： 存储过程：可以有0个返回，也可以有多个返回，适合做批量插入、批量更新 函数：有且仅有1 个返回，适合做处理数据后返回一个结果 123456789101112131415161718192021222324252627282930313233343536373839404142#创建语法CREATE FUNCTION 函数名(参数列表) RETURNS 返回类型BEGIN 函数体END&#x2F;*注意：1.参数列表 包含两部分：参数名 参数类型2.函数体：肯定会有return语句，如果没有会报错如果return语句没有放在函数体的最后也不报错，但不建议return 值;3.函数体中仅有一句话，则可以省略begin end4.使用 delimiter语句设置结束标记*&#x2F;#调用语法SELECT 函数名(参数列表)#案例：返回公司的员工个数CREATE FUNCTION myf1() RETURNS INTBEGIN DECLARE c INT DEFAULT 0;#定义局部变量 SELECT COUNT(*) INTO c#赋值 FROM employees; RETURN c;END $SELECT myf1()$#案例 根据部门名，返回该部门的平均工资CREATE FUNCTION myf3(deptName VARCHAR(20)) RETURNS DOUBLEBEGIN DECLARE sal DOUBLE ; SELECT AVG(salary) INTO sal FROM employees e JOIN departments d ON e.department_id &#x3D; d.department_id WHERE d.department_name&#x3D;deptName; RETURN sal;END $SELECT myf3(&#39;IT&#39;)$#查看和删除SHOW CREATE FUNCTION myf3;DROP FUNCTION myf3; 流程控制结构分支结构if函数 语法：if(条件,值1，值2) 功能：实现双分支，应用任何地方。 if结构 语法： if 条件1 then 语句1; elseif 条件2 then 语句2; …. else 语句n; end if; 功能：类似于多重if 只能应用在begin end 中 12345678910111213141516171819202122#案例 创建函数，实现传入成绩，如果成绩&gt;90,返回A，如果成绩&gt;80,返回B，如果成绩&gt;60,返回C，否则返回DCREATE FUNCTION test_if(score FLOAT) RETURNS CHARBEGIN DECLARE ch CHAR DEFAULT &#39;A&#39;; IF score&gt;90 THEN SET ch&#x3D;&#39;A&#39;; ELSEIF score&gt;80 THEN SET ch&#x3D;&#39;B&#39;; ELSEIF score&gt;60 THEN SET ch&#x3D;&#39;C&#39;; ELSE SET ch&#x3D;&#39;D&#39;; END IF; RETURN ch;END $SELECT test_if(87)$#案例 创建存储过程，如果工资&lt;2000,则删除，如果5000&gt;工资&gt;2000,则涨工资1000，否则涨工资500CREATE PROCEDURE test_if_pro(IN sal DOUBLE)BEGIN IF sal&lt;2000 THEN DELETE FROM employees WHERE employees.salary&#x3D;sal; ELSEIF sal&gt;&#x3D;2000 AND sal&lt;5000 THEN UPDATE employees SET salary&#x3D;salary+1000 WHERE employees.&#96;salary&#96;&#x3D;sal; ELSE UPDATE employees SET salary&#x3D;salary+500 WHERE employees.&#96;salary&#96;&#x3D;sal; END IF;END $CALL test_if_pro(2100)$ case结构 语法： 情况1：类似于switch case 变量 | 表达式 | 字段 when 值1 then 语句1; when 值2 then 语句2; … else 语句n或要返回的值n; end 情况2： case when 条件1 then 语句1; when 条件2 then 语句2; … else 语句n或要返回的值n; end 作为独立的语句只能在begin end 中或外面；作为表达式，嵌套在其他语句中使用，可以用在任何地方。 else可以省略，若else省略，并when的所有条件都不满足，则返回null。 12345678910111213#案例 创建函数，实现传入成绩，如果成绩&gt;90,返回A，如果成绩&gt;80,返回B，如果成绩&gt;60,返回C，否则返回DCREATE FUNCTION test_case(score FLOAT) RETURNS CHARBEGIN DECLARE ch CHAR DEFAULT &#39;A&#39;; CASE WHEN score&gt;90 THEN SET ch&#x3D;&#39;A&#39;; WHEN score&gt;80 THEN SET ch&#x3D;&#39;B&#39;; WHEN score&gt;60 THEN SET ch&#x3D;&#39;C&#39;; ELSE SET ch&#x3D;&#39;D&#39;; END CASE; RETURN ch;END $SELECT test_case(56)$ 循环结构分类：while、loop、repeat 循环控制：iterate类似于 continue，继续，结束本次循环，继续下一次；leave 类似于 break，跳出，结束当前所在的循环。 while 语法： 【标签:】while 循环条件 do ​ 循环体; end while【 标签】; 123456789101112131415161718192021222324252627#案例：批量插入，根据次数插入到admin表中多条记录DROP PROCEDURE pro_while1$CREATE PROCEDURE pro_while1(IN insertCount INT)BEGIN DECLARE i INT DEFAULT 1; WHILE i&lt;&#x3D;insertCount DO INSERT INTO admin(username,&#96;password&#96;) VALUES(CONCAT(&#39;Rose&#39;,i),&#39;666&#39;); SET i&#x3D;i+1; END WHILE;END $CALL pro_while1(100)$#添加leave语句#案例 批量插入，根据次数插入到admin表中多条记录，如果次数&gt;20则停止TRUNCATE TABLE admin$DROP PROCEDURE test_while1$CREATE PROCEDURE test_while1(IN insertCount INT)BEGIN DECLARE i INT DEFAULT 1; a:WHILE i&lt;&#x3D;insertCount DO INSERT INTO admin(username,&#96;password&#96;) VALUES(CONCAT(&#39;xiaohua&#39;,i),&#39;0000&#39;); IF i&gt;&#x3D;20 THEN LEAVE a; END IF; SET i&#x3D;i+1; END WHILE a;END $CALL test_while1(100)$ loop 语法： 【标签:】loop ​ 循环体; end loop 【标签】; 可以用来模拟简单的死循环 repeat 语法：使用 leave 跳出循环。 【标签：】repeat ​ 循环体; until 结束循环的条件 end repeat 【标签】; 12345678910111213141516171819202122232425262728&#x2F;*已知表stringcontent其中字段：id 自增长content varchar(20)向该表插入指定个数的，随机的字符串*&#x2F;DROP TABLE IF EXISTS stringcontent;CREATE TABLE stringcontent( id INT PRIMARY KEY AUTO_INCREMENT, content VARCHAR(20));DELIMITER $CREATE PROCEDURE test_randstr_insert(IN insertCount INT)BEGIN DECLARE i INT DEFAULT 1; DECLARE str VARCHAR(26) DEFAULT &#39;abcdefghijklmnopqrstuvwxyz&#39;; DECLARE startIndex INT;#代表初始索引 DECLARE len INT;#代表截取的字符长度 WHILE i&lt;&#x3D;insertcount DO SET startIndex&#x3D;FLOOR(RAND()*26+1);#代表初始索引，随机范围1-26 SET len&#x3D;FLOOR(RAND()*(20-startIndex+1)+1);#代表截取长度，随机范围1-（20-startIndex+1） INSERT INTO stringcontent(content) VALUES(SUBSTR(str,startIndex,len)); SET i&#x3D;i+1; END WHILE;END $CALL test_randstr_insert(10)$","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"TCL语言","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL05-TCL.html","text":"TCL语言Transaction Control Language 事务控制语言 事务：一个或一组sql语句组成一个执行单元，这个执行单元要么全部执行，要么全部不执行。 事务的特性：ACID 原子性：一个事务不可再分割，要么都执行要么都不执行 一致性：一个事务执行会使数据从一个一致状态切换到另外一个一致状态 隔离性：一个事务的执行不受其他事务的干扰 持久性：一个事务一旦提交，则会永久的改变数据库的数据. 事务的创建 隐式事务：事务没有明显的开启和结束的标记比如insert、update、delete语句 显式事务：事务具有明显的开启和结束的标记前提：必须先设置自动提交功能为禁用：set autocommit=0; 步骤1：开启事务 set autocommit=0; start transaction;可选的 步骤2：编写事务中的sql语句(select insert update delete) ​ 语句1; ​ 语句2; ​ … 步骤3：结束事务 commit;提交事务 或 rollback;回滚事务 savepoint 节点名;设置保存点 DML语句是insert ,update, delete —-&gt;这个是要COMMIT才能存储在ORACLE数据库里 DDL语句是系统自动提交的，不需要手动COMMIT。这是修改表结构，执行完成，就生效了。 脏读: 对于两个事务T1, T2, T1 读取了已经被T2 更新但还没有被提交的字段. 之后, 若T2 回滚, T1读取的内容就是临时且无效的. 不可重复读: 对于两个事务T1, T2, T1 读取了一个字段, 然后T2 更新了该字段. 之后, T1再次读取同一个字段, 值就不同了. 幻读: 对于两个事务T1, T2, T1 从一个表中读取了一个字段, 然后T2 在该表中插入了一些新的行. 之后, 如果T1 再次读取同一个表, 就会多出几行. 事务的隔离级别： ​ 脏读 不可重复读 幻读 read uncommitted：√ √ √ read committed： × √ √ repeatable read： × × √ serializable × × × 每启动一个mysql 程序, 就会获得一个单独的数据库连接. 每个数据库连接都有一个全局变量@@tx_isolation, 表示当前的事务隔离级别. 查看当前的隔离级别: SELECT @@tx_isolation; 设置当前mySQL 连接的隔离级别:set transaction isolation level read committed; 设置数据库系统的全局的隔离级别:set globaltransaction isolation level read committed; 123456789101112131415161718192021222324252627282930#开启事务SET autocommit&#x3D;0;START TRANSACTION;#编写一组事务的语句UPDATE account SET balance &#x3D; 1000 WHERE username&#x3D;&#39;张无忌&#39;;UPDATE account SET balance &#x3D; 1000 WHERE username&#x3D;&#39;赵敏&#39;;#结束事务ROLLBACK;#commit;SELECT * FROM account;#2.演示事务对于delete和truncate的处理的区别SET autocommit&#x3D;0;START TRANSACTION;DELETE FROM account;ROLLBACK;#3.演示savepoint 的使用SET autocommit&#x3D;0;START TRANSACTION;DELETE FROM account WHERE id&#x3D;25;SAVEPOINT a;#设置保存点DELETE FROM account WHERE id&#x3D;28;ROLLBACK TO a;#回滚到保存点SELECT * FROM account;","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"JDBC概述&数据库连接方式","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/04-JDBC/JDBC-01 概览&连接数据库方式.html","text":"web技术概览 第1章：JDBC概述1.1 数据的持久化 持久化(persistence)：把数据保存到可掉电式存储设备中以供之后使用。大多数情况下，特别是企业级应用，数据持久化意味着将内存中的数据保存到硬盘上加以”固化”，而持久化的实现过程大多通过各种关系数据库来完成。 持久化的主要应用是将内存中的数据存储在关系型数据库中，当然也可以存储在磁盘文件、XML数据文件中。 1.2 Java中的数据存储技术 在Java中，数据库存取技术可分为如下几类： JDBC直接访问数据库 JDO (Java Data Object )技术 第三方O/R工具，如Hibernate, Mybatis 等 JDBC是java访问数据库的基石，JDO、Hibernate、MyBatis等只是更好的封装了JDBC。 1.3 JDBC介绍 JDBC(Java Database Connectivity)是一个独立于特定数据库管理系统、通用的SQL数据库存取和操作的公共接口（一组API），定义了用来访问数据库的标准Java类库，（java.sql,javax.sql）使用这些类库可以以一种标准的方法、方便地访问数据库资源。 JDBC为访问不同的数据库提供了一种统一的途径，为开发者屏蔽了一些细节问题。 JDBC的目标是使Java程序员使用JDBC可以连接任何提供了JDBC驱动程序的数据库系统，这样就使得程序员无需对特定的数据库系统的特点有过多的了解，从而大大简化和加快了开发过程。 如果没有JDBC，那么Java程序访问数据库时是这样的： 有了JDBC，Java程序访问数据库时是这样的： 总结如下： 1.4 JDBC体系结构 JDBC接口（API）包括两个层次： 面向应用的API：Java API，抽象接口，供应用程序开发人员使用（连接数据库，执行SQL语句，获得结果）。 面向数据库的API：Java Driver API，供开发商开发数据库驱动程序用。 JDBC是sun公司提供一套用于数据库操作的接口，java程序员只需要面向这套接口编程即可。 不同的数据库厂商，需要针对这套接口，提供不同实现。不同的实现的集合，即为不同数据库的驱动。 ————面向接口编程 1.5 JDBC程序编写步骤 补充：ODBC(Open Database Connectivity，开放式数据库连接)，是微软在Windows平台下推出的。使用者在程序中只需要调用ODBC API，由 ODBC 驱动程序将调用转换成为对特定的数据库的调用请求。 第2章：获取数据库连接2.1 要素一：Driver接口实现类2.1.1 Driver接口介绍 java.sql.Driver 接口是所有 JDBC 驱动程序需要实现的接口。这个接口是提供给数据库厂商使用的，不同数据库厂商提供不同的实现。 在程序中不需要直接去访问实现了 Driver 接口的类，而是由驱动程序管理器类(java.sql.DriverManager)去调用这些Driver实现。 Oracle的驱动：oracle.jdbc.driver.OracleDriver mySql的驱动： com.mysql.jdbc.Driver 将上述jar包拷贝到Java工程的一个目录中，习惯上新建一个lib文件夹。 在驱动jar上右键—&gt;Build Path—&gt;Add to Build Path 注意：如果是Dynamic Web Project（动态的web项目）话，则是把驱动jar放到WebContent（有的开发工具叫WebRoot）目录中的WEB-INF目录中的lib目录下即可 2.1.2 加载与注册JDBC驱动 加载驱动：加载 JDBC 驱动需调用 Class 类的静态方法 forName()，向其传递要加载的 JDBC 驱动的类名 Class.forName(“com.mysql.jdbc.Driver”); 注册驱动：DriverManager 类是驱动程序管理器类，负责管理驱动程序 使用DriverManager.registerDriver(com.mysql.jdbc.Driver)来注册驱动 通常不用显式调用 DriverManager 类的 registerDriver() 方法来注册驱动程序类的实例，因为 Driver 接口的驱动程序类都包含了静态代码块，在这个静态代码块中，会调用 DriverManager.registerDriver() 方法来注册自身的一个实例。下图是MySQL的Driver实现类的源码： 2.2 要素二：URL JDBC URL 用于标识一个被注册的驱动程序，驱动程序管理器通过这个 URL 选择正确的驱动程序，从而建立到数据库的连接。 JDBC URL的标准由三部分组成，各部分间用冒号分隔。 jdbc:子协议:子名称 协议：JDBC URL中的协议总是jdbc 子协议：子协议用于标识一个数据库驱动程序 子名称：一种标识数据库的方法。子名称可以依不同的子协议而变化，用子名称的目的是为了定位数据库提供足够的信息。包含主机名(对应服务端的ip地址)，端口号，数据库名 举例： 几种常用数据库的 JDBC URL MySQL的连接URL编写方式： jdbc:mysql://主机名称:mysql服务端口号/数据库名称?参数=值&amp;参数=值 jdbc:mysql://localhost:3306/atguigu jdbc:mysql://localhost:3306/atguigu?useUnicode=true&amp;characterEncoding=utf8（如果JDBC程序与服务器端的字符集不一致，会导致乱码，那么可以通过参数指定服务器端的字符集） jdbc:mysql://localhost:3306/atguigu?user=root&amp;password=123456 Oracle 9i的连接URL编写方式： jdbc:oracle:thin:@主机名称:oracle服务端口号:数据库名称 jdbc:oracle:thin:@localhost:1521:atguigu SQLServer的连接URL编写方式： jdbc:sqlserver://主机名称:sqlserver服务端口号:DatabaseName=数据库名称 jdbc:sqlserver://localhost:1433:DatabaseName=atguigu 2.3 要素三：用户名和密码 user,password可以用“属性名=属性值”方式告诉数据库 可以调用 DriverManager 类的 getConnection() 方法建立到数据库的连接 2.4 数据库连接方式举例2.4.1 连接方式一12345678910111213141516171819202122@Test public void testConnection1() &#123; try &#123; //1.提供java.sql.Driver接口实现类的对象 Driver driver = null; driver = new com.mysql.jdbc.Driver(); //2.提供url，指明具体操作的数据 String url = \"jdbc:mysql://localhost:3306/test\"; //3.提供Properties的对象，指明用户名和密码 Properties info = new Properties(); info.setProperty(\"user\", \"root\"); info.setProperty(\"password\", \"abc123\"); //4.调用driver的connect()，获取连接 Connection conn = driver.connect(url, info); System.out.println(conn); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; 说明：上述代码中显式出现了第三方数据库的API 2.4.2 连接方式二1234567891011121314151617181920212223@Test public void testConnection2() &#123; try &#123; //1.实例化Driver String className = \"com.mysql.jdbc.Driver\"; Class clazz = Class.forName(className); Driver driver = (Driver) clazz.newInstance(); //2.提供url，指明具体操作的数据 String url = \"jdbc:mysql://localhost:3306/test\"; //3.提供Properties的对象，指明用户名和密码 Properties info = new Properties(); info.setProperty(\"user\", \"root\"); info.setProperty(\"password\", \"abc123\"); //4.调用driver的connect()，获取连接 Connection conn = driver.connect(url, info); System.out.println(conn); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 说明：相较于方式一，这里使用反射实例化Driver，不在代码中体现第三方数据库的API。体现了面向接口编程思想。 2.4.3 连接方式三123456789101112131415161718192021@Test public void testConnection3() &#123; try &#123; //1.数据库连接的4个基本要素： String url = \"jdbc:mysql://localhost:3306/test\"; String user = \"root\"; String password = \"abc123\"; String driverName = \"com.mysql.jdbc.Driver\"; //2.实例化Driver Class clazz = Class.forName(driverName); Driver driver = (Driver) clazz.newInstance(); //3.注册驱动 DriverManager.registerDriver(driver); //4.获取连接 Connection conn = DriverManager.getConnection(url, user, password); System.out.println(conn); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 说明：使用DriverManager实现数据库的连接。体会获取连接必要的4个基本要素。 2.4.4 连接方式四12345678910111213141516171819202122232425262728293031@Test public void testConnection4() &#123; try &#123; //1.数据库连接的4个基本要素： String url = \"jdbc:mysql://localhost:3306/test\"; String user = \"root\"; String password = \"abc123\"; String driverName = \"com.mysql.jdbc.Driver\"; //2.加载驱动 （①实例化Driver ②注册驱动） Class.forName(driverName); //Driver driver = (Driver) clazz.newInstance(); //3.注册驱动 //DriverManager.registerDriver(driver); /* 可以注释掉上述代码的原因，是因为在mysql的Driver类中声明有： static &#123; try &#123; DriverManager.registerDriver(new Driver()); &#125; catch (SQLException var1) &#123; throw new RuntimeException(\"Can't register driver!\"); &#125; &#125; */ //3.获取连接 Connection conn = DriverManager.getConnection(url, user, password); System.out.println(conn); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 说明：不必显式的注册驱动了。因为在DriverManager的源码中已经存在静态代码块，实现了驱动的注册。 2.4.5 连接方式五(最终版)123456789101112131415161718192021@Test public void testConnection5() throws Exception &#123; //1.加载配置文件 InputStream is = ConnectionTest.class.getClassLoader().getResourceAsStream(\"jdbc.properties\"); Properties pros = new Properties(); pros.load(is); //2.读取配置信息 String user = pros.getProperty(\"user\"); String password = pros.getProperty(\"password\"); String url = pros.getProperty(\"url\"); String driverClass = pros.getProperty(\"driverClass\"); //3.加载驱动 Class.forName(driverClass); //4.获取连接 Connection conn = DriverManager.getConnection(url,user,password); System.out.println(conn); &#125; 其中，配置文件声明在工程的src目录下：【jdbc.properties】 1234user=rootpassword=abc123url=jdbc:mysql://localhost:3306/testdriverClass=com.mysql.jdbc.Driver 说明：使用配置文件的方式保存配置信息，在代码中加载配置文件 使用配置文件的好处： ①实现了代码和数据的分离，如果需要修改配置信息，直接在配置文件中修改，不需要深入代码②如果修改了配置信息，省去重新编译的过程。","tags":[{"name":"JDBC","slug":"JDBC","permalink":"https://mtcai.github.io/tags/JDBC/"}]},{"title":"数据库事务&DAO","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/04-JDBC/JDBC-03 数据库事务&DAO.html","text":"第6章： 数据库事务6.1 数据库事务介绍 事务：一组逻辑操作单元,使数据从一种状态变换到另一种状态。 事务处理（事务操作）：保证所有事务都作为一个工作单元来执行，即使出现了故障，都不能改变这种执行方式。当在一个事务中执行多个操作时，要么所有的事务都被提交(commit)，那么这些修改就永久地保存下来；要么数据库管理系统将放弃所作的所有修改，整个事务回滚(rollback)到最初状态。 为确保数据库中数据的一致性，数据的操纵应当是离散的成组的逻辑单元：当它全部完成时，数据的一致性可以保持，而当这个单元中的一部分操作失败，整个事务应全部视为错误，所有从起始点以后的操作应全部回退到开始状态。 6.2 JDBC事务处理 数据一旦提交，就不可回滚。 哪些操作会导致数据的自动提交？ DDL操作一旦执行，都会自动提交。 set autocommit = false 对DDL操作失效 DML默认情况下，一旦执行，就会自动提交。 我们可以通过set autocommit = false的方式取消DML操作的自动提交。 默认在关闭连接时，会自动的提交数据 数据什么时候意味着提交？ 当一个连接对象被创建时，默认情况下是自动提交事务：每次执行一个 SQL 语句时，如果执行成功，就会向数据库自动提交，而不能回滚。 关闭数据库连接，数据就会自动的提交。如果多个操作，每个操作使用的是自己单独的连接，则无法保证事务。即同一个事务的多个操作必须在同一个连接下。 JDBC程序中为了让多个 SQL 语句作为一个事务执行： 调用 Connection 对象的 setAutoCommit(false); 以取消自动提交事务 在所有的 SQL 语句都成功执行后，调用 commit(); 方法提交事务 在出现异常时，调用 rollback(); 方法回滚事务 若此时 Connection 没有被关闭，还可能被重复使用，则需要恢复其自动提交状态 setAutoCommit(true)。尤其是在使用数据库连接池技术时，执行close()方法前，建议恢复自动提交状态。 【案例：用户AA向用户BB转账100】 12345678910111213141516171819202122232425262728293031323334353637public void testJDBCTransaction() &#123; Connection conn = null; try &#123; // 1.获取数据库连接 conn = JDBCUtils.getConnection(); // 2.开启事务 conn.setAutoCommit(false); // 3.进行数据库操作 String sql1 = \"update user_table set balance = balance - 100 where user = ?\"; update(conn, sql1, \"AA\"); // 模拟网络异常 //System.out.println(10 / 0); String sql2 = \"update user_table set balance = balance + 100 where user = ?\"; update(conn, sql2, \"BB\"); // 4.若没有异常，则提交事务 conn.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); // 5.若有异常，则回滚事务 try &#123; conn.rollback(); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; &#125; finally &#123; try &#123; //6.恢复每次DML操作的自动提交功能 conn.setAutoCommit(true); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; //7.关闭连接 JDBCUtils.closeResource(conn, null, null); &#125; &#125; 其中，对数据库操作的方法为： 12345678910111213141516171819//使用事务以后的通用的增删改操作（version 2.0）public void update(Connection conn ,String sql, Object... args) &#123; PreparedStatement ps = null; try &#123; // 1.获取PreparedStatement的实例 (或：预编译sql语句) ps = conn.prepareStatement(sql); // 2.填充占位符 for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; // 3.执行sql语句 ps.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭资源 JDBCUtils.closeResource(null, ps); &#125;&#125; 6.3 事务的ACID属性 原子性（Atomicity） 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency） 事务必须使数据库从一个一致性状态变换到另外一个一致性状态。 隔离性（Isolation） 事务的隔离性是指一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 持久性（Durability） 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来的其他操作和数据库故障不应该对其有任何影响。 6.3.1 数据库的并发问题 对于同时运行的多个事务, 当这些事务访问数据库中相同的数据时, 如果没有采取必要的隔离机制, 就会导致各种并发问题: 脏读: 对于两个事务 T1, T2, T1 读取了已经被 T2 更新但还没有被提交的字段。之后, 若 T2 回滚, T1读取的内容就是临时且无效的。 不可重复读: 对于两个事务T1, T2, T1 读取了一个字段, 然后 T2 更新了该字段。之后, T1再次读取同一个字段, 值就不同了。 幻读: 对于两个事务T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入了一些新的行。之后, 如果 T1 再次读取同一个表, 就会多出几行。 数据库事务的隔离性: 数据库系统必须具有隔离并发运行各个事务的能力, 使它们不会相互影响, 避免各种并发问题。 一个事务与其他事务隔离的程度称为隔离级别。数据库规定了多种事务隔离级别, 不同隔离级别对应不同的干扰程度, 隔离级别越高, 数据一致性就越好, 但并发性越弱。 6.3.2 四种隔离级别 数据库提供的4种事务隔离级别： Oracle 支持的 2 种事务隔离级别：READ COMMITED, SERIALIZABLE。 Oracle 默认的事务隔离级别为: READ COMMITED 。 Mysql 支持 4 种事务隔离级别。Mysql 默认的事务隔离级别为: REPEATABLE READ。 6.3.3 在MySql中设置隔离级别 每启动一个 mysql 程序, 就会获得一个单独的数据库连接. 每个数据库连接都有一个全局变量 @@tx_isolation, 表示当前的事务隔离级别。 查看当前的隔离级别: 1SELECT @@tx_isolation; 设置当前 mySQL 连接的隔离级别: 1set transaction isolation level read committed; 设置数据库系统的全局的隔离级别: 1set global transaction isolation level read committed; 补充操作： 创建mysql数据库用户： 1create user tom identified by &#39;abc123&#39;; 授予权限 12345#授予通过网络方式登录的tom用户，对所有库所有表的全部权限，密码设为abc123.grant all privileges on *.* to tom@&#39;%&#39; identified by &#39;abc123&#39;; #给tom用户使用本地命令行方式，授予atguigudb这个库下的所有表的插删改查的权限。grant select,insert,delete,update on atguigudb.* to tom@localhost identified by &#39;abc123&#39;; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// ***************************************************** @Test public void testTransactionSelect() throws Exception&#123; Connection conn = JDBCUtils.getConnection(); //获取当前连接的隔离级别 System.out.println(conn.getTransactionIsolation()); //设置数据库的隔离级别： conn.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED); //取消自动提交数据 conn.setAutoCommit(false); String sql = \"select user,password,balance from user_table where user = ?\"; User user = getInstance(conn, User.class, sql, \"CC\"); System.out.println(user); &#125; @Test public void testTransactionUpdate() throws Exception&#123; Connection conn = JDBCUtils.getConnection(); //取消自动提交数据 conn.setAutoCommit(false); String sql = \"update user_table set balance = ? where user = ?\"; update(conn, sql, 5000,\"CC\"); Thread.sleep(15000); System.out.println(\"修改结束\"); &#125; //通用的查询操作，用于返回数据表中的一条记录（version 2.0：考虑上事务） public &lt;T&gt; T getInstance(Connection conn,Class&lt;T&gt; clazz,String sql, Object... args) &#123; PreparedStatement ps = null; ResultSet rs = null; try &#123; ps = conn.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; rs = ps.executeQuery(); // 获取结果集的元数据 :ResultSetMetaData ResultSetMetaData rsmd = rs.getMetaData(); // 通过ResultSetMetaData获取结果集中的列数 int columnCount = rsmd.getColumnCount(); if (rs.next()) &#123; T t = clazz.newInstance(); // 处理结果集一行数据中的每一个列 for (int i = 0; i &lt; columnCount; i++) &#123; // 获取列值 Object columValue = rs.getObject(i + 1); // 获取每个列的列名 // String columnName = rsmd.getColumnName(i + 1); String columnLabel = rsmd.getColumnLabel(i + 1); // 给t对象指定的columnName属性，赋值为columValue：通过反射 Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columValue); &#125; return t; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, ps, rs); &#125; return null; &#125; 第7章：DAO及相关实现类 DAO：Data Access Object访问数据信息的类和接口，包括了对数据的CRUD（Create、Retrival、Update、Delete），而不包含任何业务相关的信息。有时也称作：BaseDAO 作用：为了实现功能的模块化，更有利于代码的维护和升级。 下面是尚硅谷JavaWeb阶段书城项目中DAO使用的体现： 层次结构： 【BaseDAO.java】123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.atguigu.bookstore.dao;import java.lang.reflect.ParameterizedType;import java.lang.reflect.Type;import java.sql.Connection;import java.sql.SQLException;import java.util.List;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanHandler;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.apache.commons.dbutils.handlers.ScalarHandler;/** * 定义一个用来被继承的对数据库进行基本操作的Dao * @author HanYanBing * @param &lt;T&gt; */public abstract class BaseDao&lt;T&gt; &#123; private QueryRunner queryRunner = new QueryRunner(); // 定义一个变量来接收泛型的类型 private Class&lt;T&gt; type; // 获取T的Class对象，获取泛型的类型，泛型是在被子类继承时才确定 public BaseDao() &#123; // 获取子类的类型 Class clazz = this.getClass(); // 获取父类的类型 // getGenericSuperclass()用来获取当前类的父类的类型 // ParameterizedType表示的是带泛型的类型 ParameterizedType parameterizedType = (ParameterizedType) clazz.getGenericSuperclass(); // 获取具体的泛型类型 getActualTypeArguments获取具体的泛型的类型 // 这个方法会返回一个Type的数组 Type[] types = parameterizedType.getActualTypeArguments(); // 获取具体的泛型的类型· this.type = (Class&lt;T&gt;) types[0]; &#125; /** * 通用的增删改操作 * * @param sql * @param params * @return */ public int update(Connection conn,String sql, Object... params) &#123; int count = 0; try &#123; count = queryRunner.update(conn, sql, params); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return count; &#125; /** * 获取一个对象 * * @param sql * @param params * @return */ public T getBean(Connection conn,String sql, Object... params) &#123; T t = null; try &#123; t = queryRunner.query(conn, sql, new BeanHandler&lt;T&gt;(type), params); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return t; &#125; /** * 获取所有对象 * * @param sql * @param params * @return */ public List&lt;T&gt; getBeanList(Connection conn,String sql, Object... params) &#123; List&lt;T&gt; list = null; try &#123; list = queryRunner.query(conn, sql, new BeanListHandler&lt;T&gt;(type), params); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return list; &#125; /** * 获取一个但一值得方法，专门用来执行像 select count(*)...这样的sql语句 * * @param sql * @param params * @return */ public Object getValue(Connection conn,String sql, Object... params) &#123; Object count = null; try &#123; // 调用queryRunner的query方法获取一个单一的值 count = queryRunner.query(conn, sql, new ScalarHandler&lt;&gt;(), params); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return count; &#125;&#125; 【BookDAO.java】123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.atguigu.bookstore.dao;import java.sql.Connection;import java.util.List;import com.atguigu.bookstore.beans.Book;import com.atguigu.bookstore.beans.Page;public interface BookDao &#123; /** * 从数据库中查询出所有的记录 * * @return */ List&lt;Book&gt; getBooks(Connection conn); /** * 向数据库中插入一条记录 * * @param book */ void saveBook(Connection conn,Book book); /** * 从数据库中根据图书的id删除一条记录 * * @param bookId */ void deleteBookById(Connection conn,String bookId); /** * 根据图书的id从数据库中查询出一条记录 * * @param bookId * @return */ Book getBookById(Connection conn,String bookId); /** * 根据图书的id从数据库中更新一条记录 * * @param book */ void updateBook(Connection conn,Book book); /** * 获取带分页的图书信息 * * @param page：是只包含了用户输入的pageNo属性的page对象 * @return 返回的Page对象是包含了所有属性的Page对象 */ Page&lt;Book&gt; getPageBooks(Connection conn,Page&lt;Book&gt; page); /** * 获取带分页和价格范围的图书信息 * * @param page：是只包含了用户输入的pageNo属性的page对象 * @return 返回的Page对象是包含了所有属性的Page对象 */ Page&lt;Book&gt; getPageBooksByPrice(Connection conn,Page&lt;Book&gt; page, double minPrice, double maxPrice);&#125; 【UserDAO.java】12345678910111213141516171819202122232425262728293031package com.atguigu.bookstore.dao;import java.sql.Connection;import com.atguigu.bookstore.beans.User;public interface UserDao &#123; /** * 根据User对象中的用户名和密码从数据库中获取一条记录 * * @param user * @return User 数据库中有记录 null 数据库中无此记录 */ User getUser(Connection conn,User user); /** * 根据User对象中的用户名从数据库中获取一条记录 * * @param user * @return true 数据库中有记录 false 数据库中无此记录 */ boolean checkUsername(Connection conn,User user); /** * 向数据库中插入User对象 * * @param user */ void saveUser(Connection conn,User user);&#125; 【BookDaoImpl.java】1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package com.atguigu.bookstore.dao.impl;import java.sql.Connection;import java.util.List;import com.atguigu.bookstore.beans.Book;import com.atguigu.bookstore.beans.Page;import com.atguigu.bookstore.dao.BaseDao;import com.atguigu.bookstore.dao.BookDao;public class BookDaoImpl extends BaseDao&lt;Book&gt; implements BookDao &#123; @Override public List&lt;Book&gt; getBooks(Connection conn) &#123; // 调用BaseDao中得到一个List的方法 List&lt;Book&gt; beanList = null; // 写sql语句 String sql = \"select id,title,author,price,sales,stock,img_path imgPath from books\"; beanList = getBeanList(conn,sql); return beanList; &#125; @Override public void saveBook(Connection conn,Book book) &#123; // 写sql语句 String sql = \"insert into books(title,author,price,sales,stock,img_path) values(?,?,?,?,?,?)\"; // 调用BaseDao中通用的增删改的方法 update(conn,sql, book.getTitle(), book.getAuthor(), book.getPrice(), book.getSales(), book.getStock(),book.getImgPath()); &#125; @Override public void deleteBookById(Connection conn,String bookId) &#123; // 写sql语句 String sql = \"DELETE FROM books WHERE id = ?\"; // 调用BaseDao中通用增删改的方法 update(conn,sql, bookId); &#125; @Override public Book getBookById(Connection conn,String bookId) &#123; // 调用BaseDao中获取一个对象的方法 Book book = null; // 写sql语句 String sql = \"select id,title,author,price,sales,stock,img_path imgPath from books where id = ?\"; book = getBean(conn,sql, bookId); return book; &#125; @Override public void updateBook(Connection conn,Book book) &#123; // 写sql语句 String sql = \"update books set title = ? , author = ? , price = ? , sales = ? , stock = ? where id = ?\"; // 调用BaseDao中通用的增删改的方法 update(conn,sql, book.getTitle(), book.getAuthor(), book.getPrice(), book.getSales(), book.getStock(), book.getId()); &#125; @Override public Page&lt;Book&gt; getPageBooks(Connection conn,Page&lt;Book&gt; page) &#123; // 获取数据库中图书的总记录数 String sql = \"select count(*) from books\"; // 调用BaseDao中获取一个单一值的方法 long totalRecord = (long) getValue(conn,sql); // 将总记录数设置都page对象中 page.setTotalRecord((int) totalRecord); // 获取当前页中的记录存放的List String sql2 = \"select id,title,author,price,sales,stock,img_path imgPath from books limit ?,?\"; // 调用BaseDao中获取一个集合的方法 List&lt;Book&gt; beanList = getBeanList(conn,sql2, (page.getPageNo() - 1) * Page.PAGE_SIZE, Page.PAGE_SIZE); // 将这个List设置到page对象中 page.setList(beanList); return page; &#125; @Override public Page&lt;Book&gt; getPageBooksByPrice(Connection conn,Page&lt;Book&gt; page, double minPrice, double maxPrice) &#123; // 获取数据库中图书的总记录数 String sql = \"select count(*) from books where price between ? and ?\"; // 调用BaseDao中获取一个单一值的方法 long totalRecord = (long) getValue(conn,sql,minPrice,maxPrice); // 将总记录数设置都page对象中 page.setTotalRecord((int) totalRecord); // 获取当前页中的记录存放的List String sql2 = \"select id,title,author,price,sales,stock,img_path imgPath from books where price between ? and ? limit ?,?\"; // 调用BaseDao中获取一个集合的方法 List&lt;Book&gt; beanList = getBeanList(conn,sql2, minPrice , maxPrice , (page.getPageNo() - 1) * Page.PAGE_SIZE, Page.PAGE_SIZE); // 将这个List设置到page对象中 page.setList(beanList); return page; &#125;&#125; 【UserDaoImpl.java】123456789101112131415161718192021222324252627282930313233343536373839package com.atguigu.bookstore.dao.impl;import java.sql.Connection;import com.atguigu.bookstore.beans.User;import com.atguigu.bookstore.dao.BaseDao;import com.atguigu.bookstore.dao.UserDao;public class UserDaoImpl extends BaseDao&lt;User&gt; implements UserDao &#123; @Override public User getUser(Connection conn,User user) &#123; // 调用BaseDao中获取一个对象的方法 User bean = null; // 写sql语句 String sql = \"select id,username,password,email from users where username = ? and password = ?\"; bean = getBean(conn,sql, user.getUsername(), user.getPassword()); return bean; &#125; @Override public boolean checkUsername(Connection conn,User user) &#123; // 调用BaseDao中获取一个对象的方法 User bean = null; // 写sql语句 String sql = \"select id,username,password,email from users where username = ?\"; bean = getBean(conn,sql, user.getUsername()); return bean != null; &#125; @Override public void saveUser(Connection conn,User user) &#123; //写sql语句 String sql = \"insert into users(username,password,email) values(?,?,?)\"; //调用BaseDao中通用的增删改的方法 update(conn,sql, user.getUsername(),user.getPassword(),user.getEmail()); &#125;&#125; 【Book.java】1234567891011121314151617package com.atguigu.bookstore.beans;/** * 图书类 * @author songhongkang * */public class Book &#123; private Integer id; private String title; // 书名 private String author; // 作者 private double price; // 价格 private Integer sales; // 销量 private Integer stock; // 库存 private String imgPath = \"static/img/default.jpg\"; // 封面图片的路径 //构造器，get()，set()，toString()方法略&#125; 【Page.java】123456789101112131415package com.atguigu.bookstore.beans;import java.util.List;/** * 页码类 * @author songhongkang * */public class Page&lt;T&gt; &#123; private List&lt;T&gt; list; // 每页查到的记录存放的集合 public static final int PAGE_SIZE = 4; // 每页显示的记录数 private int pageNo; // 当前页// private int totalPageNo; // 总页数，通过计算得到 private int totalRecord; // 总记录数，通过查询数据库得到 【User.java】123456789101112package com.atguigu.bookstore.beans;/** * 用户类 * @author songhongkang * */public class User &#123; private Integer id; private String username; private String password; private String email;","tags":[{"name":"JDBC","slug":"JDBC","permalink":"https://mtcai.github.io/tags/JDBC/"}]},{"title":"数据库连接池&DBUtils","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/04-JDBC/JDBC-04 数据库连接池&DBUtils.html","text":"第8章：数据库连接池8.1 JDBC数据库连接池的必要性 在使用开发基于数据库的web程序时，传统的模式基本是按以下步骤： 在主程序（如servlet、beans）中建立数据库连接 进行sql操作 断开数据库连接 这种模式开发，存在的问题: 普通的JDBC数据库连接使用 DriverManager 来获取，每次向数据库建立连接的时候都要将 Connection 加载到内存中，再验证用户名和密码(得花费0.05s～1s的时间)。需要数据库连接的时候，就向数据库要求一个，执行完成后再断开连接。这样的方式将会消耗大量的资源和时间。数据库的连接资源并没有得到很好的重复利用。若同时有几百人甚至几千人在线，频繁的进行数据库连接操作将占用很多的系统资源，严重的甚至会造成服务器的崩溃。 对于每一次数据库连接，使用完后都得断开。否则，如果程序出现异常而未能关闭，将会导致数据库系统中的内存泄漏，最终将导致重启数据库。（回忆：何为Java的内存泄漏？） 这种开发不能控制被创建的连接对象数，系统资源会被毫无顾及的分配出去，如连接过多，也可能导致内存泄漏，服务器崩溃。 8.2 数据库连接池技术 为解决传统开发中的数据库连接问题，可以采用数据库连接池技术。 数据库连接池的基本思想：就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。 数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。 数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。 工作原理： 数据库连接池技术的优点 1. 资源重用 由于数据库连接得以重用，避免了频繁创建，释放连接引起的大量性能开销。在减少系统消耗的基础上，另一方面也增加了系统运行环境的平稳性。 2. 更快的系统反应速度 数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于连接池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接，避免了数据库连接初始化和释放过程的时间开销，从而减少了系统的响应时间 3. 新的资源分配手段 对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接池的配置，实现某一应用最大可用数据库连接数的限制，避免某一应用独占所有的数据库资源 4. 统一的连接管理，避免数据库连接泄漏 在较为完善的数据库连接池实现中，可根据预先的占用超时设定，强制回收被占用连接，从而避免了常规数据库连接操作中可能出现的资源泄露 8.3 多种开源的数据库连接池 JDBC 的数据库连接池使用 javax.sql.DataSource 来表示，DataSource 只是一个接口，该接口通常由服务器(Weblogic, WebSphere, Tomcat)提供实现，也有一些开源组织提供实现： DBCP 是Apache提供的数据库连接池。tomcat 服务器自带dbcp数据库连接池。速度相对c3p0较快，但因自身存在BUG，Hibernate3已不再提供支持。 C3P0 是一个开源组织提供的一个数据库连接池，速度相对较慢，稳定性还可以。hibernate官方推荐使用 Proxool 是sourceforge下的一个开源项目数据库连接池，有监控连接池状态的功能，稳定性较c3p0差一点 BoneCP 是一个开源组织提供的数据库连接池，速度快 Druid 是阿里提供的数据库连接池，据说是集DBCP 、C3P0 、Proxool 优点于一身的数据库连接池，但是速度不确定是否有BoneCP快 DataSource 通常被称为数据源，它包含连接池和连接池管理两个部分，习惯上也经常把 DataSource 称为连接池 DataSource用来取代DriverManager来获取Connection，获取速度快，同时可以大幅度提高数据库访问速度。 特别注意： 数据源和数据库连接不同，数据源无需创建多个，它是产生数据库连接的工厂，因此整个应用只需要一个数据源即可。 当数据库访问结束后，程序还是像以前一样关闭数据库连接：conn.close(); 但conn.close()并没有关闭数据库的物理连接，它仅仅把数据库连接释放，归还给了数据库连接池。 8.3.1 C3P0数据库连接池 获取连接方式一 12345678910111213141516//使用C3P0数据库连接池的方式，获取数据库的连接：不推荐public static Connection getConnection1() throws Exception&#123; ComboPooledDataSource cpds = new ComboPooledDataSource(); cpds.setDriverClass(\"com.mysql.jdbc.Driver\"); cpds.setJdbcUrl(\"jdbc:mysql://localhost:3306/test\"); cpds.setUser(\"root\"); cpds.setPassword(\"abc123\"); cpds.setInitiaPoolAize(10);// 设置初始时数据库连接池中的连接数// cpds.setMaxPoolSize(100); Connection conn = cpds.getConnection(); return conn; // 销毁c3p0数据库连接池 // DataSources.destory(cpds);&#125; 获取连接方式二 123456//使用C3P0数据库连接池的配置文件方式，获取数据库的连接：推荐private static DataSource cpds = new ComboPooledDataSource(\"helloc3p0\");public static Connection getConnection2() throws SQLException&#123; Connection conn = cpds.getConnection(); return conn;&#125; 其中，src下的配置文件为：【c3p0-config.xml】 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;c3p0-config&gt; &lt;named-config name=\"helloc3p0\"&gt; &lt;!-- 获取连接的4个基本信息 --&gt; &lt;property name=\"user\"&gt;root&lt;/property&gt; &lt;property name=\"password\"&gt;abc123&lt;/property&gt; &lt;property name=\"jdbcUrl\"&gt;jdbc:mysql:///test&lt;/property&gt; &lt;property name=\"driverClass\"&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;!-- 涉及到数据库连接池的管理的相关属性的设置 --&gt; &lt;!-- 若数据库中连接数不足时, 一次向数据库服务器申请多少个连接 --&gt; &lt;property name=\"acquireIncrement\"&gt;5&lt;/property&gt; &lt;!-- 初始化数据库连接池时连接的数量 --&gt; &lt;property name=\"initialPoolSize\"&gt;5&lt;/property&gt; &lt;!-- 数据库连接池中的最小的数据库连接数 --&gt; &lt;property name=\"minPoolSize\"&gt;5&lt;/property&gt; &lt;!-- 数据库连接池中的最大的数据库连接数 --&gt; &lt;property name=\"maxPoolSize\"&gt;10&lt;/property&gt; &lt;!-- C3P0 数据库连接池可以维护的 Statement 的个数 --&gt; &lt;property name=\"maxStatements\"&gt;20&lt;/property&gt; &lt;!-- 每个连接同时可以使用的 Statement 对象的个数 --&gt; &lt;property name=\"maxStatementsPerConnection\"&gt;5&lt;/property&gt; &lt;/named-config&gt;&lt;/c3p0-config&gt; 8.3.2 DBCP数据库连接池 DBCP 是 Apache 软件基金组织下的开源连接池实现，该连接池依赖该组织下的另一个开源系统：Common-pool。如需使用该连接池实现，应在系统中增加如下两个 jar 文件： Commons-dbcp.jar：连接池的实现 Commons-pool.jar：连接池实现的依赖库 Tomcat 的连接池正是采用该连接池来实现的。该数据库连接池既可以与应用服务器整合使用，也可由应用程序独立使用。 数据源和数据库连接不同，数据源无需创建多个，它是产生数据库连接的工厂，因此整个应用只需要一个数据源即可。 当数据库访问结束后，程序还是像以前一样关闭数据库连接：conn.close(); 但上面的代码并没有关闭数据库的物理连接，它仅仅把数据库连接释放，归还给了数据库连接池。 配置属性说明 属性 默认值 说明 initialSize 0 连接池启动时创建的初始化连接数量 maxActive 8 连接池中可同时连接的最大的连接数 maxIdle 8 连接池中最大的空闲的连接数，超过的空闲连接将被释放，如果设置为负数表示不限制 minIdle 0 连接池中最小的空闲的连接数，低于这个数量会被创建新的连接。该参数越接近maxIdle，性能越好，因为连接的创建和销毁，都是需要消耗资源的；但是不能太大。 maxWait 无限制 最大等待时间，当没有可用连接时，连接池等待连接释放的最大时间，超过该时间限制会抛出异常，如果设置-1表示无限等待 poolPreparedStatements false 开启池的Statement是否prepared maxOpenPreparedStatements 无限制 开启池的prepared 后的同时最大连接数 minEvictableIdleTimeMillis 连接池中连接，在时间段内一直空闲， 被逐出连接池的时间 removeAbandonedTimeout 300 超过时间限制，回收没有用(废弃)的连接 removeAbandoned false 超过removeAbandonedTimeout时间后，是否进 行没用连接（废弃）的回收 获取连接方式一： 1234567891011121314public static Connection getConnection3() throws Exception &#123; BasicDataSource source = new BasicDataSource(); source.setDriverClassName(\"com.mysql.jdbc.Driver\"); source.setUrl(\"jdbc:mysql:///test\"); source.setUsername(\"root\"); source.setPassword(\"abc123\"); // source.setInitialSize(10); Connection conn = source.getConnection(); return conn;&#125; 获取连接方式二： 12345678910111213141516171819202122//使用dbcp数据库连接池的配置文件方式，获取数据库的连接：推荐private static DataSource source = null;static&#123; try &#123; Properties pros = new Properties(); InputStream is = DBCPTest.class.getClassLoader().getResourceAsStream(\"dbcp.properties\"); pros.load(is); //根据提供的BasicDataSourceFactory创建对应的DataSource对象 source = BasicDataSourceFactory.createDataSource(pros); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;public static Connection getConnection4() throws Exception &#123; Connection conn = source.getConnection(); return conn;&#125; 其中，src下的配置文件为：【dbcp.properties】 1234567driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=true&amp;useServerPrepStmts=falseusername=rootpassword=abc123initialSize=10#... 8.3.3 Druid（德鲁伊）数据库连接池Druid是阿里巴巴开源平台上一个数据库连接池实现，它结合了C3P0、DBCP、Proxool等DB池的优点，同时加入了日志监控，可以很好的监控DB池连接和SQL的执行情况，可以说是针对监控而生的DB连接池，可以说是目前最好的连接池之一。 123456789101112131415161718192021package com.atguigu.druid;import java.sql.Connection;import java.util.Properties;import javax.sql.DataSource;import com.alibaba.druid.pool.DruidDataSourceFactory;public class TestDruid &#123; public static void main(String[] args) throws Exception &#123; // 可以放到静态代码块 Properties pro = new Properties(); pro.load(TestDruid.class.getClassLoader().getResourceAsStream(\"druid.properties\")); DataSource ds = DruidDataSourceFactory.createDataSource(pro); Connection conn = ds.getConnection(); System.out.println(conn); &#125;&#125; 其中，src下的配置文件为：【druid.properties】 123456789url=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=trueusername=rootpassword=123456driverClassName=com.mysql.jdbc.DriverinitialSize=10maxActive=20maxWait=1000filters=wall 详细配置参数： 配置 缺省 说明 name 配置这个属性的意义在于，如果存在多个数据源，监控的时候可以通过名字来区分开来。 如果没有配置，将会生成一个名字，格式是：”DataSource-” + System.identityHashCode(this) url 连接数据库的url，不同数据库不一样。例如：mysql : jdbc:mysql://10.20.153.104:3306/druid2 oracle : jdbc:oracle:thin:@10.20.149.85:1521:ocnauto username 连接数据库的用户名 password 连接数据库的密码。如果你不希望密码直接写在配置文件中，可以使用ConfigFilter。详细看这里：https://github.com/alibaba/druid/wiki/%E4%BD%BF%E7%94%A8ConfigFilter driverClassName 根据url自动识别 这一项可配可不配，如果不配置druid会根据url自动识别dbType，然后选择相应的driverClassName(建议配置下) initialSize 0 初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时 maxActive 8 最大连接池数量 maxIdle 8 已经不再使用，配置了也没效果 minIdle 最小连接池数量 maxWait 获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。 poolPreparedStatements false 是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql下建议关闭。 maxOpenPreparedStatements -1 要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题，可以把这个数值配置大一些，比如说100 validationQuery 用来检测连接是否有效的sql，要求是一个查询语句。如果validationQuery为null，testOnBorrow、testOnReturn、testWhileIdle都不会其作用。 testOnBorrow true 申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 testOnReturn false 归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能 testWhileIdle false 建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。 timeBetweenEvictionRunsMillis 有两个含义： 1)Destroy线程会检测连接的间隔时间2)testWhileIdle的判断依据，详细看testWhileIdle属性的说明 numTestsPerEvictionRun 不再使用，一个DruidDataSource只支持一个EvictionRun minEvictableIdleTimeMillis connectionInitSqls 物理连接初始化的时候执行的sql exceptionSorter 根据dbType自动识别 当数据库抛出一些不可恢复的异常时，抛弃连接 filters 属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有： 监控统计用的filter:stat日志用的filter:log4j防御sql注入的filter:wall proxyFilters 类型是List，如果同时配置了filters和proxyFilters，是组合关系，并非替换关系 第9章：Apache-DBUtils实现CRUD操作9.1 Apache-DBUtils简介 commons-dbutils 是 Apache 组织提供的一个开源 JDBC工具类库，它是对JDBC的简单封装，学习成本极低，并且使用dbutils能极大简化jdbc编码的工作量，同时也不会影响程序的性能。 API介绍： org.apache.commons.dbutils.QueryRunner org.apache.commons.dbutils.ResultSetHandler 工具类：org.apache.commons.dbutils.DbUtils API包说明： 9.2 主要API的使用9.2.1 DbUtils DbUtils ：提供如关闭连接、装载JDBC驱动程序等常规工作的工具类，里面的所有方法都是静态的。主要方法如下： public static void close(…) throws java.sql.SQLException： DbUtils类提供了三个重载的关闭方法。这些方法检查所提供的参数是不是NULL，如果不是的话，它们就关闭Connection、Statement和ResultSet。 public static void closeQuietly(…): 这一类方法不仅能在Connection、Statement和ResultSet为NULL情况下避免关闭，还能隐藏一些在程序中抛出的SQLEeception。 public static void commitAndClose(Connection conn)throws SQLException： 用来提交连接的事务，然后关闭连接 public static void commitAndCloseQuietly(Connection conn)： 用来提交连接，然后关闭连接，并且在关闭连接时不抛出SQL异常。 public static void rollback(Connection conn)throws SQLException：允许conn为null，因为方法内部做了判断 public static void rollbackAndClose(Connection conn)throws SQLException rollbackAndCloseQuietly(Connection) public static boolean loadDriver(java.lang.String driverClassName)：这一方装载并注册JDBC驱动程序，如果成功就返回true。使用该方法，你不需要捕捉这个异常ClassNotFoundException。 9.2.2 QueryRunner类 该类简单化了SQL查询，它与ResultSetHandler组合在一起使用可以完成大部分的数据库操作，能够大大减少编码量。 QueryRunner类提供了两个构造器： 默认的构造器 需要一个 javax.sql.DataSource 来作参数的构造器 QueryRunner类的主要方法： 更新 public int update(Connection conn, String sql, Object… params) throws SQLException:用来执行一个更新（插入、更新或删除）操作。 …… 插入 public T insert(Connection conn,String sql,ResultSetHandler rsh, Object… params) throws SQLException：只支持INSERT语句，其中 rsh - The handler used to create the result object from the ResultSet of auto-generated keys. 返回值: An object generated by the handler.即自动生成的键值 …. 批处理 public int[] batch(Connection conn,String sql,Object[][] params)throws SQLException： INSERT, UPDATE, or DELETE语句 public T insertBatch(Connection conn,String sql,ResultSetHandler rsh,Object[][] params)throws SQLException：只支持INSERT语句 ….. 查询 public Object query(Connection conn, String sql, ResultSetHandler rsh,Object… params) throws SQLException：执行一个查询操作，在这个查询中，对象数组中的每个元素值被用来作为查询语句的置换参数。该方法会自行处理 PreparedStatement 和 ResultSet 的创建和关闭。 …… 测试 123456789101112// 测试添加@Testpublic void testInsert() throws Exception &#123; QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3();//Druid连接方式 String sql = \"insert into customers(name,email,birth)values(?,?,?)\"; int count = runner.update(conn, sql, \"何成飞\", \"he@qq.com\", \"1992-09-08\"); System.out.println(\"添加了\" + count + \"条记录\"); JDBCUtils.closeResource(conn, null);&#125; 123456789101112131415161718/** * 使用Druid数据库连接池技术 */ private static DataSource source1; static&#123; try &#123; Properties pros = new Properties(); InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(\"druid.properties\"); pros.load(is); source1 = DruidDataSourceFactory.createDataSource(pros); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static Connection getConnection3() throws SQLException&#123; Connection conn = source1.getConnection(); return conn; &#125; 12345678910111213// 测试删除@Testpublic void testDelete() throws Exception &#123; QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); String sql = \"delete from customers where id &lt; ?\"; int count = runner.update(conn, sql,3); System.out.println(\"删除了\" + count + \"条记录\"); JDBCUtils.closeResource(conn, null);&#125; 9.2.3 ResultSetHandler接口及实现类 该接口用于处理 java.sql.ResultSet，将数据按要求转换为另一种形式。 ResultSetHandler 接口提供了一个单独的方法：Object handle (java.sql.ResultSet .rs)。 接口的主要实现类： ArrayHandler：把结果集中的第一行数据转成对象数组。 ArrayListHandler：把结果集中的每一行数据都转成一个数组，再存放到List中。 BeanHandler：将结果集中的第一行数据封装到一个对应的JavaBean实例中。 BeanListHandler：将结果集中的每一行数据都封装到一个对应的JavaBean实例中，存放到List里。 ColumnListHandler：将结果集中某一列的数据存放到List中。 KeyedHandler(name)：将结果集中的每一行数据都封装到一个Map里，再把这些map再存到一个map里，其key为指定的key。 MapHandler：将结果集中的第一行数据封装到一个Map里，key是列名，value就是对应的值。 MapListHandler：将结果集中的每一行数据都封装到一个Map里，然后再存放到List ScalarHandler：查询单个值对象 测试 123456789101112131415161718/* * 测试查询:查询一条记录 * 使用ResultSetHandler的实现类：BeanHandler */@Testpublic void testQueryInstance() throws Exception&#123; QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); String sql = \"select id,name,email,birth from customers where id = ?\"; // BeanHandler&lt;Customer&gt; handler = new BeanHandler&lt;&gt;(Customer.class); Customer customer = runner.query(conn, sql, handler, 23); System.out.println(customer); JDBCUtils.closeResource(conn, null);&#125; 12345678910111213141516171819/* * 测试查询:查询多条记录构成的集合 * 使用ResultSetHandler的实现类：BeanListHandler */@Testpublic void testQueryList() throws Exception&#123; QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); String sql = \"select id,name,email,birth from customers where id &lt; ?\"; // BeanListHandler&lt;Customer&gt; handler = new BeanListHandler&lt;&gt;(Customer.class); List&lt;Customer&gt; list = runner.query(conn, sql, handler, 23); list.forEach(System.out::println); JDBCUtils.closeResource(conn, null);&#125; 12345678910111213141516171819202122232425262728293031323334353637/* * 自定义ResultSetHandler的实现类 */@Testpublic void testQueryInstance1() throws Exception&#123; QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); String sql = \"select id,name,email,birth from customers where id = ?\"; ResultSetHandler&lt;Customer&gt; handler = new ResultSetHandler&lt;Customer&gt;() &#123; @Override public Customer handle(ResultSet rs) throws SQLException &#123; System.out.println(\"handle\");// return new Customer(1,\"Tom\",\"tom@126.com\",new Date(123323432L)); if(rs.next())&#123; int id = rs.getInt(\"id\"); String name = rs.getString(\"name\"); String email = rs.getString(\"email\"); Date birth = rs.getDate(\"birth\"); return new Customer(id, name, email, birth); &#125; return null; &#125; &#125;; Customer customer = runner.query(conn, sql, handler, 23); System.out.println(customer); JDBCUtils.closeResource(conn, null);&#125; 12345678910111213141516171819202122232425/* * 如何查询类似于最大的，最小的，平均的，总和，个数相关的数据， * 使用ScalarHandler * */@Testpublic void testQueryValue() throws Exception&#123; QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); //测试一：// String sql = \"select count(*) from customers where id &lt; ?\";// ScalarHandler handler = new ScalarHandler();// long count = (long) runner.query(conn, sql, handler, 20);// System.out.println(count); //测试二： String sql = \"select max(birth) from customers\"; ScalarHandler handler = new ScalarHandler(); Date birth = (Date) runner.query(conn, sql, handler); System.out.println(birth); JDBCUtils.closeResource(conn, null);&#125; JDBC总结1234567891011121314151617181920212223242526272829303132总结@Testpublic void testUpdateWithTx() &#123; Connection conn = null; try &#123; //1.获取连接的操作（ //① 手写的连接：JDBCUtils.getConnection(); //② 使用数据库连接池：C3P0;DBCP;Druid //2.对数据表进行一系列CRUD操作 //① 使用PreparedStatement实现通用的增删改、查询操作（version 1.0 \\ version 2.0)//version2.0的增删改public void update(Connection conn,String sql,Object ... args)&#123;&#125;//version2.0的查询 public &lt;T&gt; T getInstance(Connection conn,Class&lt;T&gt; clazz,String sql,Object ... args)&#123;&#125; //② 使用dbutils提供的jar包中提供的QueryRunner类 //提交数据 conn.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); try &#123; //回滚数据 conn.rollback(); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; &#125;finally&#123; //3.关闭连接等操作 //① JDBCUtils.closeResource(); //② 使用dbutils提供的jar包中提供的DbUtils类提供了关闭的相关操作 &#125;&#125;","tags":[{"name":"JDBC","slug":"JDBC","permalink":"https://mtcai.github.io/tags/JDBC/"}]},{"title":"概述&安装与源码编译","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop01-概述、运行模式&源码编译.html","text":"大数据概述 Hadoop三大发行版本：Apache、Cloudera、Hortonworks。 Apache版本最原始（最基础）的版本，对于入门学习最好。 官网地址：http://hadoop.apache.org/releases.html 下载地址：https://archive.apache.org/dist/hadoop/common/ Cloudera在大型互联网企业中用的较多。 官网地址：https://www.cloudera.com/downloads/cdh/5-10-0.html 下载地址：http://archive-primary.cloudera.com/cdh5/cdh/5/ CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强 Hortonworks文档较好。 官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform Hadoop组成 HDFS（Hadoop Distributed File System）的架构概述 YARN (Yet Another Resource Negotiator) 架构概述 RM 作用： 处理客户端请求 监控 NodeManager 启动或监控 ApplicationMaster 资源的分配与调度 NodeManger 作用： 管理单个节点的资源 处理来自RM的命令 处理来自AM的命令 ApplicationMaster 作用： 负责数据的切分 为应用程序申请资源，并分配给内部的任务 任务的监控与容错 Container 作用： Container 是Yarn中资源的抽象，它封装了某个节点上的多维度资源，如内存，cpu，磁盘，网络等 MapReduce架构概述MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据 2）Reduce阶段对Map结果进行汇总 HDFS、YARN、MapReduce 三者关系 大数据技术生态体系 图中涉及的技术名词解释如下： Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。 Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 支持通过Kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。 Storm：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。 Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。 Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多 Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。 Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。 Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。 R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。 Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库。 ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 推荐系统框架图 Hadoop目录结构 drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 bin drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 etc drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 include drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 lib drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 libexec -rw-r—r—. 1 atguigu atguigu 15429 5月 22 2017 LICENSE.txt -rw-r—r—. 1 atguigu atguigu 101 5月 22 2017 NOTICE.txt -rw-r—r—. 1 atguigu atguigu 1366 5月 22 2017 README.txt drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 sbin drwxr-xr-x. 4 atguigu atguigu 4096 5月 22 2017 share bin目录：存放对Hadoop相关服务（HDFS,YARN,MapRed）进行操作的脚本 etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能） sbin目录：存放启动或停止Hadoop相关服务的脚本 share目录：存放Hadoop的依赖jar包、文档、和官方案例 伪分布式运行模式启动HDFS并运行MapReduce程序1 执行步骤 （a）配置：hadoop-env.sh Linux系统中获取JDK的安装路径： [atguigu@ hadoop101 ~]# echo $JAVA_HOME/opt/module/jdk1.8.0_144 修改JAVA_HOME 路径： export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置：core-site.xml 12345678910&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;&lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; （c）配置：hdfs-site.xml 12345&lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 2 启动集群 （a）格式化NameNode（第一次启动时格式化，以后就不要总格式化） [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs namenode -format （b）启动NameNode [atguigu@hadoop1 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode （c）启动DataNode [atguigu@hadoop1 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode 3 查看集群 a）查看是否启动成功 [atguigu@hadoop1 hadoop-2.7.2]$ jps 13586 NameNode 13668 DataNode 13786 Jps 注意：jps是JDK中的命令，不是Linux命令。不安装JDK不能使用jps （b）web端查看HDFS文件系统 http://hadoop1:50070/dfshealth.html#tab-overview 注意：如果不能查看，看如下帖子处理：http://www.cnblogs.com/zlslch/p/6604189.html （c）查看产生的Log日志 说明：在企业中遇到Bug时，经常根据日志提示信息去分析问题、解决Bug。 当前目录：/opt/module/hadoop-2.7.2/logs [atguigu@hadoop101 logs]$ ls hadoop-atguigu-datanode-hadoop.atguigu.com.log hadoop-atguigu-datanode-hadoop.atguigu.com.out hadoop-atguigu-namenode-hadoop.atguigu.com.log hadoop-atguigu-namenode-hadoop.atguigu.com.out SecurityAuth-root.audit [atguigu@hadoop101 logs]# cat hadoop-atguigu-datanode-hadoop101.log （d）思考：为什么不能一直格式化NameNode，格式化NameNode，要注意什么？ [atguigu@hadoop1 hadoop-2.7.2]$ cd data/tmp/dfs/name/current/ [atguigu@hadoop1 current]$ cat VERSION clusterID=CID-f0330a58-36fa-4a2a-a65f-2688269b5837** [atguigu@hadoop1 hadoop-2.7.2]$ cd data/tmp/dfs/data/current/clusterID=CID-f0330a58-36fa-4a2a-a65f-2688269b5837 注意：格式化NameNode，会产生新的集群id,导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。 4 操作集群 ​ （a）在HDFS文件系统上创建一个input文件夹 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/atguigu/input ​ （b）将测试文件内容上传到文件系统上 [atguigu@hadoop1 hadoop-2.7.2]$bin/hdfs dfs -put wcinput/wc.input /user/atguigu/input/ ​ （c）查看上传的文件是否正确 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -ls /user/atguigu/input/ [atguigu@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/atguigu/ input/wc.input ​ （d）运行MapReduce程序 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input/ /user/atguigu/output ​ （e）查看输出结果 命令行查看： [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/atguigu/output/* 浏览器查看，如图 ​ （f）将测试文件内容下载到本地 [atguigu@hadoop1 hadoop-2.7.2]$ hdfs dfs -get /user/atguigu/output/part-r-00000 ./wcoutput/ （g）删除输出结果 [atguigu@hadoop1 hadoop-2.7.2]$ hdfs dfs -rm -r /user/atguigu/output 启动YARN并运行MapReduce程序1 配置集群 （a）配置yarn-env.sh 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置yarn-site.xml 12345678910&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt;&lt;/property&gt; （c）配置：mapred-env.sh 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 （d）配置： (对mapred-site.xml.template重新命名为) mapred-site.xml [atguigu@hadoop1 hadoop]$ mv mapred-site.xml.template mapred-site.xml [atguigu@hadoop1 hadoop]$ vi mapred-site.xml 12345&lt;!-- 指定MR运行在YARN上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 2 启动集群 （a）启动前必须保证NameNode和DataNode已经启动 （b）启动ResourceManager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager （c）启动NodeManager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager 3 集群操作 （a）YARN的浏览器页面查看，如图所示 http://hadoop101:8088/cluster （b）删除文件系统上的output文件 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/atguigu/output （c）执行MapReduce程序 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input /user/atguigu/output （d）查看运行结果，如图2-36所示 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/atguigu/output/* 配置历史服务器为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下： 配置mapred-site.xml [atguigu@hadoop1 hadoop]$ vi mapred-site.xml 在该文件里面增加如下配置。 1234567891011&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop1:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop1:19888&lt;/value&gt;&lt;/property&gt; 启动历史服务器 [atguigu@hadoop1 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver 查看历史服务器是否启动 [atguigu@hadoop1 hadoop-2.7.2]$ jps 查看JobHistory http://hadoop1:19888/jobhistory 配置日志的聚集日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。 日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。 注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager。 开启日志聚集功能具体步骤如下： 配置yarn-site.xml [atguigu@hadoop1 hadoop]$ vi yarn-site.xml 在该文件里面增加如下配置。 1234567891011121314151617&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置日志聚集服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 关闭NodeManager 、ResourceManager和HistoryManager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh stop historyserver 启动NodeManager 、ResourceManager和HistoryManager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver 删除HDFS上已经存在的输出文件 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/atguigu/output 执行WordCount程序 [atguigu@hadoop11 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input /user/atguigu/output 查看日志，如图所示 http://hadoop1:19888/jobhistory 配置文件说明Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。 （1）默认配置文件： 要获取的默认文件 文件存放在Hadoop的jar包中的位置 [core-default.xml] hadoop-common-2.7.2.jar/ core-default.xml [hdfs-default.xml] hadoop-hdfs-2.7.2.jar/ hdfs-default.xml [yarn-default.xml] hadoop-yarn-common-2.7.2.jar/ yarn-default.xml [mapred-default.xml] hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml （2）自定义配置文件： ​ core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置 完全分布式运行模式rsync 远程同步工具rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。 （1）基本语法 rsync -rvl \\$pdir/​\\$fname ​\\$user@hadoop\\$host:\\$pdir/​\\$fname 命令 选项参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 ​ 选项参数说明 选项 功能 -r 递归 -v 显示复制过程 -l 拷贝符号连接 xsync集群分发脚本（1）需求：循环复制文件到所有节点的相同目录下 （2）需求分析： rsync命令原始拷贝： rsync -rvl /opt/module root@hadoop103:/opt/ 期望脚本： xsync 要同步的文件名称 说明：在/home/atguigu/bin这个目录下存放的脚本，atguigu用户可以在系统任何地方直接执行。 （3）脚本实现 在/home/atguigu目录下创建bin目录，并在bin目录下xsync创建文件，文件内容如下： [atguigu@hadoop102 ~]$ mkdir bin [atguigu@hadoop102 ~]$ cd bin/ [atguigu@hadoop102 bin]$ touch xsync [atguigu@hadoop102 bin]$ vi xsync ​ 在该文件中编写如下代码 1234567891011121314151617181920212223242526#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 修改脚本 xsync 具有执行权限 [atguigu@hadoop2 bin]$ chmod 777 xsync 调用脚本形式：xsync 文件名称 [atguigu@hadoop2 bin]$ xsync /home/atguigu/bin 注意：如果将xsync放到/home/atguigu/bin目录下仍然不能实现全局使用，可以将xsync移动到/usr/local/bin目录下。 集群配置➢ ➢ 集群部署规划 hadoop102 —-&gt;hadoop1 hadoop103 —-&gt;hadoop2 hadoop104 —-&gt;hadoop3 hadoop102 hadoop103 hadoop104 HDFS NameNodeDataNode DataNode SecondaryNameNodeDataNode YARN NodeManager ResourceManagerNodeManager NodeManager NameNode和SecondaryNameNode使用相同大小的内存，故一般不放在同一个服务器上 ResourceManager也比较耗内存，故不和NameNode、SecondaryNameNode放在同一个服务器 配置集群（1）核心配置文件 配置core-site.xml [atguigu@hadoop102 hadoop]$ vi core-site.xml 在该文件中编写如下配置 123456789101112131415161718&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt;&lt;!-- Hadoop3.x 配置 HDFS 网页登录使用的静态用户为 atguigu --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt; （2）HDFS配置文件 配置hadoop-env.sh [atguigu@hadoop102 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置hdfs-site.xml [atguigu@hadoop102 hadoop]$ vi hdfs-site.xml 在该文件中编写如下配置 1234567891011121314151617&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- nn web 端访问地址 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;!-- 2 nn web 端访问地址 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt; （3）YARN配置文件 配置yarn-env.sh [atguigu@hadoop102 hadoop]$ vi yarn-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置yarn-site.xml [atguigu@hadoop102 hadoop]$ vi yarn-site.xml 在该文件中增加如下配置 12345678910111213141516171819202122232425262728293031323334353637&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置日志聚集服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;&lt;!-- 环境变量的继承 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE, HADOOP_YARN_HOME, HADOOP_MAPRED_HOME &lt;/value&gt;&lt;/property&gt; （4）MapReduce配置文件 配置mapred-env.sh [atguigu@hadoop102 hadoop]$ vi mapred-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置mapred-site.xml [atguigu@hadoop102 hadoop]$ cp mapred-site.xml.template mapred-site.xml [atguigu@hadoop102 hadoop]$ vi mapred-site.xml 在该文件中增加如下配置 12345678910111213141516&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop102:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器 web 端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;19888&lt;/value&gt;&lt;/property&gt; 在集群上分发配置好的Hadoop配置文件 [atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/ 查看文件分发情况 [atguigu@hadoop103 hadoop]$ cat /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml 集群单点启动（1）如果集群是第一次启动，需要格式化NameNode，==启动前要关闭所有服务== [atguigu@hadoop102 hadoop-2.7.2]$ hadoop namenode -format （2）在hadoop102上启动NameNode [atguigu@hadoop102 hadoop-2.7.2]$ hadoop-daemon.sh start namenode [atguigu@hadoop102 hadoop-2.7.2]$ jps 3461 NameNode （3）在hadoop102、hadoop103以及hadoop104上分别启动DataNode [atguigu@hadoop102 hadoop-2.7.2]$ hadoop-daemon.sh start datanode [atguigu@hadoop102 hadoop-2.7.2]$ jps 3461 NameNode 3608 Jps 3561 DataNode [atguigu@hadoop103 hadoop-2.7.2]$ hadoop-daemon.sh start datanode [atguigu@hadoop103 hadoop-2.7.2]$ jps 3190 DataNode 3279 Jps [atguigu@hadoop104 hadoop-2.7.2]$ hadoop-daemon.sh start datanode [atguigu@hadoop104 hadoop-2.7.2]$ jps 3237 Jps 3163 DataNode （4）思考：每次都一个一个节点启动，如果节点数增加到1000个怎么办？ ​ 早上来了开始一个一个节点启动，到晚上下班刚好完成，下班？ SSH无密登录配置 生成公钥和私钥 [atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登录的目标机器上 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104 注意： 还需要在hadoop102上采用 root 账号，配置一下无密登录到hadoop102、hadoop103、hadoop104； 还需要在hadoop103上采用 atguigu 账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。 还需要在hadoop104上采用 atguigu 账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。 .ssh文件夹下（~/.ssh）的文件功能解释 known_hosts 记录ssh访问过计算机的公钥(public key) id_rsa 生成的私钥 id_rsa.pub 生成的公钥 authorized_keys 存放授权过得无密登录服务器公钥 群起集群配置slaves/workers12345678hadoop2.x修改 /opt/module/hadoop-2.7.2/etc/hadoop/slaveshadoop3.x修改 /opt/module/hadoop-3.1.3/etc/hadoop/workers[atguigu@hadoop102 hadoop]$ vi slaves在该文件中增加如下内容：注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。hadoop102hadoop103hadoop104 同步所有节点配置文件: [atguigu@hadoop102 hadoop]$ xsync slaves 启动集群 如果集群是第一次启动，需要格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据） 注意：格式化 NameNode 会产生新的集群 id，导致 NameNode 和 DataNode的集群 id不一致，集群找不到已往数据。 如果集群在运行过程中报错，需要重新格式化 NameNode的话， 一定要 先停止 namenode和 datanode进程， 并且要 删除 所有机器的 data和 logs目录，然后再进行格式化。 启动HDFS [atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [atguigu@hadoop102 hadoop-2.7.2]$ jps 4166 NameNode 4482 Jps 4263 DataNode [atguigu@hadoop103 hadoop-2.7.2]$ jps 3218 DataNode 3288 Jps [atguigu@hadoop104 hadoop-2.7.2]$ jps 3221 DataNode 3283 SecondaryNameNode 3364 Jps 启动YARN [atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh 注意：NameNode和ResourceManger如果不是同一台机器，不能在NameNode上启动 YARN，应该在ResouceManager所在的机器上启动YARN。 Web端查看SecondaryNameNode 浏览器中输入：http://hadoop104:50090/status.html 常用命令12345678910111213141516171819202122232425262728starthadoop_home=/usr/local/hadoop$hadoop_home/sbin/hadoop-daemon.sh start namenode$hadoop_home/sbin/hadoop-daemon.sh start datanode$hadoop_home/sbin/yarn-daemon.sh start resourcemanager$hadoop_home/sbin/yarn-daemon.sh start nodemanager$hadoop_home/sbin/yarn-daemon.sh start historyserver$hadoop_home/sbin/mr-jobhistory-daemon.sh start historyserverstophadoop_home=/usr/local/hadoop$hadoop_home/sbin/hadoop-daemon.sh stop namenode$hadoop_home/sbin/hadoop-daemon.sh stop datanode$hadoop_home/sbin/yarn-daemon.sh stop resourcemanager$hadoop_home/sbin/yarn-daemon.sh stop nodemanager$hadoop_home/sbin/yarn-daemon.sh stop historyserver$hadoop_home/sbin/mr-jobhistory-daemon.sh stop historyserverhdfs --daemon start/stop namenode/datanode/secondarynamenodeyarn --daemon start/stop resourcemanager/nodemanagerbin/mapred --daemon start/stop historyserver#各个模块分开启动/停止（配置ssh是前提）#整体启动/停止HDFSstart-dfs.sh / stop-dfs.sh#整体启动/停止YARNstart-yarn.sh / stop-yarn.sh 集群时间同步 时间服务器配置（必须root用户) 检查 ntp 是否安装 [root@hadoop102 桌面]# rpm -qa|grep ntp ntp-4.2.6p5-10.el6.centos.x86_64 fontpackages-filesystem-1.41-1.1.el6.noarch ntpdate-4.2.6p5-10.el6.centos.x86_64 查看 所有节点 ntpd服务 状态 和 开机自启动 状态 sudo systemctl status ntpd sudo systemctl start ntpd sudo systemctl is-enabled ntpd 修改ntp配置文件 [root@hadoop102 桌面]# vi /etc/ntp.conf 修改内容如下 a）修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间） #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap 为 restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap b）修改2（集群在局域网中，不使用其他互联网上的时间） server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst为 #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步） server 127.127.1.0 fudge 127.127.1.0 stratum 10 修改/etc/sysconfig/ntpd 文件 root@hadoop102 桌面]# vim /etc/sysconfig/ntpd 增加内容如下（让硬件时间与系统时间一起同步） SYNC_HWCLOCK=yes 重新启动ntpd服务 [root@hadoop102 桌面]# service ntpd start 设置ntpd服务开机启动 [root@hadoop102 桌面]# chkconfig ntpd on 其他机器配置（必须root用户） 在其他机器配置10分钟与时间服务器同步一次 [root@hadoop103桌面]# crontab -e 编写定时任务如下： /10 * /usr/sbin/ntpdate hadoop102 关闭 所有节点上 ntp服务和自启动 sudo systemctl stop ntpd sudo systemctl disable ntpd 修改任意机器时间 [root@hadoop103桌面]# date -s “2017-9-11 11:11:11” 十分钟后查看机器是否与时间服务器同步 [root@hadoop103桌面]# date Hadoop编译源码（面试重点）准备工作 （1）系统联网，或者有yum源 （2）hadoop-2.7.2-src.tar.gz 进入hadoop-2.7.2-src文件夹，查看BUILDING.txt cd hadoop-2.7.2-src more BUILDING.txt 可以看到编译所需的库或者工具 （3）jdk-8u144-linux-x64.tar.gz （4）apache-ant-1.9.9-bin.tar.gz（build工具，打包用的） （5）apache-maven-3.0.5-bin.tar.gz （6）protobuf-2.5.0.tar.gz（序列化的框架） （7）apache-tomcat-6.0.44.tar.gz 配置jdk验证命令：java -version 配置Maven [root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml ==== ​ ==nexus-aliyun== ​ ==central== ​ ==Nexus aliyun== ​ ==[http://maven.aliyun.com/nexus/content/groups/public](http://maven.aliyun.com/nexus/content/groups/public)== ​ ==== [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile MAVEN_HOMEexport MAVEN_HOME=/opt/module/apache-maven-3.0.5 export PATH=\\$PATH:\\$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn -version 配置ant [root@hadoop101 apache-ant-1.9.9]# vi /etc/profile ANT_HOMEexport ANT_HOME=/opt/module/apache-ant-1.9.9 export PATH=\\$PATH:\\$ANT_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：ant -version 安装 g++、make、cmake等库 [root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers [root@hadoop101 apache-ant-1.9.9]# yum -y install svn ncurses-devel gcc* [root@hadoop101 apache-ant-1.9.9]# yum install make [root@hadoop101 apache-ant-1.9.9]# yum install cmake [root@hadoop101 apache-ant-1.9.9]# yum -y install lzo-devel zlib-devel autoconf automake libtool cmake openssl-devel 安装protobuf [root@hadoop101 protobuf-2.5.0]#./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make check [root@hadoop101 protobuf-2.5.0]# make install [root@hadoop101 protobuf-2.5.0]# ldconfig [root@hadoop101 hadoop-dist]# vi /etc/profile LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0 export PATH=\\$PATH:\\$LD_LIBRARY_PATH [root@hadoop101 software]#source /etc/profile 验证命令：protoc —version 安装findbugs 解压：tar -zxvf findbugs-3.0.1.tar.gz -C /opt/moudles/ 配置环境变量: 在 /etc/profile 文件末尾添加： export FINDBUGS_HOME=/opt/findbugs-3.0.1 export PATH=\\$PATH:\\$FINDBUGS_HOME/bin 保存退出，并使更改生效。 验证命令：findbugs -version 编译源码 1.进入到源码目录 [root@hadoop101 hadoop-2.7.2-src]# pwd /opt/hadoop-2.7.2-src 2.通过maven执行编译命令 [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 编译过程中会下载 apache-tomcat-6.0.44.tar.gz，速度非常慢，把提前下载好的文件放到如下目录： 注：编译前这两个目录并不存在，编译过程中及时中断，然后复制文件 hadoop-2.7.2-src/hadoop-common-project/hadoop-kms/downloads/ hadoop-2.7.2-src/hadoop-hdfs-project/hadoop-hdfs-httpfs/downloads 等待时间30分钟左右，最终成功是全部SUCCESS，如图 成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下 编译源码过程中常见的问题及解决方案（1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） （2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar （3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐2.7.0版本的问题汇总帖子 http://www.tuicool.com/articles/IBn63qf","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"HDFS概述","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop02-HDFS概述、shell&客户端操作.html","text":"HDFS概述HDFS（Hadoop Distributed File System）是一种分布式文件管理系统。通过目录树定位文件；其次有很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 应用场景：适合一次写入，多次读出的场景，且不支持文件的修改，适合用来做数据分析，不适合用来做网盘应用。 优缺点优点： 高容错性 数据自动保存多个副本，通过增加副本的形式，提高容错性 某一个副本丢失后，他可以自动恢复 适合处理大数据 数据规模：能够处理数据规模达到GB，TB，甚至PB级别的数据 文件规模：能够处理百万规模以上的文件数量 可构建在廉价的机器上，通过多副本的机制，提高可靠性 缺点： 不适合低延时数据访问，比如毫秒级别的存储数据 无法高效地对大量小文件进行存储 存储大量小文件的话，会占用 NameNode 大量的内存来存储文件目录和块信息 小文件的存储的寻址时间会超过读取时间，违反了HDFS的设计目标 不支持文件并发写入，随机修改 一个文件只能有一个写，不允许多个线程同时写 仅支持数据 append，不支持文件的随机修改 组成架构 文件块大小（面试重点）HDFS中的文件在物理上是分块存储(block)，块的大小可以通过配置参数（dfs.blocksize）规定，yarn集群的默认大小在Hadoop2.x/3.x中是128M，Hadoop1.x 是64M，在本地运行时32M。 使用存储块的好处 假如上传的一个文件非常大，没有任何一块磁盘能够存储，这样这个文件就没法上传了，如果使用块的概念，会把文件分割成许多块，这样这个文件可以使用集群中的任意节点进行存储。 数据存储要考虑容灾备份，以块为单位非常有利于进行备份，HDFS默认每个块备份3份，这样如果这个块上或这个节点坏掉，可以直接找其他节点上的备份块。还有就是，有的时候需要将备份数量提高， 这样能够分散机群的读取负载，因为可以在多个节点中寻找到目标数据，减少单个节点读取。 问：为什么块的大小不能太大，也不能太小？ 如果块设置过大， 从磁盘传输数据的时间会明显大于寻址时间，导致程序在处理这块数据时，变得非常慢； mapreduce中的map任务通常一次只处理一个块中的数据，如果块过大运行速度也会很慢。 在数据读写计算的时候,需要进行网络传输。如果block过大会导致网络传输时间增长，程序卡顿/超时/无响应. 任务执行的过程中拉取其他节点的block或者失败重试的成本会过高. namenode监管容易判断数据节点死亡.导致集群频繁产生/移除副本, 占用cpu,网络,内存资源. 如果块设置过小， 存放大量小文件会占用NameNode中大量内存来存储元数据，而NameNode的物理内存是有限的； 文件块过小，寻址时间增大，导致程序一直在找block的开始位置。 操作系统对目录中的小文件处理存在性能问题.比如同一个目录下文件数量操作100万,执行”fs -l “之类的命令会卡死. 则会频繁的进行文件传输,对严重占用网络/CPU资源. 主要取决于磁盘/网络的传输速率。[其实就是CPU,磁盘,网卡之间的协同效率 即 跨物理机/机架之间文件传输速率] 为什么分片大小需要与HDFS数据块（分块）大小一致 hadoop将mapReduce的输入数据划分为等长的小数据块，称为输入分片或者分片，hadoop为每个分片构建一个map任务。 hadoop在存储有输入数据（HDFS中的数据）的节点上运行map任务，可以获得高性能，这就是所谓的数据本地化。所以最佳分片的大小应该与HDFS上的块大小一样，因为如果分片跨越2个数据块，对于任何一个HDFS节点（基本不肯能同时存储这2个数据块），分片中的另外一块数据就需要通过网络传输到map任务节点，与使用本地数据运行map任务相比，效率则更低！ 为什么不能远大于64MB或者128MB或256MB？ 这里主要从上层的MapReduce框架来讨论 （1）Map崩溃问题：系统需要重新启动，启动过程需要重新加载数据，数据块越大，数据加载时间越长，系统恢复过程越长。 （2）监管时间问题：主节点监管其他节点的情况，每个节点会周期性的把完成的工作和状态的更新报告回来。如果一个节点保持沉默超过一个预设的时间间隔，主节点记录下这个节点状态为死亡，并把分配给这个节点的数据发到别的节点。对于这个“预设的时间间隔”，这是从数据块的角度大概估算的。假如是对于64MB的数据块，我可以假设你10分钟之内无论如何也能解决了吧， 超过10分钟也没反应，那就是死了。可对于640MB或是1G以上的数据，我应该要估算个多长的时间呢？估算的时间短了，那就误判死亡了，更坏的情况是所有节点都会被判死亡。 估算的时间长了，那等待的时间就过长了。所以对于过大的数据块，这个“预设的时间间隔”不好估算。 （3）Map任务上：因为MapReducer中一般一个map处理一个块上的数据，如果块很大，任务数会很少(少于集群中的节点个数)这样执行效率会明显降低。 HDFS的Shell操作 bin/hadoop fs XXXX bin/hdfs dfs XXXX dfs是fs的实现类 全部命令1234567891011121314151617181920212223242526272829303132333435363738[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs** [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] #追加一个文件到已经存在的文件末尾** [-cat [-ignoreCrc] &lt;src&gt; ...] #显示文件内容 [-checksum &lt;src&gt; ...]* [-chgrp [-R] GROUP PATH...] #和Linux文件系统中的用法一样，修改文件所属权限* [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]* [-chown [-R] [OWNER][:[GROUP]] PATH...]* [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] #从本地文件系统中 拷贝 文件到 HDFS路径去* [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] #从 HDFS拷贝到本地 [-count [-q] &lt;path&gt; ...]** [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;] #从 HDFS的一个路径拷贝到 HDFS的另一个路径 [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]]** [-du [-s] [-h] &lt;path&gt; ...] #统计文件夹的大小信息 [-expunge]** [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] #等同于 copyToLocal [-getfacl [-R] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]]** [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] #显示目录信息** [-mkdir [-p] &lt;path&gt; ...] #创建路径* [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] #从本地 剪切 粘贴到 HDFS [-moveToLocal &lt;src&gt; &lt;localdst&gt;]** [-mv &lt;src&gt; ... &lt;dst&gt;] #在 HDFS目录中移动文件** [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] #等同于 copyFromLocal [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]** [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] #删除文件或文件夹 [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]* [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] #设置 HDFS中文件的副本数量 [-stat [format] &lt;path&gt; ...]* [-tail [-f] &lt;file&gt;] #显示一个文件的末尾 1kb的数据 [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]] 123456789101112131415161718192021222324hadoop fs -help rm; -help：输出这个命令参数hadoop fs -ls /; -ls: 显示目录信息hadoop fs -mkdir -p /sanguo/shuguo; -mkdir：在HDFS上创建目录hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo; -moveFromLocal：从本地剪切粘贴到HDFShadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt; -appendToFile：追加一个文件到已经存在的文件末尾hadoop fs -cat /sanguo/shuguo/kongming.txt; -cat：显示文件内容adoop fs -chmod 666 /sanguo/shuguo/kongming.txt; -chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限hadoop fs -copyFromLocal README.txt /; -copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./; -copyToLocal：从HDFS拷贝到本地hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt; -cp ：从HDFS的一个路径拷贝到HDFS的另一个路径hadoop fs -mv /zhuge.txt /sanguo/shuguo/; -mv：在HDFS目录中移动文件hadoop fs -get /sanguo/shuguo/kongming.txt ./; -get：等同于copyToLocal，就是从HDFS下载文件到本地hadoop fs -getmerge /user/atguigu/test/* ./zaiyiqi.txt; -getmerge：合并下载多个文件hadoop fs -put ./zaiyiqi.txt /user/atguigu/test/; -put：等同于copyFromLocalhadoop fs -tail /sanguo/shuguo/kongming.txt; -tail：显示一个文件的末尾hadoop fs -rm /user/atguigu/test/jinlian2.txt; -rm：删除文件或文件夹hadoop fs -rmdir /test; -rmdir：删除空目录hadoop fs -du -s -h /user/atguigu/test; -du统计文件夹的大小信息&gt; 2.7 K /user/atguigu/testhadoop fs -du -h /user/atguigu/test&gt; 1.3 K /user/atguigu/test/README.txt&gt; 15 /user/atguigu/test/jinlian.txt&gt; 1.4 K /user/atguigu/test/zaiyiqi.txthadoop fs -setrep 10 /sanguo/shuguo/kongming.txt; -setrep：设置HDFS中文件的副本数量 HDFS的客户端操作客户端环境准备配置HADOOP_HOME环境变量；配置Path环境变量 创建一个Maven工程HdfsClientDemo，在pom.xml添加依赖： 12345678910111213141516171819202122232425262728293031323334&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 注意：如果Eclipse/Idea打印不出日志，在控制台上只显示 1231.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). 2.log4j:WARN Please initialize the log4j system properly. 3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入: 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 创建HdfsClient类 12345678910111213141516171819public class HdfsClient&#123; @Testpublic void testMkdirs() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 // configuration.set(\"fs.defaultFS\", \"hdfs://hadoop102:9000\"); // FileSystem fs = FileSystem.get(configuration); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 创建目录 fs.mkdirs(new Path(\"/1108/daxian/banzhang\")); // 3 关闭资源 fs.close(); &#125;&#125; 由于需要连接集群操作，所以需要在idea中设置连接集群的账户名： HDFS的API操作HDFS文件上传（测试参数优先级）12345678910111213141516@Testpublic void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException &#123; // 1 获取文件系统 Configuration configuration = new Configuration(); configuration.set(\"dfs.replication\", \"2\"); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 上传文件 fs.copyFromLocalFile(new Path(\"e:/banzhang.txt\"), new Path(\"/banzhang.txt\")); // 3 关闭资源 fs.close(); System.out.println(\"over\");&#125; 将hdfs-site.xml拷贝到项目的根目录下 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 参数优先级 参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置 HDFS文件下载1234567891011121314151617@Testpublic void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 执行下载操作 // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件校验 fs.copyToLocalFile(false, new Path(\"/banzhang.txt\"), new Path(\"e:/banhua.txt\"), true); // 3 关闭资源 fs.close();&#125; HDFS文件夹删除12345678910111213@Testpublic void testDelete() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 执行删除 fs.delete(new Path(\"/0508/\"), true); // 3 关闭资源 fs.close();&#125; HDFS文件名更改12345678910111213@Testpublic void testRename() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 修改文件名称 fs.rename(new Path(\"/banzhang.txt\"), new Path(\"/banhua.txt\")); // 3 关闭资源 fs.close();&#125; HDFS文件详情查看查看文件名称、权限、长度、块信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Testpublic void testListFiles() throws IOException, InterruptedException, URISyntaxException&#123; // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true); while(listFiles.hasNext())&#123; LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 System.out.println(status.getPath().getName()); // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 分组 System.out.println(status.getGroup()); System.out.println(status.getOwner()); System.out.println(status.getModificationTime()); System.out.println(status.getReplication()); System.out.println(status.getBlockSize()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(\"-----------分割线----------\"); &#125;// 3 关闭资源fs.close();&#125; HDFS文件和文件夹判断1234567891011121314151617181920212223@Testpublic void testListStatus() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件配置信息 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 判断是文件还是文件夹 FileStatus[] listStatus = fs.listStatus(new Path(\"/\")); for (FileStatus fileStatus : listStatus) &#123; // 如果是文件 if (fileStatus.isFile()) &#123; System.out.println(\"f:\"+fileStatus.getPath().getName()); &#125;else &#123; System.out.println(\"d:\"+fileStatus.getPath().getName()); &#125; &#125; // 3 关闭资源 fs.close();&#125; HDFS的I/O流操作HDFS文件上传需求：把本地e盘上的banhua.txt文件上传到HDFS根目录 123456789101112131415161718192021@Testpublic void putFileToHDFS() throws IOException, InterruptedException, URISyntaxException &#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 创建输入流 FileInputStream fis = new FileInputStream(new File(\"e:/banhua.txt\")); // 3 获取输出流 FSDataOutputStream fos = fs.create(new Path(\"/banhua.txt\")); // 4 流对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close();&#125; HDFS文件下载需求：从HDFS上下载banhua.txt文件到本地e盘上 12345678910111213141516171819202122// 文件下载@Testpublic void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(\"/banhua.txt\")); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/banhua.txt\")); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close();&#125; 定位文件读取需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 下载第一块@Testpublic void readFileSeek1() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(\"/hadoop-2.7.2.tar.gz\")); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/hadoop-2.7.2.tar.gz.part1\")); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++)&#123; fis.read(buf); fos.write(buf); &#125; // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);fs.close();&#125;// 下载第二块@Testpublic void readFileSeek2() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(\"/hadoop-2.7.2.tar.gz\")); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/hadoop-2.7.2.tar.gz.part2\")); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);&#125; 在Window命令窗口中进入到目录E:\\，然后执行如下命令，对数据进行合并 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"HDFS读写流程","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop03-HDFS读写流程&NN和2NN.html","text":"HDFS数据流HDFS写数据流程剖析文件写入 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 NameNode返回是否可以上传。 客户端请求第一个 Block上传到哪几个DataNode服务器上。 NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 dn1、dn2、dn3逐级应答客户端。 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步） 网络拓扑-节点距离计算节点距离：两个节点到达最近的共同祖先的距离总和。 机架感知（副本存储节点选择） HDFS读数据流程 客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode地址。 挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。 DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。 客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。 NN和2NNNN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？ 首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。==因此产生在磁盘中备份元数据的FsImage。== 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。==因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。==这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。 但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。==因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。== NN和2NN工作机制： Fsimage和Edits解析 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求。 （3）NameNode记录操作日志，更新滚动日志。 （4）NameNode在内存中对数据进行增删改。 第二阶段：Secondary NameNode工作 （1）Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode是否检查结果。 （2）Secondary NameNode 请求执行 CheckPoint。 （3）NameNode 滚动正在写的 Edits日志。 （4）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。 （5）Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。 （6）生成新的镜像文件 fsimage.chkpoint。 （7）拷贝 fsimage.chkpoint 到 NameNode。 （8）NameNode将 fsimage.chkpoint 重新命名成 fsimage。 ==NN和2NN工作机制详解== Fsimage：NameNode内存中元数据序列化后形成的文件。HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。 Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。 seen_txid：文件保存的是一个数字，就是最后一个edits_的数字 每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。 NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。 由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。 SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。 oiv 查看 Fsimage文件 hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径 12345678910111213141516171819202122232425262728293031323334353637383940414243[atguigu@hadoop102 current]$ pwd/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current[atguigu@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml[atguigu@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml# 将显示的xml文件内容拷贝到Eclipse中创建的xml文件中，并格式化。部分显示结果如下。&lt;inode&gt; &lt;id&gt;16386&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;user&lt;/name&gt; &lt;mtime&gt;1512722284477&lt;/mtime&gt; &lt;permission&gt;atguigu:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt; &lt;id&gt;16387&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;atguigu&lt;/name&gt; &lt;mtime&gt;1512790549080&lt;/mtime&gt; &lt;permission&gt;atguigu:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt; &lt;id&gt;16389&lt;/id&gt; &lt;type&gt;FILE&lt;/type&gt; &lt;name&gt;wc.input&lt;/name&gt; &lt;replication&gt;3&lt;/replication&gt; &lt;mtime&gt;1512722322219&lt;/mtime&gt; &lt;atime&gt;1512722321610&lt;/atime&gt; &lt;perferredBlockSize&gt;134217728&lt;/perferredBlockSize&gt; &lt;permission&gt;atguigu:supergroup:rw-r--r--&lt;/permission&gt; &lt;blocks&gt; &lt;block&gt; &lt;id&gt;1073741825&lt;/id&gt; &lt;genstamp&gt;1001&lt;/genstamp&gt; &lt;numBytes&gt;59&lt;/numBytes&gt; &lt;/block&gt; &lt;/blocks&gt;&lt;/inode &gt; 思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？ 在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。 oev 查看Edits文件 hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192[atguigu@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml[atguigu@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml# 将显示的xml文件内容拷贝到Eclipse中创建的xml文件中，并格式化。显示结果如下&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;EDITS&gt; &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;129&lt;/TXID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;130&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;16407&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943607866&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_-1544295051_1&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;192.168.1.5&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;atguigu&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;RPC_CLIENTID&gt;908eafd4-9aec-4288-96f1-e8011d181561&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ALLOCATE_BLOCK_ID&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;131&lt;/TXID&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_SET_GENSTAMP_V2&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;132&lt;/TXID&gt; &lt;GENSTAMPV2&gt;1016&lt;/GENSTAMPV2&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD_BLOCK&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;133&lt;/TXID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;0&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;RPC_CLIENTID&gt;&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;-2&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_CLOSE&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;134&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;0&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943608761&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;false&lt;/OVERWRITE&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;25&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;atguigu&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;/DATA&gt; &lt;/RECORD&gt;&lt;/EDITS &gt; 思考：NameNode如何确定下次开机启动的时候合并哪些Edits？ CheckPoint时间设置 通常情况下，SecondaryNameNode每隔一小时执行一次 123456[hdfs-default.xml]&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt; 一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 1234567891011&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt;&lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt;&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; NameNode故障处理NameNode故障后，可以采用如下两种方法恢复数据。 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录； kill -9 NameNode进程 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） 拷贝SecondaryNameNode中数据到原NameNode存储数据目录 重新启动NameNode 12345[atguigu@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/[atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中 修改hdfs-site.xml中的 123456789&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; kill -9 NameNode进程 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） 1[atguigu@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 123456789[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./[atguigu@hadoop102 namesecondary]$ rm -rf in_use.lock[atguigu@hadoop102 dfs]$ pwd/opt/module/hadoop-2.7.2/data/tmp/dfs[atguigu@hadoop102 dfs]$ lsdata name namesecondary 导入检查点数据（等待一会ctrl+c结束掉） 1[atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint 启动NameNode 1[atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 集群安全模式 集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。 bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态） NameNode多目录配置NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性 具体配置如下: 在hdfs-site.xml文件中增加如下内容 1234&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 停止集群，删除data和logs中所有数据 格式化集群并启动 查看结果 12345[atguigu@hadoop102 dfs]$ ll总用量 12drwx------. 3 atguigu atguigu 4096 12月 11 08:03 datadrwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"HDFS-DataNode","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop04-HDFS-DataNode.html","text":"DataNode工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。 12345678910111213&lt;!-- DN 向 NN 汇报当前解读信息的时间间隔，默认6 小时 --&gt;&lt;property&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; &lt;value&gt;21600000&lt;/value&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;&lt;/property&gt;&lt;!-- DN 扫描自己节点块信息列表的时间，默认6 小时 --&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.directoryscan.interval&lt;/name&gt; &lt;value&gt;21600s&lt;/value&gt; &lt;description&gt;Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.&lt;/description&gt;&lt;/property&gt; 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。 集群运行中可以安全加入和退出一些机器。 数据完整性如下是DataNode节点保证数据完整性的方法： 当DataNode读取Block的时候，它会计算CheckSum。 如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。 Client读取其他DataNode上的Block。 DataNode在其文件创建后周期验证CheckSum，如图3-16所示 掉线时限参数设置 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为==毫秒==，dfs.heartbeat.interval的单位为==秒==。 12345678&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 服役新数据节点 直接启动DataNode，即可关联到集群 12[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager 如果数据不均衡，可以用命令实现集群的再平衡 123[atguigu@hadoop102 sbin]$ ./start-balancer.shstarting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.outTime Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 退役旧数据节点添加白名单添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。 配置白名单的具体步骤如下： 在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件 123456789[atguigu@hadoop102 hadoop]$ pwd/opt/module/hadoop-2.7.2/etc/hadoop[atguigu@hadoop102 hadoop]$ touch dfs.hosts[atguigu@hadoop102 hadoop]$ vi dfs.hosts# 添加如下主机名称（不添加hadoop105）hadoop102hadoop103hadoop104 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 配置文件分发 1[atguigu@hadoop102 hadoop]$ xsync hdfs-site.xml 刷新NameNode 1[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes 更新ResourceManager节点 12[atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes17/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 如果数据不均衡，可以用命令实现集群的再平衡 123[atguigu@hadoop102 sbin]$ ./start-balancer.shstarting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.outTime Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 黑名单退役在黑名单上面的主机都会被强制退出。 在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件 1234567[atguigu@hadoop102 hadoop]$ pwd/opt/module/hadoop-2.7.2/etc/hadoop[atguigu@hadoop102 hadoop]$ touch dfs.hosts.exclude[atguigu@hadoop102 hadoop]$ vi dfs.hosts.exclude# 添加如下主机名称（要退役的节点）hadoop105 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 刷新NameNode、刷新ResourceManager 12345[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodesRefresh nodes successful[atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点，如图所示 等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役，如图所示 12345[atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanodestopping datanode[atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanagerstopping nodemanager 如果数据不均衡，可以用命令实现集群的再平衡 123[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.outTime Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 注意：不允许白名单和黑名单中同时出现同一个主机名称。 Datanode多目录配置DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt;","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"HDFS2.x新特性与HA","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop05-HDFS2.X新特性和高可用(HA).html","text":"HDFS 2.X新特性集群间数据拷贝采用distcp命令实现两个Hadoop集群之间的递归数据复制： 1[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp hdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt 小文件存档 每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。 HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存。 案例： 需要启动YARN进程：start-yarn.sh 归档文件 把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。 1[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName input.har –p /user/atguigu/input /user/atguigu/output 查看归档 12[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/atguigu/output/input.har[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/atguigu/output/input.har 解归档文件 1[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/atguigu/output/input.har/* /user/atguigu 回收站开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。 开启回收站功能参数说明： 默认值fs.trash.interval=0，0表示禁用回收站;其他值表示设置文件的存活时间。 默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。 要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。 回收站工作机制： 启用回收站 修改core-site.xml，配置垃圾回收时间为1分钟。 1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 查看回收站 回收站在集群中的路径：/user/atguigu/.Trash/…. 修改访问垃圾回收站用户名称 进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户. 修改core-site.xml 1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt; 通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 Trash trash = New Trash(conf); trash.moveToTrash(path); 恢复回收站数据 1[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/atguigu/.Trash/Current/user/atguigu/input /user/atguigu/input 清空回收站 1[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -expunge 快照管理快照相当于对目录做一个备份。并不会立即复制所有文件，而是记录文件变化。 参数描述： hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能） hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用） hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照） hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照） hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照） hdfs lsSnapshottableDir （功能描述：列出当前用户所有可快照目录） hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处） hdfs dfs -deleteSnapshot （功能描述：删除快照） 开启/禁用指定目录的快照功能 123[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/atguigu/input[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/atguigu/input 对目录创建快照 123[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/input# 通过web访问hdfs://hadoop102:50070/user/atguigu/input/.snapshot/s…..// 快照和源文件使用相同数据[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/atguigu/input/.snapshot/ 指定名称创建快照 1[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/input miao170508 重命名快照 1[atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/atguigu/input/ miao170508 atguigu170508 列出当前用户所有可快照目录 1[atguigu@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir 比较两个快照目录的不同之处 1[atguigu@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/atguigu/input/ . .snapshot/atguigu170508 恢复快照 1atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/atguigu/input/.snapshot/s20170708-134303.027 /user HDFS HA高可用HA概述所谓HA（High Available），即高可用（7*24小时不中断服务）。实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。 Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。NameNode主要在以下两个方面影响HDFS集群： NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启 NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用 HDFS HA功能通过配置Active/Standby两个NameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。 HDFS-HA工作机制通过双NameNode消除单点故障 工作要点 元数据管理方式需要改变 内存中各自保存一份元数据； Edits日志只有Active状态的NameNode节点可以做写操作； 两个NameNode都可以读取Edits； 共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）； 需要一个状态管理功能模块 实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在NameNode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。 必须保证两个NameNode之间能够ssh无密码登录 隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务 自动故障转移工作机制前面学习了使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程，如图所示。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能： 故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。 现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。 ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责： 健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。 ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。 基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为Active状态。 HDFS-HA集群配置规划集群 hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager NodeManager NodeManager NodeManager 配置HDFS-HA集群 复制已有的 hadoop-2.7.2 到新目录下； 配置core-site.xml 12345678910111213141516&lt;configuration&gt; &lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/module/HA/hadoop-2.7.2/data/tmp/jn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置hdfs-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;configuration&gt; &lt;!-- 完全分布式集群名称 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群中NameNode节点都有哪些 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/atguigu/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 声明journalnode服务器存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/jn&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭权限检查--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 拷贝配置好的hadoop环境到其他节点 注意：这些参数读取后为K-V对儿，参数写错文件也没事。 启动HDFS-HA集群 三台机器分别启动zookeeper。 在各个JournalNode节点上，输入以下命令启动journalnode服务 sbin/hadoop-daemon.sh start journalnode 在[nn1]上，对其进行格式化，并启动： bin/hdfs namenode -format sbin/hadoop-daemon.sh start namenode 在[nn2]上，同步nn1的元数据信息，并启动 [nn2] bin/hdfs namenode -bootstrapStandby sbin/hadoop-daemon.sh start namenode 查看web页面显示 在[nn1]上，启动所有datanode sbin/hadoop-daemons.sh start datanode 将[nn1]切换为Active bin/hdfs haadmin -transitionToActive nn1 查看是否Active bin/hdfs haadmin -getServiceState nn1 为了防止脑裂，手动模式下两个NameNode必须都起来才能切换NameNode。自动故障转移会自动切换NameNode，因为zkfc会kill -9 原先的NameNode进程，防止脑裂。 配置HDFS-HA自动故障转移 增加 hdfs-site.xml 配置 1234&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 增加 core-site.xml 配置 1234&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;&lt;/property&gt; 启动 1234567891011121314# 关闭所有HDFS服务：NameNode、DataNode、journalnode、zkfcsbin/stop-dfs.sh# 在各个机器启动Zookeeper集群bin/zkServer.sh start# 初始化HA在Zookeeper中状态，然后在Zookeeper根目录下有集群节点bin/hdfs zkfc -formatZK# 启动HDFS服务sbin/start-dfs.sh# 在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNodesbin/hadoop-daemin. sh start zkfc YARN-HA配置YARN-HA工作机制 配置YARN-HA集群 hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager ResourceManager NodeManager NodeManager NodeManager 配置 yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--启用resourcemanager ha--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--声明两台resourcemanager的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster-yarn1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop102&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper集群的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 同步更新其他节点的配置信息 启动hdfs 123# 已经格式化NameNode了，这里不需要再格式化# 启动HFDSsbin/start-dfs.sh 启动YARN 12345# 在hadoop102中执行sbin/start-yarn.sh# 在hadoop103中执行sbin/yarn-daemon.sh start resourcemanager 查看服务状态 注意：执行start-yarn.sh并不能启动103的RM，需要单独在103启动RM，此时102为active。访问103:8088会自动跳转到102:8088。如果kill -9 102RM，无法访问102:8088，103RM自动提升为active，103:8088不再跳转。 HDFS Federation结构设计 NameNode架构的局限性 Namespace（命名空间）的限制 由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB。 隔离问题 由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。 性能的瓶颈 由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量。 HDFS Federation架构设计 能不能有多个NameNode | NameNode | NameNode | NameNode || ———— | ———— | ————————- || 元数据 | 元数据 | 元数据 || Log | machine | 电商数据/话单数据 | HDFS Federation应用思考 不同应用可以使用不同NameNode进行数据管理 图片业务、爬虫业务、日志审计业务 Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"MapReduce概述与序列化","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop06-MapReduce概述&序列化.html","text":"MapReduce概述定义MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。 MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。 优点 MapReduce 易于编程：它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 良好的扩展性：当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力 高容错性：MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。 适合PB级以上海量数据的离线处理：可以实现上千台服务器集群并发工作，提供数据处理能力。 缺点 不擅长实时计算：MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果 不擅长式计算：流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 不擅长DAG（有向图）计算：多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 核心思想 分布式的运算程序往往需要分成至少2个阶段。 第一个阶段的MapTask并发实例，完全并行运行，互不相干。 第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。 MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。 总结：分析WordCount数据流走向深入理解MapReduce核心思想。 MapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调。 MapTask：负责Map阶段的整个数据处理流程 ReduceTask：负责Reduce阶段的整个数据处理流程。 常用数据序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable MapReduce编程规范用户编写的程序分成三个部分：Mapper、Reducer和Driver。 Mapper阶段 用户自定义的Mapper要继承自己的父类 Mapper的输入数据是KV对的形式（KV的类型可自定义） Mapper中的业务逻辑写在map()方法中 Mapper的输出数据是KV对的形式（KV的类型可自定义） map()方法（MapTask进程）对每一个调用一次 Reducer阶段 用户自定义的Reducer要继承自己的父类 Reducer的输入数据类型对应Mapper的输出数据类型，也是KV Reducer的业务逻辑写在reduce()方法中 ReduceTask进程对每一组相同k的组调用一次reduce()方法 Driver阶段相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象。 实现WordCount需求在给定的文本文件中统计输出每一个单词出现的总次数。 1234567891011121314151617# 输入atguigu atguiguss sscls clsjiaobanzhangxuehadoop# 输出atguigu 2banzhang 1cls 2hadoop 1jiao 1ss 2xue 1 环境准备创建maven工程，在pom.xml文件中添加如下依赖：（可选：utf-8编码，maven镜像源） 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入：（目的是打印日志） 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 写Mapper类12345678910111213141516171819202122232425package com.atguigu.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割 String[] words = line.split(\" \"); // 3 输出 for (String word : words) &#123; k.set(word); context.write(k, v); &#125; &#125;&#125; 写Reduce类1234567891011121314151617181920212223package com.atguigu.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; int sum; IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; // 1 累加求和 sum = 0; for (IntWritable count : values) &#123; sum += count.get(); &#125; // 2 输出 v.set(sum); context.write(key,v); &#125;&#125; 写Driver驱动类123456789101112131415161718192021222324252627282930313233343536373839404142package com.atguigu.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息以及封装任务 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 关联map和reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 测试1 本地测试： 编译Hadoop，在Windows环境上配置HADOOP_HOME环境变量。直接run 2 集群上测试：需要先生成jar包，然后cp到集群运行 pom.xml添加： 1234567891011121314151617181920212223242526272829303132333435&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin &lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!-- Driver类的全名 --&gt; &lt;mainClass&gt;com.atguigu.mr.WordcountDriver&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 运行 maven install。等待编译完成就会在项目的target文件夹中生成jar包。修改不带依赖的jar包名称为wc.jar，并拷贝该jar包到Hadoop集群。 12[atguigu@hadoop102 software]$ hadoop jar wc.jar com.atguigu.wordcount.WordcountDriver /user/atguigu/input /user/atguigu/output Hadoop序列化为什么需要序列化序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。 一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。 问：为什么不用Java的序列化 Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）。 Hadoop序列化特点： 紧凑 ：高效使用存储空间。 快速：读写数据的额外开销小。 可扩展：随着通信协议的升级而可升级 互操作：支持多语言的交互 自定义bean对象实现序列化自定义序列化要求在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。 必须实现Writable接口：重写序列化方法和重写反序列化方法 123456789101112131415// 重写序列化方法@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow);&#125;// 重写反序列化方法@Overridepublic void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong();&#125; 注意反序列化的顺序和序列化的顺序完全一致 反序列化时，需要反射调用空参构造函数，所以必须有空参构造 123public FlowBean() &#123; super();&#125; 要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。 如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。详见后面排序案例。 12345@Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 案例需求：统计每一个手机号耗费的总上行流量、下行流量、总流量。 key为手机号，values为上行，下行流量；则基本数据类型不够用，需要传输对象，计算中存在集群间拷贝数据，还需要序列化对象。 123456789101112131415161718192021222324252627// 输入数据1 13736230513 192.196.100.1 www.atguigu.com 2481 24681 2002 13846544121 192.196.100.2 264 0 2003 13956435636 192.196.100.3 132 1512 2004 13966251146 192.168.100.1 240 0 4045 18271575951 192.168.100.2 www.atguigu.com 1527 2106 2006 84188413 192.168.100.3 www.atguigu.com 4116 1432 2007 13590439668 192.168.100.4 1116 954 2008 15910133277 192.168.100.5 www.hao123.com 3156 2936 2009 13729199489 192.168.100.6 240 0 20010 13630577991 192.168.100.7 www.shouhu.com 6960 690 20011 15043685818 192.168.100.8 www.baidu.com 3659 3538 20012 15959002129 192.168.100.9 www.atguigu.com 1938 180 50013 13560439638 192.168.100.10 918 4938 20014 13470253144 192.168.100.11 180 180 20015 13682846555 192.168.100.12 www.qq.com 1938 2910 20016 13992314666 192.168.100.13 www.gaga.com 3008 3720 20017 13509468723 192.168.100.14 www.qinghua.com 7335 110349 40418 18390173782 192.168.100.15 www.sogou.com 9531 2412 20019 13975057813 192.168.100.16 www.baidu.com 11058 48243 20020 13768778790 192.168.100.17 120 120 20021 13568436656 192.168.100.18 www.alibaba.com 2481 24681 20022 13568436656 192.168.100.19 1116 954 200 // 期望输出格式13560436666 1116 954 2070手机号码 上行流量 下行流量 总流量 流量统计的Bean对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;// 1 实现writable接口public class FlowBean implements Writable&#123; private long upFlow; private long downFlow; private long sumFlow; //2 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; //3 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; //4 反序列化方法 //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 6 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125; 写Mapper类1234567891011121314151617181920212223242526272829303132333435package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split(\"\\t\"); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); k.set(phoneNum); v.set(downFlow, upFlow); // 4 写出 context.write(k, v); &#125;&#125; 写Reducer类1234567891011121314151617181920212223242526package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context)throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_downFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sum_upFlow += flowBean.getUpFlow(); sum_downFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow); // 3 写出 context.write(key, resultBean); &#125;&#125; 写Driver驱动类1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置args = new String[] &#123; \"e:/input/inputflow\", \"e:/output1\" &#125;; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"CRUD","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/04-JDBC/JDBC-02 CRUD.html","text":"使用PreparedStatement实现CRUD操作操作和访问数据库 数据库连接被用于向数据库服务器发送命令和 SQL 语句，并接受数据库服务器返回的结果。其实一个数据库连接就是一个Socket连接。 在 java.sql 包中有 3 个接口分别定义了对数据库的调用的不同方式： Statement：用于执行静态 SQL 语句并返回它所生成结果的对象。 PrepatedStatement：SQL 语句被预编译并存储在此对象中，可以使用此对象多次高效地执行该语句。 CallableStatement：用于执行 SQL 存储过程 使用Statement操作数据表的弊端 通过调用 Connection 对象的 createStatement() 方法创建该对象。该对象用于执行静态的 SQL 语句，并且返回执行结果。 Statement 接口中定义了下列方法用于执行 SQL 语句： 12int excuteUpdate(String sql)：执行更新操作INSERT、UPDATE、DELETEResultSet executeQuery(String sql)：执行查询操作SELECT 但是使用Statement操作数据表存在弊端： 问题一：存在拼串操作，繁琐 问题二：存在SQL注入问题 SQL 注入是利用某些系统没有对用户输入的数据进行充分的检查，而在用户输入数据中注入非法的 SQL 语句段或命令(如：SELECT user, password FROM user_table WHERE user=’a’ OR 1 = ‘ AND password = ‘ OR ‘1’ = ‘1’) ，从而利用系统的 SQL 引擎完成恶意行为的做法。 对于 Java 而言，要防范 SQL 注入，只要用 PreparedStatement(从Statement扩展而来) 取代 Statement 就可以了。 代码演示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106public class StatementTest &#123; // 使用Statement的弊端：需要拼写sql语句，并且存在SQL注入的问题 @Test public void testLogin() &#123; Scanner scan = new Scanner(System.in); System.out.print(\"用户名：\"); String userName = scan.nextLine(); System.out.print(\"密 码：\"); String password = scan.nextLine(); // SELECT user,password FROM user_table WHERE USER = '1' or ' AND PASSWORD = '='1' or '1' = '1'; String sql = \"SELECT user,password FROM user_table WHERE USER = '\" + userName + \"' AND PASSWORD = '\" + password + \"'\"; User user = get(sql, User.class); if (user != null) &#123; System.out.println(\"登陆成功!\"); &#125; else &#123; System.out.println(\"用户名或密码错误！\"); &#125; &#125;// 使用Statement实现对数据表的查询操作 public &lt;T&gt; T get(String sql, Class&lt;T&gt; clazz) &#123; T t = null; Connection conn = null; Statement st = null; ResultSet rs = null; try &#123; // 1.加载配置文件 InputStream is = StatementTest.class.getClassLoader().getResourceAsStream(\"jdbc.properties\"); Properties pros = new Properties(); pros.load(is); // 2.读取配置信息 String user = pros.getProperty(\"user\"); String password = pros.getProperty(\"password\"); String url = pros.getProperty(\"url\"); String driverClass = pros.getProperty(\"driverClass\"); // 3.加载驱动 Class.forName(driverClass); // 4.获取连接 conn = DriverManager.getConnection(url, user, password); st = conn.createStatement(); rs = st.executeQuery(sql); // 获取结果集的元数据 ResultSetMetaData rsmd = rs.getMetaData(); // 获取结果集的列数 int columnCount = rsmd.getColumnCount(); if (rs.next()) &#123; t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) &#123; // //1. 获取列的名称 // String columnName = rsmd.getColumnName(i+1); // 1. 获取列的别名 String columnName = rsmd.getColumnLabel(i + 1); // 2. 根据列名获取对应数据表中的数据 Object columnVal = rs.getObject(columnName); // 3. 将数据表中得到的数据，封装进对象 Field field = clazz.getDeclaredField(columnName); field.setAccessible(true); field.set(t, columnVal); &#125; return t; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭资源 if (rs != null) &#123; try &#123; rs.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (st != null) &#123; try &#123; st.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (conn != null) &#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return null; &#125;&#125; 综上： PreparedStatement的使用PreparedStatement介绍 可以通过调用 Connection 对象的 preparedStatement(String sql) 方法获取 PreparedStatement 对象 PreparedStatement 接口是 Statement 的子接口，它表示一条预编译过的 SQL 语句 PreparedStatement 对象所代表的 SQL 语句中的参数用问号(?)来表示，调用 PreparedStatement 对象的 setXxx() 方法来设置这些参数. setXxx() 方法有两个参数，第一个参数是要设置的 SQL 语句中的参数的索引(从 1 开始)，第二个是设置的 SQL 语句中的参数的值 PreparedStatement vs Statement 代码的可读性和可维护性。 PreparedStatement 能最大可能提高性能： DBServer会对预编译语句提供性能优化。因为预编译语句有可能被重复调用，所以语句在被DBServer的编译器编译后的执行代码被缓存下来，那么下次调用时只要是相同的预编译语句就不需要编译，只要将参数直接传入编译过的语句执行代码中就会得到执行。 在statement语句中,即使是相同操作但因为数据内容不一样,所以整个语句本身不能匹配,没有缓存语句的意义.事实是没有数据库会对普通语句编译后的执行代码缓存。这样每执行一次都要对传入的语句编译一次。 (语法检查，语义检查，翻译成二进制命令，缓存) PreparedStatement 可以防止 SQL 注入 Java与SQL对应数据类型转换表 Java类型 SQL类型 boolean BIT byte TINYINT short SMALLINT int INTEGER long BIGINT String CHAR,VARCHAR,LONGVARCHAR byte array BINARY , VAR BINARY java.sql.Date DATE java.sql.Time TIME java.sql.Timestamp TIMESTAMP 使用PreparedStatement实现增、删、改操作123456789101112131415161718192021222324252627282930/*除了解决Statement的拼串、sql问题之外，PreparedStatement还有哪些好处呢？ 1.PreparedStatement操作Blob的数据，而Statement做不到。 2.PreparedStatement可以实现更高效的批量操作。*/ //通用的增、删、改操作（体现一：增、删、改 ； 体现二：针对于不同的表） public void update(String sql,Object ... args)&#123; Connection conn = null; PreparedStatement ps = null; try &#123; //1.获取数据库的连接 conn = JDBCUtils.getConnection(); //2.获取PreparedStatement的实例 (或：预编译sql语句) ps = conn.prepareStatement(sql); //3.填充占位符 for(int i = 0;i &lt; args.length;i++)&#123; ps.setObject(i + 1, args[i]); &#125; //4.执行sql语句 ps.execute(); // ps.executeUpdate() 返回影响的行数 // ps.executeUpdate(); 说明：返回值为影响的行数 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; //5.关闭资源 JDBCUtils.closeResource(conn, ps); &#125; &#125; JDBCUtils.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.io.InputStream;import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;/** * @Description 操作数据库的工具类 * @version * @date 上午9:10:02 */public class JDBCUtils &#123; /** * @Description 获取数据库的连接 * @return * @throws Exception */ public static Connection getConnection() throws Exception &#123; // 1.读取配置文件中的4个基本信息 InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(\"jdbc.properties\"); Properties pros = new Properties(); pros.load(is); String user = pros.getProperty(\"user\"); String password = pros.getProperty(\"password\"); String url = pros.getProperty(\"url\"); String driverClass = pros.getProperty(\"driverClass\"); // 2.加载驱动 Class.forName(driverClass); // 3.获取连接 Connection conn = DriverManager.getConnection(url, user, password); return conn; &#125; /** * @Description 关闭连接和Statement的操作 * @param conn * @param ps */ public static void closeResource(Connection conn,Statement ps)&#123; try &#123; if(ps != null) ps.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; if(conn != null) conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; /** * @Description 关闭资源操作 * @param conn * @param ps * @param rs */ public static void closeResource(Connection conn,Statement ps,ResultSet rs)&#123; try &#123; if(ps != null) ps.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; if(conn != null) conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; if(rs != null) rs.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 使用PreparedStatement实现查询操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/*针对于表的字段名与类的属性名不相同的情况：1. 必须声明sql时，使用类的属性名来命名字段的别名2. 使用ResultSetMetaData时，需要使用getColumnLabel()来替换getColumnName(),获取列的别名。 说明：如果sql中没有给字段其别名，getColumnLabel()获取的就是列名*/// 通用的针对于不同表的查询:返回一个对象 (version 1.0) public &lt;T&gt; T getInstance(Class&lt;T&gt; clazz, String sql, Object... args) &#123; Connection conn = null; PreparedStatement ps = null; ResultSet rs = null; try &#123; // 1.获取数据库连接 conn = JDBCUtils.getConnection(); // 2.预编译sql语句，得到PreparedStatement对象 ps = conn.prepareStatement(sql); // 3.填充占位符 for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; // 4.执行executeQuery(),得到结果集：ResultSet rs = ps.executeQuery(); // 5.得到结果集的元数据：ResultSetMetaData ResultSetMetaData rsmd = rs.getMetaData(); // 6.1通过ResultSetMetaData得到columnCount,columnLabel；通过ResultSet得到列值 int columnCount = rsmd.getColumnCount(); if (rs.next()) &#123; T t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) &#123;// 遍历每一个列 // 获取列值 Object columnVal = rs.getObject(i + 1); //获取列的列名：getColumnName() --不推荐使用 // 获取列的别名:列的别名，使用类的属性名充当 String columnLabel = rsmd.getColumnLabel(i + 1); // 6.2使用反射，给对象的相应属性赋值 Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columnVal); &#125; return t; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 7.关闭资源 JDBCUtils.closeResource(conn, ps, rs); &#125; return null; &#125;// 查询一组数据 public &lt;T&gt; List&lt;T&gt; getForList(Class&lt;T&gt; clazz,String sql, Object... args)&#123; Connection conn = null; PreparedStatement ps = null; ResultSet rs = null; try &#123; conn = JDBCUtils.getConnection(); ps = conn.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; rs = ps.executeQuery(); // 获取结果集的元数据 :ResultSetMetaData ResultSetMetaData rsmd = rs.getMetaData(); // 通过ResultSetMetaData获取结果集中的列数 int columnCount = rsmd.getColumnCount(); //创建集合对象 ArrayList&lt;T&gt; list = new ArrayList&lt;T&gt;(); while (rs.next()) &#123; T t = clazz.newInstance(); // 处理结果集一行数据中的每一个列:给t对象指定的属性赋值 for (int i = 0; i &lt; columnCount; i++) &#123; // 获取列值 Object columValue = rs.getObject(i + 1); // 获取每个列的列名 // String columnName = rsmd.getColumnName(i + 1); String columnLabel = rsmd.getColumnLabel(i + 1); // 给t对象指定的columnName属性，赋值为columValue：通过反射 Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columValue); &#125; list.add(t); &#125; return list; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(conn, ps, rs); &#125; return null; &#125; 说明：使用PreparedStatement实现的查询操作可以替换Statement实现的查询操作，解决Statement拼串和SQL注入问题。 ResultSet与ResultSetMetaDataResultSet 查询需要调用PreparedStatement 的 executeQuery() 方法，查询结果是一个ResultSet 对象 ResultSet 对象以逻辑表格的形式封装了执行数据库操作的结果集，ResultSet 接口由数据库厂商提供实现 ResultSet 返回的实际上就是一张数据表。有一个指针指向数据表的第一条记录的前面。 ResultSet 对象维护了一个指向当前数据行的游标，初始的时候，游标在第一行之前，可以通过 ResultSet 对象的 next() 方法移动到下一行。调用 next()方法检测下一行是否有效。若有效，该方法返回 true，且指针下移。相当于Iterator对象的 hasNext() 和 next() 方法的结合体。 当指针指向一行时, 可以通过调用 getXxx(int index) 或 getXxx(int columnName) 获取每一列的值。 例如: getInt(1), getString(“name”) 注意：Java与数据库交互涉及到的相关Java API中的索引都从1开始。 ResultSet 接口的常用方法： boolean next() getString() … ResultSetMetaData 可用于获取关于 ResultSet 对象中列的类型和属性信息的对象 ResultSetMetaData meta = rs.getMetaData(); getColumnName(int column)：获取指定列的名称 getColumnLabel(int column)：获取指定列的别名 getColumnCount()：返回当前 ResultSet 对象中的列数。 getColumnTypeName(int column)：检索指定列的数据库特定的类型名称。 getColumnDisplaySize(int column)：指示指定列的最大标准宽度，以字符为单位。 isNullable(int column)：指示指定列中的值是否可以为 null。 isAutoIncrement(int column)：指示是否自动为指定列进行编号，这样这些列仍然是只读的。 问题1：得到结果集后, 如何知道该结果集中有哪些列 ？ 列名是什么？ 需要使用一个描述 ResultSet 的对象， 即 ResultSetMetaData 问题2：关于ResultSetMetaData 如何获取 ResultSetMetaData： 调用 ResultSet 的 getMetaData() 方法即可 获取 ResultSet 中有多少列：调用 ResultSetMetaData 的 getColumnCount() 方法 获取 ResultSet 每一列的列的别名是什么：调用 ResultSetMetaData 的getColumnLabel() 方法 资源的释放 释放ResultSet, Statement,Connection。 数据库连接（Connection）是非常稀有的资源，用完后必须马上释放，如果Connection不能及时正确的关闭将导致系统宕机。Connection的使用原则是尽量晚创建，尽量早的释放。 可以在finally中关闭，保证及时其他代码出现异常，资源也一定能被关闭。 JDBC API小结 两种思想 面向接口编程的思想 ORM思想(object relational mapping) 一个数据表对应一个java类 表中的一条记录对应java类的一个对象 表中的一个字段对应java类的一个属性 sql是需要结合列名和表的属性名来写。注意起别名。 两种技术 JDBC结果集的元数据：ResultSetMetaData 获取列数：getColumnCount() 获取列的别名：getColumnLabel() 通过反射，创建指定类的对象，获取指定的属性并赋值 章节练习练习题1：从控制台向数据库的表customers中插入一条数据，表结构如下： 练习题2：创立数据库表 examstudent，表结构如下： 向数据表中添加如下数据： 代码实现1：插入一个新的student 信息 请输入考生的详细信息 Type:IDCard:ExamCard:StudentName:Location:Grade: 信息录入成功! 代码实现2：在 eclipse中建立 java 程序：输入身份证号或准考证号可以查询到学生的基本信息。结果如下： 代码实现3：完成学生信息的删除功能 操作BLOB类型字段MySQL BLOB类型 MySQL中，BLOB是一个二进制大型对象，是一个可以存储大量数据的容器，它能容纳不同大小的数据。 插入BLOB类型的数据必须使用PreparedStatement，因为BLOB类型的数据无法使用字符串拼接写的。 MySQL的四种BLOB类型(除了在存储的最大信息量上不同外，他们是等同的) 实际使用中根据需要存入的数据大小定义不同的BLOB类型。 需要注意的是：如果存储的文件过大，数据库的性能会下降。 如果在指定了相关的Blob类型以后，还报错：xxx too large，那么在mysql的安装目录下，找my.ini文件加上如下的配置参数： max_allowed_packet=16M。同时注意：修改了my.ini文件之后，需要重新启动mysql服务。 向数据表中插入大数据类型123456789101112131415161718//获取连接Connection conn = JDBCUtils.getConnection(); String sql = \"insert into customers(name,email,birth,photo)values(?,?,?,?)\";PreparedStatement ps = conn.prepareStatement(sql);// 填充占位符ps.setString(1, \"徐海强\");ps.setString(2, \"xhq@126.com\");ps.setDate(3, new Date(new java.util.Date().getTime()));// 操作Blob类型的变量FileInputStream fis = new FileInputStream(\"xhq.png\");ps.setBlob(4, fis);//执行ps.execute(); fis.close();JDBCUtils.closeResource(conn, ps); 修改数据表中的Blob类型字段1234567891011121314Connection conn = JDBCUtils.getConnection();String sql = \"update customers set photo = ? where id = ?\";PreparedStatement ps = conn.prepareStatement(sql);// 填充占位符// 操作Blob类型的变量FileInputStream fis = new FileInputStream(\"coffee.png\");ps.setBlob(1, fis);ps.setInt(2, 25);ps.execute();fis.close();JDBCUtils.closeResource(conn, ps); 从数据表中读取大数据类型1234567891011121314151617181920212223242526272829303132333435363738String sql = \"SELECT id, name, email, birth, photo FROM customer WHERE id = ?\";conn = getConnection();ps = conn.prepareStatement(sql);ps.setInt(1, 8);rs = ps.executeQuery();if(rs.next())&#123; // 方式1 //Integer id = rs.getInt(1); //String name = rs.getString(2); //String email = rs.getString(3); //Date birth = rs.getDate(4); // 方式2 Integer id = rs.getInt(\"id\"); String name = rs.getString(\"name\"); String email = rs.getString(\"email\"); Date birth = rs.getDate(\"birth\"); Customer cust = new Customer(id, name, email, birth); System.out.println(cust); //读取Blob类型的字段 //Blob photo = rs.getBlob(5); Blob photo = rs.getBlob(\"photo\"); InputStream is = photo.getBinaryStream(); OutputStream os = new FileOutputStream(\"c.jpg\"); byte [] buffer = new byte[1024]; int len = 0; while((len = is.read(buffer)) != -1)&#123; os.write(buffer, 0, len); &#125; JDBCUtils.closeResource(conn, ps, rs); if(is != null)&#123; is.close(); &#125; if(os != null)&#123; os.close(); &#125;&#125; 批量插入批量执行SQL语句当需要成批插入或者更新记录时，可以采用Java的批量更新机制，这一机制允许多条语句一次性提交给数据库批量处理。通常情况下比单独提交处理更有效率 JDBC的批量处理语句包括下面三个方法： addBatch(String)：添加需要批量处理的SQL语句或是参数； executeBatch()：执行批量处理语句； clearBatch():清空缓存的数据 通常我们会遇到两种批量执行SQL语句的情况： 多条SQL语句的批量处理； 一个SQL语句的批量传参； 高效的批量插入举例：向数据表中插入20000条数据 数据库中提供一个goods表。创建如下： 1234CREATE TABLE goods(id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20)); 实现层次一：使用Statement123456Connection conn = JDBCUtils.getConnection();Statement st = conn.createStatement();for(int i = 1;i &lt;= 20000;i++)&#123; String sql = \"insert into goods(name) values('name_' + \"+ i +\")\"; st.executeUpdate(sql);&#125; 实现层次二：使用PreparedStatement123456789101112131415long start = System.currentTimeMillis(); Connection conn = JDBCUtils.getConnection(); String sql = \"insert into goods(name)values(?)\";PreparedStatement ps = conn.prepareStatement(sql);for(int i = 1;i &lt;= 20000;i++)&#123; ps.setString(1, \"name_\" + i); ps.executeUpdate();&#125; long end = System.currentTimeMillis();System.out.println(\"花费的时间为：\" + (end - start));//82340JDBCUtils.closeResource(conn, ps); 实现层次三12345678910111213141516171819202122232425262728/* * 修改1： 使用 addBatch() / executeBatch() / clearBatch() * 修改2：mysql服务器默认是关闭批处理的，我们需要通过一个参数，让mysql开启批处理的支持。 * ?rewriteBatchedStatements=true 写在配置文件的url后面 * 修改3：使用更新的mysql 驱动：mysql-connector-java-5.1.37-bin.jar * */@Testpublic void testInsert1() throws Exception&#123; long start = System.currentTimeMillis(); Connection conn = JDBCUtils.getConnection(); String sql = \"insert into goods(name)values(?)\"; PreparedStatement ps = conn.prepareStatement(sql); for(int i = 1;i &lt;= 1000000;i++)&#123; ps.setString(1, \"name_\" + i); //1.“攒”sql ps.addBatch(); if(i % 500 == 0)&#123; //2.执行 ps.executeBatch(); //3.清空 ps.clearBatch(); &#125; &#125; long end = System.currentTimeMillis(); System.out.println(\"花费的时间为：\" + (end - start));//20000条：625 //1000000条:14733 JDBCUtils.closeResource(conn, ps);&#125; 实现层次四12345678910111213141516171819202122232425262728293031323334/** 层次四：在层次三的基础上操作* 使用Connection 的 setAutoCommit(false) / commit()*/@Testpublic void testInsert2() throws Exception&#123; long start = System.currentTimeMillis(); Connection conn = JDBCUtils.getConnection(); //1.设置为不自动提交数据 conn.setAutoCommit(false); String sql = \"insert into goods(name)values(?)\"; PreparedStatement ps = conn.prepareStatement(sql); for(int i = 1;i &lt;= 1000000;i++)&#123; ps.setString(1, \"name_\" + i); //1.“攒”sql ps.addBatch(); if(i % 500 == 0)&#123; //2.执行 ps.executeBatch(); //3.清空 ps.clearBatch(); &#125; &#125; //2.提交数据 conn.commit(); long end = System.currentTimeMillis(); System.out.println(\"花费的时间为：\" + (end - start));//1000000条:4978 JDBCUtils.closeResource(conn, ps);&#125;","tags":[{"name":"JDBC","slug":"JDBC","permalink":"https://mtcai.github.io/tags/JDBC/"}]},{"title":"Hadoop数据压缩","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop08-Hadoop数据压缩.html","text":"概述压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、 Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。 鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。 压缩是提高Hadoop运行效率的一种优化策略。通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度。注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能。 运算密集型的job，少用压缩 IO密集型的job，多用压缩 MR支持的压缩编码压缩编码 压缩格式 hadoop自带 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFLATE 是，直接使用 DEFLATE .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFLATE .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s 压缩方式选择Gzip： 优点：压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便。 缺点：不支持Split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。 Bzip2： 优点：支持Split；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便。 缺点：压缩/解压速度慢。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split，而且兼容之前的应用程序的情况。 Lzo： 优点：压缩/解压速度也比较快，合理的压缩率；支持Split，是Hadoop中最流行的压缩格式；可以在Linux系统下安装lzop命令，使用方便。 缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理（为了支持Split需要建索引，还需要指定InputFormat为Lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显。 Snappy： 优点：高速压缩速度和合理的压缩率。 缺点：不支持Split；压缩率比Gzip要低；Hadoop本身不支持，需要安装。 应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入。 压缩位置压缩可以在MapReduce作用的任意阶段启用。 压缩参数为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 要在Hadoop中启用压缩，可以配置如下参数： 参数 默认值 阶段 建议 io.compression.codecs（在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress（在mapred-site.xml中配置） false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec mapper输出 企业多使用LZO或Snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置） false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置） RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 案例Map输出端采用压缩、Reduce输出端采用压缩即使你的MapReduce的输入输出文件都是未压缩的文件，仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可。 只修改驱动类即可，无需修改map和reduce类。压缩map端输出不会影响最终输出格式，reduce读取时会自动解压；压缩reduce段输出会改变最终输出格式，内容不变。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.BZip2Codec; import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); // 开启map端输出压缩 configuration.setBoolean(\"mapreduce.map.output.compress\", true); // 设置map端输出压缩方式 configuration.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class, CompressionCodec.class); Job job = Job.getInstance(configuration); job.setJarByClass(WordCountDriver.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 设置reduce端输出压缩开启 FileOutputFormat.setCompressOutput(job, true); // 设置压缩的方式 FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); // FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); // FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 1 : 0); &#125;&#125;","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"Yarn资源调度","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop09-Yarn资源调度.html","text":"Yarn基本架构Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。 Yarn工作机制 MR程序提交到客户端所在的节点。 YarnRunner 向 ResourceManager 申请一个 Application。 RM 将该应用程序的资源路径返回给 YarnRunner。 该程序将运行所需资源提交到 HDFS 上。 程序资源提交完毕后，申请运行 mrAppMaster。 RM 将用户的请求初始化成一个 Task。 其中一个NodeManager 领取到Task任务。 该 NodeManager 创建容器 Container，并产生 MRAppmaster。 Container 从 HDFS 上拷贝资源到本地。 MRAppmaster 向 RM 申请运行 MapTask 资源。 RM 将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 NodeManager 分别启动 MapTask，MapTask 对数据分区排序。 MrAppMaster 等待所有 MapTask 运行完毕后，向 RM 申请容器，运行 ReduceTask。 ReduceTask 向 MapTask 获取相应分区的数据。 程序运行完毕后，MRAppmaster 会向 RM 申请注销自己。 作业提交过程作业提交过程之HDFS&amp;MapReduce 作业提交过程之YARN （1）作业提交 第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。 第2步：Client向 RM 申请一个作业 id。 第3步：RM 给 Client 返回该 job 资源的提交路径和作业 id。 第4步：Client 提交 jar 包、切片信息和配置文件到指定的资源提交路径。 第5步：Client 提交完资源后，向 RM 申请运行 MrAppMaster。 （2）作业初始化 第6步：当 RM 收到 Client 的请求后，将该 job 添加到容量调度器中。 第7步：某一个空闲的 NM 领取到该 Job。 第8步：该 NM 创建 Container，并产生 MRAppmaster。 第9步：下载 Client 提交的资源到本地。 （3）任务分配 第10步：MrAppMaster 向 RM 申请运行多个 MapTask 任务资源。 第11步：RM 将运行 MapTask 任务分配给另外两 个NodeManager，另两个 NodeManager 分别领取任务并创建容器。 （4）任务运行 第12步：MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 NodeManager 分别启动 MapTask，MapTask 对数据分区排序。 第13步：MrAppMaster 等待所有 MapTask 运行完毕后，向 RM 申请容器，运行 ReduceTask。 第14步：ReduceTask 向 MapTask 获取相应分区的数据。 第15步：程序运行完毕后，MR 会向 RM 申请注销自己。 （5）进度和状态更新 YARN 中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。 （6）作业完成 除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。 资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2/3.1.3 默认的资源调度器是Capacity Scheduler。CDH框架默认调度器是Fair Scheduler。 具体设置详见：yarn-default.xml文件 12345&lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 公平调度器并发高，要求集群配置高 先进先出调度器并发最低，要求集群配置低 容量调度器并发和配置中等 先进先出调度器（FIFO） 优点：简单易懂； 缺点：不支持多队列，生产环境很少使用； 容量调度器（Capacity Scheduler）Capacity Scheduler 是Yahoo 开发的多用户调度器 公平调度器（Fair Scheduler）Fair Schedulere 是Facebook 开发的多用户调度器。 任务的推测执行作业完成时间取决于最慢的任务完成时间 一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。 思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。 执行推测任务的前提条件： 每个Task只能有一个备份任务; 当前Job已完成的Task必须不小于0.05（5%）； 开启推测执行参数设置。mapred-site.xml文件中默认是打开的。 1234567891011&lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; 不能启用推测执行机制情况 任务间存在严重的负载倾斜；(如Task执行10亿数据，Task2执行5条数据，Task2执行完后，不能再给Task1开备份任务。) 特殊任务，比如任务向数据库中写数据。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"生产调优手册","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop10-生产调优手册.html","text":"HDFS核心参数NameNode内存生产配置每个文件块大概占用 150byte，一台服务器 128G内存为例 128 * 1024 * 1024 * 1024 / 150Byte = 9.1亿 hadoop-env.sh hadoop2.x：NameNode内存默认 2000m，如果服务器内存 4G， NameNode内存可以配置 3G。 HADOOP_NAMENODE_OPTS=-Xmx 3072 m hadoop3.x：hadoop-env.sh中描述 Hadoop的内存是动态分配的，[ jmap -heap 线程号] 可以查看使用的最大内存。 export HDFS_NAMENODE_OPTS=”-Dhadoop.security.logger=INFO,RFAS -Xmx 1024 m” export HDFS_DATANODE_OPTS=”-Dhadoop.security.logger=ERROR,RFAS -Xmx 1024 m NameNode心跳并发配置 hdfs-site.xml 1234567NameNode 有一个工作线程池，用来处理不同 DataNode 的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大该参数 。 默认值是 10。&lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;21&lt;/value&gt;&lt;/property&gt; \\text{dfs.namenode.handler.count}=20*log_e^{Cluster\\_size}开启回收站配置 core-site.xml 123456789&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;!-- 单位是分钟 --&gt;&lt;/property&gt;&lt;!--1. 默认值 fs.trash.interval = 0, 0表示禁用回收站 其他值表示设置文件的存活时间。2. 默认值 fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为 0，则该值设置和 fs.trash.interval的参数值相等。3. 要求 fs.trash.checkpoint.interval &lt;= fs.trash.interval--&gt; 回收站目录在 HDFS集群中的路径： /user/atguigu/.Trash/…. 通过网页上直接删除的文件也不会走回收站。 通过程序删除的文件不会经过回收站，需要调用 moveToTrash()才 进入回收站 只有在命令行利用 hadoop fs -rm命令删除的文件才会走回收站。 恢复回收站数据: $ hadoop fs mv/user/atguigu/.Trash/Current/user/atguigu/input/user/atguigu/input HDFS集群压测写性能1234567891011121314$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB2021 02 09 10:43:16,853 INFO fs.TestDFSIO: TestDFSIO : write2021 02 09 10:43:16,854 INFO fs.Te stDFSIO: Date &amp; time: Tue Feb 09 10:43:16 CST 20212021 02 09 10:43:16,854 INFO fs.TestDFSIO: Number of files: 102021 02 09 10:43:16,854 INFO fs.TestDFSIO: Total MBytes processed: 12802021 02 09 10:43:16,854 INFO fs.TestDFSIO: Throughput mb/sec: 1.612021 02 09 10:43:16,854 INFO fs.TestDFSIO: Average IO rate mb/sec: 1.92021 02 09 10:43:16,854 INFO fs.TestDFSIO: IO rate std deviation: 0.762021 02 09 10:43:16,854 INFO fs.TestDFSIO: Test exec time sec: 133.052021 02 09 10:43:16,854 INFO fs.TestDFSIO: nrFiles 为生成 mapTask的数量，设置为 CPU核数-1 Number of files：生成 mapTask数量 ，一般是集群中 CPU核数 -1） Total MBytes processed：单个 map处理的文件大小 Throughput mb/sec：单个 mapTak的吞吐量 Average IO rate mb/sec：平均 mapTak的吞吐量 IO rate std deviation：方差、反映各个 mapTask处理的差值，越小越均衡 注意：如果测试过程中，出现异常，在 yarn-site.xml中设置虚拟内存检测为 false 12345678&lt;!-- 是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 true--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem check enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;分发配置并重启 Yarn集群 测试结果分析： 由于副本1在本地，则改副本不参与测试。共计10个文件*2个副本=20个文件。 1.61*20=32M/s 三台服务器均为100Mbps，100/8*3=30M/s 可以认为网络资源都已经用满。 如果实测速度远远小于网络，并且实测速度不能满足工作需求，可以考虑采用固态硬盘或者增加磁盘个数。 如果客服端不在集群节点，则三个副本都参与计算。 读性能1234567891011121314151617$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient 3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB2021 02 09 11:34:15,847 INFO fs.TestDFSIO: TestDFSIO : read2021 02 09 11:34:15,847 INFO fs.TestDFSIO: Date &amp; time: Tue Feb 09 11:34:15 CST 20212021 02 09 11:34:15,847 INFO fs.TestDFSIO: Number of files: 102021 02 09 11:34:15,847 INFO fs.TestDFSIO: Total MBytes processed: 12802021 02 09 11:3 4:15,848 INFO fs.TestDFSIO: Throughput mb/sec: 200.282021 02 09 11:34:15,848 INFO fs.TestDFSIO: Average IO rate mb/sec: 266.742021 02 09 11:34:15,848 INFO fs.TestDFSIO: IO rate std deviation: 143.122021 02 09 11:34:15,848 INFO fs.TestDFSIO: Test exec time sec: 20.83删除测试生成数据$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient 3.1.3-tests.jar TestDFSIO -clean 测试结果分析：为什么读取文件速度大于网络带宽 由于目前只有三台服务器，且有三个副本，数据读取就近原则，相当于都是读取的本地磁盘数据，没有走网络。 HDFS多目录NameNode多目录配置NameNode的本地目录可以配置成多个， 且每个目录存放内容相同，增加了可靠性。 hdfs-site.xml 1234567&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt; file://$&#123;hadoop.tmp.dir&#125;/dfs/name1,file://$&#123;hadoop.tmp.dir&#125;/dfs/name2 &lt;/value&gt; &lt;/property&gt; DataNode多目录配置DataNode可以配置成多个目录， 每个目录存储的数据不一样，数据不是副本。 hdfs-site.xml 1234567&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt; file://$&#123;hadoop.tmp.dir&#125;/dfs/data1,file://$&#123;hadoop.tmp.dir&#125;/dfs/data2 &lt;/value&gt; &lt;/property&gt; 集群数据均衡之磁盘间数据均衡(Hadoop3.x)生产环境，由于硬盘空间不足，往往需要增加一块硬盘。刚加载的硬盘没有数据时，可以执行磁盘数据均衡命令。 （Hadoop3.x新特性） 1234567891011生成均衡计划 (我们只有一块磁盘，不会生成计划)hdfs diskbalancer -plan hadoop103执行均衡计划hdfs diskbalancer -execute hadoop103.plan.json查看当前均衡任务的执行情况hdfs diskbalancer -query hadoop103取消均衡任务hdfs diskbalancer -cancel hadoop103.plan.json HDFS集群扩容及缩容添加白名单白名单：表示在白名单的主机IP地址可以，用来存储数据。 企业中：配置白名单，可以尽量防止黑客恶意访问攻击。 在 NameNode节点的 /opt/module/hadoop-3.1.3/etc/hadoop目录下 分别创 建 whitelist 和blacklist文件，在 whitelist 中写入可访问节点的主机名称，blacklist 保持空即可 hdfs-site.xml 1234567891011白名单&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/whitelist&lt;/value&gt;&lt;/property&gt;黑名单&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;&lt;/property&gt; 分发配置文件 whitelist， hdfs-site.xml 第一次添加白名单必须重启集群，不是第一次，只需要刷新 NameNode节点即可。hdfs dfsadmin -refreshNodes 服役新服务器 启动DataNode和NodeManager 添加白名单，分发文件，重启集群 刷新NameNode：hdfs dfsadmin -refreshNodes 服务器间数据均衡节点之间数据不均衡。 开启数据均衡命令： sbin/start-balancer.sh-threshold 10 对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过 10%，可根据实际情况进行调整。 停止数据均衡命令: sbin/stop-balancer.sh 由于 HDFS需要启动单独的 Rebalance Server 来执行 Rebalance 操作， 所以尽量不要在 NameNode上执行 start-balancer.sh，而是找一台比较空闲的机器。 黑名单退役服务器黑名单：表示在黑名单的主机IP地址不可以，用来存储数据。 企业中：配置黑名单，用来退役服务器。 hdfs-site.xml配置文件中增加主机，分发文件。第一次添加黑名单必须重启集群，不是第一次，只需要刷新 NameNode节点即可。 12345黑名单&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt;&lt;/property&gt; 检查 Web浏览器 ，退役节点的状态为 decommission in progress（退役中）， 说明数据节点正在复制块到其他节点 等待退役节点状态为 decommissioned （所有块已经复制完成 ），停止该节点及节点资源管理器。 注意 ：如果副本数是 3， 服役的节点小于等于 3，是不能退役成功的，需要修改副本数后才能退役 HDFS存储优化MapReduce 生产经验MR跑得慢的原因MapReduce 程序效率的瓶颈在于两点： 计算机性能：CPU、内存、磁盘健康、网络 I/O 操作优化 数据倾斜 Map和Reduce数设置不合理 Map运行时间太长，导致Reduce等待过久 小文件过多 大量的不可分块的超大文件 Spill次数过多 Merge次数过多等 MR调优参数 数据倾斜问题倾斜现象： 数据频率倾斜——某一个区域的数据量要远远大于其他区域。 数据大小倾斜——部分记录的大小远远大于平均值。 减少数据倾斜的方法： 首先检查是否空值过多造成的数据倾斜，生产环境，可以直接过滤掉空值；如果想保留空值，就自定义分区，将空值加随机数打散。最后再二次聚合 。 抽样和范围分区：可以通过对原始数据进行抽样得到的结果集来预设分区边界值。 自定义分区：基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例。 Combine：使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据。 采用Map Join，尽量避免Reduce Join MR优化方法MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。 数据输入 合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。 采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 Map阶段 减少溢写（Spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO。 减少合并（Merge）次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。 在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少 I/O。 Reduce阶段 合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误。 设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。 规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。 合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整。 IO传输 采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器。 使用SequenceFile二进制文件。 常用的调优参数。资源相关参数以下参数是在用户自己的MR应用程序中配置就可以生效（mapred-default.xml） 配置参数 参数说明 mapreduce.map.memory.mb 一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。 mapreduce.reduce.memory.mb 一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。 mapreduce.map.cpu.vcores 每个MapTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.cpu.vcores 每个ReduceTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.shuffle.parallelcopies 每个Reduce去Map中取数据的并行数。默认值是5 mapreduce.reduce.shuffle.merge.percent Buffer中的数据达到多少比例开始写入磁盘。默认值0.66 mapreduce.reduce.shuffle.input.buffer.percent Buffer大小占Reduce可用内存的比例。默认值0.7 mapreduce.reduce.input.buffer.percent 指定多少比例的内存用来存放Buffer中的数据，默认值是0.0 应该在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml） 配置参数 参数说明 yarn.scheduler.minimum-allocation-mb 给应用程序Container分配的最小内存，默认值：1024 yarn.scheduler.maximum-allocation-mb 给应用程序Container分配的最大内存，默认值：8192 yarn.scheduler.minimum-allocation-vcores 每个Container申请的最小CPU核数，默认值：1 yarn.scheduler.maximum-allocation-vcores 每个Container申请的最大CPU核数，默认值：32 yarn.nodemanager.resource.memory-mb 给Containers分配的最大物理内存，默认值：8192 Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml） 配置参数 参数说明 mapreduce.task.io.sort.mb Shuffle的环形缓冲区大小，默认100m mapreduce.map.sort.spill.percent 环形缓冲区溢出的阈值，默认80% 容错相关参数 配置参数 参数说明 mapreduce.map.maxattempts 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 mapreduce.reduce.maxattempts 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 mapreduce.task.timeout Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。 HDFS小文件优化方法DFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。 小文件的优化无非以下几种方式： 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。 在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。 在MapReduce处理时，可采用CombineTextInputFormat提高效率。 Hadoop Archive：是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了NameNode的内存使用。 Sequence File： Sequence File由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。 CombineFileInputFormat：CombineFileInputFormat是一种新的InputFormat，用于将多个文件合并成一个单独的Split，另外，它会考虑数据的存储位置。 开启JVM重用：对于大量小文件Job，可以开启JVM重用会减少45%运行时间。JVM重用原理：一个Map运行在一个JVM上，开启重用的话，该Map在JVM上运行完毕后，JVM继续运行其他Map。具体设置：mapreduce.job.jvm.numtasks值在10-20之间。","tags":[]},{"title":"源码解析","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop11-源码解析.html","text":"NameNode 启动源码解析 DataNode 启动源码解析 HDFS上传源码解析 Yarn 源码解析","tags":[]},{"title":"MapReduce框架原理","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop07-MapReduce框架原理.html","text":"InputFormat数据输入数据切片理解MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。 思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ 数据块：Block是HDFS物理上把数据分成一块一块。针对物理上的储存。 数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce程序计算输入数据的单位 ，一个切片会对应启动一个 MapTask。针对程序上的读取。有多种方式切片，下图只是一种。12345678FileInputFormat //父类，抽象类 --NLineInputFormat //N行记录为一个切片，一次返回一行 &lt;起始字节偏移量，一行内容(不包含换行符和回车符)&gt;；输入文件总行数/N=切片数，若不整除，切片数=商+1 --TextInputFormat //默认 //按照单个文件大小切片，一次返回一行 &lt;起始字节偏移量，一行内容(不包含换行符和回车符)&gt; --CombineFileInputFormat //按照多个文件大小切片，一次返回一行 &lt;起始字节偏移量，一行内容(不包含换行符和回车符)&gt; --FixedLengthInputFormat // --KeyValueTextInputFormat //按照文件大小切片，一次返回一行，一行内容根据指定分隔符分为'key \\t value', 返回&lt;key, value&gt; --SequenceFileInputFormat // --自定义InputFormat Job提交流程源码和切片源码详解Job提交流程源码详解： 1234567891011121314151617181920212223242526272829303132333435363738394041waitForCompletion()submit();// 1建立连接 connect(); // 1）创建提交Job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交job submitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);//会创建临时文件目录 // 2）获取jobid ，并创建Job路径 JobID jobId = submitClient.getNewJobID();//临时文件目录/mapred/staging/use_id/.staging/ // 3）拷贝jar包到集群 submitJobDir = new Path(jobStagingArea, jobId.toString()); copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // jobSubmitDir=submitJobDir // 4）计算切片，生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);////////进行切片的函数 splitSize = Math.max(minSize, Math.min(maxSize,blockSize)); minsize=1; maxsize=Long.MAXValue; // 故，默认情况下，blockSize=128M。 // 5）向Stag路径写XML配置文件 writeConf(conf, submitJobFile); conf.writeXml(out); // 6）提交Job,返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); // 7) 清空 submitJobDir FileInputFormat切片源码解析(input.getSplits(job))： TextInputFormat 切片机制 源码中计算切片大小的公式 1234Math.max(minSize, Math.min(maxSize,blockSize));mapreduce.input.fileinputformat.split.minsize=1;mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue;// 故，默认情况下，blockSize=128M。 &gt; 129M 的文件，&lt;font color = #3333ff face=&quot;宋体&quot;&gt;HDFS上存 2 块，切片只有 1 块&lt;/font&gt;。 切片大小设置 maxSize：比blockSize小，则会让切片大小等于maxSize minSize：比blockSize大，则会让切片大小等于minSize 获取切片信息API 1234// 获取切片的文件名称String name = inputSplit.getPath().getName();// 根据文件类型获取切片信息FileSplit inputSplit = (FileSplit) contex.getInputSplit(); TextInputFormat 是默认的FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，LongWritable类型。值是这行的内容，不包括任何行终止符（ 换行符和回车符）， Text类型。 CombineTextInputFormat 切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。 CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。 虚拟存储切片最大值设置： CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。 生成切片过程包括：虚拟存储过程和切片过程两部分。 虚拟存储过程 将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。 例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。 切片过程 （a）判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。 （b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。 （c）测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为： 1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M） 最终会形成3个切片，大小分别为： （1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M 案例： 需求：将输入的大量小文件合并成一个切片统一处理。 不做任何处理，运行WordCount案例程序，观察切片个数为4 在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数 驱动类中添加代码如下 12345// 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class); //虚拟存储切片最大值设置4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304); //运行结果为3个切片 驱动中添加代码如下 12345// 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class); //虚拟存储切片最大值设置20mCombineTextInputFormat.setMaxInputSplitSize(job, 20971520)// 运行结果为1个切片 KeyValueTextInputFormat 使用案例需求：统计输入文件中每一行的第一个单词相同的行数。 123456789// 输入数据banzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhang// 期望结果数据banzhang 2xihuan 2 写Mapper类： 1234567891011121314151617181920package com.atguigu.mapreduce.KeyValueTextInputFormat;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class KVTextMapper extends Mapper&lt;Text, Text, Text, LongWritable&gt;&#123; // 1 设置value LongWritable v = new LongWritable(1); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123;// banzhang ni hao // 2 写出 context.write(key, v); &#125;&#125; 写Reducer类： 1234567891011121314151617181920212223242526package com.atguigu.mapreduce.KeyValueTextInputFormat;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class KVTextReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;&#123; LongWritable v = new LongWritable(); @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0L; // 1 汇总统计 for (LongWritable value : values) &#123; sum += value.get(); &#125; v.set(sum); // 2 输出 context.write(key, v); &#125;&#125; 写Driver类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.atguigu.mapreduce.keyvaleTextInputFormat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class KVTextDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); // 设置切割符 conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \" \"); // 1 获取job对象 Job job = Job.getInstance(conf); // 2 设置jar包位置，关联mapper和reducer job.setJarByClass(KVTextDriver.class); job.setMapperClass(KVTextMapper.class); job.setReducerClass(KVTextReducer.class); // 3 设置map输出kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 4 设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); // 5 设置输入输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); // 设置输入格式 job.setInputFormatClass(KeyValueTextInputFormat.class); // 6 设置输出数据路径 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交job job.waitForCompletion(true); &#125;&#125; NLineInputFormat使用案例需求：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中。 123456789101112131415// 输入数据banzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhang banzhang ni haoxihuan hadoop banzhang// 期望输出数据Number of splits:4 Mapper类： 12345678910111213141516171819202122232425package com.atguigu.mapreduce.nline;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class NLineMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;&#123; private Text k = new Text(); private LongWritable v = new LongWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割 String[] splited = line.split(\" \"); // 3 循环写出 for (int i = 0; i &lt; splited.length; i++) &#123; k.set(splited[i]); context.write(k, v); &#125; &#125;&#125; 写Reducer类： 1234567891011121314151617181920212223package com.atguigu.mapreduce.nline;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class NLineReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;&#123; LongWritable v = new LongWritable(); @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0; // 1 汇总 for (LongWritable value : values) &#123; sum += value.get(); &#125; v.set(sum); // 2 输出 context.write(key, v); &#125;&#125; 写Driver类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.atguigu.mapreduce.nline;import java.io.IOException;import java.net.URISyntaxException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class NLineDriver &#123; public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[] &#123; \"e:/input/inputword\", \"e:/output1\" &#125;; // 1 获取job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 7设置每个切片InputSplit中划分三条记录 NLineInputFormat.setNumLinesPerSplit(job, 3); // 8使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class); // 2设置jar包位置，关联mapper和reducer job.setJarByClass(NLineDriver.class); job.setMapperClass(NLineMapper.class); job.setReducerClass(NLineReducer.class); // 3设置map输出kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 4设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); // 5设置输入输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6提交job job.waitForCompletion(true); &#125;&#125; 自定义InputFormat步骤 继承FileInputFormat 改写RecordReader，实现一次读取一个完整文件封装为KV 在输出时使用SequenceFileOutPutFormat输出合并文件。 案例无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义InputFormat实现小文件的合并。 将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式），SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。 自定义一个类继承FileInputFormat 重写isSplitable()方法，返回false不可切割 重写createRecordReader()，创建自定义的RecordReader对象，并初始化 改写RecordReader，实现一次读取一个完整文件封装为KV 采用IO流一次读取一个文件输出到value中，因为设置了不可切片，最终把所有文件都封装到了value中 获取文件路径信息+名称，并设置key 设置Driver 1234// （1）设置输入的inputFormatjob.setInputFormatClass(WholeFileInputformat.class);// （2）设置输出的outputFormatjob.setOutputFormatClass(SequenceFileOutputFormat.class); 自定义InputFromat—WholeFileInputformat12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;// 定义类继承FileInputFormatpublic class WholeFileInputformat extends FileInputFormat&lt;Text, BytesWritable&gt;&#123; @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; 自定义RecordReader类—WholeRecordReader123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package com.atguigu.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class WholeRecordReader extends RecordReader&lt;Text, BytesWritable&gt;&#123; private Configuration configuration; private FileSplit split; private boolean isProgress= true; private BytesWritable value = new BytesWritable(); private Text k = new Text(); @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.split = (FileSplit)split; configuration = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (isProgress) &#123; // 1 定义缓存区 byte[] contents = new byte[(int)split.getLength()]; FileSystem fs = null; FSDataInputStream fis = null; try &#123; // 2 获取文件系统 Path path = split.getPath(); fs = path.getFileSystem(configuration); // 3 读取数据 fis = fs.open(path); // 4 读取文件内容 IOUtils.readFully(fis, contents, 0, contents.length); // 5 输出文件内容 value.set(contents, 0, contents.length); // 6 获取文件路径及名称 String name = split.getPath().toString(); // 7 设置输出的key值 k.set(name); &#125; catch (Exception e) &#123; &#125;finally &#123; IOUtils.closeStream(fis); &#125; isProgress = false; return true; &#125; return false; &#125; @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return k; &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; @Override public void close() throws IOException &#123; &#125;&#125; SequenceFileMapper类12345678910111213141516package com.atguigu.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class SequenceFileMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt;&#123; @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125; SequenceFileReducer类1234567891011121314package com.atguigu.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, values.iterator().next()); &#125;&#125; SequenceFileDriver类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.atguigu.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;public class SequenceFileDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[] &#123; \"e:/input/inputinputformat\", \"e:/output1\" &#125;; // 1 获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 设置jar包存储位置、关联自定义的mapper和reducer job.setJarByClass(SequenceFileDriver.class); job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 7设置输入的inputFormat job.setInputFormatClass(WholeFileInputformat.class); // 8设置输出的outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); // 3 设置map输出端的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); // 4 设置最终输出端的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); // 5 设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; MapReduce工作流程绿色框均可自定义！ 对于 MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 对于 ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 第8步，分区内 快排，只能使用字典顺序排序，无法修改。 第9步，环形缓冲区只有100M，达到80%时会溢出到磁盘。一个MapTask的数据可能很多，会多次溢出到磁盘，形成多个文件。 第10步，MapTask将溢出的多个文件合并，采用 归并排序 第11步，可选流程。采用 归并排序 第13步，将多个MapTask的中同一分区的数据合并，采用 归并排序 流程详解 上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下： MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序 ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据 ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序） 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法） Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。 缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M。 1234567891011context.write(k, NullWritable.get()); output.write(key, value); collector.collect(key, value,partitioner.getPartition(key, value, partitions)); HashPartitioner(); collect() close() collect.flush() sortAndSpill() sort() QuickSort mergeParts(); collector.close(); Shuffle机制Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。 map方法和reduce方法之间的都是shuffle机制的内容。 Partition分区默认分区： 123456public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt;&#123; public int getPartition(K key, V value, int numReduceTasks)&#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125;// 用户无法控制哪个key存储在哪个分区。 自定义Partitioner分区： 自定义类继承Partitioner，重写getPartition()方法 1234567public class Custompartitioner exends Partitioner&lt;Text, FlowBean&gt;&#123; @Override public int getPartition(Text key, FlowBeam value, int numPartitions)&#123; ........... return partition; &#125;&#125; 在Job驱动中，设置自定义Partitioner job.setPartitionerClass(Custompartitioner.class); 自定义Partition后，根据自定义Partitioner的逻辑设置相应数量的ReduceTask job.setBumReduceTask(5); 分区总结: 如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx； 如果1","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"数据结构02-复杂度分析（上）","date":"2020-06-03T11:19:39.000Z","path":"2020/06/03/01-数据结构/数据结构02-复杂度分析(上).html","text":"[toc] 复杂度分析(上)如何分析、统计算法的执行效率和资源消耗？数据结构和算法本身解决的是“快”和“省”的问题，即，如何让代码运行得更快，如何让代码更省存储空间。 事后统计法事后统计法：把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。 局限性： 测试结果非常依赖测试环境，相同代码不同机子，测试结果不一致。 测试结果受数据规模的影响很大，同一个排序算法对有序度不同的数据测试结果不一致。 大O表示法算法的执行效率，粗略地讲，就是算法代码执行的时间。但是，如何在不运行代码的情况下，用“肉眼”得到一段代码的执行时间呢？ 这里有段非常简单的代码，求 1,2,3…n 的累加和。现在，我就带你一块来估算一下这段代码的执行时间。 12345678int cal(int n) &#123; int sum = 0; int i = 1; for (; i &lt;= n; ++i) &#123; sum = sum + i; &#125; return sum; &#125; 从 CPU 的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的 CPU 执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？ 第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 2nunit_time 的执行时间，所以这段代码总的执行时间就是 (2n+2)unit_time。可以看出来，所有代码的执行时间 T(n) 与每行代码的执行次数成正比。 按照这个分析思路，我们再来看这段代码。 12345678910int cal(int n) &#123; int sum = 0; int i = 1; int j = 1; for (; i &lt;= n; ++i) &#123; j = 1; for (; j &lt;= n; ++j) &#123; sum = sum + i * j; &#125; &#125; 我们依旧假设每个语句的执行时间是 unit_time。那这段代码的总执行时间 T(n) 是多少呢？ 第 2、3、4 行代码，每行都需要 1 个 unit_time 的执行时间，第 5、6 行代码循环执行了 n 遍，需要 2n unit_time 的执行时间，第 7、8 行代码循环执行了 $n^2$遍，所以需要 $2n^2$ unit_time 的执行时间。所以，整段代码总的执行时间 T(n) = ($2n^2+2n+3$)*unit_time。 尽管我们不知道 unit_time 的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。 我们可以把这个规律总结成一个公式。注意，大 O 就要登场了！ T(n)=O(f(n))具体解释一下这个公式。其中，T(n) 我们已经讲过了，它表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和。因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比。 所以，第一个例子中的 T(n) = O(2n+2)，第二个例子中的 T(n) = O($2n^2+2n+3$)。这就是大 O 时间复杂度表示法。大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。 当 n 很大时，你可以把它想象成 10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大 O 表示法表示刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n)； T(n) = O(n^2)。 时间复杂度分析 只关注循环执行次数最多的一段代码 12345678int cal(int n) &#123; int sum = 0; int i = 1; for (; i &lt;= n; ++i) &#123; sum = sum + i; &#125; return sum;&#125; 其中第 2、3 行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第 4、5 行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了 n 次，所以总的时间复杂度就是 O(n)。 加法法则：总复杂度等于量级最大的那段代码的复杂度 12345678910111213141516171819202122232425int cal(int n) &#123; int sum_1 = 0; int p = 1; for (; p &lt; 100; ++p) &#123; sum_1 = sum_1 + p; &#125; int sum_2 = 0; int q = 1; for (; q &lt; n; ++q) &#123; sum_2 = sum_2 + q; &#125; int sum_3 = 0; int i = 1; int j = 1; for (; i &lt;= n; ++i) &#123; j = 1; for (; j &lt;= n; ++j) &#123; sum_3 = sum_3 + i * j; &#125; &#125; return sum_1 + sum_2 + sum_3; &#125; 第一段的时间复杂度是多少呢？这段代码循环执行了 100 次，所以是一个常量的执行时间，跟 n 的规模无关。 这里我要再强调一下，即便这段代码循环 10000 次、100000 次，只要是一个已知的数，跟 n 无关，照样也是常量级的执行时间。当 n 无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。 那第二段代码和第三段代码的时间复杂度是多少呢？答案是 O(n) 和 O(n2)。 综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为 O(n2)。也就是说：总的时间复杂度就等于量级最大的那段代码的时间复杂度。那我们将这个规律抽象成公式就是： T1(n)=O(f(n))\\\\ T2(n)=O(g(n))\\\\ T(n)=T1(n)+T2(n)=max(O(f(n)),O(g(n)))=O(max(f(n),f(n))) 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 可以把乘法法则看成是嵌套循环 12345678910111213141516int cal(int n) &#123; int ret = 0; int i = 1; for (; i &lt; n; ++i) &#123; ret = ret + f(i); &#125; &#125; int f(int n) &#123; int sum = 0; int i = 1; for (; i &lt; n; ++i) &#123; sum = sum + i; &#125; return sum; &#125; 我们单独看 cal() 函数。假设 f() 只是一个普通的操作，那第 4～6 行的时间复杂度就是，T1(n) = O(n)。但 f() 函数本身不是一个简单的操作，它的时间复杂度是 T2(n) = O(n)，所以，整个 cal() 函数的时间复杂度就是，T(n) = T1(n) * T2(n) = O(n*n) = O(n^2)。 T1(n)=O(f(n))\\\\ T2(n)=O(g(n))\\\\ T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))几种常见时间复杂度实例分析 对于刚罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2^n) 和 O(n!)。当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。因此，关于 NP 时间复杂度我就不展开讲了。 我们主要来看几种常见的多项式时间复杂度。 O(1)首先必须明确一个概念，O(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这段代码，即便有 3 行，它的时间复杂度也是 O(1），而不是 O(3)。只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。 123int i = 8;int j = 6;int sum = i + j; $O(logn)、O(nlogn)$对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。我通过一个例子来说明一下。 1234i=1;while (i &lt;= n) &#123; i = i * 2;&#125; 通过 $2^x=n$ 求解 x 这个问题我们想高中应该就学过了，我就不多说了。x=\\log_2n，所以，这段代码的时间复杂度就是 $O(\\log^2n)$。 现在，我把代码稍微改下，你再看看，这段代码的时间复杂度是多少？ 1234i=1;while (i &lt;= n) &#123; i = i * 3;&#125; 这段代码的时间复杂度为 $O(\\log_3n)$。 我们可以把所有对数阶的时间复杂度都记为 O(logn)，\\log_3n=\\log_32*\\log_2n，所以O(\\log_3n) = O(C*\\log_2n)，其中C=\\log_32是一个常量。基于我们前面的一个理论：在采用大 O 标记复杂度的时候，可以忽略系数，即 O(C*f(n)) = O(f(n))。所以，O(\\log_2n)就等于 $O(\\log_3n)$。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 $O(\\log n)$。 那O(n \\log n) 就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是 $O(\\log n)$，我们循环执行 n 遍，时间复杂度就是O(n \\log n) 了。而且，O(n \\log n)也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是O(n \\log n)。 O(m+n)、O(m*n)代码的复杂度由两个数据的规模来决定。 123456789101112131415int cal(int m, int n) &#123; int sum_1 = 0; int i = 1; for (; i &lt; m; ++i) &#123; sum_1 = sum_1 + i; &#125; int sum_2 = 0; int j = 1; for (; j &lt; n; ++j) &#123; sum_2 = sum_2 + j; &#125; return sum_1 + sum_2;&#125; 从代码中可以看出，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。 针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)T2(n) = O(f(m) f(n))。 空间复杂度分析时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 1234567891011void print(int n) &#123; int i = 0; int[] a = new int[n]; for (i; i &lt;n; ++i) &#123; a[i] = i * i; &#125; for (i = n-1; i &gt;= 0; --i) &#123; print out a[i] &#125;&#125; 跟时间复杂度分析一样，我们可以看到，第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 我们常见的空间复杂度就是 $O(1)、O(n)、O(n^2 )$，像O(\\log n)、O(n\\log n)这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我说的这些内容已经足够了。 内容小结基础复杂度分析的知识到此就讲完了，总结一下。 复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(\\log n)、O(n)、O(n\\log n)、O(n^2 )。等学完整个专栏之后，就会发现几乎所有的数据结构和算法的复杂度都跑不出这几个。 O(1)","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Ubuntu16.04 安装OpenCV","date":"2020-01-20T07:18:50.000Z","path":"2020/01/20/00-环境/99-Ubuntu16.04 安装opencv（C++版本）.html","text":"[toc] Ubuntu16.04 安装OpenCV 3.4.x（C++版本） 1.安装依赖 2.编译OpenCV 3.测试 4.踩坑 安装依赖1.安装ffmpegffmpeg安装 2.安装其他依赖 1234567sudo apt-get install build-essentialsudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-devsudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev # 处理图像所需的包sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev liblapacke-devsudo apt-get install libxvidcore-dev libx264-dev # 处理视频所需的包，可选sudo apt-get install libatlas-base-dev gfortran # 优化opencv功能，可选sudo apt-get install ffmpeg 编译opencv3.安装 12345cd opencv-3.4.6 #进入opencv的目录mkdir build &amp;&amp; cd buildcmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. sudo make -j4 # 使用四线程，电脑线程多可以改大sudo mkae install 4.配置环境变量 1234567891011121314151617181920212223sudo vim /etc/ld.so.conf.d/opencv.conf # 这个文件并不存在，我们新建一个然后，在里面写入：/usr/local/lib 退出vimsudo vim /etc/ld.so.conf添加： include /etc/ld.so.conf.d/*.conf/usr/local/lib 退出vimsudo ldconfig # 使更改生效sudo vim /etc/bash.bashrc 在最末尾添加:PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfigexport PKG_CONFIG_PATH 退出vimsource /etc/bash.bashrcsudo updatedbpkg-config opencv --modversion # 查看opencv版本 5.测试 1234567进入到opencv的example目录cd cd samples/cpp/example_cmake/make./opencv_example # 会输出以下内容，并打开摄像头Built with OpenCV 3.4.6Capture is opened 踩坑1.ippicv 1234567在编译opencv时，会碰到ippicv_2019_lnx_intel64_general_20180723.tgz这个文件下载超级慢的问题下载网址1：https:&#x2F;&#x2F;github.com&#x2F;opencv&#x2F;opencv_3rdparty&#x2F;tree&#x2F;ippicv&#x2F;master_20180723&#x2F;ippicv下载网址2：https:&#x2F;&#x2F;links.jianshu.com&#x2F;go?to&#x3D;https%3A%2F%2Fraw.githubusercontent.com%2Fopencv%2Fopencv_3rdparty%2Fippicv%2Fmaster_20180723%2Fippicv%2Fippicv_2019_lnx_intel64_general_20180723.tgz下载后放到自己方便的目录下，随便哪个都行然后修改 .&#x2F;3rdparty&#x2F;ippicv&#x2F;ippicv.cmake #就是这个文件的第47行&quot;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;opencv&#x2F;opencv_3rdparty&#x2F;$&#123;IPPICV_COMMIT&#125;ippicv&#x2F;&quot; 修改为：&quot;file:~&#x2F;Downloads&#x2F;&quot; #（仅供参考，根据自己的路径填写，填写绝对路径） 2.recipe for target ‘lib/libopencv_videoio.so.3.4.6’ failed报错信息为： 123456modules&#x2F;videoio&#x2F;CMakeFiles&#x2F;opencv_videoio.dir&#x2F;build.make:333: recipe for target ‘lib&#x2F;libopencv_videoio.so.3.4.2’ failedmake[2]: *** [lib&#x2F;libopencv_videoio.so.3.4.2] Error 1CMakeFiles&#x2F;Makefile2:5205: recipe for target ‘modules&#x2F;videoio&#x2F;CMakeFiles&#x2F;opencv_videoio.dir&#x2F;all’ failedmake[1]: *** [modules&#x2F;videoio&#x2F;CMakeFiles&#x2F;opencv_videoio.dir&#x2F;all] Error 2Makefile:162: recipe for target ‘all’ failedmake: *** [all] Error 2 经过查资料发现时ffmpeg的问题，没有编译好，编译ffmpeg时缺少参数https://github.com/DeaDBeeF-Player/deadbeef/issues/1691解决办法： 编译ffmpeg时添加 —extra-cflags=”-fPIC” 参数./configure —extra-cflags=”-fPIC” ok至此，Ubuntu16.04下安装OpenCV已全部结束，请小量食用OpenCV！","tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://mtcai.github.io/tags/OpenCV/"},{"name":"C++","slug":"C","permalink":"https://mtcai.github.io/tags/C/"}]},{"title":"ffmpeg安装","date":"2020-01-20T07:18:50.000Z","path":"2020/01/20/00-环境/99-安装ffmpeg.html","text":"[toc] ffmpeg安装下载ffmpeg 1.git下载 git clone git://source.ffmpeg.org/ffmpeg.git ffmpeg # 网速太慢 2.官网下载 http://www.ffmpeg.org/download.html # 我使用的这个 安装依赖123sudo apt-get install yasm # 必选sudo apt-get install xorg-dev # 可选sudo apt-get install libsdl1.2-dev # 可选 开始编译安装1234./configure --enable-shared --extra-cflags=\"-fPIC\"sudo makesudo make install # 编译完成，查看/usr/local/lib和/usr/local/include是否生成ffmpeg的库 测试1ffmpeg -version # 输出正常即可 测试用例1： 1234567891011121314151617181920212223242526新建test.c，写入以下代码：#include &lt;stdio.h&gt;#include &lt;libavutil/avstring.h&gt;#include &lt;libavutil/eval.h&gt;#include &lt;libavutil/mathematics.h&gt;#include &lt;libavutil/pixdesc.h&gt;#include &lt;libavutil/imgutils.h&gt;#include &lt;libavutil/dict.h&gt;#include &lt;libavutil/parseutils.h&gt;#include &lt;libavutil/samplefmt.h&gt;#include &lt;libavutil/avassert.h&gt;#include &lt;libavutil/time.h&gt;#include &lt;libavformat/avformat.h&gt;#include &lt;libavdevice/avdevice.h&gt;#include &lt;libswscale/swscale.h&gt;#include &lt;libavutil/opt.h&gt;#include &lt;libavcodec/avfft.h&gt;#include &lt;libswresample/swresample.h&gt;int main(int argc, char* argv[])&#123; printf(\"this is a test program for ffmpeg\\n\"); av_register_all(); return 0;&#125; 在终端输入以下指令：gcc test.c -o test -I /usr/local/include -L /usr/local/lib -lavutil -lavformat -lavcodec -lavutil -lswresample -lm -lrt -lpthread -lz./test 测试用例2 123456789101112131415161718192021222324252627test1.c#include &lt;stdio.h&gt;#include &lt;libavutil/avstring.h&gt;#include &lt;libavutil/eval.h&gt;#include &lt;libavutil/mathematics.h&gt;#include &lt;libavutil/pixdesc.h&gt;#include &lt;libavutil/imgutils.h&gt;#include &lt;libavutil/dict.h&gt;#include &lt;libavutil/parseutils.h&gt;#include &lt;libavutil/samplefmt.h&gt;#include &lt;libavutil/avassert.h&gt;#include &lt;libavutil/time.h&gt;#include &lt;libavformat/avformat.h&gt;#include &lt;libavdevice/avdevice.h&gt;#include &lt;libswscale/swscale.h&gt;#include &lt;libavutil/opt.h&gt;#include &lt;libavcodec/avfft.h&gt;#include &lt;libswresample/swresample.h&gt;int main(int argc, char* argv[])&#123; printf(\"this is a test program for ffmpeg\\n\"); printf(\"%s\", avcodec_configuration()); getchar(); return 0;&#125; 在终端输入以下指令：gcc test1.c -o test -I /usr/local/include -L /usr/local/lib -lavutil -lavformat -lavcodec -lavutil -lswresample -lm -lrt -lpthread -lz./test 踩坑 1.ffmpeg: error while loading shared libraries: libavdevice.so.58: cannot open shared object file: No such file or directory因为我们使用源码编译，没有添加环境变量 123456789sudo vim &#x2F;etc&#x2F;ld.so.conf添加： &#x2F;usr&#x2F;local&#x2F;lib 退出vimsudo ldconfig # 更改生效sudo vim &#x2F;etc&#x2F;profile添加： export PATH&#x3D;&quot;&#x2F;usr&#x2F;local&#x2F;bin:$PATH&quot;source &#x2F;etc&#x2F;profile # 更改生效 再在终端输入ffmpeg -version 就会有信息输出了","tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://mtcai.github.io/tags/ffmpeg/"}]}]