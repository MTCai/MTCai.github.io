[{"title":"00-环境/00-MySQL安装","date":"2021-04-12T02:11:07.946Z","path":"2021/04/12/00-环境/00-MySQL安装.html","text":"MySQL安装安装文件：mysql-5.5.15-winx64.msi 接下来，需要注意：1）Typical 是经典安装，包含服务端和自带的客户端；2）Custom 是自定义安装；3）Complete 是完全安装。这里选择自定义安装。同时修改安装路径。 然后期间会出现mysql的广告，点击下一步即可。然后会继续安装，安装成功后，保证下面是勾选状态（默认也是勾选的），到这里仅是安装好了服务，还没配置。 如果取消了勾选，或配置时中途退出，也可以在安装目录下重新运行配置程序。 D:\\Program Files\\MySQL\\MySQL Server 5.5\\bin\\MySQLInstanceConfig.exe 配置界面有两个选项：1）Detailed XX 是精确配置；2）Standard XXX是标准配置。这里使用精确配置。然后选择服务类型，从上到下依次是开发机、服务器和专用服务器，占用的内存也依次递增。一般选择开发机即可。 接下来选择数据库类型，1）多功能型数据库；2）事务型数据库；3）非事务型数据库。存储引擎有事务性和非事务性，多功能型数据库在两种存储引擎速度都比较快，事务型数据库在事务型引擎较快，非事务型数据库在非事务型引擎速度较快。一般选择多功能型数据库。下一个界面直接下一步。 接下来配置数据库并发连接数：1）策略式，支持20个连接；2）在线式，允许500个连接；3）自定义，自己设定连接数。一般选择第一个即可。然后是配置端口号，开发中一般需要修改，防止别人恶意攻击。学习使用可先不改。 接下来选择字符集：使用第三个，然后下拉选择utf8。下一个界面中起一个服务名，其中绿线是开机自启，红线是添加环境变量。 接下来，设置root账户密码，同时勾选允许远程连接。然后点击“Execute”执行，等待完成，如下图。 图像化界面安装文件：SQLyog-10.0.0-0.exe 激活码： Name: any key: dd987f34-f358-4894-bd0f-21f3f04be9c1 一路下一步即可。然后新建一个连接，输入刚才设置的密码，连接。 连接成功就完成全部配置。 linux 安装mysql 查看mysql 是否安装，如果安装了，卸载mysql rpm -qa|grep mysql # 查询 rpm -e --nodeps mysql-libs-XXXXX.x86_64 # 卸载 安装mysql 服务端：rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm 查看产生的随机密码：cat /root/.mysql_secret 查看mysql 状态：service mysql status 启动mysql服务：service mysql start 安装MySql 客户端：rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm 链接mysql：mysql -uroot -pOEXaQuS8IWkG19Xs 修改密码：mysql&gt;SET PASSWORD=PASSWORD(‘000000’); 退出：mysql&gt;exit; 配置只要是root 用户+密码，在任何主机上都能登录MySQL 数据库。 [root@hadoop102 mysql-libs]# mysql -uroot -p000000 mysql&gt;use mysql; mysql&gt;show tables; mysql&gt;select User, Host, Password from user; mysql&gt;update user set host=&#39;%&#39; where host=&#39;localhost&#39;; mysql&gt;delete from user where Host=&#39;hadoop102&#39;; mysql&gt;delete from user where Host=&#39;127.0.0.1&#39;; mysql&gt;delete from user where Host=&#39;::1&#39;; mysql&gt;flush privileges; mysql&gt;quit;","tags":[]},{"title":"安装Hadoop","date":"2021-04-12T02:00:50.000Z","path":"2021/04/12/00-环境/01-Hadoop安装.html","text":"安装Hadoop 解压安装包 [atguigu@hadoop102 software]$ tar -zxvf hadoop-3.1.3.tar.gz -C/opt/module 添加环境变量 [atguigu@hadoop102 hadoop-3.1.3]$ sudo vim/etc/profile.d/my_env.sh 在 my_env.sh文件末尾添加如下内容 (shift+g) :’ #HADOOP_HOME ‘ export HADOOP_HOME=/opt/module/hadoop-3.1.3 export PATH=\\$PATH:\\$HADOOP_HOME/bin export PATH=\\$PATH:\\$HADOOP_HOME/sbin 测试是否安装成功 [atguigu@hadoop102 hadoop-3.1.3 ]$ hadoop version Hadoop 3.1.3 配置SSH免密登陆SSH无密登录配置 生成公钥和私钥 [atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登录的目标机器上 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104 注意： 还需要在hadoop102上采用 root 账号，配置一下无密登录到hadoop102、hadoop103、hadoop104； 还需要在hadoop103上采用 atguigu 账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。 还需要在hadoop104上采用 atguigu 账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。 配置集群核心配置文件配置core-site.xml [atguigu@hadoop102 hadoop]$ vi core-site.xml 在该文件中编写如下配置 &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- Hadoop3.x 配置 HDFS 网页登录使用的静态用户为 atguigu --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt; &lt;/property&gt; HDFS配置文件配置hadoop-env.sh [atguigu@hadoop102 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置hdfs-site.xml [atguigu@hadoop102 hadoop]$ vi hdfs-site.xml 在该文件中编写如下配置 &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- nn web 端访问地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http address &lt;/&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt; &lt;!-- 2 nn web 端访问地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt; &lt;/property&gt; YARN配置文件配置yarn-env.sh [atguigu@hadoop102 hadoop]$ vi yarn-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置yarn-site.xml [atguigu@hadoop102 hadoop]$ vi yarn-site.xml 在该文件中增加如下配置 &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!-- 日志聚集功能使能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置日志聚集服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;!-- 日志保留时间设置7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt; &lt;!-- 环境变量的继承 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE, HADOOP_YARN_HOME, HADOOP_MAPRED_HOME &lt;/value&gt; &lt;/property&gt; MapReduce配置文件配置mapred-env.sh [atguigu@hadoop102 hadoop]$ vi mapred-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置mapred-site.xml [atguigu@hadoop102 hadoop]$ cp mapred-site.xml.template mapred-site.xml [atguigu@hadoop102 hadoop]$ vi mapred-site.xml 在该文件中增加如下配置 &lt;!-- 指定MR运行在Yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop102:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器 web 端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;19888&lt;/value&gt; &lt;/property&gt; 在集群上分发配置好的Hadoop配置文件 [atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/ 配置slaves/workershadoop2.x修改 /opt/module/hadoop-2.7.2/etc/hadoop/slaves hadoop3.x修改 /opt/module/hadoop-3.1.3/etc/hadoop/workers [atguigu@hadoop102 hadoop]$ vi slaves 在该文件中增加如下内容：注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。 hadoop102 hadoop103 hadoop104 同步所有节点配置文件: [atguigu@hadoop102 hadoop]$ xsync slaves 启动如果集群是第一次启动，需要格式化NameNode，==启动前要关闭所有服务==（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据） 注意：格式化 NameNode 会产生新的集群 id，导致 NameNode 和 DataNode的集群 id不一致，集群找不到已往数据。 如果集群在运行过程中报错，需要重新格式化 NameNode的话， 一定要 先停止 namenode和 datanode进程， 并且要 删除 所有机器的 data和 logs目录，然后再进行格式化。 [atguigu@hadoop102 hadoop-2.7.2]$ hadoop namenode -format 集群启动/停止HDFS start-dfs.sh / stop-dfs.sh 集群启动/停止YARN start-yarn.sh / stop-yarn.sh","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"Hadoop编译源码","date":"2021-04-12T02:00:50.000Z","path":"2021/04/12/00-环境/02-Hadoop编译源码.html","text":"Hadoop编译源码（面试重点）准备工作 （1）系统联网，或者有yum源 （2）hadoop-2.7.2-src.tar.gz 进入hadoop-2.7.2-src文件夹，查看BUILDING.txt cd hadoop-2.7.2-src more BUILDING.txt 可以看到编译所需的库或者工具 （3）jdk-8u144-linux-x64.tar.gz （4）apache-ant-1.9.9-bin.tar.gz（build工具，打包用的） （5）apache-maven-3.0.5-bin.tar.gz （6）protobuf-2.5.0.tar.gz（序列化的框架） （7）apache-tomcat-6.0.44.tar.gz 配置jdk验证命令：java -version 配置Maven [root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml &lt;mirrors&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile :&#39; #MAVEN_HOME &#39; export MAVEN_HOME=/opt/module/apache-maven-3.0.5 export PATH=$PATH:$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn -version 配置ant [root@hadoop101 apache-ant-1.9.9]# vi /etc/profile :&#39; #ANT_HOME &#39; export ANT_HOME=/opt/module/apache-ant-1.9.9 export PATH=$PATH:$ANT_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：ant -version 安装 g++、make、cmake等库 [root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers [root@hadoop101 apache-ant-1.9.9]# yum -y install svn ncurses-devel gcc* [root@hadoop101 apache-ant-1.9.9]# yum install make [root@hadoop101 apache-ant-1.9.9]# yum install cmake [root@hadoop101 apache-ant-1.9.9]# yum -y install lzo-devel zlib-devel autoconf automake libtool cmake openssl-devel 安装protobuf [root@hadoop101 protobuf-2.5.0]#./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make check [root@hadoop101 protobuf-2.5.0]# make install [root@hadoop101 protobuf-2.5.0]# ldconfig [root@hadoop101 hadoop-dist]# vi /etc/profile :&#39; #LD_LIBRARY_PATH &#39; export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0 export PATH=$PATH:$LD_LIBRARY_PATH [root@hadoop101 software]#source /etc/profile 验证命令：protoc —version 安装findbugs 解压：tar -zxvf findbugs-3.0.1.tar.gz -C /opt/moudles/ 配置环境变量: 在 /etc/profile 文件末尾添加： export FINDBUGS_HOME=/opt/findbugs-3.0.1 export PATH=$PATH:$FINDBUGS_HOME/bin 保存退出，并使更改生效。 验证命令：findbugs -version 编译源码 1.进入到源码目录 [root@hadoop101 hadoop-2.7.2-src]# pwd /opt/hadoop-2.7.2-src 2.通过maven执行编译命令 [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 编译过程中会下载 apache-tomcat-6.0.44.tar.gz，速度非常慢，把提前下载好的文件放到如下目录： 注：编译前这两个目录并不存在，编译过程中及时中断，然后复制文件 hadoop-2.7.2-src/hadoop-common-project/hadoop-kms/downloads/ hadoop-2.7.2-src/hadoop-hdfs-project/hadoop-hdfs-httpfs/downloads 等待时间30分钟左右，最终成功是全部SUCCESS，如图 成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下 编译源码过程中常见的问题及解决方案（1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） （2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar （3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐2.7.0版本的问题汇总帖子 http://www.tuicool.com/articles/IBn63qf","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"Zookeeper安装","date":"2021-04-12T02:00:50.000Z","path":"2021/04/12/00-环境/03-Zookeeper安装.html","text":"官网首页：https://zookeeper.apache.org/ 本地模式安装部署安装 安装JDK 解压Zookeeper安装包 [atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 修改zoo.cfg配置、创建zkData目录 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件，修改dataDir路径： dataDir=/opt/module/zookeeper-3.4.10/zkData 在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹 mkdir zkData 操作Zookeeper 启动Zookeeper：bin/zkServer.sh start 查看进程是否启动 [atguigu@hadoop102 zookeeper-3.4.10]$ jps 4020 Jps 4001 QuorumPeerMain 查看状态 [atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: standalone 启动客户端：bin/zkCli.sh 退出客户端：[zk: localhost:2181(CONNECTED) 0] quit 停止Zookeeper：bin/zkServer.sh stop 配置参数解读Zookeeper中的配置文件zoo.cfg中参数含义解读如下： tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) initLimit =10：Leader 和 Fllower初始通信时限 集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 syncLimit =5：LF同步通信时限 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。 dataDir：数据文件目录+数据持久化路径 主要用于保存Zookeeper中的数据。 clientPort =2181：客户端连接端口 监听客户端连接的端口。 分布式安装部署在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper 解压安装# 解压Zookeeper安装包到/opt/module/目录下 [atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ # 同步/opt/module/zookeeper-3.4.10目录内容到hadoop103、hadoop104 [atguigu@hadoop102 module]$ xsync zookeeper-3.4.10/ 配置服务器编号 在/opt/module/zookeeper-3.4.10/这个目录下创建zkData：mkdir -p zkData 在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件：touch myid 注：添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 在myid文件中添加与server对应的编号：2 可以不从0开始，但是需要是唯一编号！ 拷贝配置好的zookeeper到其他机器上：[atguigu@hadoop102 zkData]$ xsync myid 并分别在hadoop102、hadoop103上修改myid文件中内容为3、4 修改zoo.cfg配置 重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg 修改zoo.cfg配置 dataDir=/opt/module/zookeeper-3.4.10/zkData #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 同步zoo.cfg配置文件 配置参数解读：server.A=B:C:D。 A是一个数字，表示这个是第几号服务器；集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群操作分别启动Zookeeper[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start [atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start [atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start 查看状态[atguigu@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [atguigu@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [atguigu@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Flume安装","date":"2021-04-12T02:00:50.000Z","path":"2021/04/12/00-环境/05-Flume安装.html","text":"安装地址 Flume 官网地址：http://flume.apache.org/ 文档查看地址：http://flume.apache.org/FlumeUserGuide.html 下载地址：http://archive.apache.org/dist/flume/ 安装部署 解压 apache flume 1.7.0 bin.tar.gz 到 /opt/ 目录下 将 flume/conf 下的 flume env.sh.template 文件修改为 flume env.sh ，并 配置 flumeenv.sh 文件 [atguigu@hadoop102 conf]$ mv flume env.sh.template flume env.sh [atguigu@hadoop102 conf]$ vi flume env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144","tags":[{"name":"Flume","slug":"Flume","permalink":"https://mtcai.github.io/tags/Flume/"}]},{"title":"数据结构32-拓扑排序","date":"2021-03-09T12:12:12.000Z","path":"2021/03/09/01-数据结构/数据结构32-拓扑排序 - 副本 (2).html","text":"[toc]","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构32-拓扑排序","date":"2021-03-09T12:12:12.000Z","path":"2021/03/09/01-数据结构/数据结构32-拓扑排序 - 副本 (3).html","text":"[toc]","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构32-拓扑排序","date":"2021-03-09T12:12:12.000Z","path":"2021/03/09/01-数据结构/数据结构32-拓扑排序.html","text":"[toc]","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构32-拓扑排序","date":"2021-03-09T12:12:12.000Z","path":"2021/03/09/01-数据结构/数据结构32-拓扑排序 - 副本.html","text":"[toc]","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构31-动态规划实战","date":"2021-03-09T08:49:12.000Z","path":"2021/03/09/01-数据结构/数据结构31-动态规划实战.html","text":"[toc] 搜索引擎中的拼写纠错功能利用 Trie 树，可以实现搜索引擎的关键词提示功能，这样可以节省用户输入搜索关键词的时间。实际上，搜索引擎在用户体验方面的优化还有很多，比如你可能经常会用的拼写纠错功能。当你在搜索框中，一不小心输错单词时，搜索引擎会非常智能地检测出你的拼写错误，并且用对应的正确单词来进行搜索。 编辑距离（Edit Distance）指的是将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。可以用来量化两个字符串之间的相似程度。编辑距离越大，说明两个字符串的相似程度越小；相反，编辑距离就越小，说明两个字符串的相似程度越大。对于两个完全相同的字符串来说，编辑距离就是 0。 根据所包含的编辑操作种类的不同，编辑距离有多种不同的计算方式，比较著名的有莱文斯坦距离（Levenshtein distance）和最长公共子串长度（Longest common substring length）。其中，莱文斯坦距离允许增加、删除、替换字符这三个编辑操作，最长公共子串长度只允许增加、删除字符这两个编辑操作。莱文斯坦距离和最长公共子串长度，从两个截然相反的角度，分析字符串的相似程度。莱文斯坦距离的大小，表示两个字符串差异的大小；而最长公共子串的大小，表示两个字符串相似程度的大小。 当用户在搜索框内，输入一个拼写错误的单词时，就拿这个单词跟词库中的单词一一进行比较，计算编辑距离，将编辑距离最小的单词，作为纠正之后的单词，提示给用户。 这就是拼写纠错最基本的原理。不过，真正用于商用的搜索引擎，拼写纠错功能显然不会就这么简单。一方面，单纯利用编辑距离来纠错，效果并不一定好；另一方面，词库中的数据量可能很大，搜索引擎每天要支持海量的搜索，所以对纠错的性能要求很高。 针对纠错效果不好的问题，有很多种优化思路，这里介绍几种。 我们并不仅仅取出编辑距离最小的那个单词，而是取出编辑距离最小的 TOP 10，然后根据其他参数，决策选择哪个单词作为拼写纠错单词。比如使用搜索热门程度来决定哪个单词作为拼写纠错单词。 我们还可以用多种编辑距离计算方法，比如今天讲到的两种，然后分别编辑距离最小的 TOP 10，然后求交集，用交集的结果，再继续优化处理。 我们还可以通过统计用户的搜索日志，得到最常被拼错的单词列表，以及对应的拼写正确的单词。搜索引擎在拼写纠错的时候，首先在这个最长被拼错单词列表中查找。如果一旦找到，直接返回对应的正确的单词。这样纠错的效果非常好。 我们还有更加高级一点的做法，引入个性化因素。针对每个用户，维护这个用户特有的搜索喜好，也就是常用的搜索关键词。当用户输入错误的单词的时候，我们首先在这个用户常用的搜索关键词中，计算编辑距离，查找编辑距离最小的单词。 针对纠错性能方面，我也有相应的优化方式。有两种分治的优化思路。 如果纠错功能的 TPS 不高，我们可以部署多台机器，每台机器运行一个独立的纠错功能。当有一个纠错请求的时候，我们通过负载均衡，分配到其中一台机器，来计算编辑距离，得到纠错单词。 如果纠错系统的响应时间太长，也就是，每个纠错请求处理时间过长，我们可以将纠错的词库，分割到很多台机器。当有一个纠错请求的时候，我们就将这个拼写错误的单词，同时发送到这多台机器，让多台机器并行处理，分别得到编辑距离最小的单词，然后再比对合并，最终决定出一个最优的纠错单词。 编程计算莱文斯坦距离回溯是一个递归处理的过程。如果 a[i] 与 b[j] 匹配，我们递归考察 a[i+1] 和 b[j+1]。如果 a[i] 与 b[j] 不匹配，那我们有多种处理方式可选： 可以删除 a[i]，然后递归考察 a[i+1] 和 b[j]； 可以删除 b[j]，然后递归考察 a[i] 和 b[j+1]； 可以在 a[i] 前面添加一个跟 b[j] 相同的字符，然后递归考察 a[i] 和 b[j+1]; 可以在 b[j] 前面添加一个跟 a[i] 相同的字符，然后递归考察 a[i+1] 和 b[j]； 可以将 a[i] 替换成 b[j]，或者将 b[j] 替换成 a[i]，然后递归考察 a[i+1] 和 b[j+1]。 // 回溯算法 private char[] a = &quot;mitcmu&quot;.toCharArray(); private char[] b = &quot;mtacnu&quot;.toCharArray(); private int n = 6; private int m = 6; private int minDist = Integer.MAX_VALUE; // 存储结果 // 调用方式 lwstBT(0, 0, 0); public lwstBT(int i, int j, int edist) { if (i == n || j == m) { if (i &lt; n) edist += (n-i); if (j &lt; m) edist += (m - j); if (edist &lt; minDist) minDist = edist; return; } if (a[i] == b[j]) { // 两个字符匹配 lwstBT(i+1, j+1, edist); } else { // 两个字符不匹配 lwstBT(i + 1, j, edist + 1); // 删除 a[i] 或者 b[j] 前添加一个字符 lwstBT(i, j + 1, edist + 1); // 删除 b[j] 或者 a[i] 前添加一个字符 lwstBT(i + 1, j + 1, edist + 1); // 将 a[i] 和 b[j] 替换为相同字符 } } 根据回溯算法的代码实现，我们可以画出递归树，看是否存在重复子问题。如果存在重复子问题，那我们就可以考虑能否用动态规划来解决；如果不存在重复子问题，那回溯就是最好的解决方法。 在递归树中，每个节点代表一个状态，状态包含三个变量 (i, j, edist)，其中，edist 表示处理到 a[i] 和 b[j] 时，已经执行的编辑操作的次数。 在递归树中，(i, j) 两个变量重复的节点很多，比如 (3, 2) 和 (2, 3)。对于 (i, j) 相同的节点，我们只需要保留 edist 最小的，继续递归处理就可以了，剩下的节点都可以舍弃。所以，状态就从 (i, j, edist) 变成了 (i, j, min_edist)，其中 min_edist 表示处理到 a[i] 和 b[j]，已经执行的最少编辑次数。 看到这里，你有没有觉得，这个问题跟上两节讲的动态规划例子非常相似？不过，这个问题的状态转移方式，要比之前两节课中讲到的例子都要复杂很多。上一节我们讲的矩阵最短路径问题中，到达状态 (i, j) 只能通过 (i-1, j) 或 (i, j-1) 两个状态转移过来，而今天这个问题，状态 (i, j) 可能从 (i-1, j)，(i, j-1)，(i-1, j-1) 三个状态中的任意一个转移过来。 如果：a[i]!=b[j]，那么：min_edist(i, j) 就等于： min(min_edist(i-1,j)+1, min_edist(i,j-1)+1, min_edist(i-1,j-1)+1) 如果：a[i]==b[j]，那么：min_edist(i, j) 就等于： min(min_edist(i-1,j)+1, min_edist(i,j-1)+1，min_edist(i-1,j-1)) 其中，min 表示求三数中的最小值。 // 动态规划 public int lwstDP(char[] a, int n, char[] b, int m) { int[][] minDist = new int[n][m]; for (int j = 0; j &lt; m; ++j) { // 初始化第 0 行:a[0..0] 与 b[0..j] 的编辑距离 if (a[0] == b[j]) minDist[0][j] = j; else if (j != 0) minDist[0][j] = minDist[0][j-1]+1; else minDist[0][j] = 1; } for (int i = 0; i &lt; n; ++i) { // 初始化第 0 列:a[0..i] 与 b[0..0] 的编辑距离 if (a[i] == b[0]) minDist[i][0] = i; else if (i != 0) minDist[i][0] = minDist[i-1][0]+1; else minDist[i][0] = 1; } for (int i = 1; i &lt; n; ++i) { // 按行填表 for (int j = 1; j &lt; m; ++j) { if (a[i] == b[j]) minDist[i][j] = min( minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]); else minDist[i][j] = min( minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]+1); } } return minDist[n-1][m-1]; } private int min(int x, int y, int z) { int minv = Integer.MAX_VALUE; if (x &lt; minv) minv = x; if (y &lt; minv) minv = y; if (z &lt; minv) minv = z; return minv; } 编程计算最长公共子串长度每个状态还是包括三个变量 (i, j, max_lcs)，max_lcs 表示 a[0…i] 和 b[0…j] 的最长公共子串长度。那 (i, j) 这个状态都是由哪些状态转移过来的呢？ 我们先来看回溯的处理思路。我们从 a[0] 和 b[0] 开始，依次考察两个字符串中的字符是否匹配。 如果 a[i] 与 b[j] 互相匹配，我们将最大公共子串长度加一，并且继续考察 a[i+1] 和 b[j+1]。 如果 a[i] 与 b[j] 不匹配，最长公共子串长度不变，这个时候，有两个不同的决策路线： 删除 a[i]，或者在 b[j] 前面加上一个字符 a[i]，然后继续考察 a[i+1] 和 b[j]； 删除 b[j]，或者在 a[i] 前面加上一个字符 b[j]，然后继续考察 a[i] 和 b[j+1]。 反过来也就是说，如果我们要求 a[0…i] 和 b[0…j] 的最长公共长度 max_lcs(i, j)，我们只有可能通过下面三个状态转移过来： (i-1, j-1, max_lcs)，其中 max_lcs 表示 a[0…i-1] 和 b[0…j-1] 的最长公共子串长度； (i-1, j, max_lcs)，其中 max_lcs 表示 a[0…i-1] 和 b[0…j] 的最长公共子串长度； (i, j-1, max_lcs)，其中 max_lcs 表示 a[0…i] 和 b[0…j-1] 的最长公共子串长度。 如果：a[i]==b[j]，那么：max_lcs(i, j) 就等于： max(max_lcs(i-1,j-1)+1, max_lcs(i-1, j), max_lcs(i, j-1))； 如果：a[i]!=b[j]，那么：max_lcs(i, j) 就等于： max(max_lcs(i-1,j-1), max_lcs(i-1, j), max_lcs(i, j-1))； 其中 max 表示求三数中的最大值。 // 动态规划 public int lcs(char[] a, int n, char[] b, int m) { int[][] maxlcs = new int[n][m]; for (int j = 0; j &lt; m; ++j) {// 初始化第 0 行：a[0..0] 与 b[0..j] 的 maxlcs if (a[0] == b[j]) maxlcs[0][j] = 1; else if (j != 0) maxlcs[0][j] = maxlcs[0][j-1]; else maxlcs[0][j] = 0; } for (int i = 0; i &lt; n; ++i) {// 初始化第 0 列：a[0..i] 与 b[0..0] 的 maxlcs if (a[i] == b[0]) maxlcs[i][0] = 1; else if (i != 0) maxlcs[i][0] = maxlcs[i-1][0]; else maxlcs[i][0] = 0; } for (int i = 1; i &lt; n; ++i) { // 填表 for (int j = 1; j &lt; m; ++j) { if (a[i] == b[j]) maxlcs[i][j] = max( maxlcs[i-1][j], maxlcs[i][j-1], maxlcs[i-1][j-1]+1); else maxlcs[i][j] = max( maxlcs[i-1][j], maxlcs[i][j-1], maxlcs[i-1][j-1]); } } return maxlcs[n-1][m-1]; } private int max(int x, int y, int z) { int maxv = Integer.MIN_VALUE; if (x &gt; maxv) maxv = x; if (y &gt; maxv) maxv = y; if (z &gt; maxv) maxv = z; return maxv; }","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构30-动态规划理论","date":"2021-03-09T03:41:12.000Z","path":"2021/03/09/01-数据结构/数据结构29-动态规划理论.html","text":"[toc] 一个模型三个特征一个模型：多阶段决策最优解模型。一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。 三个特征： 最优子结构 最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，后面阶段的状态可以通过前面阶段的状态推导出来。 无后效性 无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。 重复子问题 不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。 假设我们有一个 n 乘以 n 的矩阵 w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？ 从 (0, 0) 走到 (n-1, n-1)，总共要走 2*(n-1) 步，也就对应着 2*(n-1) 个阶段。每个阶段都有向右走或者向下走两种决策，并且每个阶段都会对应一个状态集合。 我们把状态定义为 min_dist(i, j)，其中 i 表示行，j 表示列。min_dist 表达式的值表示从 (0, 0) 到达 (i, j) 的最短路径长度。所以，这个问题是一个多阶段决策最优解问题，符合动态规划的模型。 可以用回溯算法来解决这个问题，画一下递归树，就会发现，递归树中有重复的节点。重复的节点表示，从左上角到节点对应的位置，有多种路线，这也能说明这个问题中存在重复子问题。 如果我们走到 (i, j) 这个位置，我们只能通过 (i-1, j)，(i, j-1) 这两个位置移动过来，也就是说，我们想要计算 (i, j) 位置对应的状态，只需要关心 (i-1, j)，(i, j-1) 两个位置对应的状态，并不关心棋子是通过什么样的路线到达这两个位置的。而且，我们仅仅允许往下和往右移动，不允许后退，所以，前面阶段的状态确定之后，不会被后面阶段的决策所改变，所以，这个问题符合“无后效性”这一特征。 刚刚定义状态的时候，我们把从起始位置 (0, 0) 到 (i, j) 的最小路径，记作 min_dist(i, j)。因为我们只能往右或往下移动，所以，我们只有可能从 (i, j-1) 或者 (i-1, j) 两个位置到达 (i, j)。也就是说，到达 (i, j) 的最短路径要么经过 (i, j-1)，要么经过 (i-1, j)，而且到达 (i, j) 的最短路径肯定包含到达这两个位置的最短路径之一。换句话说就是，min_dist(i, j) 可以通过 min_dist(i, j-1) 和 min_dist(i-1, j) 两个状态推导出来。这就说明，这个问题符合“最优子结构”。 min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j)) 两种动态规划解题思路总结状态转移表法在递归树中检查是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。 找到重复子问题之后，接下来，我们有两种处理思路，第一种是直接用回溯加“备忘录”的方法，来避免重复子问题。从执行效率上来讲，这跟动态规划的解决思路没有差别。第二种是使用动态规划的解决方法，状态转移表法。 我们先画出一个状态表。状态表一般都是二维的，所以你可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。 尽管大部分状态表都是二维的，但是如果问题的状态比较复杂，需要很多变量来表示，那对应的状态表可能就是高维的，比如三维、四维。那这个时候，我们就不适合用状态转移表法来解决了。一方面是因为高维状态转移表不好画图表示，另一方面是因为人脑确实很不擅长思考高维的东西。 现在，我们来看一下，如何套用这个状态转移表法，来解决之前那个矩阵最短路径的问题？ 从起点到终点，我们有很多种不同的走法。我们可以穷举所有走法，然后对比找出一个最短走法。不过如何才能无重复又不遗漏地穷举出所有走法呢？我们可以用回溯算法这个比较有规律的穷举算法。 回溯算法的代码实现如下所示。代码很短。 private int minDist = Integer.MAX_VALUE; // 全局变量或者成员变量 // 调用方式：minDistBacktracing(0, 0, 0, w, n); public void minDistBT(int i, int j, int dist, int[][] w, int n) { // 到达了 n-1, n-1 这个位置了，这里看着有点奇怪哈，你自己举个例子看下 if (i == n &amp;&amp; j == n) { if (dist &lt; minDist) minDist = dist; return; } if (i &lt; n) { // 往下走，更新 i=i+1, j=j minDistBT(i + 1, j, dist+w[i][j], w, n); } if (j &lt; n) { // 往右走，更新 i=i, j=j+1 minDistBT(i, j+1, dist+w[i][j], w, n); } } 有了回溯代码之后，接下来，我们要画出递归树，以此来寻找重复子问题。在递归树中，一个状态（也就是一个节点）包含三个变量 (i, j, dist)，其中 i，j 分别表示行和列，dist 表示从起点到达 (i, j) 的路径长度。从图中，我们看出，尽管 (i, j, dist) 不存在重复的，但是 (i, j) 重复的有很多。对于 (i, j) 重复的节点，我们只需要选择 dist 最小的节点，继续递归求解，其他节点就可以舍弃了。 既然存在重复子问题，我们就可以尝试看下，用动态规划来解决。我们画出一个二维状态表，表中的行、列表示棋子所在的位置，表中的数值表示从起点到这个位置的最短路径。我们按照决策过程，通过不断状态递推演进，将状态表填好。为了方便代码实现，我们按行来进行依次填充。 public int minDistDP(int[][] matrix, int n) { int[][] states = new int[n][n]; int sum = 0; for (int j = 0; j &lt; n; ++j) { // 初始化 states 的第一行数据 sum += matrix[0][j]; states[0][j] = sum; } sum = 0; for (int i = 0; i &lt; n; ++i) { // 初始化 states 的第一列数据 sum += matrix[i][0]; states[i][0] = sum; } for (int i = 1; i &lt; n; ++i) { for (int j = 1; j &lt; n; ++j) { states[i][j] = matrix[i][j] + Math.min(states[i][j-1], states[i-1][j]); } } return states[n-1][n-1]; } 状态转移方程法状态转移方程法有点类似递归的解题思路。我们需要分析，某个问题如何通过子问题来递归求解，也就是所谓的最优子结构。根据最优子结构，写出递归公式，也就是所谓的状态转移方程。有了状态转移方程，代码实现就非常简单了。一般情况下，我们有两种代码实现方法，一种是递归加“备忘录”，另一种是迭代递推。 min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j)) 状态转移方程是解决动态规划的关键。如果我们能写出状态转移方程，那动态规划问题基本上就解决一大半了，而翻译成代码非常简单。但是很多动态规划问题的状态本身就不好定义，状态转移方程也就更不好想到。 private int[][] matrix = {{1，3，5，9}, {2，1，3，4}，{5，2，6，7}，{6，8，4，3}}; private int n = 4; private int[][] mem = new int[4][4]; public int minDist(int i, int j) { // 调用 minDist(n-1, n-1); if (i == 0 &amp;&amp; j == 0) return matrix[0][0]; if (mem[i][j] &gt; 0) return mem[i][j]; int minLeft = Integer.MAX_VALUE; if (j-1 &gt;= 0) { minLeft = minDist(i, j-1); } int minUp = Integer.MAX_VALUE; if (i-1 &gt;= 0) { minUp = minDist(i-1, j); } int currMinDist = matrix[i][j] + Math.min(minLeft, minUp); mem[i][j] = currMinDist; return currMinDist; } 四种算法思想比较分析贪心、回溯、动态规划可以归为一类，而分治单独可以作为一类。前三个算法解决问题的模型，都可以抽象成我们今天讲的那个多阶段决策最优解模型，而分治算法解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。 回溯算法是个“万金油”。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。回溯算法相当于穷举搜索。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。对于大规模数据的问题，用回溯算法解决的执行效率就很低了。 尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。 贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性。其中，最优子结构、无后效性跟动态规划中的无异。“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构19-递归树：分析递归算法时间复杂度","date":"2021-03-02T03:15:12.000Z","path":"2021/03/02/01-数据结构/数据结构19-递归树.html","text":"[toc] 递归树与时间复杂度分析递归代码的时间复杂度分析起来很麻烦，除了用递推公式这种比较复杂的分析方法，还可以借助递归树来分析递归算法的时间复杂度。 递归的思想就是，将大问题分解为小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止。 如果我们把这个一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫作递归树。节点里的数字表示数据的规模，一个节点的求解可以分解为左右子节点两个问题的求解。 把归并排序画成递归树，就是下面这个样子： 因为每次分解都是一分为二，所以代价很低，我们把时间上的消耗记作常量 1。归并算法中比较耗时的是归并操作，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记作 n。 现在，我们只需要知道这棵树的高度 h，用高度 h 乘以每一层的时间消耗 n，就可以得到总的时间复杂度 O(n*h)。 从归并排序的原理和递归树，可以看出来，归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是 log~2~n，所以，归并排序递归实现的时间复杂度就是 O(nlogn)。这里的时间复杂度都是估算的，对树的高度的计算也没有那么精确，但是这并不影响复杂度的计算结果。 实战一：分析快速排序的时间复杂度快速排序在最好情况下，每次分区都能一分为二，这个时候用递推公式 T(n)=2T(\\frac{h}{2})+n，很容易就能推导出时间复杂度是 O(nlogn)。但是，我们并不可能每次分区都这么幸运，正好一分为二。 假设平均情况下，每次分区之后，两个分区的大小比例为 1:k。当 k=9 时，如果用递推公式的方法来求解时间复杂度的话，递推公式就写成T(n)=T(\\frac{n}{10})+T(\\frac{9n}{10})+n。 快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是 n。我们现在只要求出递归树的高度 h，这个快排过程遍历的数据个数就是 h∗n ，也就是说，时间复杂度就是 O(h∗n)。 快速排序结束的条件就是待排序的小区间，大小为 1，也就是说叶子节点里的数据规模是 1。从根节点 n 到叶子节点 1，递归树中最短的一个路径每次都乘以 1/10，最长的一个路径每次都乘以 9/10。通过计算，我们可以得到，从根节点到叶子节点的最短路径是 $log_{10}n$，最长的路径是 $log_{\\frac{10}{9}}⁡n$。 所以，遍历数据的个数总和就介于 $nlog_{10}n$ 和 $nlog_{\\frac{10}{9}}⁡n$ 之间。根据复杂度的大 O 表示法，对数复杂度的底数不管是多少，我们统一写成 logn，所以，当分区大小比例是 1:9 时，快速排序的时间复杂度仍然是 O(nlogn)。 也就是说，对于 k 等于 99，9999，甚至是 999999，99999999……，只要 k 的值不随 n 变化，是一个事先确定的常量，那快排的时间复杂度就是 O(nlogn)。所以，从概率论的角度来说，快排的平均时间复杂度就是 O(nlog⁡n)。 实战二：分析斐波那契数列的时间复杂度 从根节点走到叶子节点，每条路径是长短不一的。如果每次都是 −1，那最长路径大约就是 n；如果每次都是 −2，那最短路径大约就是 n/2。 每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作 1。所以，从上往下，第一层的总时间消耗是 1，第二层的总时间消耗是 2，第三层的总时间消耗就是 2^2^。依次类推，第 k 层的时间消耗就是 2^k−1^，那整个算法的总的时间消耗就是每一层时间消耗之和。 如果路径长度都为 n，那这个总和就是 2^n^−1。 如果路径长度都是 n/2 ，那整个算法的总的时间消耗就是 2^n/2^−1。 实战三：分析全排列的时间复杂度// 调用方式： // int[]a = a={1, 2, 3, 4}; printPermutations(a, 4, 4); // k 表示要处理的子数组的数据个数 public void printPermutations(int[] data, int n, int k) { if (k == 1) { for (int i = 0; i &lt; n; ++i) { System.out.print(data[i] + &quot; &quot;); } System.out.println(); } for (int i = 0; i &lt; k; ++i) { int tmp = data[i]; data[i] = data[k-1]; data[k-1] = tmp; printPermutations(data, n, k - 1); tmp = data[i]; data[i] = data[k-1]; data[k-1] = tmp; } } 如果我们确定了最后一位数据，那就变成了求解剩下 n−1 个数据的排列问题。而最后一位数据可以是 n 个数据中的任意一个，因此它的取值就有 nn种情况。所以，“n 个数据的排列”问题，就可以分解成 n 个“n−1 个数据的排列”的子问题。 第一层分解有 nn次交换操作，第二层有 n 个节点，每个节点分解需要 n−1 次交换，所以第二层总的交换次数是 n∗(n−1)。第三层有 n∗(n−1)n∗(n−1) 个节点，每个节点分解需要 n−2次交换，所以第三层总的交换次数是 n∗(n−1)∗(n−2)n∗(n−1)∗(n−2)。第 kk 层总的交换次数就是 n∗(n−1)∗(n−2)∗…∗(n−k+1)。最后一层为n∗(n−1)∗(n−2)∗…∗2∗1等于 n!。也就是说，全排列的递归算法的时间复杂度大于O(n!)，小于 O(n∗n!)。 小结有些代码比较适合用递推公式来分析，比如归并排序的时间复杂度、快速排序的最好情况时间复杂度；有些比较适合采用递归树来分析，比如快速排序的平均时间复杂度。而有些可能两个都不怎么适合使用，比如二叉树的递归前中后序遍历。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构18-红黑树","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构18-红黑树.html","text":"[toc] 平衡二叉查找树平衡二叉树的定义：二叉树中任意一个节点的左右子树的高度相差不能大于 1。完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。 发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。所以，平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。 如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比 log~2~n 大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。 什么是红黑树红黑树的英文是“Red-Black Tree”，简称 R-B Tree。它是一种不严格的平衡二叉查找树。 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点； 这里的第二点要求“叶子节点都是黑色的空节点”，稍微有些奇怪，它主要是为了简化红黑树的代码实现而设置的，只要满足这一条要求，那在任何时刻，红黑树的平衡操作都可以归结为特定的几类。暂时不考虑这一点，所以，在画图和讲解的时候，将黑色的、空的叶子节点都省略掉了。 红黑树“近似平衡”平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化的太严重。 如果我们将红色节点从红黑树中去掉，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。 红黑树的定义里有这么一条：从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。 完全二叉树的高度近似 log~2~n，这里的四叉“黑树”的高度要低于完全二叉树，所以去掉红色节点的“黑树”的高度也不会超过 log~2~n。 现在把红色节点加回去，在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过 log~2~n，所以加入红色节点之后，最长路径不会超过 2log~2~n，也就是说，红黑树的高度近似 2log~2~n。 所以，红黑树的高度只比高度平衡的 AVL 树的高度（log~2~n）仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。 Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。 AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了。 红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 AVL 树要低。 所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。 实现红黑树的基本思想左旋（rotate left）全称其实是叫围绕某个节点的左旋，右旋（rotate right）的全称叫围绕某个节点的右旋。 在插入、删除节点的过程中，第三、第四点要求可能会被破坏。 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点； 插入操作的平衡调整红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。 关于插入操作的平衡调整，有这样两种特殊情况： 如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。 如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。 除此之外，其他情况都会违背红黑树的定义，于是就需要进行调整，调整的过程包含两种基础的操作：左右旋转和改变颜色。 红黑树的平衡调整过程是一个迭代的过程。把正在处理的节点叫作关注节点。关注节点会随着不停地迭代处理，而不断发生变化。最开始的关注节点就是新插入的节点。 新节点插入之后，如果红黑树的平衡被打破，那一般会有下面三种情况。我们只需要根据每种情况的特点，不停地调整，就可以让红黑树继续符合定义，也就是继续保持平衡。 我们下面依次来看每种情况的调整过程。为了简化描述，把父节点的兄弟节点叫作叔叔节点，父节点的父节点叫作祖父节点。 CASE 1：如果关注节点是 a，它的叔叔节点 d 是红色 将关注节点 a 的父节点 b、叔叔节点 d 的颜色都设置成黑色； 将关注节点 a 的祖父节点 c 的颜色设置成红色； 关注节点变成 a 的祖父节点 c； 跳到 CASE 2 或者 CASE 3。 CASE 2：如果关注节点是 a，它的叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的右子节点 关注节点变成节点 a 的父节点 b； 围绕新的关注节点 b 左旋； 跳到 CASE 3。 CASE 3：如果关注节点是 a，它的叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的左子节点 围绕关注节点 a 的祖父节点 c 右旋； 将关注节点 a 的父节点 b、兄弟节点 c 的颜色互换。 调整结束。 删除操作的平衡调整红黑树插入操作的平衡调整还不是很难，但是它的删除操作的平衡调整相对就要难多了。不过原理都是类似的，依旧只需要根据关注节点与周围节点的排布特点，按照一定的规则去调整就行了。 删除操作的平衡调整分为两步，第一步是针对删除节点初步调整。初步调整只是保证整棵红黑树在一个节点删除之后，仍然满足最后一条定义的要求，也就是说，每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；第二步是针对关注节点进行二次调整，让它满足红黑树的第三条定义，即不存在相邻的两个红色节点。 针对删除节点初步调整红黑树的定义中“只包含红色节点和黑色节点”，经过初步调整之后，为了保证满足红黑树定义的最后一条要求，有些节点会被标记成两种颜色，“红 - 黑”或者“黑 - 黑”。如果一个节点被标记为了“黑 - 黑”，那在计算黑色节点个数的时候，要算成两个黑色节点。 在下面的讲解中，如果一个节点既可以是红色，也可以是黑色，在画图的时候，我会用一半红色一半黑色来表示。如果一个节点是“红 - 黑”或者“黑 - 黑”，我会用左上角的一个小黑点来表示额外的黑色。 CASE 1：如果要删除的节点是 a，它只有一个子节点 b 删除节点 a，并且把节点 b 替换到节点 a 的位置，这一部分操作跟普通的二叉查找树的删除操作一样； 节点 a 的位置只能是黑色，节点 b的位置也只能是红色（任意节点到可达子节点经过的黑色节点数量一致，任意节点的左右子树高度相差不超过1），其他情况均不符合红黑树的定义。这种情况下，我们把节点 b 改为黑色； 调整结束，不需要进行二次调整。 CASE 2：如果要删除的节点 a 有两个非空子节点，并且它的后继节点就是节点 a 的右子节点 c 如果节点 a 的后继节点就是右子节点 c，那右子节点 c 肯定没有左子树。我们把节点 a 删除，并且将节点 c 替换到节点 a 的位置。这一部分操作跟普通的二叉查找树的删除操作无异； 然后把节点 c 的颜色设置为跟节点 a 相同的颜色； 如果节点 c 是黑色，为了不违反红黑树的最后一条定义，我们给节点 c 的右子节点 d 多加一个黑色，这个时候节点 d 就成了“红 - 黑”或者“黑 - 黑”； 这个时候，关注节点变成了节点 d，第二步的调整操作就会针对关注节点来做。 CASE 3：如果要删除的是节点 a，它有两个非空子节点，并且节点 a 的后继节点不是右子节点 找到后继节点 d，并将它删除，删除后继节点 d 的过程参照 CASE 1； 将节点 a 替换成后继节点 d； 把节点 d 的颜色设置为跟节点 a 相同的颜色； 如果节点 d 是黑色，为了不违反红黑树的最后一条定义，我们给节点 d 的右子节点 c 多加一个黑色，这个时候节点 c 就成了“红 - 黑”或者“黑 - 黑”； 这个时候，关注节点变成了节点 c，第二步的调整操作就会针对关注节点来做。 针对关注节点进行二次调整 经过初步调整之后，关注节点变成了“红 - 黑”或者“黑 - 黑”节点。针对这个关注节点，我们再分四种情况来进行二次调整。二次调整是为了让红黑树中不存在相邻的红色节点。 CASE 1：如果关注节点是 a，它的兄弟节点 c 是红色的 围绕关注节点 a 的父节点 b 左旋； 关注节点 a 的父节点 b 和祖父节点 c 交换颜色； 关注节点不变； 继续从四种情况中选择适合的规则来调整。 CASE 2：如果关注节点是 a，它的兄弟节点 c 是黑色的，并且节点 c 的左右子节点 d、e 都是黑色的 将关注节点 a 的兄弟节点 c 的颜色变成红色； 从关注节点 a 中去掉一个黑色，这个时候节点 a 就是单纯的红色或者黑色； 给关注节点 a 的父节点 b 添加一个黑色，这个时候节点 b 就变成了“红 - 黑”或者“黑 - 黑”； 关注节点从 a 变成其父节点 b； 继续从四种情况中选择符合的规则来调整。 CASE 3：如果关注节点是 a，它的兄弟节点 c 是黑色，c 的左子节点 d 是红色，c 的右子节点 e 是黑色 围绕关注节点 a 的兄弟节点 c 右旋； 节点 c 和节点 d 交换颜色； 关注节点不变； 跳转到 CASE 4，继续调整。 CASE 4：如果关注节点 a 的兄弟节点 c 是黑色的，并且 c 的右子节点是红色的 围绕关注节点 a 的父节点 b 左旋； 将关注节点 a 的兄弟节点 c 的颜色，跟关注节点 a 的父节点 b 设置成相同的颜色； 将关注节点 a 的父节点 b 的颜色设置为黑色； 从关注节点 a 中去掉一个黑色，节点 a 就变成了单纯的红色或者黑色； 将关注节点 a 的叔叔节点 e 设置为黑色； 调整结束。 小结第一点，把红黑树的平衡调整的过程比作魔方复原，不要过于深究这个算法的正确性。你只需要明白，只要按照固定的操作步骤，保持插入、删除的过程，不破坏平衡树的定义就行了。 第二点，找准关注节点，不要搞丢、搞错关注节点。因为每种操作规则，都是基于关注节点来做的，只有弄对了关注节点，才能对应到正确的操作规则中。在迭代的调整过程中，关注节点在不停地改变，所以，这个过程一定要注意，不要弄丢了关注节点。 第三点，插入操作的平衡调整比较简单，但是删除操作就比较复杂。针对删除操作，我们有两次调整，第一次是针对要删除的节点做初步调整，让调整后的红黑树继续满足第四条定义，“每个节点到可达叶子节点的路径都包含相同个数的黑色节点”。但是这个时候，第三条定义就不满足了，有可能会存在两个红色节点相邻的情况。第二次调整就是解决这个问题，让红黑树不存在相邻的红色节点。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构21-图的表示","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构21-图的表示.html","text":"[toc] 图 图中的元素我们就叫作顶点（vertex）。 图中的一个顶点可以与任意其他顶点建立连接关系，这种建立的关系叫作边（edge）。 无向图中顶点相连接的边的条数叫作顶点的度（degree）。 有向图中，度分为入度（In-degree）和出度（Out-degree）：顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。 在带权图中，每条边都有一个权重（weight）。 邻接矩阵存储方法图最直观的一种存储方法就是，邻接矩阵（Adjacency Matrix）。 邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 $A[i][j]$ 和 $A[j][i]$ 标记为 1；对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 $A[i][j]$ 标记为 1。同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 $A[j][i]$ 标记为 1。对于带权图，数组中就存储相应的权重。 用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。对于无向图来说，如果 $A[i][j]$ 等于 1，那 $A[j][i]$ 也肯定等于 1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。如果我们存储的是稀疏图（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。 邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。 用邻接矩阵存储图的另外一个好处是方便计算。这是因为，用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。 邻接表存储方法邻接表（Adjacency List）每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。 图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点， 邻接表存储起来比较节省空间，但是使用起来就比较耗时间。可以将邻接表中的链表改成平衡二叉查找树。实际开发中，可以选择用红黑树。这样，可以更加快速地查找两个顶点之间是否存在边。二叉查找树也可以换成其他动态数据结构，比如跳表、散列表等。除此之外，还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构20-堆排序","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构20-堆和堆排序.html","text":"堆 堆是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。 第一点，堆必须是一个完全二叉树。完全二叉树要求除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。 第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。 对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆”。 其中第 1 个和第 2 个是大顶堆，第 3 个是小顶堆，第 4 个不是堆。 一个包含 n 个节点的完全二叉树，树的高度不会超过 log~2~⁡n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)。 插入如果把新插入的元素放到堆的最后，就不符合堆的特性了，于是就需要进行调整，让其重新满足堆的特性，这个过程起了一个名字，就叫作堆化（heapify）。 堆化实际上有两种，从下往上和从上往下。这里先讲从下往上的堆化方法。 让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。 public class Heap { private int[] a; // 数组，从下标 1 开始存储数据 private int n; // 堆可以存储的最大数据个数 private int count; // 堆中已经存储的数据个数 public Heap(int capacity) { a = new int[capacity + 1]; n = capacity; count = 0; } public void insert(int data) { if (count &gt;= n) return; // 堆满了 ++count; a[count] = data; int i = count; while (i/2 &gt; 0 &amp;&amp; a[i] &gt; a[i/2]) { // 自下往上堆化 swap(a, i, i/2); // swap() 函数作用：交换下标为 i 和 i/2 的两个元素 i = i/2; } } } 删除假设我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。 实际上，我们稍微改变一下思路，就可以解决这个问题。你看我画的下面这幅图。我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。 public void removeMax() { if (count == 0) return -1; // 堆中没有数据 a[1] = a[count]; --count; heapify(a, count, 1); } private void heapify(int[] a, int n, int i) { // 自上往下堆化 while (true) { int maxPos = i; if (i*2 &lt;= n &amp;&amp; a[i] &lt; a[i*2]) maxPos = i*2; if (i*2+1 &lt;= n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; } } 堆排序建堆我们首先将数组原地建成一个堆。所谓“原地”就是，不借助另一个数组，就在原数组上操作。建堆的过程，有两种思路。 第一种是借助前面讲的，在堆中插入一个元素的思路。尽管数组中包含 n 个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为 1 的数据。然后，我们调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含 n 个数据的数组，组织成了堆。 第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且每个数据都是从上往下堆化。因为叶子节点往下堆化只能自己跟自己比较，所以直接从第一个非叶子节点开始，依次堆化就行了。 对于完全二叉树来说，下标从n/2+1 到 n 的节点都是叶子节点。 private static void buildHeap(int[] a, int n) { for (int i = n/2; i &gt;= 1; --i) { heapify(a, n, i); } } private static void heapify(int[] a, int n, int i) { while (true) { int maxPos = i; if (i*2 &lt;= n &amp;&amp; a[i] &lt; a[i*2]) maxPos = i*2; if (i*2+1 &lt;= n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; } } 因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度 kk成正比。 将每个非叶子节点的高度求和： S=1*h+2^1*(h-1)+2^2*(h-2)+...+2^{h-1}*1\\\\ S=2^{h+1}-h-2\\\\ h=\\log_2n所以，建堆的时间复杂度就是 O(n)。 排序建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。这个过程有点类似上面讲的“删除堆顶元素”的操作，当堆顶元素移除之后，我们把下标为 nn的元素放到堆顶，然后再通过堆化的方法，将剩下的 n−1 个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是 n−1 的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。 // n 表示数据的个数，数组 a 中的数据从下标 1 到 n 的位置。 public static void sort(int[] a, int n) { buildHeap(a, n); int k = n; while (k &gt; 1) { swap(a, 1, k); --k; heapify(a, k, 1); } } 整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlog⁡n)，所以，堆排序整体的时间复杂度是 O(nlogn)。 堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。 在前面的讲解以及代码中，我都假设，堆中的数据是从数组下标为 1 的位置开始存储。那如果从 0 开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了.如果节点的下标是 i，那左子节点的下标就是 2∗i+1，右子节点的下标就是 2∗i+2，父节点的下标就是 $\\frac{i-1}{2}$。 特点 堆排序数据访问的方式没有快速排序友好。 对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。 比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶节点进行堆化，会依次访问数组下标是 1，2，4，8 的元素，而不是像快速排序那样，局部顺序访问，所以，这样对 CPU 缓存是不友好的。 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序 在讲排序的时候，提过两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多。 但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。 堆的应用优先级队列优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。 堆和优先级队列非常相似，一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。 合并有序小文件假设有 100 个小文件，每个文件的大小是 100MB，每个文件中存储的都是有序的字符串。希望将这些 100 个小文件合并成一个有序的大文件。这里就会用到优先级队列。 整体思路有点像归并排序中的合并函数。从这 100 个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。 假设，这个最小的字符串来自于 13.txt 这个小文件，我们就再从这个小文件取下一个字符串，并且放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，并且将它从数组中删除。依次类推，直到所有的文件中的数据都放入到大文件为止。 这里用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。 很好的方法是用优先级队列，将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中。 删除堆顶数据和往堆中插入数据的时间复杂度都是 O(logn)，n 表示堆中的数据个数，这里就是 100。比原来数组存储的方式高效了很多 高性能定时器假设有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。 但是，这样每过 1 秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。 针对这些问题，就可以用优先级队列来解决。按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。 这样，定时器就不需要每隔 1 秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 T。 这个时间间隔 T 就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在 T 秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。 当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。 这样，定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。 求 Top K求 Top K 的问题抽象成两类。一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。 针对静态数据，如何在一个包含 n 个数据的数组中，查找前 K 大数据呢？我们可以维护一个大小为 K 的小顶堆，顺序遍历数组，从数组中取出取数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。遍历数组需要 O(n) 的时间复杂度，一次堆化操作需要 O(logK) 的时间复杂度，所以最坏情况下，n 个元素都入堆一次，所以时间复杂度就是 O(nlogK)。 针对动态数据求得 Top K 就是实时 Top K。怎么理解呢？举一个例子。一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前 K 大数据。 如果每次询问前 K 大数据，我们都基于当前的全部数据重新计算的话，那时间复杂度就是 O(nlogK)，n 表示当前的数据的大小。实际上，可以一直都维护一个 K 大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 K 大数据，我们都可以里立刻返回给他。 求中位数中位数，顾名思义，就是处在中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第 $\\frac{n}{2}+1$ 个数据就是中位数；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第 $\\frac{n}{2}$ 个和第 $\\frac{n}{2}+1$个数据，这个时候，可以随意取一个作为中位数。 对于一组静态数据，中位数是固定的，我们可以先排序，第 $\\frac{n}{2}$个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。所以，尽管排序的代价比较大，但是边际成本会很小。但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。 借助堆这种数据结构，不用排序，就可以非常高效地实现求中位数操作，需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。 也就是说，如果有 n 个数据，n 是偶数，我们从小到大排序，那前 n/2个数据存储在大顶堆中，后 n/2个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果 n 是奇数，情况是类似的，大顶堆就存储 n/2+1 个数据，小顶堆中就存储 n/2 个数据。 如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；如果新加入的数据大于等于小顶堆的堆顶元素，我们就将这个新数据插入到小顶堆。 这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果 n 是偶数，两个堆中的数据个数都是 n/2；如果 n 是奇数，大顶堆有 n/2+1 个数据，小顶堆有 n/2 个数据。这个时候，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。 于是，我们就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间复杂度变成了 O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是 O(1)。 实际上，利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 n*99% 个数据，小顶堆中保存 n*1% 个数据。大顶堆堆顶的数据就是我们要找的 99% 分位数据。 每次插入一个数据的时候，要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。 但是，为了保持大顶堆中的数据占 99%，小顶堆中的数据占 1%，在每次新插入数据之后，都要重新计算大顶堆和小顶堆中的数据个数，是否还符合 99:1 这个比例。如果不符合，就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法。 通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是 O(logn)。每次求 99% 响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是 O(1)。 class MedianFinder { Queue&lt;Integer&gt; A, B; public MedianFinder() { A = new PriorityQueue&lt;&gt;(); // 小顶堆，保存较大的一半 B = new PriorityQueue&lt;&gt;((x, y) -&gt; (y - x)); // 大顶堆，保存较小的一半 } public void addNum(int num) { if(A.size() != B.size()) { A.add(num); B.add(A.poll()); } else { B.add(num); A.add(B.poll()); } } public double findMedian() { return A.size() != B.size() ? A.peek() : (A.peek() + B.peek()) / 2.0; } }","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构22-深度和广度优先搜索","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构22-深度和广度优先搜索.html","text":"[toc] 深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。 public class Graph { // 无向图 private int v; // 顶点的个数 private LinkedList&lt;Integer&gt; adj[]; // 邻接表 public Graph(int v) { this.v = v; adj = new LinkedList[v]; for (int i=0; i&lt;v; ++i) { adj[i] = new LinkedList&lt;&gt;(); } } public void addEdge(int s, int t) { // 无向图一条边存两次 adj[s].add(t); adj[t].add(s); } } 广度优先搜索广度优先搜索（Breadth-First-Search）,简称为 BFS，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。 public void bfs(int s, int t) { //其中 s 表示起始顶点，t 表示终止顶点。 if (s == t) return; boolean[] visited = new boolean[v]; visited[s]=true; Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); queue.add(s); int[] prev = new int[v]; for (int i = 0; i &lt; v; ++i) { prev[i] = -1; } while (queue.size() != 0) { int w = queue.poll(); for (int i = 0; i &lt; adj[w].size(); ++i) { int q = adj[w].get(i); if (!visited[q]) { prev[q] = w; if (q == t) { print(prev, s, t); return; } visited[q] = true; queue.add(q); } } } } private void print(int[] prev, int s, int t) { // 递归打印 s-&gt;t 的路径 if (prev[t] != -1 &amp;&amp; t != s) { print(prev, s, prev[t]); } System.out.print(t + &quot; &quot;); } visited是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点 q 被访问，那相应的 visited[q] 会被设置为 true。 queue是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第 k 层的顶点都访问完成之后，才能访问第 k+1 层的顶点。当我们访问到第 k 层的顶点的时候，我们需要把第 k 层的顶点记录下来，稍后才能通过第 k 层的顶点来找第 k+1 层的顶点。所以，我们用这个队列来实现记录的功能。 prev用来记录搜索路径。当我们从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。不过，这个路径是反向存储的。prev[w] 存储的是，顶点 w 是从哪个前驱顶点遍历过来的。比如，我们通过顶点 2 的邻接表访问到顶点 3，那 prev[3] 就等于 2。为了正向打印出路径，我们需要递归地来打印，你可以看下 print() 函数的实现方式。 最坏情况下，终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是 O(V+E)，其中，V 表示顶点的个数，E 表示边的个数。当然，对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，E 肯定要大于等于 V-1，所以，广度优先搜索的时间复杂度也可以简写为 O(E)。 广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是 O(V)。 深度优先搜索深度优先搜索（Depth-First-Search），简称 DFS。 最直观的例子就是“走迷宫”。假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。 这里面实线箭头表示遍历，虚线箭头表示回退。从图中可以看出，深度优先搜索找出来的路径，并不是顶点 s 到顶点 t 的最短路径。 实际上，深度优先搜索用的是一种比较著名的算法思想，回溯思想。这种思想解决问题的过程，非常适合用递归来实现。 boolean found = false; // 全局变量或者类成员变量 public void dfs(int s, int t) { found = false; boolean[] visited = new boolean[v]; int[] prev = new int[v]; for (int i = 0; i &lt; v; ++i) { prev[i] = -1; } recurDfs(s, t, visited, prev); print(prev, s, t); } private void recurDfs(int w, int t, boolean[] visited, int[] prev) { if (found == true) return; visited[w] = true; if (w == t) { found = true; return; } for (int i = 0; i &lt; adj[w].size(); ++i) { int q = adj[w].get(i); if (!visited[q]) { prev[q] = w; recurDfs(q, t, visited, prev); } } } 深度优先搜索代码实现也用到了 prev、visited 变量以及 print() 函数，它们跟广度优先搜索代码实现里的作用是一样的。不过，深度优先搜索代码实现里，有个比较特殊的变量 found，它的作用是，当已经找到终止顶点 t 之后，就不再递归地继续查找了。 从我面画的图可以看出，每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是 O(E)，E 表示边的个数。 深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是 O(V)。 小结广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，比如 A、IDA 等，要简单粗暴，没有什么优化，所以，也被叫作暴力搜索算法。所以，这两种搜索算法仅适用于状态空间不大，也就是说图不大的搜索。 广度优先搜索，通俗的理解就是，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先搜索的时间复杂度都是 O(E)，空间复杂度是 O(V)。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构24-Trie 树","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构24-Trie树.html","text":"[toc] Trie 树Trie 树，也叫“字典树”。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。 这样一个问题可以有多种解决方法，比如散列表、红黑树，或者前面几节的一些字符串匹配算法，但是，Trie 树在这个问题的解决上，有它特有的优点。不仅如此，Trie 树能解决的问题也不限于此。 有 6 个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这 6 个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？ 可以先对这 6 个字符串做一下预处理，组织成 Trie 树的结构，之后每次查找，都是在 Trie 树中进行匹配查找。Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。 其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）。 public class Trie { private TrieNode root = new TrieNode(&#39;/&#39;); // 存储无意义字符 // 往 Trie 树中插入一个字符串 public void insert(char[] text) { TrieNode p = root; for (int i = 0; i &lt; text.length; ++i) { int index = text[i] - &#39;a&#39;; if (p.children[index] == null) { TrieNode newNode = new TrieNode(text[i]); p.children[index] = newNode; } p = p.children[index]; } p.isEndingChar = true; } // 在 Trie 树中查找一个字符串 public boolean find(char[] pattern) { TrieNode p = root; for (int i = 0; i &lt; pattern.length; ++i) { int index = pattern[i] - &#39;a&#39;; if (p.children[index] == null) { return false; // 不存在 pattern } p = p.children[index]; } if (p.isEndingChar == false) return false; // 不能完全匹配，只是前缀 else return true; // 找到 pattern } public class TrieNode { public char data; public TrieNode[] children = new TrieNode[26]; public boolean isEndingChar = false; public TrieNode(char data) { this.data = data; } } } 如果要在一组字符串中，频繁地查询某些字符串，用 Trie 树会非常高效。构建 Trie 树的过程，需要扫描所有的字符串，时间复杂度是 O(n)（n 表示所有字符串的长度和）。但是一旦构建成功之后，后续的查询操作会非常高效。 每次查询时，如果要查询的字符串长度是 k，那我们只需要比对大约 k 个节点，就能完成查询操作。跟原本那组字符串的长度和个数没有任何关系。所以说，构建好 Trie 树后，在其中查找字符串的时间复杂度是 O(k)，k 表示要查找的字符串的长度。 Trie 树是非常耗内存的，用的是一种空间换时间的思路 Trie 树的实现的时候，讲到用数组来存储一个节点的子节点的指针。如果字符串中包含从 a 到 z 这 26 个字符，那每个节点都要存储一个长度为 26 的数组，并且每个数组存储一个 8 字节指针（或者是 4 字节，这个大小跟 CPU、操作系统、编译器等有关）。而且，即便一个节点只有很少的子节点，远小于 26 个，比如 3、4 个，我们也要维护一个长度为 26 的数组。 Trie 树的本质是避免重复存储一组字符串的相同前缀子串，但是现在每个字符（对应一个节点）的存储远远大于 1 个字节。在重复的前缀并不多的情况下，Trie 树不但不能节省内存，还有可能会浪费更多的内存。 将每个节点中的数组换成其他数据结构，比如有序数组、跳表、散列表、红黑树等。 假设我们用有序数组，数组中的指针按照所指向的子节点中的字符的大小顺序排列。查询的时候，我们可以通过二分查找的方法，快速查找到某个字符应该匹配的子节点的指针。但是，在往 Trie 树中插入一个字符串的时候，我们为了维护数组中数据的有序性，就会稍微慢了点。 实际上，Trie 树的变体有很多，都可以在一定程度上解决内存消耗的问题。比如，缩点优化，就是对只有一个子节点的节点，而且此节点不是一个串的结束节点，可以将此节点与子节点合并。这样可以节省空间，但却增加了编码难度。 Trie 树与散列表、红黑树的比较实际上，字符串的匹配问题，笼统上讲，其实就是数据的查找问题。对于支持动态数据高效操作的数据结构，比如散列表、红黑树、跳表等等。实际上，这些数据结构也可以实现在一组字符串中查找字符串的功能。 在一组字符串中查找字符串，Trie 树实际上表现并不好。它对要处理的字符串有严苛的要求。 第一，字符串中包含的字符集不能太大。如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。 第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。 第三，如果要用 Trie 树解决问题，那就要自己从零开始实现一个 Trie 树，还要保证没有 bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。 第四，通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。 综合这几点，针对在一组字符串中查找字符串的问题，在工程中，更倾向于用散列表或者红黑树。因为这两种数据结构，都不需要自己去实现，直接利用编程语言中提供的现成类库就行了。 实际上，Trie 树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适合的是查找前缀匹配的字符串。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构22-深度和广度优先搜索","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构26-贪心算法.html","text":"[toc] 贪心算法（greedy algorithm）贪心算法有很多经典的应用，比如霍夫曼编码（Huffman Coding）、Prim 和 Kruskal 最小生成树算法、还有 Dijkstra 单源最短路径算法。 假设我们有一个可以容纳 100kg 物品的背包，可以装各种物品。我们有以下 5 种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？ 只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆，所以，我们可以往背包里装 20kg 黑豆、30kg 绿豆、50kg 红豆。 总结一下贪心算法解决问题的步骤： 第一步，当看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。类比到刚刚的例子，限制值就是重量不能超过 100kg，期望值就是物品的总价值。这组数据就是 5 种豆子。我们从中选出一部分，满足重量不超过 100kg，并且总价值最大。 第二步，尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。类比到刚刚的例子，我们每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。 第三步，举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。 实际上，用贪心算法解决问题的思路，并不总能给出最优解。 在一个有权图中，从顶点 S 开始，找一条到顶点 T 的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点 T。按照这种思路，我们求出的最短路径是 S-&gt;A-&gt;E-&gt;T，路径长度是 1+4+4=9。 但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径 S-&gt;B-&gt;D-&gt;T 才是最短路径，因为这条路径的长度是 2+2+2=6。 在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果我们第一步从顶点 S 走到顶点 A，那接下来面对的顶点和边，跟第一步从顶点 S 走到顶点 B，是完全不同的。所以，即便我们第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。 贪心算法实战分析分糖果我们有 m 个糖果和 n 个孩子。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m&lt;n），所以糖果只能分配给一部分孩子。 每个糖果的大小不等，这 m 个糖果的大小分别是 s1，s2，s3，……，sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这 n 个孩子对糖果大小的需求分别是 g1，g2，g3，……，gn。如何分配糖果，能尽可能满足最多数量的孩子？ 我们可以把这个问题抽象成，从 n 个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数（期望值）是最大的。这个问题的限制值就是糖果个数 m。 对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。 我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。 钱币找零这个问题在我们的日常生活中更加普遍。假设我们有 1 元、2 元、5 元、10 元、20 元、50 元、100 元这些面额的纸币，它们的张数分别是 c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付 K 元，最少要用多少张纸币呢？ 在生活中，我们肯定是先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用 1 元来补齐。 在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。直觉告诉我们，这种处理方法就是最好的。实际上，要严谨地证明这种贪心算法的正确性，需要比较复杂的。 区间覆盖假设我们有 n 个区间，区间的起始端点和结束端点分别是 [l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。我们从这 n 个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？ 这个问题的处理思路稍微不是那么好懂，不过，我建议你最好能弄懂，因为这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。 这个问题的解决思路是这样的：我们假设这 n 个区间中最左端点是 lmin，最右端点是 rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将 [lmin, rmax] 覆盖上。我们按照起始端点从小到大的顺序对这 n 个区间排序。 我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。 霍夫曼编码霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在 20%～90% 之间。 霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。 对于等长的编码来说，我们解压缩起来很简单。用 3 个 bit 表示一个字符，在解压缩的时候，我们每次从文本中读取 3 位二进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取 1 位还是 2 位、3 位等等来解压缩呢？这个问题就导致霍夫曼编码解压缩起来比较复杂。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。 假设这 6 个字符出现的频率从高到低依次是 a、b、c、d、e、f。我们把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀，在解压缩的时候，我们每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这 1000 个字符只需要 2100bits 就可以了。 尽管霍夫曼编码的思想并不难理解，但是如何根据字符出现频率的不同，给不同的字符进行不同长度的编码呢？这里的处理稍微有些技巧。 我们把每个字符看作一个节点，并且辅带着把频率放到优先级队列中。我们从队列中取出频率最小的两个节点 A、B，然后新建一个节点 C，把频率设置为两个节点的频率之和，并把这个新节点 C 作为节点 A、B 的父节点。最后再把 C 节点放入到优先级队列中。重复这个过程，直到队列中没有数据。 现在，我们给每一条边加上画一个权值，指向左子节点的边我们统统标记为 0，指向右子节点的边，我们统统标记为 1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构27-分治算法","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构27-分治算法.html","text":"[toc] 分治算法分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。 分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作： 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题。 分治算法能解决的问题，一般需要满足下面这几个条件： 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。 应用举例分析假设我们有 n 个数据，我们期望数据从小到大排列，那完全有序的数据的有序度就是 n(n-1)/2，逆序度等于 0；相反，倒序排列的数据的有序度就是 0，逆序度是 n(n-1)/2。除了这两种极端情况外，我们通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。 现在的问题是，如何编程求出一组数据的有序对个数或者逆序对个数呢？因为有序对个数和逆序对个数的求解方式是类似的，所以你可以只思考逆序对个数的求解方法。 最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作 k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的 k 值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是 O(n^2)。那有没有更加高效的处理方法呢？ 套用分治的思想来求数组 A 的逆序对个数。我们可以将数组分成前后两半 A1 和 A2，分别计算 A1 和 A2 的逆序对个数 K1 和 K2，然后再计算 A1 与 A2 之间的逆序对个数 K3。那数组 A 的逆序对个数就等于 K1+K2+K3。 private int num = 0; // 全局变量或者成员变量 public int count(int[] a, int n) { num = 0; mergeSortCounting(a, 0, n-1); return num; } private void mergeSortCounting(int[] a, int p, int r) { if (p &gt;= r) return; int q = (p+r)/2; mergeSortCounting(a, p, q); mergeSortCounting(a, q+1, r); merge(a, p, q, r); } private void merge(int[] a, int p, int q, int r) { int i = p, j = q+1, k = 0; int[] tmp = new int[r-p+1]; while (i&lt;=q &amp;&amp; j&lt;=r) { if (a[i] &lt;= a[j]) { tmp[k++] = a[i++]; } else { num += (q-i+1); // 统计 p-q 之间，比 a[j] 大的元素个数 tmp[k++] = a[j++]; } } while (i &lt;= q) { // 处理剩下的 tmp[k++] = a[i++]; } while (j &lt;= r) { // 处理剩下的 tmp[k++] = a[j++]; } for (i = 0; i &lt;= r-p; ++i) { // 从 tmp 拷贝回 a a[p+i] = tmp[i]; } } 有很多同学经常说，某某算法思想如此巧妙，我是怎么也想不到的。实际上，确实是的。有些算法确实非常巧妙，并不是每个人短时间都能想到的。比如这个问题，并不是每个人都能想到可以借助归并排序算法来解决，不夸张地说，如果之前没接触过，绝大部分人都想不到。但是，如果我告诉你可以借助归并排序算法来解决，那你就应该要想到如何改造归并排序，来求解这个问题了，只要你能做到这一点，我觉得就很棒了。 关于分治算法，我这还有两道比较经典的问题，你可以自己练习一下。 二维平面上有 n 个点，如何快速计算出两个距离最近的点对？ 有两个 n*n 的矩阵 A，B，如何快速求解两个矩阵的乘积 C=A*B？ 分治思想在海量数据处理中的应用比如，给 10GB 的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有 10GB，而我们的机器的内存可能只有 2、3GB 这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。 要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。我们可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。 可以先扫描一遍订单，根据订单的金额，将 10GB 的文件划分为几个金额区间。比如订单金额为 1 到 100 元的放到一个小文件，101 到 200 之间的放到另一个文件，以此类推。这样每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终有序的 10GB 订单数据了。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构25-AC自动机","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构25-AC自动机.html","text":"[toc] AC自动机实现一个高性能的敏感词过滤系统 基于单模式串和 Trie 树实现的敏感词过滤BF 算法、RK 算法、BM 算法、KMP 算法，还有 Trie 树。前面四种算法都是单模式串匹配算法，只有 Trie 树是多模式串匹配算法。 单模式串匹配算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串。多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。 对敏感词字典进行预处理，构建成 Trie 树结构。这个预处理的操作只需要做一次，如果敏感词字典动态更新了，比如删除、添加了一个敏感词，那只需要动态更新一下 Trie 树就可以了。 当用户输入一个文本内容后，把用户输入的内容作为主串，从第一个字符（假设是字符 C）开始，在 Trie 树中匹配。当匹配到 Trie 树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符 C 的下一个字符开始，重新在 Trie 树中匹配。 基于 Trie 树的这种处理方法，有点类似单模式串匹配的 BF 算法。我们知道，单模式串匹配算法中，KMP 算法对 BF 算法进行改进，引入了 next 数组，让匹配失败时，尽可能将模式串往后多滑动几位。借鉴单模式串的优化改进方法，能否对多模式串 Trie 树进行改进，进一步提高 Trie 树的效率呢？这就要用到 AC 自动机算法了。 经典的多模式串匹配算法：AC 自动机AC 自动机算法，全称是 Aho-Corasick 算法。其实，Trie 树跟 AC 自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟 KMP 算法之间的关系一样，只不过前者针对的是多模式串而已。所以，AC 自动机实际上就是在 Trie 树之上，加了类似 KMP 的 next 数组，只不过此处的 next 数组是构建在树上罢了。 所以，AC 自动机的构建，包含两个操作： 将多个模式串构建成 Trie 树； 在 Trie 树上构建失败指针（相当于 KMP 中的失效函数 next 数组）。 构建好 Trie 树之后，如何在它之上构建失败指针？","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构28-回溯算法","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构28-回溯算法.html","text":"[toc] 深度优先搜索算法利用的是回溯算法思想。这个算法思想非常简单，但是应用却非常广泛。它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。 理解“回溯算法”笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。 回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。 理论的东西还是过于抽象，老规矩，我还是举例说明一下。我举一个经典的回溯例子，我想你可能已经猜到了，那就是八皇后问题。 我们有一个 8x8 的棋盘，希望往里放 8 个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。你可以看我画的图，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。 我们把这个问题划分成 8 个阶段，依次将 8 个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前的方法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种方法，继续尝试。 int[] result = new int[8];// 全局或成员变量, 下标表示行, 值表示 queen 存储在哪一列 public void cal8queens(int row) { // 调用方式：cal8queens(0); if (row == 8) { // 8 个棋子都放置好了，打印结果 printQueens(result); return; // 8 行棋子都放好了，已经没法再往下递归了，所以就 return } for (int column = 0; column &lt; 8; ++column) { // 每一行都有 8 中放法 if (isOk(row, column)) { // 有些放法不满足要求 result[row] = column; // 第 row 行的棋子放到了 column 列 cal8queens(row+1); // 考察下一行 } } } private boolean isOk(int row, int column) {// 判断 row 行 column 列放置是否合适 int leftup = column - 1, rightup = column + 1; for (int i = row-1; i &gt;= 0; --i) { // 逐行往上考察每一行 if (result[i] == column) return false; // 第 i 行的 column 列有棋子吗？ if (leftup &gt;= 0) { // 考察左上对角线：第 i 行 leftup 列有棋子吗？ if (result[i] == leftup) return false; } if (rightup &lt; 8) { // 考察右上对角线：第 i 行 rightup 列有棋子吗？ if (result[i] == rightup) return false; } --leftup; ++rightup; } return true; } private void printQueens(int[] result) { // 打印出一个二维矩阵 for (int row = 0; row &lt; 8; ++row) { for (int column = 0; column &lt; 8; ++column) { if (result[row] == column) System.out.print(&quot;Q &quot;); else System.out.print(&quot;* &quot;); } System.out.println(); } System.out.println(); } 经典应用0-1 背包0-1 背包问题有很多变体，这里介绍一种比较基础的。我们有一个背包，背包总的承载重量是 Wkg。现在我们有 n 个物品，每个物品的重量不等，并且不可分割。我们现在期望选择几件物品，装载到背包中。在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大？ 今天讲的这个背包问题，物品是不可分割的，要么装要么不装，所以叫 0-1 背包问题。显然，这个问题已经无法通过贪心算法来解决了。我们现在来看看，用回溯算法如何来解决。 对于每个物品来说，都有两种选择，装进背包或者不装进背包。对于 n 个物品来说，总的装法就有 2^n 种，去掉总重量超过 Wkg 的，从剩下的装法中选择总重量最接近 Wkg 的。不过，我们如何才能不重复地穷举出这 2^n 种装法呢？ 这里就可以用回溯的方法。我们可以把物品依次排列，整个问题就分解为了 n 个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。描述起来很费劲，我们直接看代码，反而会更加清晰一些。 这里还稍微用到了一点搜索剪枝的技巧，就是当发现已经选择的物品的重量超过 Wkg 之后，我们就停止继续探测剩下的物品。 public int maxW = Integer.MIN_VALUE; // 存储背包中物品总重量的最大值 // cw 表示当前已经装进去的物品的重量和；i 表示考察到哪个物品了； // w 背包重量；items 表示每个物品的重量；n 表示物品个数 // 假设背包可承受重量 100，物品个数 10，物品重量存储在数组 a 中，那可以这样调用函数： // f(0, 0, a, 10, 100) public void f(int i, int cw, int[] items, int n, int w) { if (cw == w || i == n) { // cw==w 表示装满了 ;i==n 表示已经考察完所有的物品 if (cw &gt; maxW) maxW = cw; return; } f(i+1, cw, items, n, w);// 借助回溯过程，实现了以每一个可能的物品，作为第一个装入背包的，以尝试所有物品组合。 if (cw + items[i] &lt;= w) {// 已经超过可以背包承受的重量的时候，就不要再装了 f(i+1,cw + items[i], items, n, w); } } 正则表达式正则表达式中，最重要的就是通配符，通配符结合在一起，可以表达非常丰富的语义。为了方便讲解，我假设正表达式中只包含 “*” 和 “?” 这两种通配符，并且对这两个通配符的语义稍微做些改变，其中，“*”匹配任意多个（大于等于 0 个）任意字符，“\\?”匹配零个或者一个任意字符。基于以上背景假设，我们看下，如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？ 我们依次考察正则表达式中的每个字符，当是非通配符时，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。 如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口，比如“*”有多种匹配方案，可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。 public class Pattern { private boolean matched = false; private char[] pattern; // 正则表达式 private int plen; // 正则表达式长度 public Pattern(char[] pattern, int plen) { this.pattern = pattern; this.plen = plen; } public boolean match(char[] text, int tlen) { // 文本串及长度 matched = false; rmatch(0, 0, text, tlen); return matched; } private void rmatch(int ti, int pj, char[] text, int tlen) { if (matched) return; // 如果已经匹配了，就不要继续递归了 if (pj == plen) { // 正则表达式到结尾了 if (ti == tlen) matched = true; // 文本串也到结尾了 return; } if (pattern[pj] == &#39;*&#39;) { // * 匹配任意个字符 for (int k = 0; k &lt;= tlen-ti; ++k) { rmatch(ti+k, pj+1, text, tlen); } } else if (pattern[pj] == &#39;?&#39;) { // ? 匹配 0 个或者 1 个字符 rmatch(ti, pj+1, text, tlen); rmatch(ti+1, pj+1, text, tlen); } else if (ti &lt; tlen &amp;&amp; pattern[pj] == text[ti]) { // 纯字符匹配才行 rmatch(ti+1, pj+1, text, tlen); } } } 回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。 尽管回溯算法的原理非常简单，但是却可以解决很多问题，比如我们开头提到的深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配等等。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构23-字符串匹配","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构23-字符串匹配.html","text":"[toc] BF (Brute Force) 算法BF 算法( Brute Force )，中文叫作暴力匹配算法，也叫朴素匹配算法。 在字符串 A 中查找字符串 B，那字符串 A 就是主串，字符串 B 就是模式串。把主串的长度记作 n，模式串的长度记作 m。因为我们是在主串中查找模式串，所以 n&gt;m。 BF 算法的思想可以用一句话来概括，那就是，在主串中，检查起始位置分别是 0、1、2…n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的。 从上面的算法思想和例子，可以看出，在极端情况下，比如主串是“aaaaa…aaaaaa”（省略号表示有很多重复的字符 a），模式串是“aaaaab”。我们每次都比对 m 个字符，要比对 n-m+1 次，所以，这种算法的最坏情况时间复杂度是 O(n*m)。 尽管理论上，BF 算法的时间复杂度很高，是 O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。 第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把 m 个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。 第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有 bug 也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。 所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。 RK (Rabin-Karp) 算法RK 算法的全称叫 Rabin-Karp 算法，是由它的两位发明者 Rabin 和 Karp 的名字来命名的。 RK 算法的思路是这样的：通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。 不过，通过哈希算法计算子串的哈希值的时候，需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。有没有方法可以提高哈希算法计算子串哈希值的效率呢？ 假设要匹配的字符串的字符集中只包含 K 个字符，我们可以用一个 K 进制数来表示一个子串，这个 K 进制数转化成十进制数，作为子串的哈希值。比如要处理的字符串只包含 a～z 这 26 个小写字母，那我们就用二十六进制来表示一个字符串。我们把 a～z 这 26 个字符映射到 0～25 这 26 个数字，a 就表示 0，b 就表示 1，以此类推，z 表示 25。在十进制的表示法中，一个数字的值是通过下面的方式计算出来的。对应到二十六进制，一个包含 a 到 z 这 26 个字符的字符串，计算哈希的时候，我们只需要把进位从 10 改成 26 就可以。 这种哈希算法有一个特点，在主串中，相邻两个子串的哈希值的计算公式有一定关系：相邻两个子串 s[i-1] 和 s[i]（i 表示子串在主串中的起始位置，子串的长度都为 m），对应的哈希值计算公式有交集，也就是说，我们可以使用 s[i-1] 的哈希值很快的计算出 s[i] 的哈希值。 不过，这里有一个小细节需要注意，那就是 26^(m-1) 这部分的计算，我们可以通过查表的方法来提高效率。我们事先计算好 26^0^、26^1^、26^2^……26^(m-1)，并且存储在一个长度为 m 的数组中，公式中的“次方”就对应数组的下标。当我们需要计算 26 的 x 次方的时候，就可以从数组的下标为 x 的位置取值，直接使用，省去了计算的时间。 整个 RK 算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是 O(n)。 模式串哈希值与每个子串哈希值之间的比较的时间复杂度是 O(1)，总共需要比较 n-m+1 个子串的哈希值，所以，这部分的时间复杂度也是 O(n)。所以，RK 算法整体的时间复杂度就是 O(n)。 这里还有一个问题就是，模式串很长，相应的主串中的子串也会很长，通过上面的哈希算法计算得到的哈希值就可能很大，如果超过了计算机中整型数据可以表示的范围，那该如何解决呢？比如将每一个字母从小到大对应一个素数，而不是 1，2，3……这样的自然数，这样冲突的概率就会降低一些。 RK 算法的时间复杂度是 O(n)，跟 BF 算法相比，效率提高了很多。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为 O(n*m)。 BM (Boyer-Moore)算法核心思想模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF 算法和 RK 算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。 在这个例子里，主串中的 c，在模式串中是不存在的，所以，模式串向后滑动的时候，只要 c 与模式串有重合，肯定无法匹配。所以，我们可以一次性把模式串往后多滑动几位，把模式串移动到 c 的后面。 BM 算法本质上其实就是在寻找这种规律。借助这种规律，在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位。 原理分析BM 算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）。 坏字符规则BM 算法的匹配顺序是按照模式串下标从大到小的顺序，倒着匹配的。 当发生不匹配的时候，把坏字符对应的模式串中的字符下标记作 si。如果坏字符在模式串中存在，把这个坏字符在模式串中的下标记作 xi。如果不存在，我们把 xi 记作 -1。那模式串往后移动的位数就等于 si-xi。（注意，我这里说的下标，都是字符在模式串的下标）。如果坏字符在模式串里多处出现，那我们在计算 xi 的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。 利用坏字符规则，BM 算法在最好情况下的时间复杂度非常低，是 O(n/m)。比如，主串是 aaabaaabaaabaaab，模式串是 aaaa。每次比对，模式串都可以直接后移四位，所以，匹配具有类似特点的模式串和主串的时候，BM 算法非常高效。 不过，单纯使用坏字符规则还是不够的。因为根据 si-xi 计算出来的移动位数，有可能是负数，比如主串是 aaaaaaaaaaaaaaaa，模式串是 baaa。不但不会向后滑动模式串，还有可能倒退。所以，BM 算法还需要用到“好后缀规则”。 好后缀规则依然是倒序匹配，我们把已经匹配的 bc 叫作好后缀，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。 如果在模式串中找不到另一个等于{u}的子串，我们不仅要看好后缀在模式串中，是否有另一个匹配的子串，我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。 我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的，假设是{v}，然后将模式串滑动到如图所示的位置。 当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？ 我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。 代码实现假设字符串的字符集不是很大，每个字符长度是 1 字节，我们用大小为 256 的数组，来记录每个字符在模式串中最后出现的位置。数组的下标对应字符的 ASCII 码值，数组中存储这个字符在模式串中出现的位置。 掌握了坏字符规则之后，先把 BM 算法代码的大框架写好，先不考虑好后缀规则，仅用坏字符规则，并且不考虑 si-xi 计算得到的移动位数可能会出现负数的情况。 public int bm(char[] a, int n, char[] b, int m) { int[] bc = new int[SIZE]; // 记录模式串中每个字符最后出现的位置 generateBC(b, m, bc); // 构建坏字符哈希表 int i = 0; // i 表示主串与模式串对齐的第一个字符 while (i &lt;= n - m) { int j; for (j = m - 1; j &gt;= 0; --j) { // 模式串从后往前匹配 if (a[i+j] != b[j]) break; // 坏字符对应模式串中的下标是 j } if (j &lt; 0) { return i; // 匹配成功，返回主串与模式串第一个匹配的字符的位置 } // 这里等同于将模式串往后滑动 j-bc[(int)a[i+j]] 位 i = i + (j - bc[(int)a[i+j]]); } return -1; } 因为好后缀也是模式串本身的后缀子串，所以，我们可以在模式串和主串正式匹配之前，通过预处理模式串，预先计算好模式串的每个后缀子串，对应的另一个可匹配子串的位置。 因为后缀子串的最后一个字符的位置是固定的，下标为 m-1，只需要记录长度就可以了。通过长度，可以确定一个唯一的后缀子串。 现在，要引入最关键的变量 suffix 数组。suffix 数组的下标 k，表示后缀子串的长度，下标对应的数组值存储的是，在模式串中跟好后缀{u}相匹配的子串{u*}的起始下标值。如，在模式串中和后一个b匹配的下标是2。如果模式串中有多个（大于 1 个）子串跟后缀子串{u}匹配，存储模式串中最靠后的那个子串的起始位置，也就是下标最大的那个子串的起始位置，避免模式串往后滑动得过头了。 // b 表示模式串，m 表示长度，suffix，prefix 数组事先申请好了 private void generateGS(char[] b, int m, int[] suffix, boolean[] prefix) { for (int i = 0; i &lt; m; ++i) { // 初始化 suffix[i] = -1; prefix[i] = false; } for (int i = 0; i &lt; m - 1; ++i) { // b[0, i] int j = i; int k = 0; // 公共后缀子串长度 while (j &gt;= 0 &amp;&amp; b[j] == b[m-1-k]) { // 与 b[0, m-1] 求公共后缀子串 --j; ++k; suffix[k] = j+1; //j+1 表示公共后缀子串在 b[0, i] 中的起始下标 } i if (j == -1) prefix[k] = true; // 如果公共后缀子串也是模式串的前缀子串 } } // a,b 表示主串和模式串；n，m 表示主串和模式串的长度。 public int bm(char[] a, int n, char[] b, int m) { int[] bc = new int[SIZE]; // 记录模式串中每个字符最后出现的位置 generateBC(b, m, bc); // 构建坏字符哈希表 int[] suffix = new int[m]; boolean[] prefix = new boolean[m]; generateGS(b, m, suffix, prefix); int i = 0; // j 表示主串与模式串匹配的第一个字符 while (i &lt;= n - m) { int j; for (j = m - 1; j &gt;= 0; --j) { // 模式串从后往前匹配 if (a[i+j] != b[j]) break; // 坏字符对应模式串中的下标是 j } if (j &lt; 0) { return i; // 匹配成功，返回主串与模式串第一个匹配的字符的位置 } int x = j - bc[(int)a[i+j]]; int y = 0; if (j &lt; m-1) { // 如果有好后缀的话 y = moveByGS(j, m, suffix, prefix); } i = i + Math.max(x, y); } return -1; } // j 表示坏字符对应的模式串中的字符下标 ; m 表示模式串长度 private int moveByGS(int j, int m, int[] suffix, boolean[] prefix) { int k = m - 1 - j; // 好后缀长度 if (suffix[k] != -1) return j - suffix[k] +1; for (int r = j+2; r &lt;= m-1; ++r) { if (prefix[m-r] == true) { return r; } } return m; } 性能分析及优化整个算法用到了额外的 3 个数组，其中 bc 数组的大小跟字符集大小有关，suffix 数组和 prefix 数组的大小跟模式串长度 m 有关。 如果我们处理字符集很大的字符串匹配问题，bc 数组对内存的消耗就会比较多。因为好后缀和坏字符规则是独立的，如果我们运行的环境对内存要求苛刻，可以只使用好后缀规则，不使用坏字符规则，这样就可以避免 bc 数组过多的内存消耗。不过，单纯使用好后缀规则的 BM 算法效率就会下降一些了。 BM 算法的时间复杂度分析起来是非常复杂，这篇论文“A new proof of the linearity of the Boyer-Moore string searching algorithm”证明了在最坏情况下，BM 算法的比较次数上限是 5n。这篇论文“Tight bounds on the complexity of the Boyer-Moore string matching algorithm”证明了在最坏情况下，BM 算法的比较次数上限是 3n。 BM 算法核心思想是，利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。BM 算法构建的规则有两类，坏字符规则和好后缀规则。好后缀规则可以独立于坏字符规则使用。因为坏字符规则的实现比较耗内存，为了节省内存，我们可以只用好后缀规则来实现 BM 算法。 KMP 算法基本原理KMP 算法是根据三位作者（D.E.Knuth，J.H.Morris 和 V.R.Pratt）的名字来命名的，算法的全称是 Knuth Morris Pratt 算法，简称为 KMP 算法。 在模式串和主串匹配的过程中，把不能匹配的那个字符仍然叫作坏字符，把已经匹配的那段字符串叫作好前缀。 看不懂！！！","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构29-动态规划入门","date":"2021-01-02T12:58:12.000Z","path":"2021/01/02/01-数据结构/数据结构30-动态规划入门案例.html","text":"[toc] 动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。 大部分动态规划能解决的问题，都可以通过回溯算法来解决，只不过回溯算法解决起来效率比较低，时间复杂度是指数级的。动态规划算法，在执行效率方面，要高很多。尽管执行效率提高了，但是动态规划的空间复杂度也提高了，所以，很多时候，我们会说，动态规划是一种空间换时间的算法思想。 正月点灯笼—动态规划 0-1 背包问题对于一组不同重量、不可分割的物品，我们需要选择一些装入背包，在满足背包最大重量限制的前提下，求背包中物品总重量的最大值。 把整个求解过程分为 n 个阶段，每个阶段会决策一个物品是否放到背包中。每个物品决策（放入或者不放入背包）完之后，背包中的物品的重量会有多种情况，也就是说，会达到多种不同的状态，对应到递归树中，就是有很多不同的节点。 把每一层重复的状态（节点）合并，只记录不同的状态，然后基于上一层的状态集合，来推导下一层的状态集合。我们可以通过合并每一层重复的状态，这样就保证每一层不同状态的个数都不会超过 w 个（w 表示背包的承载重量）。于是，我们就成功避免了每层状态个数的指数级增长。 我们用一个二维数组 states[n][w+1]，来记录每层可以达到的不同状态。 private int[] weight = {2，2，4，6，3}; // 物品重量 private int n = 5; // 物品个数 private int w = 9; // 背包承受的最大重量 第 0 个（下标从 0 开始编号）物品的重量是 2，要么装入背包，要么不装入背包，决策完之后，会对应背包的两种状态，背包中物品的总重量是 0 或者 2。我们用 states[0][0]=true 和 states[0][2]=true 来表示这两种状态。 第 1 个物品的重量也是 2，基于之前的背包状态，在这个物品决策完之后，不同的状态有 3 个，背包中物品总重量分别是 0(0+0)，2(0+2 or 2+0)，4(2+2)。我们用 states[1][0]=true，states[1][2]=true，states[1][4]=true 来表示这三种状态。 以此类推，直到考察完所有的物品后，整个 states 状态数组就都计算好了。我把整个计算的过程画了出来，你可以看看。图中 0 表示 false，1 表示 true。我们只需要在最后一层，找一个值为 true 的最接近 w（这里是 9）的值，就是背包中物品总重量的最大值。 weight: 物品重量，n: 物品个数，w: 背包可承载重量 public int knapsack(int[] weight, int n, int w) { boolean[][] states = new boolean[n][w+1]; // 默认值 false states[0][0] = true; // 第一行的数据要特殊处理，可以利用哨兵优化 states[0][weight[0]] = true; for (int i = 1; i &lt; n; ++i) { // 动态规划状态转移 for (int j = 0; j &lt;= w; ++j) {// 不把第 i 个物品放入背包 if (states[i-1][j] == true) states[i][j] = states[i-1][j]; } for (int j = 0; j &lt;= w-weight[i]; ++j) {// 把第 i 个物品放入背包 if (states[i-1][j]==true) states[i][j+weight[i]] = true; } } for (int i = w; i &gt;= 0; --i) { // 输出结果 if (states[n-1][i] == true) return i; } return 0; } 时间复杂度是 O(n*w)。n 表示物品个数，w 表示背包可以承载的总重量。用回溯算法解决这个问题的时间复杂度 O(2^n^)，是指数级的。 实际上，我们只需要一个大小为 w+1 的一维数组就可以解决这个问题。动态规划状态转移的过程，都可以基于这个一维数组来操作。 public static int knapsack2(int[] items, int n, int w) { boolean[] states = new boolean[w+1]; // 默认值 false states[0] = true; // 第一行的数据要特殊处理，可以利用哨兵优化 states[items[0]] = true; for (int i = 1; i &lt; n; ++i) { // 动态规划 for (int j = w-items[i]; j &gt;= 0; --j) {// 把第 i 个物品放入背包 if (states[j]==true) states[j+items[i]] = true; } } for (int i = w; i &gt;= 0; --i) { // 输出结果 if (states[i] == true) return i; } return 0; } 强调一下代码中的第 6 行，j 需要从大到小来处理。如果我们按照 j 从小到大处理的话，会出现 for 循环重复计算的问题。 0-1 背包问题升级版于一组不同重量、不同价值、不可分割的物品，我们选择将某些物品装入背包，在满足背包最大重量限制的前提下，求背包中可装入物品的最大总价值 回溯算法: private int maxV = Integer.MIN_VALUE; // 结果放到 maxV 中 private int[] items = {2，2，4，6，3}; // 物品的重量 private int[] value = {3，4，8，9，6}; // 物品的价值 private int n = 5; // 物品个数 private int w = 9; // 背包承受的最大重量 public void f(int i, int cw, int cv) { // 调用 f(0, 0, 0) if (cw == w || i == n) { // cw==w 表示装满了，i==n 表示物品都考察完了 if (cv &gt; maxV) maxV = cv; return; } f(i+1, cw, cv); // 选择不装第 i 个物品 if (cw + weight[i] &lt;= w) { f(i+1,cw+weight[i], cv+value[i]); // 选择装第 i 个物品 } } 动态规划: 用一个二维数组 states[n][w+1]，来记录每层可以达到的不同状态。不过这里数组存储的值不再是 boolean 类型的了，而是当前状态对应的最大总价值。我们把每一层中 (i, cw) 重复的状态（节点）合并，只记录 cv 值最大的那个状态，然后基于这些状态来推导下一层的状态。 public static int knapsack3(int[] weight, int[] value, int n, int w) { int[][] states = new int[n][w+1]; for (int i = 0; i &lt; n; ++i) { // 初始化 states for (int j = 0; j &lt; w+1; ++j) { states[i][j] = -1; } } states[0][0] = 0; states[0][weight[0]] = value[0]; for (int i = 1; i &lt; n; ++i) { // 动态规划，状态转移 for (int j = 0; j &lt;= w; ++j) { // 不选择第 i 个物品 if (states[i-1][j] &gt;= 0) states[i][j] = states[i-1][j]; } for (int j = 0; j &lt;= w-weight[i]; ++j) { // 选择第 i 个物品 if (states[i-1][j] &gt;= 0) { int v = states[i-1][j] + value[i]; if (v &gt; states[i][j+weight[i]]) { states[i][j+weight[i]] = v; } } } } // 找出最大值 int maxvalue = -1; for (int j = 0; j &lt;= w; ++j) { if (states[n-1][j] &gt; maxvalue) maxvalue = states[n-1][j]; } return maxvalue; } 时间复杂度是 O(n*w)，空间复杂度也是 O(n*w)。 满减淘宝的“双十一”购物节有各种促销活动，比如“满 200 元减 50 元”。假设你女朋友的购物车中有 n 个（n&gt;100）想买的商品，她希望从里面选几个，在凑够满减条件的前提下，让选出来的商品价格总和最大程度地接近满减条件（200 元），这样就可以极大限度地“薅羊毛”。 不过，这个问题不仅要求大于等于 200 的总价格中的最小的，我们还要找出这个最小总价格对应都要购买哪些商品。实际上，我们可以利用 states 数组，倒推出这个被选择的商品序列。 // items 商品价格，n 商品个数, w 表示满减条件，比如 200 public static void double11advance(int[] items, int n, int w) { boolean[][] states = new boolean[n][3*w+1];// 超过 3 倍就没有薅羊毛的价值了 states[0][0] = true; // 第一行的数据要特殊处理 states[0][items[0]] = true; for (int i = 1; i &lt; n; ++i) { // 动态规划 for (int j = 0; j &lt;= 3*w; ++j) {// 不购买第 i 个商品 if (states[i-1][j] == true) states[i][j] = states[i-1][j]; } for (int j = 0; j &lt;= 3*w-items[i]; ++j) {// 购买第 i 个商品 if (states[i-1][j]==true) states[i][j+items[i]] = true; } } int j; for (j = w; j &lt; 3*w+1; ++j) { if (states[n-1][j] == true) break; // 输出结果大于等于 w 的最小值 } if (j == 3*w+1) return; // 没有可行解 for (int i = n-1; i &gt;= 1; --i) { // i 表示二维数组中的行，j 表示列 if(j-items[i] &gt;= 0 &amp;&amp; states[i-1][j-items[i]] == true) { System.out.print(items[i] + &quot; &quot;); // 购买这个商品 j = j - items[i]; } // else 没有购买这个商品，j 不变。 } if (j != 0) System.out.print(items[0]); }","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构17-二叉树基础","date":"2020-12-26T03:19:17.000Z","path":"2020/12/26/01-数据结构/数据结构17-二叉树基础.html","text":"[toc] 二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？ 树（Tree）比如下面这幅图，A 节点就是 B 节点的父节点，B 节点是 A 节点的子节点。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫作根节点，也就是图中的节点 E。我们把没有子节点的节点叫作叶子节点或者叶节点，比如图中的 G、H、I、J、K、L 都是叶子节点。 节点的高度（Height）：节点到叶子节点的最长路径（边数） 节点的深度（Depth）：根节点到这个节点所经历的边的个数 节点的层（Level）：节点的深度+1 树的高度：根节点的高度 二叉树（Binary Tree）二叉树，顾名思义，每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子\\**节**\\点**。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。 编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。 编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作完全二叉树。 要理解完全二叉树定义的由来，我们需要先了解，如何表示（或者存储）一棵二叉树？ 想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。 我们先来看比较简单、直观的链式存储法。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。 我们再来看，基于数组的顺序存储法。我们把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置。如果节点 X 存储在数组中下标为 i 的位置，下标为 2 i 的位置存储的就是左子节点，下标为 2 i + 1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为 1 的位置），这样就可以通过下标计算，把整棵树都串起来。 刚刚举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为 0 的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。 所以，如果某棵二叉树是一棵==完全二叉树，那用数组存储无疑是最节省内存==的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。 当我们讲到堆和堆排序的时候，你会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。 二叉树的遍历如何将所有节点都遍历打印出来呢？经典的方法有三种，前序遍历、中序遍历和后序遍历。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。 实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。 void preOrder(Node* root) { if (root == null) return; print root // 此处为伪代码，表示打印 root 节点 preOrder(root-&gt;left); preOrder(root-&gt;right); } void inOrder(Node* root) { if (root == null) return; inOrder(root-&gt;left); print root // 此处为伪代码，表示打印 root 节点 inOrder(root-&gt;right); } void postOrder(Node* root) { if (root == null) return; postOrder(root-&gt;left); postOrder(root-&gt;right); print root // 此处为伪代码，表示打印 root 节点 } 从前、中、后序遍历的顺序图，可以看出来，每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数 n 成正比，也就是说二叉树遍历的时间复杂度是 O(n)。 二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。满二叉树又是完全二叉树的一种特殊情况。 二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。 二叉查找树（Binary Search Tree）二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。 这些都依赖于二叉查找树的特殊结构。二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。 二叉查找树的查找操作我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。 public class BinarySearchTree { private Node tree; public Node find(int data) { Node p = tree; while (p != null) { if (data &lt; p.data) p = p.left; else if (data &gt; p.data) p = p.right; else return p; } return null; } public static class Node { private int data; private Node left; private Node right; public Node(int data) { this.data = data; } } } 二叉查找树的插入操作二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。 如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。 public void insert(int data) { if (tree == null) { tree = new Node(data); return; } Node p = tree; while (p != null) { if (data &gt; p.data) { if (p.right == null) { p.right = new Node(data); return; } p = p.right; } else { // data &lt; p.data if (p.left == null) { p.left = new Node(data); return; } p = p.left; } } } 二叉查找树的删除操作二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了 。针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。 第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null。比如图中的删除节点 55。 第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点 13。 第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点 18。 public void delete(int data) { Node p = tree; // p 指向要删除的节点，初始化指向根节点 Node pp = null; // pp 记录的是 p 的父节点 while (p != null &amp;&amp; p.data != data) { pp = p; if (data &gt; p.data) p = p.right; else p = p.left; } if (p == null) return; // 没有找到 // 要删除的节点有两个子节点 if (p.left != null &amp;&amp; p.right != null) { // 查找右子树中最小节点 Node minP = p.right; Node minPP = p; // minPP 表示 minP 的父节点 while (minP.left != null) { minPP = minP; minP = minP.left; } p.data = minP.data; // 将 minP 的数据替换到 p 中 p = minP; // 下面就变成了删除 minP 了 pp = minPP; } // 删除节点是叶子节点或者仅有一个子节点 Node child; // p 的子节点 if (p.left != null) child = p.left; else if (p.right != null) child = p.right; else child = null; if (pp == null) tree = child; // 删除的是根节点 else if (pp.left == p) pp.left = child; else pp.right = child; } 实际上，关于二叉查找树的删除操作，还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为“已删除”，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。而且，这种处理方法也并没有增加插入、查找操作代码实现的难度。 二叉查找树的其他操作除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。 二叉查找树除了支持上面几个操作之外，还有一个重要的特性，就是中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。 二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是 O(logn)。 不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于 log2n 的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到 O(n)。 支持重复数据的二叉查找树前面讲二叉查找树的时候，我们默认树中节点存储的都是数字。很多时候，在实际的软件开发中，我们在二叉查找树中存储的，是一个包含很多字段的对象。我们利用对象的某个字段作为键值（key）来构建二叉查找树。我们把对象中的其他字段叫作卫星数据。 前面我们讲的二叉查找树的操作，针对的都是不存在键值相同的情况。那如果存储的两个对象键值相同，这种情况该怎么处理呢？我这里有两种解决方法。 第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。 第二种方法比较不好理解，不过更加优雅。 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。 当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。 对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。 二叉查找树的时间复杂度分析实际上，二叉查找树的形态各式各样。比如这个图中，对于同一组数据，我们构造了三种二叉查找树。它们的查找、插入、删除操作的执行效率都是不一样的。图中第一种二叉查找树，根节点的左右子树极度不平衡，已经退化成了链表，所以查找的时间复杂度就变成了 O(n)。 现在来分析一个最理想的情况，二叉查找树是一棵完全二叉树（或满二叉树），不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是 O(height)。 树的高度就等于最大层数减一，为了方便计算，我们转换成层来表示。从图中可以看出，包含 n 个节点的完全二叉树中，第一层包含 1 个节点，第二层包含 2 个节点，第三层包含 4 个节点，依次类推，下面一层节点个数是上一层的 2 倍，第 K 层包含的节点个数就是 2^(K-1)。 不过，对于完全二叉树来说，最后一层的节点个数有点儿不遵守上面的规律了。它包含的节点个数在 1 个到 2^(L-1) 个之间（我们假设最大层数是 L）。如果我们把每一层的节点个数加起来就是总的节点个数 n。也就是说，如果节点的个数是 n，那么 n 满足这样一个关系： n >= 1+2+4+8+...+2^{(L-2)}+1\\\\ n","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构16-哈希算法的应用","date":"2020-12-26T02:49:49.000Z","path":"2020/12/26/01-数据结构/数据结构16-哈希算法的应用.html","text":"[toc] 什么是hash不管是“散列”还是“哈希”，这都是中文翻译的差别，英文其实就是“Hash”。所以，我们常听到有人把“散列表”叫作“哈希表”“Hash 表”，把“哈希算法”叫作“Hash 算法”或者“散列算法” 哈希算法的定义和原理非常简单，基本上一句话就可以概括了。将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。 一个优秀hash算法的要求： 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）； 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同； 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小； 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。 例如： MD5(&quot; 我今天讲哈希算法！&quot;) = 425f0d5a917188d2c3c3dc85b5e4f2cb MD5(&quot; 我今天讲哈希算法 &quot;) = a1fb91ac128e6aa37fe42c663971ac3d 应用一：安全加密说到哈希算法的应用，最先想到的应该就是安全加密。最常用于加密的哈希算法是MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法）和SHA（Secure Hash Algorithm，安全散列算法）。除了这两个之外，当然还有很多其他加密算法，比如DES（Data Encryption Standard，数据加密标准）、AES（Advanced Encryption Standard，高级加密标准）。 前面讲到的哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。 第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。所以我着重讲一下第二点。实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说呢？ 这里就基于组合数学中一个非常基础的理论，鸽巢原理（也叫抽屉原理）。这个原理本身很简单，它是说，如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个，换句话说就是，肯定有 2 只鸽子在 1 个鸽巢内。 有了鸽巢原理的铺垫之后，我们再来看，为什么哈希算法无法做到零冲突？ 哈希算法产生的哈希值的长度是固定且有限的。比如前面举的 MD5 的例子，哈希值是固定的 128 位二进制串，能表示的数据是有限的，最多能表示 2^128 个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对 2^128+1 个数据求哈希值，就必然会存在哈希值相同的情况。一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。 这两段字符串经过 MD5 哈希算法加密之后，产生的哈希值是相同的。 应用二：唯一标识如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那该如何搜索呢？ 可以给每一个图片取一个唯一标识，或者说信息摘要。比如，从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。 如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。 如果不存在，那就说明这个图片不在图库中；如果存在，再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。 应用三：数据校验BT 下载的原理是基于 P2P 协议的。我们从多个机器上并行下载一个 2GB 的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每块大约 20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。 但网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？ 具体的 BT 协议很复杂，校验方法也有很多，来理解下其中的一种思路。 通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。 应用四：散列函数散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，都可以通过开放寻址法或者链表法解决。 不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。 字典攻击如果用户信息被“脱库”，黑客虽然拿到是加密之后的密文，但可以通过“猜”的方式来破解密码，这是因为，有些用户的密码太简单。比如很多人习惯用 00000、123456 这样的简单数字组合做密码，很容易就被猜中。 那就需要维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。如果相同，基本上就可以认为，这个加密之后的密码对应的明文就是字典中的这个密码。（注意，这里说是的是“基本上可以认为”，因为根据前面的学习，哈希算法存在散列冲突，也有可能出现，尽管密文一样，但是明文并不一样的情况。） 针对字典攻击，可以引入一个盐（salt），跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。不过我这里想多说一句，我认为安全和攻击是一种博弈关系，不存在绝对的安全。所有的安全措施，只是增加攻击的成本而已。 应用五：负载均衡负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。 最直接的方法就是，维护一张映射关系表，这张表的内容是客户端 IP 地址或者会话 ID 与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端： 如果客户端很多，映射表可能会很大，比较浪费内存空间； 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大； 如果借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。 应用六：数据分片统计“搜索关键词”出现的次数假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？ 我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。 针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用 n 台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。 这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。 实际上，这里的处理过程也是 MapReduce 的基本设计思想。 快速判断图片是否在图库中如何快速判断图片是否在图库中？采用应用二：唯一标识，给每个图片取唯一标识（或者信息摘要），然后构建散列表。 假设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。 我们同样可以对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。 当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。 现在，我们来估算一下，给这 1 亿张图片构建散列表大约需要多少台机器。 散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设我们通过 MD5 来计算哈希值，那长度就是 128 比特，也就是 16 字节。文件路径长度的上限是 256 字节，我们可以假设平均长度是 128 字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用 8 字节。所以，散列表中每个数据单元就占用 152 字节（这里只是估算，并不准确）。 假设一台机器的内存大小为 2GB，散列表的装载因子为 0.75，那一台机器可以给大约 1000 万（2GB*0.75/152）张图片构建散列表。所以，如果要对 1 亿张图片构建索引，需要大约十几台机器。在工程中，这种估算还是很重要的，能让我们事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性。 实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。 应用七：分布式存储现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。 该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。 但是，如果数据增多，原来的 10 个机器已经无法承受了，我们就需要扩容了，比如扩到 11 个机器，这时候麻烦就来了。因为，这里并不是简单地加个机器就可以了。 原来的数据是通过与 10 来取模的。比如 13 这个数据，存储在编号为 3 这台机器上。但是新加了一台机器中，我们对数据按照 11 取模，原来 13 这个数据就被分配到 2 号这台机器上了。 因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。 所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了。 假设我们有 k 个机器，数据的哈希值的范围是 [0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。 一致性哈希算法的基本思想就是这么简单。除此之外，它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来。这里我就不展开讲了，如果感兴趣，你可以看下这个介绍。 除了我们上面讲到的分布式缓存，实际上，一致性哈希算法的应用非常广泛，在很多分布式存储系统中，都可以见到一致性哈希算法的影子。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构15-散列表","date":"2020-12-23T13:44:13.000Z","path":"2020/12/23/01-数据结构/数据结构15-散列表.html","text":"[toc] 散列思想散列表的英文叫“Hash Table”，我们平时也叫它“哈希表”或者“Hash 表”。 散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。 假如有 89 名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。编号用 6 位数字来表示。比如 051167，其中，前两位 05 表示年级，中间两位 11 表示班级，最后两位是 1 到 89。可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。这就是典型的散列思想。其中，参赛选手的编号我们叫作键（key）或者关键字。用它来标识一个选手。把参赛编号转化为数组下标的映射方法就叫作散列函数（或“Hash 函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash 值”“哈希值”）。 通过这个例子，可以总结出这样的规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当按照键值查询元素时，用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。 散列函数散列函数，顾名思义，它是一个函数。可以把它定义成hash(key)，其中 key 表示元素的键值，hash(key) 的值表示经过散列函数计算得到的散列值。 三点散列函数设计的基本要求： 散列函数计算得到的散列值是一个非负整数； 如果 key1 = key2，那 hash(key1) == hash(key2)； 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。 在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。 所以几乎无法找到一个完美的无冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，需要通过其他途径来解决。 如何设计散列函数？ 散列函数的设计不能太复杂，过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能 散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。 实际工作中，还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。 散列冲突再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。 比如，Java 中 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。 开放寻址法开放寻址法的核心思想是，如果出现了散列冲突，就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？比较经典的探测方法有：线性探测（Linear Probing）、二次探测（Quadratic probing）和双重散列（Double hashing）。 优点：开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。 缺点：用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。 线性探测： 往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。 在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。 对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。不能单纯地把要删除的元素设置为空。 在查找的时候，一旦通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。这个问题如何解决呢？ 可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。 线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。 二次探测： 所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2^，hash(key)+2^2^…… 双重散列： 所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。 不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，要尽可能保证散列表中有一定比例的空闲槽位，用装载因子（load factor）来表示空位的多少。 散列表的装载因子 = 填入表中的元素个数 / 散列表的长度 装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。 链表法链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。图中，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素都放到相同槽位对应的链表中。 优点：链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。 优点：链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。 实际上，对链表法稍加改造，可以实现一个更加高效的散列表。那就是，将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是 O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。 在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从 O(1) 急剧退化为 O(n)。 如果散列表中有 10 万个数据，退化后的散列表查询的效率就下降了 10 万倍。更直接点说，如果之前运行 100 次查询只需要 0.1 秒，那现在就需要 1 万秒。这样就有可能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就是散列表碰撞攻击的基本原理。 当插入的时候，只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。 那查找或删除操作的时间复杂度是多少呢？实际上，这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。 装载因子过大了怎么办？针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是 0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了 0.4。 针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。 如图，在原来的散列表中，21 这个元素原来存储在下标为 0 的位置，搬移到新的散列表中，存储在下标为 7 的位置。 插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。 实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果对空间消耗非常敏感，可以在装载因子小于某个值之后，启动动态缩容。当然，如果更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。 装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。 为了解决一次性扩容耗时过多的情况，可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，只申请新空间，但并不将老的数据搬移到新散列表中。 当有新数据要插入时，将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。 对于查询操作，为了兼容了新、老散列表中的数据，先从新散列表中查找，如果没有找到，再去老的散列表中查找。通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。 工业级散列表举例分析Java 中的 HashMap 这样一个工业级的散列表。 要求： 支持快速的查询、插入、删除操作； 内存占用合理，不能浪费过多的内存空间； 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。 设计一个合适的散列函数； 定义装载因子阈值，并且设计动态扩容策略； 选择合适的散列冲突解决方法。 初始大小HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 HashMap 的性能。 装载因子和动态扩容最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。 散列冲突解决方法HashMap 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能。 于是，在 JDK1.8 版本中，为了对 HashMap 做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。 散列函数散列函数的设计并不复杂，追求的是简单高效、分布均匀。 int hash(Object key) { int h = key.hashCode()； return (h ^ (h &gt;&gt;&gt; 16)) &amp; (capitity -1); //capicity 表示散列表的大小 } // 其中，hashCode() 返回的是 Java 对象的 hash code。比如 String 类型的对象的 hashCode() 就是下面这样： public int hashCode() { int var1 = this.hash; if(var1 == 0 &amp;&amp; this.value.length &gt; 0) { char[] var2 = this.value; for(int var3 = 0; var3 &lt; this.value.length; ++var3) { var1 = 31 * var1 + var2[var3]; } this.hash = var1; } return var1; } LRU 缓存淘汰算法缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 CPU 缓存、数据库缓存、浏览器缓存等等。 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）。 需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。 当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰算法的时间复杂很高，是 O(n)。 实际上，一个缓存（cache）系统主要包含下面这几个操作： 往缓存中添加一个数据； 从缓存中删除一个数据； 在缓存中查找一个数据。 这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是 O(n)。如果将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到 O(1)。 链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。因为散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。 如何查找一个数据。散列表中查找数据的时间复杂度接近 O(1)，所以通过散列表，可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。 如何删除一个数据。需要找到数据所在的结点，然后将结点删除。借助散列表，可以在 O(1) 时间复杂度里找到要删除的结点。因为链表是双向链表，双向链表可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要 O(1) 的时间复杂度。 如何添加一个数据。添加数据到缓存稍微有点麻烦，需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。 这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在 O(1) 的时间复杂度内完成。所以，这三个操作的时间复杂度都是 O(1)。至此，通过散列表和双向链表的组合使用，实现了一个高效的、支持 LRU 缓存淘汰算法的缓存系统原型。 散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那需要将散列表中的数据拷贝到数组中，然后排序，再遍历。 因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构14-跳表","date":"2020-11-27T09:37:28.000Z","path":"2020/11/27/01-数据结构/数据结构14-跳表.html","text":"[toc] 跳表（Skip list）二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，只需要对链表稍加改造，就可以支持类似“二分”的查找算法。把改造之后的数据结构叫作跳表（Skip list），这是一种各方面性能都比较优秀的动态数据结构，可以支持快速的插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。 对于一个单链表来讲，即便链表中存储的数据是有序的，如果要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是 O(n)。 对链表建立一级“索引”，每两个结点提取一个结点到上一级，把抽出来的那一级叫作索引或索引层。图中的 down 表示 down 指针，指向下一级结点。 如果现在要查找某个结点，比如 16。可以先在索引层遍历，当遍历到索引层中值为 13 的结点时，发现下一个结点是 17，那要查找的结点 16 肯定就在这两个结点之间。然后通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历 2 个结点，就可以找到值等于 16 的这个结点了。这样，原来如果要查找 16，需要遍历 10 个结点，现在只需要遍历 7 个结点。加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了 跟前面建立第一级索引的方式相似，在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在再来查找 16，只需要遍历 6 个结点了，需要遍历的结点数量又减少了。 从下图中可以看出，原来没有索引的时候，查找 62 需要遍历 62 个结点，现在只需要遍历 11 个结点，速度是不是提高了很多？所以，当链表的长度 n 比较大时，比如 1000、10000 的时候，在构建索引之后，查找效率的提升就会非常明显。 这种链表加多级索引的结构，就是跳表。 复杂度分析每两个结点会抽出一个结点作为上一级索引的结点，最后一层索引2个节点，索引有h层，那么n/(2^h)=2，$h=log_2n-1$，如果包含原始链表这一层，整个跳表的高度就是 log~2~n。如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*(h+1))=O(m*logn)，其中m=3，所以在跳表中查询任意数据的时间复杂度就是 O(logn)。 假设要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。 比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。如果将包含 n 个结点的单链表构造成跳表，我们需要额外再用接近 $\\frac{n}{2}+\\frac{n}{4}+\\frac{n}{8}+….+4+2=n-2$个结点的存储空间。如果每三个结点或五个结点，抽一个结点到上级索引，n/3+n/9+n/27+…+9+3+1=n/2。尽管空间复杂度还是 O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。 实际上，在软件开发中，不必太在意索引占用的额外空间。在讲数据结构和算法时，习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。 插入和删除对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，查找某个结点的的时间复杂度是 O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。 如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。 当不停地往跳表中插入数据时，如果不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。 作为一种动态数据结构，需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。 红黑树、AVL 树这样平衡二叉树是通过左右旋的方式保持左右子树的大小平衡，而跳表是通过随机函数来维护前面提到的“平衡性”。 当往跳表中插入数据的时候，可以选择同时将这个数据插入到部分索引层中。通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那就将这个结点添加到第一级到第 K 级这 K 级索引中。 随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构13-二分查找","date":"2020-11-27T07:41:26.000Z","path":"2020/11/27/01-数据结构/数据结构13-二分查找.html","text":"[toc] 二分查找（Binary Search）二分查找（Binary Search）算法，也叫折半查找算法。 二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。 假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。其中 n/2^k^=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度就是 O(k)。通过 n/2^k^=1，我们可以求得 k=log~2~n，所以时间复杂度就是 O(logn)。 对数时间复杂度是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 O(1) 的算法还要高效。 二分查找的递归与非递归实现最简单的情况就是有序数组中不存在重复元素，在其中用二分查找值等于给定值的数据。 public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &lt;= high) { int mid = (low + high) / 2; if (a[mid] == value) { return mid; } else if (a[mid] &lt; value) { low = mid + 1; } else { high = mid - 1; } } return -1; } 注意是 low&lt;=high，而不是 low&lt;high。 实际上，mid=(low+high)/2 这种写法是有问题的。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以 2 操作转化成位运算 low+((high-low)&gt;&gt;1)。因为相比除法运算来说，计算机处理位运算要快得多。 low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。比如，当 high=3，low=3 时，如果 a[3] 不等于 value，就会导致一直循环不退出。 // 二分查找的递归实现 public int bsearch(int[] a, int n, int val) { return bsearchInternally(a, 0, n - 1, val); } private int bsearchInternally(int[] a, int low, int high, int value) { if (low &gt; high) return -1; int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] == value) { return mid; } else if (a[mid] &lt; value) { return bsearchInternally(a, mid+1, high, value); } else { return bsearchInternally(a, low, mid-1, value); } } 首先，二分查找依赖的是顺序表结构，简单点说就是数组。 其次，二分查找针对的是有序数据。 再次，数据量太小不适合二分查找。不过，如果数据之间的比较操作非常耗时，不管数据量大小，都推荐使用二分查找。比如，数组中存储的都是长度超过 300 的字符串，如此长的两个字符串之间比对大小，就会非常耗时。需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。 最后，数据量太大也不适合二分查找。二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。 二分查找的变形问题 变体一：查找第一个值等于给定值的元素public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &lt;= high) { int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt;= value) { // ? high = mid - 1; } else { low = mid + 1; } } if (low &lt; n &amp;&amp; a[low]==value) return low; else return -1; } public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &lt;= high) { int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt; value) { high = mid - 1; } else if (a[mid] &lt; value) { low = mid + 1; } else { if ((mid == 0) || (a[mid - 1] != value)) return mid; else high = mid - 1; } } return -1; } 变体二：查找最后一个值等于给定值的元素public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &lt;= high) { int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt; value) { high = mid - 1; } else if (a[mid] &lt; value) { low = mid + 1; } else { if ((mid == n - 1) || (a[mid + 1] != value)) return mid; else low = mid + 1; } } return -1; } 变体三：查找第一个大于等于给定值的元素public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &lt;= high) { int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt;= value) { if ((mid == 0) || (a[mid - 1] &lt; value)) return mid; else high = mid - 1; } else { low = mid + 1; } } return -1; } 变体四：查找最后一个小于等于给定值的元素public int bsearch7(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &lt;= high) { int mid = low + ((high - low) &gt;&gt; 1); if (a[mid] &gt; value) { high = mid - 1; } else { if ((mid == n - 1) || (a[mid + 1] &gt; value)) return mid; else low = mid + 1; } } return -1; }","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构12-排序之优化","date":"2020-11-26T13:51:30.000Z","path":"2020/11/26/01-数据结构/数据结构12-排序之优化.html","text":"[toc] 如何选择合适的排序算法？ 线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。 时间复杂度是 O(nlogn) 的排序算法不止一个，已经讲过的有归并排序、快速排序，后面讲堆的时候还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。 使用归并排序的情况其实并不多，因为归并排序并不是原地排序算法，空间复杂度是 O(n)。 如何优化快速排序？最坏情况下快速排序的时间复杂度是 O(n^2^)，出现的主要原因是因为分区点选的不够合理。 最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。 三数取中法从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。 随机法随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的 O(n2) 的情况，出现的可能性不大。 举例分析排序函数O(n^2^) 时间复杂度的算法并不一定比 O(nlogn) 的算法执行时间长 O(nlogn) 在没有省略低阶、系数、常数之前可能是 O(knlogn + c)，而且 k 和 c 有可能还是一个比较大的数。假设 k=1000，c=200，当我们对小规模数据（比如 n=100）排序时，n2的值实际上比 knlogn+c 还要小。 knlogn+c = 1000 100 log100 + 200 远大于 10000 n^2 = 100*100 = 10000 对于小规模数据的排序，O(n2) 的排序算法并不一定比 O(nlogn) 排序算法执行的时间长。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构10-线性排序(桶排序、计数排序、基数排序)","date":"2020-11-26T12:56:16.000Z","path":"2020/11/26/01-数据结构/数据结构11-排序之线性排序(桶排序、计数排序、基数排序).html","text":"[toc] 三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。 这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以重点的是掌握这些排序算法的适用场景。 桶排序（Bucket sort）核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。 如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k logk)。m 个桶排序的时间复杂度就是 O(m k logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(nlog(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。 实际上，桶排序对要排序数据的要求是非常苛刻的。 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。 计数排序（Counting sort）计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。 假设对50W个高考分数排序，考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以我们可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，我们将这 50 万考生划分到这 901 个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。 假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 A[8] 中，它们分别是：2，5，3，0，2，3，0，3。期望输出R[8]={0,0,2,2,3,3,3,5} 考生的成绩从 0 到 5 分，我们使用大小为 6 的数组 C[6] 表示桶，其中下标对应分数。不过，C[6] 内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到 C[6] 的值。 如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？ 对 C[6] 数组顺序求和，C[6] 存储的数据就变成C[6]={2,2,4,7,7,8}。C[k] 里存储==小于等于==分数 k 的考生个数。 从后到前依次扫描数组 A。比如，当扫描到 3 时，我们可以从数组 C 中取出下标为 3 的值 7，也就是说，到目前为止，包括自己在内，分数小于等于 3 的考生有 7 个，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 的元素就只剩下了 6 个了，所以相应的 C[3] 要减 1，变成 6。 以此类推，当我们扫描到第 2 个分数为 3 的考生的时候，就会把它放入数组 R 中的第 6 个元素的位置（也就是下标为 5 的位置）。当我们扫描完整个数组 A 后，数组 R 内的数据就是按照分数从小到大有序排列的了。 // 计数排序，a 是数组，n 是数组大小。假设数组中存储的都是非负整数。 public void countingSort(int[] a, int n) { if (n &lt;= 1) return; // 查找数组中数据的范围 int max = a[0]; for (int i = 1; i &lt; n; ++i) { if (max &lt; a[i]) { max = a[i]; } } int[] c = new int[max + 1]; // 申请一个计数数组 c，下标大小 [0,max] for (int i = 0; i &lt;= max; ++i) { c[i] = 0; } // 计算每个元素的个数，放入 c 中 for (int i = 0; i &lt; n; ++i) { c[a[i]]++; } // 依次累加 for (int i = 1; i &lt;= max; ++i) { c[i] = c[i-1] + c[i]; } // 临时数组 r，存储排序之后的结果 int[] r = new int[n]; // 计算排序的关键步骤，有点难理解 for (int i = n - 1; i &gt;= 0; --i) { int index = c[a[i]]-1; r[index] = a[i]; c[a[i]]--; } // 将结果拷贝给 a 数组 for (int i = 0; i &lt; n; ++i) { a[i] = r[i]; } } 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。 比如，还是拿考生这个例子。如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以 10，转化成整数，然后再放到 9010 个桶内。再比如，如果要排序的数据中有负数，数据的范围是 [-1000, 1000]，那我们就需要先对每个数据都加 1000，转化成非负整数。 基数排序（Radix sort）先按照最后一位来排序，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序 注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。因为如果是非稳定排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，那么低位的排序就完全没有意义了。 根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k*n)。当 k 不大的时候，所以基数排序的时间复杂度就近似于 O(n)。 实际上，有时候要排序的数据并不都是等长的，比如我们排序牛津字典中的 20 万个英文单词，最短的只有 1 个字母，最长的我特意去查了下，有 45 个字母，中文翻译是尘肺病。对于这种不等长的数据，基数排序还适用吗？实际上，我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”，因为根据ASCII 值，所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序。这样就可以继续用基数排序了。 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。 桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构10-排序之归并&快排","date":"2020-11-26T08:33:55.000Z","path":"2020/11/26/01-数据结构/数据结构10-排序之归并&快排.html","text":"[toc] 归并排序和快速排序都用到了分治思想，非常巧妙。这两种排序算法适合大规模的数据排序。 归并排序（Merge Sort）如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。 归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。 public static int[] sort(int[] a,int low,int high){ int mid = (low+high)/2; if(low&lt;high){ sort(a,low,mid); sort(a,mid+1,high); //左右归并 merge(a,low,mid,high); } return a; } public static void merge(int[] a, int low, int mid, int high) { int[] temp = new int[high-low+1]; int i= low; int j = mid+1; int k=0; // 把较小的数先移到新数组中 while(i&lt;=mid &amp;&amp; j&lt;=high){ if(a[i]&lt;a[j]){ temp[k++] = a[i++]; }else{ temp[k++] = a[j++]; } } // 把左边剩余的数移入数组 while(i&lt;=mid){ temp[k++] = a[i++]; } // 把右边边剩余的数移入数组 while(j&lt;=high){ temp[k++] = a[j++]; } // 把新数组中的数覆盖nums数组 for(int x=0;x&lt;temp.length;x++){ a[x+low] = temp[x]; } } 第一，归并排序是稳定的排序算法吗？ 归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。在合并的过程中，如果 A[p…q] 和 A[q+1…r] 之间有值相同的元素，那我们可以像伪代码中那样，先把 A[p…q] 中的元素放入 tmp 数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。 第二，归并排序的时间复杂度是多少？ 递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。 假设对 n 个元素进行归并排序需要的时间是 T(n)，那分解成两个子数组排序的时间都是 T(n/2)。我们知道，merge() 函数合并两个有序子数组的时间复杂度是 O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是： T(1) = C； n=1 时，只需要常量级的执行时间，所以表示为 C。 T(n) = 2*T(n/2) + n； n&gt;1 得到 T(n) = 2^k^T(n/2^k^)+k\\n。当 T(n/2^k^)=T(1) 时，也就是 n/2^k^=1，我们得到 k=log~2~n 。我们将 k 值代入上面的公式，得到 T(n)=C*n+n*log~2~n 。如果我们用大 O 标记法来表示的话，T(n) 就等于 O(nlogn)。所以归并排序的时间复杂度是 O(nlogn)。 ==归并排序的执行效率与要排序的原始数组的有序程度无关==，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。 第三，归并排序的空间复杂度是多少？ ==归并排序不是原地排序算法==。这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。空间复杂度是 O(n) 快速排序（Quicksort）快速排序（Quicksort）是对冒泡排序的一种改进。 如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。 根据分治、递归的处理思想，可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。 如果希望快排是原地排序算法，那它的空间复杂度得是 O(1)，那 partition() 分区函数就不能占用太多额外的内存空间，就需要在 A[p…r] 的原地完成分区操作。 partition(A, p, r) { pivot := A[r] i := p for j := p to r-1 do { if A[j] &lt; pivot { swap A[i] with A[j] i := i+1 } } swap A[i] with A[r] return i 因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列 6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个 6 的相对先后顺序就会改变。所以，快速排序并不是一个稳定的排序算法。 import java.util.Arrays; public class QuickSort { //三数取中法。取出不大不小的那个位置 public static int getPivotPos(int[] a,int low,int high) { int mid=(low+high)/2; int pos=low; if(a[mid]&lt;a[low]) { int temp=a[low]; a[low]=a[mid]; a[mid]=temp; } if(a[high]&lt;a[low]) { int temp=a[high]; a[high]=a[low]; a[low]=temp; } if(a[high]&lt;a[mid]) { int temp=a[high]; a[high]=a[mid]; a[mid]=temp; } pos=mid; return pos; } //划分，取出枢纽位置 public static int partition(int[] a,int low,int high) { int pivotpos=getPivotPos(a,low,high); int pivot=a[pivotpos]; int temp=pivot; a[pivotpos]=a[low]; a[low]=temp; while(low&lt;high) { while(low&lt;high&amp;&amp;a[high]&gt;=pivot) high--; a[low]=a[high]; while(low&lt;high&amp;&amp;a[low]&lt;=pivot) low++; a[high]=a[low]; } a[low]=pivot; return low; } //快排 public static void quickSort(int[] a,int low,int high) { if(low&lt;high) { int pivotpos=partition(a,low,high); quickSort(a,low,pivotpos-1); quickSort(a,pivotpos+1,high); } } //重载快排 public static void quickSort(int[] a) { if(a.length==0) return; int low=0; int high=a.length-1; quickSort(a,low,high); } public static void main(String[] args) { int[] a= {12,32,24,99,54,76,48}; quickSort(a); System.out.println(Arrays.toString(a)); } } 快排是一种原地、不稳定的排序算法。快排的时间复杂度也是 O(nlogn)。举一个比较极端的例子。如果数组中的数据原来已经是有序的了，比如 1，3，5，6，8。如果每次选择最后一个元素作为 pivot，那每次分区得到的两个区间都是不均等的。需要进行大约 n 次分区操作，才能完成快排的整个过程。每次分区平均要扫描大约 n/2 个元素，这种情况下，快排的时间复杂度就从 O(nlogn) 退化成了 O(n2)。在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)。 区别 归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构09-排序之冒泡&插入&选择","date":"2020-11-26T07:48:56.000Z","path":"2020/11/26/01-数据结构/数据结构09-排序之冒泡&插入&选择.html","text":"[toc] 排序冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序. 展示动态排序网站：https://visualgo.net/zh 冒泡排序（Bubble Sort）冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。 实际上，刚讲的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。我这里还有另外一个例子，这里面给 6 个元素排序，只需要 4 次冒泡操作就可以了。 // 冒泡排序，a 表示数组，n 表示数组大小 public void bubbleSort(int[] a, int n) { if (n &lt;= 1) return; for (int i = 0; i &lt; n; ++i) { // 提前退出冒泡循环的标志位 boolean flag = false; for (int j = 0; j &lt; n - i - 1; ++j) { if (a[j] &gt; a[j+1]) { // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true; // 表示有数据交换 } } if (!flag) break; // 没有数据交换，提前退出 } } 第一，冒泡排序是原地排序算法吗？ 冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。 第二，冒泡排序是稳定的排序算法吗？ 在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。 第三，冒泡排序的时间复杂度是多少？ 最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以==最好情况时间复杂度是 O(n)==。 而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以==最坏情况时间复杂度为 O(n^2^)==。 对于包含 n 个数据的数组，这 n 个数据就有 n! 种排列方式。不同的排列方式，冒泡排序执行的时间肯定是不同的。比如我们前面举的那两个例子，其中一个要进行 6 次冒泡，而另一个只需要 4 次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我这里还有一种思路，通过“有序度”和“逆序度”这两个概念来进行分析。有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：有序元素对：a[i] &lt;= a[j], 如果 i &lt; j。同理，对于一个倒序排列的数组，比如 6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是n*(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作满有序度。逆序度的定义正好跟有序度相反（默认从小到大为有序）：逆序元素对：a[i] &gt; a[j], 如果 i &lt; j。逆序度 = 满有序度 - 有序度。排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的。对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 n*(n-1)/2 次交换。最好情况下，初始状态的有序度是 n*(n-1)/2，就不需要进行交换。我们可以取个中间值 n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。换句话说，平均情况下，需要 n*(n-1)/4 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n2)，所以==平均情况下的时间复杂度就是 O(n^2^)==。 这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。等我们讲到快排的时候，我还会再次用这种“不严格”的方法来分析平均时间复杂度。 插入排序（Insertion Sort）一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，只要遍历数组，找到数据应该插入的位置将其插入即可。 首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。 如图所示，要排序的数据是 4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。 插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。 对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。 // 插入排序，a 表示数组，n 表示数组大小 public void insertionSort(int[] a, int n) { if (n &lt;= 1) return; for (int i = 1; i &lt; n; ++i) { int value = a[i]; int j = i - 1; // 查找插入的位置 for (; j &gt;= 0; --j) { if (a[j] &gt; value) { a[j+1] = a[j]; // 数据移动 } else { break; } } a[j+1] = value; // 插入数据 } } 第一，插入排序是原地排序算法吗？ 从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。 第二，插入排序是\\**稳定**\\的排序算法吗？** 在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。 第三，插入排序的时间复杂度是多少？ 如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，==最好是时间复杂度为 O(n)==。注意，这里是从尾到头遍历已经有序的数据。 如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以==最坏情况时间复杂度为 O(n^2^)==。 还记得在数组中插入一个数据的平均时间复杂度是多少吗？没错，是 O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以==平均时间复杂度为 O(n^2^)==。 选择排序（Selection Sort）选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。 首先，选择排序空间复杂度为 O(1)，==是一种原地排序==算法。 选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度==都为 O(n^2^)==。 选择排序是一种==不稳定==的排序算法。比如 5，8，5，2，9 这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素 2，与第一个 5 交换位置，那第一个 5 和中间的 5 顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。 为什么插入排序要比冒泡排序更受欢迎呢？冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。 但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要 3 个赋值操作，而插入排序只需要 1 个： 冒泡排序中数据的交换操作： if (a[j] &gt; a[j+1]) { // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true; } 插入排序中数据的移动操作： if (a[j] &gt; value) { a[j+1] = a[j]; // 数据移动 } else { break; } 所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是 O(n^2^)，但是如果希望把性能优化做到极致，那肯定首选插入排序。插入排序的算法思路也有很大的优化空间，","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构08-递归","date":"2020-11-26T07:20:44.000Z","path":"2020/11/26/01-数据结构/数据结构08-递归.html","text":"[toc] 递归递归是一种应用非常广泛的算法（或者编程技巧）。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。 递归需要满足的三个条件： 一个问题的解可以分解为几个子问题的解 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件 写递归代码最关键的是写出递推公式，找到终止条件. 在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销， 举例假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？如果有 7 个台阶，你可以 2，2，2，1 这样子上去，也可以 1，2，1，1，2 这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？ 可以根据第一步的走法把所有走法分为两类，第一类是第一步走了 1 个台阶，另一类是第一步走了 2 个台阶。所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 加上先走 2 阶后，n-2 个台阶的走法。用公式表示就是：f(n) = f(n-1)+f(n-2) 来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法,所以 f(1)=1；有两个台阶时，只有两种走法，（一次走两步或者两次一步）所以 f(2)=2。 int f(int n) { if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2); } 计算机擅长做重复的事情，所以递归正和它的胃口。而我们人脑更喜欢平铺直叙的思维方式。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。人脑几乎没办法把整个“递”和“归”的过程一步一步都想清楚。 因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。 递归代码要警惕堆栈溢出函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。 可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如 1000）之后，我们就不继续往下再递归了，直接返回报错。 递归代码要警惕重复计算 为了避免重复计算，可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免。 public int f(int n) { if (n == 1) return 1; if (n == 2) return 2; // hasSolvedList 可以理解成一个 Map，key 是 n，value 是 f(n) if (hasSolvedList.containsKey(n)) { return hasSovledList.get(n); } int ret = f(n-1) + f(n-2); hasSovledList.put(n, ret); return ret; } 递归代码改写为非递归代码对 f(x) =f(x-1)+1 这个递推公式： int f(int n) { int ret = 1; for (int i = 2; i &lt;= n; ++i) { ret = ret + 1; } return ret; } 针对，上面举例： int f(int n) { if (n == 1) return 1; if (n == 2) return 2; int ret = 0; int pre = 2; int prepre = 1; for (int i = 3; i &lt;= n; ++i) { ret = pre + prepre; prepre = pre; pre = ret; } return ret; } 笼统地讲，所有的递归代码都可以改为这种迭代循环的非递归写法。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构07-队列","date":"2020-11-25T09:36:21.000Z","path":"2020/11/25/01-数据结构/数据结构07-队列.html","text":"[toc] 队列队列跟栈一样，也是一种操作受限的线性表数据结构。先进者先出 队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。 实现用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。 // 用数组实现的队列 public class ArrayQueue { // 数组：items，数组大小：n private String[] items; private int n = 0; // head 表示队头下标，tail 表示队尾下标 private int head = 0; private int tail = 0; // 申请一个大小为 capacity 的数组 public ArrayQueue(int capacity) { items = new String[capacity]; n = capacity; } // 入队操作，将 item 放入队尾 public boolean enqueue(String item) { // tail == n 表示队列末尾没有空间了 if (tail == n) { // tail ==n &amp;&amp; head==0，表示整个队列都占满了 if (head == 0) return false; // 数据搬移,从[head, tail]到[0, tail-head],把后面的移到前面 for (int i = head; i &lt; tail; ++i) { items[i-head] = items[i]; } // 搬移完之后重新更新 head 和 tail tail -= head; head = 0; } items[tail] = item; ++tail; return true; } // 出队 public String dequeue() { // 如果 head == tail 表示队列为空 if (head == tail) return null; String ret = items[head]; ++head; return ret; } } /** * 基于链表实现的队列 */ public class QueueBasedOnLinkedList { // 队列的队首和队尾 private Node head = null; private Node tail = null; // 入队 public void enqueue(String value) { if (tail == null) { Node newNode = new Node(value, null); head = newNode; tail = newNode; } else { tail.next = new Node(value, null); tail = tail.next; } } // 出队 public String dequeue() { if (head == null) return null; String value = head.data; head = head.next; if (head == null) { tail = null; } return value; } public void printAll() { Node p = head; while (p != null) { System.out.print(p.data + &quot; &quot;); p = p.next; } System.out.println(); } private static class Node { private String data; private Node next; public Node(String data, Node next) { this.data = data; this.next = next; } public String getData() { return data; } } } 循环队列用数组来实现队列的时候，在 tail==n 时，会有数据搬移操作，这样入队操作性能就会受到影响。循环队列可以解决。 图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后 tail 加 1 更新为 1。所以，在 a，b 依次入队之后，循环队列中的元素就变成了下面的样子： 通过这样的方法，我们成功避免了数据搬移操作。看起来不难理解，但是循环队列的代码实现难度要比前面讲的非循环队列难多了。要想写出没有 bug 的循环队列的实现代码，最关键的是，确定好队空和队满的判定条件。 队列为空的判断条件仍然是 head == tail。 当队列满时，(tail+1)%n=head。图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。 public class CircularQueue { // 数组：items，数组大小：n private String[] items; private int n = 0; // head 表示队头下标，tail 表示队尾下标 private int head = 0; private int tail = 0; // 申请一个大小为 capacity 的数组 public CircularQueue(int capacity) { items = new String[capacity]; n = capacity; } // 入队 public boolean enqueue(String item) { // 队列满了 if ((tail + 1) % n == head) return false; items[tail] = item; tail = (tail + 1) % n; return true; } // 出队 public String dequeue() { // 如果 head == tail 表示队列为空 if (head == tail) return null; String ret = items[head]; head = (head + 1) % n; return ret; } } 阻塞队列和并发队列阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。 在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？ 线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因.","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构06-栈","date":"2020-11-25T09:07:34.000Z","path":"2020/11/25/01-数据结构/数据结构06-栈.html","text":"[toc] 栈的概述栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。后进者先出，先进者后出 事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。 当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。 栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，叫作顺序栈，用链表实现的栈，叫作链式栈。 实现// 基于数组实现的顺序栈 public class ArrayStack { private String[] items; // 数组 private int count; // 栈中元素个数 private int n; // 栈的大小 // 初始化数组，申请一个大小为 n 的数组空间 public ArrayStack(int n) { this.items = new String[n]; this.n = n; this.count = 0; } // 入栈操作 public boolean push(String item) { // 数组空间不够了，直接返回 false，入栈失败。 if (count == n) return false; // 将 item 放到下标为 count 的位置，并且 count 加一 items[count] = item; ++count; return true; } // 出栈操作 public String pop() { // 栈为空，则直接返回 null if (count == 0) return null; // 返回下标为 count-1 的数组元素，并且栈中元素个数 count 减一 String tmp = items[count-1]; --count; return tmp; } } /** * 基于链表实现的栈。 * Author: Zheng */ public class StackBasedOnLinkedList { private Node top = null; public void push(int value) { Node newNode = new Node(value, null); // 判断是否栈空 if (top == null) { top = newNode; } else { newNode.next = top; top = newNode; } } /** * 用-1表示栈中没有数据。 */ public int pop() { if (top == null) return -1; int value = top.data; top = top.next; return value; } public void printAll() { Node p = top; while (p != null) { System.out.print(p.data + &quot; &quot;); p = p.next; } System.out.println(); } private static class Node { private int data; private Node next; public Node(int data, Node next) { this.data = data; this.next = next; } public int getData() { return data; } } } 支持动态扩容的顺序栈要实现一个支持动态扩容的栈，只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，就申请一个更大的数组，将原来的数据搬移到新数组中。 实际上，支持动态扩容的顺序栈，平时开发中并不常用到。主要练习一下前面的复杂度分析方法。 对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。 对于入栈操作来说，当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。 为了分析的方便，需要事先做一些假设和定义： 栈空间不够时，重新申请一个是原来大小两倍的数组； 为了简化分析，假设只有入栈操作没有出栈操作； 定义不涉及内存搬移的入栈操作为 simple-push 操作(不需要扩容时的入栈操作)，时间复杂度为 O(1)。 如果当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存，并且做 K 个数据的搬移操作，然后再入栈。但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push 操作就可以完成。 这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分情况下，入栈操作的时间复杂度 O 都是 O(1)，只有在个别时刻才会退化为 O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。 栈的应用函数调用栈操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。 int main() { int a = 1; int ret = 0; int res = 0; ret = add(3, 5); res = a + ret; printf(&quot;%d&quot;, res); reuturn 0; } int add(int x, int y) { int sum = 0; sum = x + y; return sum; } main() 函数调用了 add() 函数，获取计算结果，并且与临时变量 a 相加，最后打印 res 的值。 表达式求值为了方便解释，将算术表达式简化为只包含加减乘除四则运算，比如：3+5*8-6。对于这个四则运算，人脑可以很快求解出答案，但是对于计算机来说，理解这个表达式本身就是个挺难的事儿。 实际上，编译器就是通过两个栈来实现的。其中一个保存==操作数==的栈，另一个是保存==运算符==的栈。从左向右遍历表达式，当遇到数字，就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。 括号匹配假设表达式中只包含三种括号，圆括号 ()、方括号 [] 和花括号{}，并且它们可以任意嵌套。比如，{[{}]}或 [{()}([])] 等都为合法格式，而{[}()] 或 [({)] 为不合法的格式。现在给有一个包含三种括号的表达式字符串，如何检查它是否合法呢？ 用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。 当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构05-链表","date":"2020-11-24T08:34:47.000Z","path":"2020/11/24/01-数据结构/数据结构05-链表.html","text":"[toc] 链表介绍从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。 而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。 介绍三种最常见的链表结构，分别是：单链表、双向链表和循环链表。 单链表为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针 next。 其中有两个结点是比较特殊的，它们分别是头结点和尾结点。头结点用来记录链表的基地址；尾结点不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。 在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移（假设需要保证有序），所以时间复杂度是 O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点。 针对链表的插入和删除操作，只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。 链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点，需要 O(n) 的时间复杂度。 循环链表循环链表是一种特殊的单链表。循环链表的尾结点指针是指向链表的头结点。 和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。 双向链表双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。 双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。 从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。 双向循环链表 删除操作分析从链表中删除一个数据无外乎这两种情况： 删除结点中“值等于某个给定值”的结点；查找+删除操作=O(n+1)=O(n) 删除给定指针指向的结点。 对于第二种情况，已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-&gt;next=q，说明 p 是 q 的前驱结点。 但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了！ 同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。 对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。 在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛。 用空间换时间的设计思想。当内存空间充足的时候，如果更追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。 链表 VS 数组性能比较 在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。 数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。 开发技巧 快慢指针找链表等分点：慢指针一次走一步，快指针一次两步，可找到中点。快指针三步，可找到1/3点，以此类推 警惕指针丢失和内存泄漏： 在结点 a 和相邻的结点 b 之间插入结点 x，假设当前指针 p 指向结点 a。如果我们将代码实现变成下面这个样子，就会发生指针丢失和内存泄露。 p-&gt;next = x; // 将 p 的 next 指针指向 x 结点； x-&gt;next = p-&gt;next; // 将 x 的结点的 next 指针指向 b 结点 利用哨兵简化实现难度 重点留意边界条件处理 如果链表为空时，代码是否能正常工作？ 如果链表只包含一个结点时，代码是否能正常工作？ 如果链表只包含两个结点时，代码是否能正常工作？ 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？ 链表实现增删改查、返回长度、反转、排序。 /** * @author niumt * @create 2020-11-24 下午 07:38 * @aa 增删改查、返回长度、反转、排序。 */ public class sinListNode&lt;T extends Comparable&gt; implements nodeMethods&lt;T&gt; { // 泛型类T，需实现compareTo接口，排序会用到比较 // 内部类表示节点 private class node { private T value; private node next = null; public node() { } public node(T value) { this.value = value; } public node(T value, node next) { this.value = value; this.next = next; } @Override public String toString() { return &quot;&quot; + value; } public T getValue() { return value; } public void setValue(T value) { this.value = value; } public node getNext() { return next; } public void setNext(node next) { this.next = next; } } private node head = null; @Override public void add(T v) { if (v == null) return; node next = new node(v); if (head == null) { head = next; return; } node tem = head; while (tem.getNext() != null) { tem = tem.getNext(); } tem.setNext(next); } // index从0开始 public void add(T v, int index) { if ((v == null) || index &gt; length()) return; node pre = head; for (int i = 0; i &lt; index - 1; i++) { pre = pre.getNext(); } pre.setNext(new node(v, pre.getNext())); } @Override public int find(T v) { if (head == null) return -1; int index = 0; node tem = head; while (tem!=null) { if (tem.getValue().compareTo(v)==0){ return index; } tem = tem.getNext(); index++; } if (tem == null) return -1; return index; } @Override public void change(T v, int index) { if ((v == null) || (head == null) || (index &gt; length()-1)) return; node tem = head; for (int i = 0; i &lt; index; i++) { tem = tem.getNext(); } tem.setValue(v); } @Override public void delete(T v) { if ((v == null) || (head == null)) return; if (head.getValue().compareTo(v)==0){ head = head.getNext(); return; } node pre = head; node cur = head.getNext(); while (cur!=null) { if (cur.getValue().compareTo(v)==0){ pre.setNext(cur.getNext()); break; } pre = cur; cur = cur.getNext(); } if (cur == null) return; } @Override public void deleteByIndex(int index) { if ((head == null) || (index &gt; length())) return; if (index &gt; (length() - 1)) return; if (index == 0) { head = head.getNext(); return; } int i = 0; node pre = head; for (i = 0; i &lt; index - 1; i++) { pre = pre.getNext(); } pre.setNext(pre.getNext().getNext()); } @Override public int length() { if (head == null) return 0; int l = 0; node n = head; while (n != null) { n = n.getNext(); l++; } return l; } @Override public void printList() { if (head == null) { return; } StringBuilder info = new StringBuilder(); node n = head; while (n != null) { info.append(n.toString()).append(&quot;\\t&quot;); n = n.getNext(); } System.out.println(info); } @Override public void reverseList() { /*两种方法：正向遍历，反向递归(未写)*/ if (length()&lt;=1){ return; } // node newHead = null; // node cur = head; // while (cur!=null){ // node n = new node(cur.getValue()); // n.setNext(newHead); // newHead = n; // cur = cur.getNext(); // } // head = newHead; node pre = head; node cur = pre.getNext(); pre.setNext(null); node tem; while (cur!=null){ tem = cur.getNext(); cur.setNext(pre); pre = cur; cur = tem; } head = pre; } // 冒泡排序 @Override public void orderList() { if (head == null) return; node cur = null; node pre = null; node next = null; int l = length(); for (int i = 0; i &lt; l - 1; i++) { int j=0; cur = head; pre = head; while (cur.getNext() != null) { if (j==l-1-i){ break; } next = cur.getNext(); int result = cur.getValue().compareTo(next.getValue()); if (result &gt; 0) { cur.setNext(next.getNext()); next.setNext(cur); if (cur == pre) { head = next; pre = head; } else { pre.setNext(next); pre = next; } } else { pre = cur; cur = cur.getNext(); } // System.out.print(&quot;&quot;+i+&quot;,&quot;+j+&quot;\\t&quot;); // printList(); j++; } } } }","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构04-数组","date":"2020-11-24T08:14:20.000Z","path":"2020/11/24/01-数据结构/数据结构04-数组.html","text":"[toc] 数组概念数组（Array）是一种==线性表==数据结构。它用一组==连续的内存空间==，来存储一组具有==相同类型==的数据。 线性表（Linear List）：顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。 非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。 连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。 低效的“插入”和“删除”如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。 如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数组插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。 如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。这不就是 JVM 标记清除垃圾回收算法的核心思想吗？ 警惕数组的访问越界问题int main(int argc, char* argv[]){ int i = 0; int arr[3] = {0}; for(; i&lt;=3; i++){ arr[i] = 0; printf(&quot;hello world\\n&quot;); } return 0; } 在 C 语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。根据我们前面讲的数组寻址公式，a[3] 也会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量 i 的内存地址，那么 a[3]=0 就相当于 i=0，所以就会导致代码无限循环。 数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。 总结：数组和ArrayList Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。 要表示多维数组时，用数组往往会更加直观。比如 Object array[][] 对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。 为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式： a[k]_{address} = base_{address} + k * TypeSize但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为： a[k]_{address} = base_{address} + (k-1) * TypeSize从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。 数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"HBase00-安装","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/00-环境/06-HBase安装.html","text":"启动Zookeeper 启动Hadoop：hdfs和yarn HBase的解压 tar -zxvf HBase-1.3.1-bin.tar.gz -C /opt/module 修改HBase的配置文件 HBase-env.sh export JAVA_HOME=/opt/module/jdk1.6.0_144 export HBASE_MANAGES_ZK=false HBase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181, hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.10/zkData&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; regionservers: hadoop102 hadoop103 hadoop104 软连接hadoop配置文件到HBase [atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/HBase/conf/core-site.xml [atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/HBase/conf/hdfs-site.xml 分发HBase到其他节点 HBase服务的启动 启动方式1 bin/HBase-daemon.sh start master bin/HBase-daemon.sh start regionserver 提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。 修复提示： a、同步时间服务 b、属性：hbase.master.maxclockskew设置更大的值 &lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;Time difference of regionserver from master&lt;/description&gt; &lt;/property&gt; 启动方式2 bin/start-HBase.sh bin/stop-HBase.sh 查看HBase页面 启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：http://hadoop102:16010","tags":[{"name":"HBase","slug":"HBase","permalink":"https://mtcai.github.io/tags/HBase/"}]},{"title":"Flume00-安装","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume00-安装.html","text":"安装地址 Flume 官网地址：http://flume.apache.org/ 文档查看地址：http://flume.apache.org/FlumeUserGuide.html 下载地址：http://archive.apache.org/dist/flume/ 安装部署 解压 apache flume 1.7.0 bin.tar.gz 到 /opt/ 目录下 将 flume/conf 下的 flume env.sh.template 文件修改为 flume env.sh ，并 配置 flumeenv.sh 文件 [atguigu@hadoop102 conf]$ mv flume env.sh.template flume env.sh [atguigu@hadoop102 conf]$ vi flume env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144","tags":[]},{"title":"Flume01-概述","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume01-概述.html","text":"定义Flume 是Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。 Flume最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写入到HDFS。 基础架构 AgentAgent是一个 JVM 进程，它以事件的形式将数据从源头送至目的。 Agent主要有 3 个部分组成， Source 、 Channel 、 Sink 。 SourceSource是负责接收数据到 Flume Agent 的组件。 Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、 thrift、 exec、 jms、 spooling directory、 netcat、 sequence generator、 syslog、 http、 legacy。 SinkSink不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent 。 Sink组件目的地包括 hdfs、 logger、avro、 thrift、 ipc、 file、 HBase、 solr、自定义。 ChannelChannel是位于 Source 和 Sink 之间的缓冲区。因此， Channel 允许 Source 和 Sink 运作在不同的速率上。 Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个Sink 的读取操作。 Flume自带两种 Channel：Memory Channel 和 File Channel 以及 Kafka Channel。 Memory Channel是内存中的队列。 Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。 FileChannel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。 Event传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。Event 由 Header 和 Body 两部分组成， Header 用来存放该 event 的一些属性，为 K-V 结构，Body 用来存放该条数据，形式为字节数组。","tags":[]},{"title":"Flume02-案例","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume02-案例.html","text":"Flume入门案例监控端口数据官方案例案例需求：使用Flume 监听一个端口， 收集该端口数据 ，并打印到控制台。 实现步骤： 安装 netcat 工具 sudo yum install -y nc 判断44444 端口是否被占用 sudo netstat -tunlp | grep 44444 创建Flume Agent 配置文件flume-netcat-logger.conf 在flume-netcat-logger.conf 文件中添加如下内容: &#39;Name the components on this agent&#39; a1.sources = r1 a1.sinks = k1 a1.channels = c1 &#39;Describe/configure the source&#39; a1.sources.r1.type = netcat a1.sources.r1.bind = localhost 表示a1监听的主机 a1.sources.r1.port = 44444 表示a1监听的端口号 &#39;Describe the sink&#39; a1.sinks.k1.type = logger &#39;Use a channel which buffers events in memory&#39; a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 表示a1的channel总容量1000个event a1.channels.c1.transactionCapacity = 100 表示a1的channel传输时收集到了100条event后再去提交事务 &#39;Bind the source and sink to the channel&#39; a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 先开启flume 监听端口 bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console 或 bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console --conf/-c：表示配置文件存储在conf/目录 --name/-n：表示给agent 起名为a1 --conf-file/-f：flume 本次启动读取的配置文件是在job 文件夹下的flume-telnet.conf文件。 -Dflume.root.logger=INFO,console ：-D 表示flume 运行时动态修改flume.root.logger 参数属性值，并将控制台日志打印级别设置为INFO 级别。日志级别包括:log、info、warn、error。 使用netcat 工具向本机的44444 端口发送内容 [atguigu@hadoop102 ~]$ nc localhost 44444 hello atguigu 在Flume 监听页面观察接收数据情况 实时监控单个追加文件案例需求：实时监控Hive 日志，并上传到HDFS 中 实现步骤: Flume 要想将数据输出到HDFS，须持有Hadoop 相关jar 包 commons-configuration-1.6.jar、 hadoop-auth-2.7.2.jar、 hadoop-common-2.7.2.jar、 hadoop-hdfs-2.7.2.jar、 commons-io-2.4.jar、 htrace-core-3.1.0-incubating.jar 拷贝到/opt/module/flume/lib 文件夹下。 创建flume-file-hdfs.conf 文件 &#39; Name the components on this agent a2.sources = r2 a2.sinks = k2 a2.channels = c2 &#39; Describe/configure the source a2.source s.r2.type = exec a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log a2.sources.r2.shell = /bin/bash -c &#39; Describe the sink a2.sinks.k2.type = hdfs a2.sinks.k2.hdfs.path = hdfs://hadoop102:9000/flume/%Y%m%d/%H a2.sinks.k2.hdfs.filePrefix = logs- &#39;上传文件的前缀 a2.sinks.k2.hdfs.round = true &#39;是否按照时间滚动文件夹 a2.sinks.k2.hdfs.roundValue = 1 &#39;多少时间单位创建一个新的文件夹 a2.sinks.k2.hdfs.roundUnit = hour &#39;重新定义时间单位 a2.sinks.k2.hdfs.useLocalTimeStamp = true &#39;是否使用本地时间戳 a2.sinks.k2.hdfs.batchSize = 1000 &#39;积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k2.hdfs.fileType = DataStream &#39;设置文件类型，可支持压缩 a2.sinks.k2.hdfs.rollInterval = 30 &#39;多久生成一个新的文件 a2.sinks.k2.hdfs.rollSize = 134217700 &#39;设置每个文件的滚动大小,略小于块的大小 a2.sinks.k2.hdfs.rollCount = 0 &#39;文件的滚动与 Event 数量无关 a2.channels.c2.type = memory &#39; Use a channel which buffers events in memory a2.channels.c2.capacity = 1000 a2.channels.c2.transactionCapacity = 100 &#39; Bind the source and sink to the channel a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2 运行Flume bin/flume-ng agent —conf conf/ —name a2 —conf-file job/flume-file-hdfs.conf 开启Hadoop 和Hive 并操作Hive 产生日志 在HDFS 上查看文件。 实时监控目录下多个新文件案例需求：使用Flume 监听整个目录的文件，并上传至HDFS 创建配置文件flume-dir-hdfs.conf a3.sources = r3 a3.sinks = k3 a3.channels = c3 &#39;Describe/configure the source a3.sources.r3.type = spooldir a3.sources.r3.spoolDir = /opt/module/flume/upload a3.sources.r3.fileSuffix = .COMPLETED a3.sources.r3.fileHeader = true a3.sources.r3.ignorePattern = ([^ ]*\\.tmp) &#39;忽略所有以.tmp 结尾的文件，不上传 &#39;Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H a3.sinks.k3.hdfs.filePrefix = upload- &#39;上传文件的前缀 a3.sinks.k3.hdfs.round = true &#39;是否按照时间滚动文件夹 a3.sinks.k3.hdfs.roundValue = 1 &#39;多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundUnit = hour &#39;重新定义时间单位 a3.sinks.k3.hdfs.useLocalTimeStamp = true &#39;是否使用本地时间戳 a3.sinks.k3.hdfs.batchSize = 100 &#39;积攒多少个Event 才flush 到HDFS 一次 a3.sinks.k3.hdfs.fileType = DataStream &#39;设置文件类型，可支持压缩 a3.sinks.k3.hdfs.rollInterval = 60 &#39;多久生成一个新的文件 a3.sinks.k3.hdfs.rollSize = 134217700 &#39;设置每个文件的滚动大小大概是128M a3.sinks.k3.hdfs.rollCount = 0 &#39;文件的滚动与Event 数量无关 &#39;Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 &#39;Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3 启动监控文件夹命令 bin/flume-ng agent —conf conf/ —name a3 —conf-file job/flume-dir-hdfs.conf 说明：在使用Spooling Directory Source 时，不要在监控目录中创建并持续修改文件，上传完成的文件会以.COMPLETED 结尾，被监控文件夹每500 毫秒扫描一次文件变动 向upload 文件夹中添加文件 查看HDFS 上的数据 实时监控目录下的多个追加文件Exec source 适用于监控一个实时追加的文件，但不能保证数据不丢失；Spooldir Source 能够保证数据不丢失，且能够实现断点续传，但延迟较高，不能实时监控；而Taildir Source 既能够实现断点续传，又可以保证数据不丢失，还能够进行实时监控。 案例需求：使用Flume 监听整个目录的实时追加文件，并上传至HDFS 创建配置文件flume-taildir-hdfs.conf a3.sources = r3 a3.sinks = k3 a3.channels = c3 &#39;Describe/configure the source a3.sources.r3.type = TAILDIR a3.sources.r3.positionFile = /opt/flume/tail_dir.json a3.sources.r3.filegroups = f1 a3.sources.r3.filegroups.f1 = /opt/module/flume/files/file.* &#39;Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path =hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H a3.sinks.k3.hdfs.filePrefix = upload- &#39;上传文件的前缀&#39; a3.sinks.k3.hdfs.round = true &#39;是否按照时间滚动文件夹&#39; a3.sinks.k3.hdfs.roundValue = 1 &#39;多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundUnit = hour &#39;重新定义时间单位&#39; a3.sinks.k3.hdfs.useLocalTimeStamp = true &#39; 是否使用本地时间戳&#39; a3.sinks.k3.hdfs.batchSize = 100 &#39;积攒多少个 Event 才 flush 到 HDFS 一次&#39; a3.sinks.k3.hdfs.fileType = DataStream &#39;设置文件类型，可支持压缩&#39; a3.sinks.k3.hdfs.rollInterval = 60 &#39;多久生成一个新的文件&#39; a3.sinks.k3.hdfs.rollSize = 134217700 &#39;设置每个文件的滚动大小大概是 128M&#39; a3.sinks.k3.hdfs.rollCount = 0 &#39;文件的滚动与 Event 数量无关&#39; &#39;Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 &#39;Bind the source and sink to the channel a3.sources.r3. channels = c3 a3.sinks.k3.channel = c3 启动监控文件夹命令 bin/flume-ng agent —conf conf/ —name a3 —conf-file job/flume-taildir-hdfs.conf 向files 文件夹中追加内容 echo hello &gt;&gt; file1.txt 查看HDFS 上的数据 Taildir 说明： Taildir Source 维护了一个json 格式的position File，其会定期的往position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。PositionFile 的格式如下： {&quot;inode&quot;:2496272,&quot;pos&quot;:12,&quot;file&quot;:&quot;/opt/module/flume/files/file1.txt&quot;} {&quot;inode&quot;:2496275,&quot;pos&quot;:12,&quot;file&quot;:&quot;/opt/module/flume/files/file2.txt&quot;} 注：Linux 中储存文件元数据的区域就叫做inode，每个inode 都有一个号码，操作系统用inode 号码来识别不同的文件，Unix/Linux 系统内部不使用文件名，而使用inode 号码来识别文件。 Flume 企业开发案例复制和多路复用案例需求：使用Flume-1 监控文件变动，Flume-1 将变动内容传递给Flume-2，Flume-2 负责存储到HDFS。同时Flume-1 将变动内容传递给Flume-3，Flume-3 负责输出到Local FileSystem。 在/opt/module/datas/目录下创建flume3 文件夹 创建flume-file-flume.conf &#39;Name the components on this agent a1.sources = r1 a1.sinks = k1 k2 a1.channels = c1 c2 &#39;将数据流复制给所有 channel a1.sources.r1.selector.type = replicating &#39;Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log a1.sources.r1.shell = /bin/bash -c &#39;Describe the sink &#39;sink 端的 avro 是一个数据发送者 a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop102 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = hadoop102 a1.sinks.k2.port = 4142 &#39;Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.channels.c2.type = memory a1.channels.c2.capacity = 1000 a1.channels.c2.transactionCapacity = 100 &#39;Bi nd the source and sink to the channel a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c2 创建 flume-flume-hdfs.conf &#39;Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 &#39;Describe/configure the source &#39;source 端的 avro 是一个数据接收服务 a2.sources.r1.type = avro a2.sources.r1.bind = hadoop102 a2.sources.r1.port = 4141 &#39;Describe the sink a2.sinks.k1.type = hdfs a2.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume2/%Y%m%d/%H a2.sinks.k1.hdfs.filePrefix = flume2 &#39;上传文件的前缀 a2.sinks.k1.hdfs.round = true&#39;是否按照时间滚动文件夹 a2.sinks.k1.hdfs.roundValue = 1 &#39;多少时间单位创建一个新的文件夹 a2.sinks.k1.hdfs.roundUnit = hour &#39;重新定义时间单位 a2.sinks.k1.hdfs.useLocalTimeStamp = true &#39;是否使用 本地时间戳 a2.sinks.k1.hdfs.batchSize = 100 &#39;积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k1.hdfs.fileType = DataStream &#39;设置文件类型，可支持压缩 a2.sinks.k1.hdfs.rollInterval = 600 &#39;多久生成一个新的文件 a2.sinks.k1.hdfs.rollSize = 134217700 &#39;设置每个文件的滚动大小大概是 128M a2.sinks.k1.hdfs.rollCount = 0 &#39;文件的滚动与 Event 数量无关 &#39;Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 &#39;Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1 创建 flume-flume-dir.conf &#39;Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c2 &#39;Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop102 a3.sources.r1.port = 4142 &#39;Describe the sink &#39;提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。 a3.sinks.k1.type = file_roll a3.sinks.k1.sink.directory = /opt/module/data/flume3 &#39;Describe the channel a3.channels.c2.type = memory a3.cha nnels.c2.capacity = 1000 a3.channels.c2.transactionCapacity = 100 &#39;Bind the source and sink to the channel a3.sources.r1.channels = c2 a3.sinks.k1.channel = c2 执行配置文件 分别启动对应的 flume 进程 flume flume dir flume flume h dfs flume file flume : [atguigu@hadoop102 flume]$ bin/flume-ng agent —conf conf/ —namea3 —conf-file job/group1/flume-flume-dir.conf [atguigu@hadoop102 flume]$ bin/flume-ng agent —conf conf/ —namea2 —conf-file job/group1/flume-flume-hdfs.conf [atguigu@hadoop102 flume]$ bin/flume-ng agent —conf conf/ —namea1 —conf-file job/group1/flume-file-flume.conf 启动 Hadoop 和 Hive 检查 /opt/module/datas/flume3 目录中数据和 HDFS 上数据 负载均衡和故障转移使用Flume 1 监控一个端口，其 sink 组中的 sink 分别对接 Flume2 和Flume3 ，采用FailoverSinkProcessor ，实现故障转移的功能。 创建flume-netcat-flume.conf &#39;Name the components on this agent a1.sources = r1 a1.channels = c1 a1.sinkgroups = g1 a1.sinks = k1 k2 &#39;Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 a1.sinkgroups.g1.processor.type = failover a1.sinkgroups.g1.processor.priority.k1 = 5 a1.sinkgroups.g1.processor.priority.k2 = 10 a1.sinkgroups.g1.processor.maxpenalty = 10000 &#39;Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop102 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = hadoop102 a1.sinks.k2.port = 4142 &#39;Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 &#39;Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinkgroups.g1.sinks = k1 k2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c1 创 建 flume-flume-console1.conf &#39;Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 &#39;Describe/configure the source a2.sources.r1.type = avro a2.sources.r1.bind = hadoop102 a2.sources.r1.port = 4141 &#39;Describe the sink a2.sinks.k1.type = logger &#39;Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 &#39;Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1 flume-flume-console2.conf &#39;Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c2 &#39;Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop102 a3.sources.r1.port = 4142 &#39;escribe the sink a3.sinks.k1.type = logger &#39;escribe the channel a3.channels.c2.type = memory a3.channels.c2.capacity = 1000 a3.channels.c2.transactionCapacity = 100 &#39;ind the source and sink to the channel a3.sources.r1.channels = c2 a3.sinks.k1.channel = c2 分别开启对应配置文件：flume-flume-console2.conf, flume-flume-console1.conf, flume-netcat-flume.conf。 [atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console [atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console [atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf 使用 net cat 工具向本机的 44444 端口发送内容 查看 F lume 2 及 F lume 3 的控制台打印日志 将 Flume2 kill ，观察 Flume 3 的控制台打印情况。 注：使用jps -ml 查看 Flume 进程。 聚合案例需求：hadoop102 上的 Flume1 监控文件 opt/module /data group.log；hadoop103 上的 Flume2 监控某一个端口的数据流，Flume1 与 Flume2 将数据发送给 hadoop104 上的 Flume3, Flume3 将最终数据打印到控制台。 分发Flume 创建flume1-logger-flume.conf &#39;Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 &#39;Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /opt/module/group.log a1.sources.r1.shell = /bin/bash -c &#39;Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop104 a1.sinks.k1.port = 4141 &#39;Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 &#39;Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 创建 flume2-netcat-flume.conf &#39;Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 &#39;Describe/configure the source a2.sources.r1.type = netcat a2.sources.r1.bind = hadoop103 a2.sources.r1.port = 44444 &#39;Describe the sink a2.sinks.k1.type = avro a2.sinks.k1.hostname = hadoop104 a2.sinks.k1.port = 4141 &#39;Use a channel which buffers events in memory a2.channels.c1.type = memory a2.channels.c1.capa city = 1000 a2.channels.c1.transactionCapacity = 100 &#39;Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1 创建 flume3-flume-logger.conf &#39;Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c1 &#39;Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop10 4 a3.sources.r1.port = 4141 &#39;Describe the sink &#39;Describe the sink a3.sinks.k1.type = logger &#39;Describe the channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000 a3.channels.c1.transactionCapacity = 100 &#39;Bind the source and sink to the channel a3.sources.r1.channels = c1 a3.sinks.k1.channel = c1 执行配置文件 [atguigu@hadoop104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console [atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume1-logger-flume.conf [atguigu@hadoop103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume2-netcat-flume.conf 在 hadoop 103 上向 opt/module 目录下的 group .log 追加内容 在 hadoop 102 上向 44444 端口发送 数据 检查 hadoop 104 上数据","tags":[]},{"title":"Flume03-原理","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume03-原理.html","text":"Flume事务 Agent 内部原理 ChannelSelector ChannelSelector 的作用就是选出Event 将要被发往哪个Channel。其共有两种类型，分别是Replicating（复制）和Multiplexing（多路复用）。 ReplicatingSelector会将同一个 Event 发往所有的 Channel， Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel。 SinkProcessor SinkProcessor共有三种类型，分别是 DefaultSinkProcessor 、LoadBalancingSinkProcessor 和 FailoverSinkProcessor。 DefaultSinkProcessor 对应的是单个的 Sink，LoadBalancingSinkProcessor 和FailoverSinkProcessor 对应的是 Sink Group，LoadBalancingSinkProcessor 可以实现负载均衡的功能， FailoverSinkProcessor 可以 实现故障转移的功能。 拓扑结构 串联 这种模式是将多个flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量 flume 数量 过多不仅会影响传输速率，而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统。 复制和多路复用图 Flume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel 中，或者将不同数据分发到不同的 channel 中， sink 可以选择传送到不同的目的地。 负载均衡和故障转移 Flume 支持使用将多个 sink 逻辑上分到一个 sink 组， sink 组配合不同的 SinkProcessor可以实现负载均衡和错误恢复的功能。 聚合 这种模式是我们最常见的，也非常实用，日常web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个flume 采集日志，传送到一个集中收集日志的flume，再由此flume 上传到hdfs、hive、hbase 等，进行日志分析。 自定义Interceptor在实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要发送到不同的分析系统。此时会用到 Flume 拓扑结构中的 Multiplexing 结构， Multiplexing的原理是，根据 event 中 Header 的某个 key 的值，将不同的 event 发送到不同的 Channel中，所以我们需要自定义一个Interceptor，为不同类型的event 的Header 中的key 赋予不同的值。 在该案例中，我们以端口数据模拟日志，以数字（单个）和字母（单个）模拟不同类型的日志，我们需要自定义interceptor 区分数字和字母，将其分别发往不同的分析系统（Channel）。 实现步骤: 创建一个maven 项目，并引入以下依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; 定义CustomInterceptor 类并实现Interceptor 接口 package com.atguigu.flume.interceptor; import org.apache.flume.Context; import org.apache.flume.Event; import org.apache.flume.interceptor.Interceptor; import java.util.List; public class CustomInterceptor implements Interceptor { @Override public void initialize() { } @Override public Event intercept(Event event) { byte[] body = event.getBody(); if (body[0] &lt; &#39;z&#39; &amp;&amp; body[0] &gt; &#39;a&#39;) { event.getHeaders().put(&quot;type&quot;, &quot;letter&quot;); } else if (body[0] &gt; &#39;0&#39; &amp;&amp; body[0] &lt; &#39;9&#39;) event.getHeaders().put(&quot;type&quot;, &quot;number&quot;); return event; } @Override public List&lt;Event&gt; intercept(List&lt;Event&gt; events) { for (Event event : events) { intercept(event); } return events; } @Override public void close() {} public static class Builder implements Interceptor.Builder { @Override public Interceptor build() { return new CustomInterceptor(); } @Override public void configure(Context context) {} } } 编辑 flume 配置文件 为hadoop102 上的 Flume1 配置 1 个 netcat source, 1 个 sink group, 2 个 avro sink并配置相应的 ChannelSelector 和 interceptor。 &#39;Name the components on this agent a1.sources = r1 a1.sinks = k1 k2 a1.channels = c1 c2 &#39;Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.CustomInterceptor$Builder a1.sources.r1.selector.type = multiplexing a1.sources.r1.selector.header = type a1.sources.r1.selector.mapping.letter = c1 a1.sourc es.r1.selector.mapping.number = c2 # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop103 a1.sinks.k1.port = 4141 a1.sinks.k2.type=avro a1.sinks.k2.hostname = hadoop104 a1.sinks.k2.port = 4242 &#39;Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 &#39;Use a channel which buffers events in memory a1.channels.c2.type = memory a1.channels.c2.capacity = 1000 a1.channels.c2.transactionCapacity = 100 &#39;Bind the source and sink to the channel a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c2 为 hadoop 103 上的 F lume2 配置一个 avro source 和一个 logger sink 。 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = avro a1.sources.r1.bind = hadoop103 a1.sources.r1.port = 4141 a1.sinks.k1.type = logger a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.sinks.k1.channel = c1 a1.sources.r1.channels = c1 为hadoop 10 4 上的 Flume3 配置一个 avro source 和一个 logger sink 。 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = avro a1.sources.r1.bind = hadoop104 a1.sources.r1.port = 4242 a1.sinks.k1.type = logger a1.channels.c1.type = memory a1.channels.c1.capacity = 10 00 a1.channels.c1.transactionCapacity = 100 a1.sinks.k1.channel = c1 a1.sources.r1.channels = c1 分别在 hadoop102 hadoop103 hadoop104 上启动 flume 进程 ，注意先后顺序。 在 hadoop102 使用 netcat 向 localhost 444 44 发送字母和数字 观察 hadoop103 和 hadoop104 打印的日志 。 自定义SourceSource是负责接收数据到 Flume Agent 的组件。 Source 组件可以处理各种类型、各种格式的日志数据 包括 avro、 thrift、 exec、 jms、 spooling directory、 netcat、 sequence generator、 syslog、 http、 legacy。官方提供的 source 类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些 source。 MySource 需要继承 AbstractSource 类并实现 Configurable 和 PollableSource 接口。 实现相应方法： getBackOffSleepIncrement暂不用 getMaxBackOffSleepInterval 暂不用 configure(Context context初始化 context （读取配置文件内容) process()获取数据封装成 event 并写入 channel ，这个方法将被循环调用。使用场景：读取MySQL 数据或者其他文件系统。 需求：使用flume 接收数据，并给每条数据添加前缀，输出到控制台。前缀可从 flume 配置文件中配置。 导入pom 依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写 MySource 代码 import org.apache.flume.Context; import org.apache.flume.EventDeliveryException; import org.apache.flume.PollableSource; import org.apache.flume.conf.Configurable; import org.apache.flume.event.SimpleEvent; import org.apache.flume.source.AbstractSource; import java.util.HashMap; public class MySource extends AbstractSource implements Configurable, PollableSource { //定义配置文件将来要读取的字段 private Long delay; private String field; //初始化配置信息 @Override public void configure(Context context) { delay = context.getLong(&quot;delay&quot;); field = context.getString(&quot;field&quot;, &quot;Hello!&quot;); } @Override public Status process() throws EventDeliveryException { try { //创建事件头信息 HashMap&lt;String, String&gt; hearderMap = new HashMap&lt;&gt;(); //创建事件 SimpleEvent event = new SimpleEvent(); //循环封装事件 for (int i = 0; i &lt; 5; i++) { //给事件设置头信息 event.setHeaders(hearderMap); //给事件设置内容 event.setBody((field + i).getBytes()); //将事件写入 channel getChannelProcessor().processEvent(event); Thread.sleep(delay); } } catch (E xception e) { e.printStackTrace(); return Status.BACKOFF; } return Status.READY; } @Override public long getBackOffSleepIncrement() { return 0; } @Override public long getMaxBackOffS leepInterval() { return 0; } } 打包，将写好的代码打包，并放到flume 的 lib 目录（ opt/module/flume ）下。 配置文件 &#39;Name the components on this agent&#39; a1.sources = r1 a1.sinks = k1 a1.channels = c1 &#39;Describe/configure the source&#39; a1.sources.r1.type = com.atguigu.MySource a1.sources.r1.delay = 1000 a1.sources.r1.field = atguigu &#39;Describe the sink&#39; a1.sinks.k1.type = logger &#39;Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transac tionCapacity = 100 &#39;Bind the source and sink to the channel&#39; a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 开启任务 [atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -fjob/my-source.conf -n a1 -Dflume.root.logger=INFO,console 结果展示 自定义SinkSink不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent 。 Sink是完全事务性的。在从 Channel 批量删除数据之前，每个 Sink 用 Channel 启动一个事务。批量事件一旦成功写出到存储系统或下一个 Flume Agent Sink 就利用 Channel 提交事务。 事务一旦被提交，该 Channel 从自己的内部缓冲区删除事件。 Sink 组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的Sink 类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Sink。 MySink 需要继承AbstractSink 类并实现Configurable 接口。 实现相应方法： configure(Context context)//初始化 context（读取配置文件内容） process() //从Channel 读取获取数据（event），这个方法将被循环调用。 使用场景：读取Channel 数据写入MySQL 或者其他文件系统 需求：使用flume 接收数据，并在Sink 端给每条数据添加前缀和后缀，输出到控制台。前后缀可在flume 任务配置文件中配置。 编码 import org.apache.flume.*; import org.apache.flume.conf.Configurable; import org.apache.flume.sink.AbstractSink; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class MySink extends AbstractSink implements Configurable { //创建 Logger 对象 private static final Logger LOG = LoggerFactory.getLogger(AbstractSink.class); private String prefix; private String suffix; @Override public Status process() throws EventDeliveryException { //声明返回值状态信息 Status status; //获取当前 Sink 绑定的 Channel Channel ch = getChannel(); //获取事务 Transaction txn = ch.getTransaction(); //声明事件 Event event; //开启事务 txn.begin(); //读取 Channel 中的事件，直到读取到事件结束循环 while (true) { event = ch.take(); if (event != null) { break; } } try { //处理事件（打印） LOG.info(prefix + new String(event.getBody()) + suffix); //事务提交 txn.commit(); status = Status.READY; } catch (Exception e){ //遇到异常，事务回滚 txn.rollback(); status = Status.BACKOFF; } finally{ //关闭事务 txn.close(); } return status; } @Override public void configure(Context context) { //读取配置文件内容，有默认值 prefix = context.getString(&quot;prefix&quot;, &quot;hello:&quot;); //读取配置文件内容，无默认值 suffix = context.getString(&quot;suffix&quot;); } } 打包 将写好的代码打包，并放到flume 的 lib 目录（ （/opt/module/flume ）下。 配置文件 &#39;Name the components on this agent&#39; a1.sources = r1 a1.sinks = k1 a1.channels = c1 &#39;Describe/configure the source&#39; a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 &#39;Describe the sink&#39; a1.sinks.k1.type = com.atguigu.MySink &#39;a1.sinks.k1.prefix =&#39; a1.sinks.k1.suffix = :atguigu &#39;Use a channel which buffers events in memory&#39; a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 &#39;Bind the source and sink to the channel&#39; a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 开启任务 [atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console [atguigu@hadoop102 ~]$ nc localhost 44444 hello OK atguigu OK 结果展示 Flume数据流监控Ganglia的安装与部署 安装 httpd 服务与 php [atguigu@hadoop102 flume]$ sudo yum y install httpd php 安装其他依赖 sudo yum y install rrdtool perl rrdtoolrrdtool devel [atguigu@hadoop102 flume]$ sudo yum y install apr devel 安装 ganglia sudo rpm Uvhhttp://dl.fedoraproject.org/pub/epel/6/x86_64/epel release 68.noarch.rpm sudo yum y install ganglia gmetad sudo yum y install ganglia web sudo yum y install ganglia gmond Ganglia由 gmond、 gmetad 和 gweb 三部分组成。 gmond (Ganglia Monitoring Daemon) 是一种轻量级服务，安装在每台需要收集指标数据的节点主 机上。使用 gmond ，你可以很容易收集很多系统指标数据，如 CPU 、内存、磁盘、网络和活跃进程的数据等。 gmetad (Ganglia Meta Daemon)整合所有信息，并将其以 RRD 格式存储至磁盘的服务。 gweb (Ganglia Web Ganglia) 可视化工具， gweb 是一种利用浏览器显示 gmetad 所存储数据的 PHP 前端。在 Web 界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。 修改配置文件 /etc/httpd/conf.d/ganglia.conf # Ganglia monitoring system php web frontend Alias /ganglia /usr/share/ganglia &lt;Location/ ganglia&gt; Order deny,allow #Deny from all Allow from all ##这一行 # Allow from 127.0.0.1 # Allow from ::1 # Allow from .example.com &lt;/Location&gt; 修改配置文件 /etc/ganglia/gmetad.conf data_source “hadoop102” 192.168. 9 .102 修改配置文件 /etc/ganglia/gmond.conf cluster { name = &quot;hadoop102&quot; owner = &quot;unspecified&quot; latlong = &quot;unspecified&quot; url = &quot;unspecified&quot; } udp_send_channel { #bind_hostname = yes # Highly recommended, soon to be default. # This option tells gmond to use a source address # that resolves to the machine&#39;s hostname. Without # this, the metrics may appear to come from any # interface and the DNS names associated with # those IPs will be used to create the # mcast_join = 239.2.11.71 host = 192.168.9.102 port = 8649 ttl = 1 } udp_recv_channel { # mcast_join = 239.2.11.71 port = 8649 bind = 192.168. 9 .102 retry_bind = true # Size of the UDP buffer. If you are handling lots of metrics you really # should bump it up to e.g. 10MB or even # buffer = 10485760 } 修改配置文件 /etc/selinux/config # This file controls the state of SELinux on the # SELINUX= can take one of these three # enforcing SELinux security policy is enforced. # permissive SELinux prints warnings instead of enforcing. # disabled No SE Linux policy is loaded. SELINUX=disabled ### 这一行 # SELINUXTYPE= can take one of these two # targeted Targeted processes are protected, # mls Multi Level Security protection. SELINUXTYPE=targeted &#39;提示selinux 本次生效关闭必须重启，如果此时不想重启，可以临时生效之： &#39;[atguigu@hadoop102 flume]$ sudo setenforce 0 启动 ganglia [atguigu@hadoop102 flume]$ sudo service httpd start [atguigu@hadoop102 flume]$ sudo service gmetad start [atguigu@hadoop102 flume]$ sudo service gmond start 打开网页浏览 ganglia 页面 http://192.168.9.102/ganglia 提示：如果完成以上操作依然出现权限不足错误，请修改 /var/lib/ganglia 目录的权限： [atguigu@hadoop102 flume]$ sudo chmod R 777 /var/lib/ganglia 操作 Flume测试监控 修改 /opt/module/flume/conf 目录下的 flume env.sh 配置： JAVA_OPTS=&quot; Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts= 192.168.9.102:8649 -Xms100m -Xmx200m&quot; 启动 Flume 任务 [atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume netcat logger.conf -Dflume.root.logger==INFO,console -Dflume.monitoring.type=ganglia -Dflume.monitorin g.hosts= 192.168.9.102:8649 发送数据观察 ganglia 监测图 [atguigu@hadoop102 flume]$ nc localhost 44444 字段（图表名称） 字段含义 EventPut AttemptCount source 尝试写入 channel 的事件总数量 EventPutSuccessCount 成功写入 channel 且提交的事件总数量 EventTakeAttemptCount sink 尝试从 channel 拉取事件的总数量 E ventTakeSuccessCount sink sink 成功读取的事件的总数量 StartTime channel 启动的时间（毫秒） StopTime channel 停止的时间（毫秒） ChannelSize 目前 channel 中事件的总数量 ChannelFillPercentage channel 占用的百分比 ChannelCapacity channel 的容量","tags":[]},{"title":"Flume04-面试题","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/08-Flume/flume04-面试题.html","text":"如何实现Flume 数据传输的监控的使用第三方框架Ganglia 实时监控Flume。 Flume 的Source，Sink，Channel 的作用？你们Source 是什么类型？作用： Source 组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy Channel 组件对采集到的数据进行缓存，可以存放在Memory 或File 中。 Sink 组件是用于把数据发送到目的地的组件，目的地包括HDFS、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。 我公司采用的Source 类型为 监控后台日志：exec 监控后台产生日志的端口：netcat Flume 的Channel Selectors Flume参数调优Source增加Source 个（使用 Tair Dir Source 时可增加 FileGroups 个数）可以增大 Source 的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个 Source 以保证 Source 有足够的能力获取到新产生的数据。 batchSize参数决定 Source 一次批量运输到 Channel 的 event 条数，适当调大这个参数可以提高 Source 搬运 Event 到 Channel 时的性能。 Channeltype选择 memory 时 Channel 的性能最好，但是如果 Flume 进程意外挂掉可能会丢失数据。 type 选择 file 时 Channel 的容错性更好，但是性能上会比 memory channel 差。 使用file Channel 时 dataDirs 配置多个不同盘下的目录可以提高性能。 Capacity参数决定 Channel 可容纳最大的 event 条数。 transactionCapacity 参数决定每次 Source 往 channel 里面写的最大 event 条数和每次 Sink 从 channel 里面读的最大 event条数。 transactionCapacity 需要大于 Source 和 Sink 的 batchSize 参数。 Sink增加Sink 的个数可以增加 Sink 消费 event 的能力。 Sink 也不是越多越好够用就行，过多的 Sink 会占用系统资源，造成系统资源不必要的浪费。 batchSize参数决定 Sink 一次批量从 Channel 读取的 event 条数，适当调大这个参数可以提高 Sink 从 Channel 搬出 event 的性能。 Flume的事务机制Flume的事务机制（类似数据库的事务机制）： Flume 使用两个独立的事务分别负责从Soucrce 到 Channel ，以及从 Channel 到 Sink 的事件传递。比如 spooling directory source为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到 Channel 且提交成功，那么 Soucrce 就将该文件标记为完成。同理，事务以类似的方式处理从 Channel 到 Sink 的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到 Channel 中，等待重新传递。 Flume采集数据会丢失吗 ?根据Flume 的架构原理， Flume 是不可能丢失数据的，其内部有完善的事务机制，Source 到 Channel 是事务性的， Channel 到 Sink 是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是 Channel 采用 memory C hannel agent 宕机导致数据丢失，或者 Channel 存储数据已满，导致 Source 不再写入，未写入的数据丢失。 Flume不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由 Sink 发出，但是没有接收到响应， Sink 会再次发送数据，此时可能会导致数据的重复 。","tags":[]},{"title":"Kafka00-安装","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka00-安装.html","text":"1.解压缩安装包 [atguigu@hadoop102 software]$ tar -zxvf kafka_2.11 0.11.0.0.tgz -C/opt/module 2.创建logs文件夹: kafka根目录下 3.修改配置文件 vim /config/server.properties &#39;broker的全局唯一编号，不能重复&#39; broker.id=0 &#39;删除 topic 功能使能&#39; delete.topic.enable=true &#39;处理网络请求的线程数量&#39; num.network.threads=3 &#39;用来处理磁盘 IO 的现成数量&#39; num.io.threads=8 &#39;发送套接字的缓冲区大小&#39; socket.send.buffer.bytes=102400 &#39;接收套接字的缓冲区大小&#39; socket.receive.buffer.bytes=102400 &#39;请求套接字的缓冲区大小&#39; socket.request.max.bytes=104857600 &#39;kafka 运行日志存放的路径&#39; log.dirs=/opt/module/kafka/logs &#39;topic 在当前 broker 上的分区个数&#39; num.partitions=1 &#39;用来恢复和清理 data 下数据的线程数量&#39; num.recovery.threads.per.data.dir=1 &#39;segment文件保留的最长时间，超时将被删除&#39; log.retention.hours=168 &#39;配置连接 Zookeeper 集群地址&#39; zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181 4.配置环境变量 KAFKA_HOMEexport KAFKA_HOME=/opt/module/kafka export PATH=\\$PATH:\\$KAFKA_HOME/bin 5.其他机器修改broker.id: broker.id不得重复 6.启动 [atguigu@hadoop104 kafka]$ bin/kafka-server-start.sh -daemonconfig/server.properties 7.关闭 [atguigu@hadoop102 kafka]$ bin/kafka-server-stop.sh stop","tags":[]},{"title":"Kafka02-结构原理","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka02-架构原理.html","text":"Kafka工作流程与文件存储 Kafka 中消息是以topic 进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。 topic 是逻辑上的概念，而partition 是物理上的概念，每个partition 对应于一个log 文件，该log 文件中存储的就是producer 生产的数据。Producer 生产的数据会被不断追加到该log 文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。 由于生产者生产的消息会不断追加到log 文件末尾，为防止log 文件过大导致数据定位效率低下，Kafka 采取了分片和索引机制，将每个partition 分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称+分区序号。例如，first 这个topic 有三个分区，则其对应的文件夹为first-0,first-1,first-2。 00000000000000000000.index 00000000000000000000.log 00000000000000170410.index 00000000000000170410.log 00000000000000239430.index 00000000000000239430.log index 和log 文件以当前segment 的第一条消息的offset 命名。下图为index 文件和log文件的结构示意图。 “.index”文件存储大量的 索引信息 ，“.log”文件存储大量的数据 ，索引文件中的元数据指向对应数据文件中 message的物理偏移地址 生产者分区策略 分区的原因 方便在集群中扩展 ，每个 Partition可以通过调整以适应它所在的机器，而一个 topic又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了； 可以提高并发 ，因为可以以 Partition为单位读写了。 分区的原则 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 数据可靠性保证为保证 producer 发送的数据，能可靠的发送到指定的 topic, topic 的每个 partition 收到producer发送的数据后， 都 需要向 producer 发送 ack (acknowledgement确认收到)，如果producer收到 ack 就会进行下一轮的发送，否 则重新发送数据。 副本数据同步策略 方案 优点 缺点 半数以上完成同步，就发送ack 延迟低 选举新的leader 时，容忍n 台节点的故障，需要2n+1 个副本 全部完成同步，才发送ack 选举新的leader 时，容忍n 台节点的故障，需要n+1 个副本 延迟高 Kafka 选择了第二种方案，原因如下： 同样为了容忍n 台节点的故障，第一种方案需要2n+1 个副本，而第二种方案只需要n+1个副本，而Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。 虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka 的影响较小。 ISR采用第二种方案之后，设想以下情景：leader 收到数据，所有follower 都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader 进行同步，那leader 就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？ Leader 维护了一个动态的in-sync replica set (ISR)，意为和leader 保持同步的follower 集合。当ISR 中的follower 完成数据的同步之后，leader 就会给follower 发送ack。如果follower 长时间未向leader 同步数据， 则该follower 将被踢出ISR ， 该时间阈值由replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从ISR 中选举新的leader。 ack 应答机制对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR 中的follower 全部接收成功。所以Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。 acks=0：producer 不等待broker 的ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当broker 故障时有可能丢失数据； aks=1：producer 等待broker 的ack，partition 的leader 落盘成功后返回ack，如果在follower同步成功之前leader 故障，那么将会丢失数据; acks=-1（all）：producer 等待broker 的ack，partition 的leader 和follower 全部落盘成功后才返回ack。但是如果在follower 同步完成后，broker 发送ack 之前，leader 发生故障，那么会造成数据重复。 故障处理细节LEO：指的是每个副本最大的offset； HW：指的是消费者能见到的最大的offset，ISR 队列中最小的LEO。 follower 故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入ISR 了。 leader 故障 leader 发生故障之后，会从ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性, 其余的 follower会先将各自的 log文件 高于 HW的部分截掉 ，然后从新的 leader同步数据。 注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 Exactly Once语义将服务器的ACK级别设置为 -1，可以保证 Producer到 Server之间不会丢失数据，即 At Least Once语义 。相对的，将服务器 ACK级别设置为 0，可以保证生产者每条消息只会被发送一次，即 At Most Once语义。 At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的， At Least Once可以保证数据不重复，但是不能保证数据不丢失。 但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once语义。 在 0.11版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 0.11版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer不论向 Server发送多少次重复数据， Server端都只会持久化一条。幂等性结合 At Least Once语义，就构成了 Kafka的 Exactly Once语义。 \\text{At Least Once} + \\text{幂等性} = \\text{Exactly Once}要启用幂等性，只需要将Producer的参数中 enable.idompotence设置为 true即可。 Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer在初始化的时候会被分配一个 PID，发往同一 Partition的消息会附带 Sequence Number。而Broker端会对 做缓存，当具有相同主键的消息提交时， Broker只会持久化一条。 但是PID重启就会变化，同时不同的 Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。 消费者消费方式consumer采用 pull（拉模式从 broker中读取数据）。 push（推模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker决定的）。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer来不及处理消息， 典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。pull 模式不足之处是，如果kafka 没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer 会等待一段时间之后再返回，这段时长即为timeout。 分区分配策略一个consumer group 中有多个consumer，一个 topic 有多个partition，所以必然会涉及到partition 的分配问题，即确定那个partition 由哪个consumer 来消费。Kafka 有两种分配策略，一是RoundRobin，一是Range。 RoundRobin： Range： offset 的维护由于consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以consumer 需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。 Kafka 0.9 版本之前，consumer 默认将offset 保存在Zookeeper 中，从0.9 版本开始，consumer 默认将offset 保存在Kafka 一个内置的topic 中，该topic 为__consumer_offsets。 修改配置文件consumer.properties exclude.internal.topics=false 读取offset 0.11.0.0 之前版本: bin/kafka-console-consumer.sh —topic __consumer_offsets—zookeeper hadoop102 :2181 —formatter“kafka.coordinator.GroupMetadataManager \\\\$OffsetsMessageFormatter” —consumer.config config/consumer.properties —from-beginning 0.11.0.0之后版 本 (含 ): bin/kafka console consumer.sh —topic __consumer_offsets—zookeeper hadoop102 :2181 —formatter“kafka.coordinator.group.GroupMetadataManager \\\\$OffsetsMessageFormatter” —consumer.config config/consumer.properties —from-beginning 高效写数据顺序写磁盘： Kafka的 producer生产数据，要写入到 log文件中，写的过程是一直追加到文件末端，为顺序写 。 官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与 磁盘的机械机构有关，顺序写之所以快，是因为其省去了 大量 磁头寻址的时间 。 零复制技术： zk的作用Kafka 集群中有一个broker 会被选举为Controller，负责管理集群broker 的上下线，所有topic 的分区副本分配和leader 选举等工作。 Controller 的管理工作都是依赖于Zookeeper 的。 以下为partition 的leader 选举过程： Kafka事务Kafka 从0.11 版本开始引入了事务支持。事务可以保证Kafka 在Exactly Once 语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 Producer事务 为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将 Producer获得的 PID和 Transaction ID绑定。这样当 Producer重启后就可以通过正在进行的 Transaction ID获得原来的 PID。 为了管理Transaction Kafka引入了一个新的组件 Transaction Coordinator。 Producer就是通过和 Transaction Coordinator交互获得 Transaction ID对应的任务状态。 Transaction Coordinator还负责将事务所有写入 Kafka的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 Consumer事务 上述事务机制主要是从Producer方面考虑，对于 Consumer而言，事务的保证就会相对较弱，尤其时无法保证 Commit的信息被精确消费。这是由于 Consumer可以通过 offset访问任意信息，而且不同的 Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。","tags":[]},{"title":"Kafka01-概述&shell操作","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka01-概述&shell操作.html","text":"概述Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 消息队列MQ：异步处理 使用消息队列的好处： 解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 缓冲 有助于控制和优化数据流经过系统的速度解决生产消息和消费消息的处理速度不一致 的情况。 灵活性 &amp; 峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪 费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 消息队列的 两种模式: 点对点模式 (一对一 ，消费者主动拉取数据，消息收到后消息清除) 消息生产者生产消息发送到 Queue中 然后消息消费者从 Queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布 /订阅模式 (一对多 ，消费者消费数据之后不会清除消息) 消息生产者（发布）将消息发布到 topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic的消息会被所有订阅者消费。 Kafka基础架构 Producer ：消息生产者，就是向kafka broker 发消息的客户端； Consumer ：消息消费者，向kafka broker 取消息的客户端； Consumer Group （CG）：消费者组，由多个consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者； Broker ：一台kafka 服务器就是一个broker。一个集群由多个broker 组成。一个broker可以容纳多个topic； Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic； Partition：为了实现扩展性，一个非常大的topic 可以分布到多个broker（即服务器）上，一个topic 可以分为多个partition，每个partition 是一个有序的队列； Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka提供了副本机制，一个 topic的每个分区都有若干个副本，一个 leader和若干个 follower； leader 每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader； follower 每个分区多个副本中的“从”，实时从 leader中同步数据，保持和 leader数据的同步。 leader发生故障时，某个 follower会成为新的 follower。 shell操作&#39;查看当前服务器中的所有 topic&#39; [atguigu@hadoop102 kafka]$ bin/kafka topics.sh --zookeeper hadoop102:2181 --list &#39;创建 topic&#39; bin/kafka topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first &#39;删除 topic&#39; [atguigu@hadoop102 kafka]$ bin/kafka topics.sh --zookeeper hadoop102:2181 --delete --topic first &#39;需要 server.properties中设置 delete.topic.enable=true否则只是标记删除。&#39; &#39;发送消息&#39; [atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first &#39;消费消息&#39; [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first &#39;查看某个Topic 的详情&#39; [atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first &#39;修改分区数&#39; [atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6","tags":[]},{"title":"Kafka03-API","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka03-API.html","text":"Producer API消息发送流程Kafka的 Producer发送消息采用的是 异步发送 的方式。在消息发送的过程中，涉及到了两个线程 main线程和 Sender线程 ，以及 一个线程共享变量 RecordAccumulator。main线程将消息发送给 RecordAccumulator Sender线程不断从 RecordAccumulator中拉取消息发送到 Kafka broker。 batch.size：只有数据积累到batch.size 之后，sender 才会发送数据。 linger.ms：如果数据迟迟未达到batch.size，sender 等待linger.time 之后就会发送数据。 异步发送API导入依赖: &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt; &lt;/dependency&gt; 编写代码: KafkaProducer类：需要创建一个生产者对象，用来发送数据 ProducerConfig类：获取所需的一系列配置参数 ProducerRecord类：每条数据都要封装成一个ProducerRecord 对象 &#39;不带回调函数的API:&#39; package com.atguigu.kafka; import org.apache.kafka.clients.producer.*; import java.util.Properties; import java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException,InterruptedException { Properties props = new Properties(); //kafka 集群， broker list props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;) props.put(&quot;acks&quot;, &quot;all&quot;); //重试次数 props.put(&quot;retries&quot;, 1); //批次大小 props.put(&quot;batch.size&quot;, 16384); //等待时间 props.put(&quot;linger.ms&quot;, 1); //RecordAccumulator 缓冲区大小 props.put(&quot;buffer.memory&quot;, 33554432); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); Producer &lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i))); } producer.close(); } } &#39;带回调函数的 API&#39; &quot;回调函数会在 producer收到 ack时调用，为异步调用， 该方法有两个参数，分别是RecordMetadata 和 Exception ，如果 Exception 为 null ，说明消息发送成功，如果Exception 不为 null ，说明消息发送失败。&quot; &quot;注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。&quot; package com.atguigu.kafka; import org.apache.kafka.clients.producer.*; import java.util.Properties; import java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException, InterruptedException { Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);//kafka 集群， broker list props.put(&quot;acks&quot;, &quot;all&quot;); props.put(&quot;retries&quot;, 1);// 重试次数 props.put(&quot;batch.size&quot;, 16384);// 批次大小 props.put(&quot;linger.ms&quot;, 1);// 等待时间 props.put(&quot;buffer.memory&quot;, 33554432);//RecordAccumulator 缓冲区大小 props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i)), new Callback() { // 回调函数， 该方法会在 Producer 收到 ack 时调用，为异步调用 @Override public void onCompletion(RecordMetadata metadata, Exception exception) { if (exception == null) { System.out.println(&quot;s uccess --&gt;&quot; + metadata.offset()); } else{ exception.printStackTrace(); } } }); } producer.close(); } } 同步发送 API同步发送的意思就是，一条消息发送之后，会阻塞当前线程， 直至返回 ack。 由于send方法返回的是一个 Future对象，根据 Futrue对象 的特点，我们也可以实现 同步发送的效果 ，只需在调用 Future对象的 get方发即可。 package com.atguigu.kafka; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; import java.util.Properties; import java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException, InterruptedException { Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);//kafka 集群， broker list props.put(&quot;acks&quot;, &quot;all&quot;); props.put(&quot;retries&quot;, 1);// 重试次数 props.put(&quot;batch.size&quot;, 16384);// 批次大小 props.put(&quot;linger.ms&quot;, 1);// 等待时间 props.put(&quot;buffer.memory&quot;, 33554432);//RecordAccumulator 缓冲区大小 props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i))).get();// .get() } } } Consumer APIConsumer消费数据时的可靠性是很容易保证的，因为数据在 Kafka中是持久化的，故不用担心数据丢失问题。 由于consumer在消费过程中可能会出现断电宕机等故障， consumer恢复后，需要从故障前的位置的继续消费，所以 consumer需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。 所以offset的维护是 Consumer消费数据是必须考虑的问题。 自动提交 offset导入依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt; &lt;/dependency&gt; KafkaConsumer类： 需要创建一个 消费 者对象，用来 消费 数据 ConsumerConfig类： 获取所需的一系列配置参数 ConsuemrRecord类： 每条数据都要封装成一个 ConsumerRecord对象 enable.auto.commit 是否开启自动提交 offset功能 auto.commit.interval.ms 自动提交 offset的时间间隔 //以下为自动提交offset的代码 package com.atguigu.kafka; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays; import java.util.Properties; public class Custom Consumer { public static void main(String[] args) { Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); props.put(&quot;group.id&quot;, &quot;test&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(&quot;first&quot;)); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerR ecord&lt;String, String&gt; record : records){ System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value()); } } } } 手动提交 offset虽然自动提交offset十分简介便利，但由于其是基于时间提交的 开发人员难以把握offset提交的时机。因此 Kafka还提供了手动提交 offset的 API。 手动提交offset的方法有两种：分别是 commitSync（同步提交） 和 commitAsync（异步提交） 。两者的相同点是，都会将本次 poll的一批数据最高的偏移量提交 ；不同点是commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（ 由不可控因素导致，也会出现提交失败 ）；而 commitAsync则没有失败重试机制，故有可能提交失败。 // 由于同步提交 offset有失败重试机制，故更加可靠 ，以下为同步提交 offset的示例。 package com.atguigu.kafka.consumer; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays; import java.util.Properties; public class CustomC omsumer { public static void main(String[] args) { Properties props = new Properties(); //Kafka 集群 props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); // 消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(&quot;group.id&quot;, &quot;test&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);// 关闭自动提交 offset props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDecerializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDecerializer&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(&quot;first&quot;));// 消费者订阅主题 while (true) { //消费者拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.printf(&quot;offset = %d, key = %s, value= %s%n&quot;, record.offset(), record.key(), value()); } // 同步提交，当前线程会阻塞 直到 offset 提交成功 consumer.commitSync(); } } } // 异步提交 offset // 虽然同步提交 offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此 吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交 offset的方式。 package com.atguigu.kafka.consumer; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays; import java.util.Properties; public class CustomC omsumer { public static void main(String[] args) { Properties props = new Properties(); //Kafka 集群 props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); // 消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(&quot;group.id&quot;, &quot;test&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);// 关闭自动提交 offset props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDecerializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDecerializer&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(&quot;first&quot;));// 消费者订阅主题 while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);// 消费者拉取数据 for (ConsumerRecord&lt;String, String&gt; record : records){ System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value()); } //异步提交 consumer.commitAsync(new OffsetCommitCallback(){ @Override public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) { if (exception != null) { System.err.println(&quot;Commit failed for&quot; + offsets); } } }); } } } 数据漏消费和重复消费分析: 无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交 offset后消费，有可能造成数据的漏消费；而先消费后提交 offset，有可能会造成数据的重复消费。 自定义存储 offsetKafka 0.9版本之前， offset存储在 zookeeper 0.9版本 及 之后，默认将 offset存储在 Kafka的一个内置的 topic中。除此之外， Kafka还可以选择自定义存储 offset。 offset的维护是相当繁琐的， 因为需要考虑到 消费者的 Rebalace。当有新的消费者加入消费者组、已有的消费者推出消费者组 或者所订阅的主题的分区发生变化 ，就会触发到分区的重新分配，重新分配的过程叫做 Rebalance。 消费者发生Rebalance之后，每个消费者消费的分区就会发生变 化。 因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的offset位置继续消费。 要实现自定义存储offset，需要借助 ConsumerRebalanceListener 以 下为 示例代码 ，其中提交和获取 offset的方法，需要根据所选的 offset存储系统自行实现。 package com.atguigu.kafka.consumer; import org.apache.kafka.clients.consumer.*; import org.apache.kafka.common.TopicPartition; import java.util.*; public class CustomCo nsumer { private static Map&lt;TopicPartition, Long&gt; currentOffset = new HashMap&lt;&gt;(); public static void main(String[] args) { //创建配置信息 Properties props = new Properties(); //Kafka 集群 props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); //消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(&quot;group.id&quot;, &quot;test&quot;); //关闭自动提交 offset props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); //Key 和 Value 的反序列化类 props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserilizer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserilizer&quot;); // 创建一个消费者 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // 消费者订阅 主题 consumer.subscribe(Arrays.asList(&quot;first&quot;), new ConsumerRebalanceListener() { // 该方法会在 Rebalance 之前调用 @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) { commitOffset(currentOffset); } // 该方法会在 Rebalance 之后调用 @Override public void onPartitionsAssigned(Collection&lt; TopicPartition&gt; partitions) { currentOffset.clear(); for (TopicPartition partition : partitions) { consumer.seek(partition, getOffset(partition));//定位到最近提交的 offset 位置继续消费 } } }); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);// 消费者拉取数据 for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.printf(&quot;offset = %d, key = %s, value = %s%,&quot; record.offset(), record.key(), record.value()); currentOffset.put(new TopicPartition(record.topic(), record.partition()), record.offset()); } commitOffset(currentOffset);// 异步提交 } } // 获取某分区的最新 offset private static long getOffset(TopicPartition partition) { return 0; } // 提交该消费者所有分区的 offset private static void commitOffset(Map&lt;TopicPartition, Long&gt; currentOffset) {} } 自定义 Interceptor拦截器原理Producer拦截器 (interceptor)是在 Kafka 0.10版本被引入的，主要用于实现 clients端的定制化控制逻辑。 对于producer而言， interceptor使得用户在消息发送前以及 producer回调逻辑前有机会对消息做一些定制化需求，比如 修改消息 等。同时， producer允许用户指定多个 interceptor按序作用于同一条消息从而形成一个拦截链 (interceptor chain)。 Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其 定义的方法包括： configure(configs) 获取配置 信息 和 初始化数据时调用 onSend(ProducerRecord) 该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。 Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的 topic和分区， 否则会影响目标分区的计算 。 onAcknowledgement(RecordMetadata,Exception) 该方法会在消息从 RecordAccumulator成功 发送到 Kafka Broker之后，或者在发送过程中失败时调用。 并且通常都是在 producer回调逻辑触发之前。 onAcknowledgement运行在producer的 IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢 producer的消息发送效率 close 关闭interceptor，主要用于执行一些资源清理工作 如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外 倘若指定了多个 interceptor，则 producer将按照指定顺序调用它们 ，并仅仅是捕获每个 interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。 拦截器案例实现一个简单的双interceptor组成的拦截链。第一个 interceptor会在消息发送前将时间戳信息加到消息 value的最前部；第二个 interceptor会在消息发送后更新成功发送消息数或失败发送消息数。 增加时间戳拦截器 package com.atguigu.kafka.interceptor; import java.util.Map; import org.apache.kafka.clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apac he.kafka.clients.producer.RecordMetadata; public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; { @Override public void configure(Map&lt;String, ?&gt; configs) {} @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) { // 创建一个新的 record ，把时间戳写入消息体的最前部 return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(), System.currentTimeMillis() + &quot;,&quot; + record.value().toString()); } @Over ride public void onAcknowledgement(RecordMetadata metadata, Exception exception) {} @Override public void close() {} } 统计发送消息成功和发送失败消息数 ，并在 producer关闭时打印这两个计数器 package com.atguigu.kafka.interceptor; import java.util.Map; import org.apache.kafka. clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;{ private int errorCounter = 0; private int successCounter = 0; @Override public void configure(Map&lt;String, ?&gt; configs) {} @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) { return record; } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { // 统计成功和失败的次数 if (exception == null) { successCounter++; } else{ errorCounter++; } } @Override public void cl ose() { // 保存结果 System.out.println(&quot;Successful sent: &quot; + successCounter); System.out.println(&quot;Failed sent: &quot; + errorCounter); } } producer主程序 package com.atguigu.kafka.interceptor; import java.util.ArrayList; import java.util.List; import java.util.Properties; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.Produc erRecord; public class InterceptorProducer { public static void main(String[] args) throws Exception { // 1 设置配置信息 Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); props.put(&quot;acks&quot;, &quot;all&quot;); props.put(&quot;retr ies&quot;, 3); props.put(&quot;batch.size&quot;, 16384); props.put(&quot;linger.ms&quot;, 1); props.put(&quot;buffer.memory&quot;, 33554432); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 2 构建拦截链 List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add(&quot;com.atguigu.kafka.interceptor.TimeInterceptor&quot;); interceptors.add(&quot;com.atguigu.kafka.interceptor.CounterInterce ptor&quot;); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); String topic = &quot;first&quot;; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); // 3 发送消息 for (int i = 0; i &lt; 10; i++) { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, &quot;message&quot; + i); producer.send(record); } // 4 一定要关闭 producer ，这样才会调用 interceptor 的 close 方法 producer.close(); } } 测试 在 kafka上启动消费者 然后运行 客户端 java程序。 [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh—bootstrap-server hadoop102: 9092 —from-beginning —topicfirst 1501904047034,message0 1501904047225,message1 1501904047230,message2 1501904047234,message3 1501904047236,message4 1501904047240,message5 1501904047243,message6 1501904047246,message7 1501904047249,message8 1501904047252,mes sage 9","tags":[]},{"title":"Kafka01-概述&shell操作","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka04-监控.html","text":"概述Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 消息队列MQ：异步处理 使用消息队列的好处： 解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 缓冲 有助于控制和优化数据流经过系统的速度解决生产消息和消费消息的处理速度不一致 的情况。 灵活性 &amp; 峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪 费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 消息队列的 两种模式: 点对点模式 (一对一 ，消费者主动拉取数据，消息收到后消息清除) 消息生产者生产消息发送到 Queue中 然后消息消费者从 Queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布 /订阅模式 (一对多 ，消费者消费数据之后不会清除消息) 消息生产者（发布）将消息发布到 topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic的消息会被所有订阅者消费。 Kafka基础架构 Producer ：消息生产者，就是向kafka broker 发消息的客户端； Consumer ：消息消费者，向kafka broker 取消息的客户端； Consumer Group （CG）：消费者组，由多个consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者； Broker ：一台kafka 服务器就是一个broker。一个集群由多个broker 组成。一个broker可以容纳多个topic； Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic； Partition：为了实现扩展性，一个非常大的topic 可以分布到多个broker（即服务器）上，一个topic 可以分为多个partition，每个partition 是一个有序的队列； Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka提供了副本机制，一个 topic的每个分区都有若干个副本，一个 leader和若干个 follower； leader 每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader； follower 每个分区多个副本中的“从”，实时从 leader中同步数据，保持和 leader数据的同步。 leader发生故障时，某个 follower会成为新的 follower。 shell操作&#39;查看当前服务器中的所有 topic&#39; [atguigu@hadoop102 kafka]$ bin/kafka topics.sh --zookeeper hadoop102:2181 --list &#39;创建 topic&#39; bin/kafka topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first &#39;删除 topic&#39; [atguigu@hadoop102 kafka]$ bin/kafka topics.sh --zookeeper hadoop102:2181 --delete --topic first &#39;需要 server.properties中设置 delete.topic.enable=true否则只是标记删除。&#39; &#39;发送消息&#39; [atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first &#39;消费消息&#39; [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first &#39;查看某个Topic 的详情&#39; [atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first &#39;修改分区数&#39; [atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6","tags":[]},{"title":"Kafka05-Flume对接Kafka","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka05-Flume对接Kafka.html","text":"配置 flume(flume-kafka.conf) &#39;define&#39; a1.sources = r1 a1.sinks = k1 a1.channels = c1 &#39; source&#39; a1.sources.r1.type = exec a1.sources.r1.command = tail -F -c +0 /opt/module/data/flume.log a1.sources.r1.shell = /bin/bash -c &#39;sink&#39; a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:90 92,hadoop104:9092 a1.sinks.k1.kafka.topic = first a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 a1.sinks.k1.kafka.producer.linger.ms = 1 &#39;channel&#39; a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 &#39;bind&#39; a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 flume $ bin/flume-ng agent -c conf/ -n a1 -f jobs/flume kafka.conf 向 /opt/module/data/flume.log里追加数据，查看 kafka消费者消费情况 $ echo hello &gt;&gt;&gt; /opt/module/data/flume.log","tags":[]},{"title":"HBase01-概述","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase01-概述.html","text":"NoSQL简介关系型数据库的查询瓶颈: 当用户表的数据达到几千万甚至几亿级别的时候，对单条数据的检索将花费数秒甚至达到分钟级别。实际情况更复杂，查询的操作速度将会受到以下两个因素的影响： ①高并发的更新(插入、修改、删除)操作。大中型网站的并发操作一般能达到几十乃至几百并发，此时单条数据查询的延时将轻而易举地达到分钟级别。 ②多表关联后的复杂查询，以及频繁的group by或者order by操作，此时，性能下降较为明显。 分摊读写压力的有效方式是将单个关系型数据库扩展为分布式数据库。但是，随之而来的问题则是很难保证原子性。没有了原子性，事务也无从谈起，关系型数据库也就没有了存在的意义。 CAP定理: 20世纪90年代初期Berkerly大学有位Eric Brewer教授提出了一个CAP理论。全称是Consistency Availability and Partition tolerance。 Consistency（强一致性）：数据更新操作的一致性，所有数据变动都是同步的。 Availability（高可用性）：良好的响应性能。 Partition tolerance（高分区容错性）：可靠性。 Brewer教授给出的定理是：任何分布式系统只可同时满足二点，没法三者兼顾。 Brewer教授给出的忠告是：架构师不要将精力浪费在如何设计能满足三者的完美分布式系统，而是应该进行取舍。所以专家们始终没有办法构建出一个既有完美原子性又兼具高性能的分布式数据库。 NoSQL: 些数据库在实现性能的同时会牺牲一部分一致性，即数据在更新时，不会立刻同步，而是经过了一段时间才达到一致性。这个特性也称之为最终一致性！例如你发了一条朋友圈，你的一部分朋友立马看到了这条信息，而另一部分朋友可能要等到1分钟之后才能刷出这条消息。虽然有延时，但是对于这样一个社交的场景，这个延时是可以容忍的。而如果使用传统关系型数据库，可能这些即时通信软件就早已崩溃！ NoSQL数据库最初指不使用SQL标准的数据库，现在泛指非关系型数据库。NoSQL一词最早出现于1998年，是Carlo Strozzi开发的一个轻量、开源、不提供SQL功能的数据库。 现在NoSQL被普遍理解理解为“Not Only SQL”，意为不仅仅是SQL。NoSQL和传统的关系型数据库在很多场景下是相辅相成的，谁也不能完全替代谁。 HBase简介2006年Google技术人员Fay Chang发布了一篇文章Bigtable: ADistributed Storage System for Structured Data。该文章向世人介绍了一种分布式的数据库，这种数据库可以在局部几台服务器崩溃的情况下继续提供高性能的服务。 HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。 Hbase面向列存储，构建于Hadoop之上，类似于Google的BigTable，提供对10亿级别表数据的快速随机实时读写！ HBase的特点 海量存储 HBase适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到百毫秒内返回数据。这与HBase的极易扩展性息息相关。正式因为HBase良好的扩展性，才为海量数据的存储提供了便利。 列式存储 这里的列式存储其实说的是列族存储，HBase是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。 极易扩展 HBase的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。 通过横向添加RegionSever的机器，进行水平扩展，提升HBase上层的处理能力，提升Hbsae服务更多Region的能力。 高并发 由于目前大部分使用HBase的架构，都是采用的廉价PC，因此单个IO的延迟其实并不小，一般在几十到上百ms之间。这里说的高并发，主要是在并发的情况下，HBase的单个IO延迟下降并不多。能获得高并发、低延迟的服务。 稀疏 稀疏主要是针对HBase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。 Hbase的优缺点优点： ① HDFS有高容错，高扩展的特点，而Hbase基于HDFS实现数据的存储，因此Hbase拥有与生俱来的超强的扩展性和吞吐量。 ② HBase采用的是Key/Value的存储方式，这意味着，即便面临海量数据的增长，也几乎不会导致查询性能下降。 ③ HBase是一个列式数据库，相对于于传统的行式数据库而言。当你的单张表字段很多的时候，可以将相同的列(以regin为单位)存在到不同的服务实例上，分散负载压力。 缺点： ① 架构设计复杂，且使用HDFS作为分布式存储，因此只是存储少量数据，它也不会很快。在大数据量时，它慢的不会很明显！ ② Hbase不支持表的关联操作，因此数据分析是HBase的弱项。常见的 group by或order by只能通过编写MapReduce来实现！ ③ Hbase部分支持了ACID 适合场景：单表超千万，上亿，且高并发！ 不适合场景：主要需求是数据分析，比如做报表。数据量规模不大，对实时性要求高！ HBase数据模型逻辑结构逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构（K-V）来看，HBase更像是一个multi-dimensional map。 物理存储结构 数据模型 Name Space 命名空间，类似于关系型数据库的database概念，每个命名空间下有多个表。HBase两个自带的命名空间，分别是 hbase和 default，hbase中存放的是 HBase内置的表，default表是用户默认使用的命名空间。 一个表可以自由选择是否有命名空间，如果创建表的时候加上了命名空间后，这个表名字以&lt;Namespace&gt;:&lt;Table&gt;作为区分！ Table 类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，数据属性，比如超时时间（TTL），压缩算法（COMPRESSION）等，都在列族的定义中定义，不需要声明具体的列。 这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。 Row HBase表中的每行数据都由一个RowKey和多个Column（列）组成。一个行包含了多个列，这些列通过列族来分类,行中的数据所属列族只能从该表所定义的列族中选取,不能定义这个表中不存在的列族，否则报错NoSuchColumnFamilyException。 RowKey Rowkey由用户指定的一串不重复的字符串定义，是一行的唯一标识！数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。 如果使用了之前已经定义的RowKey，那么会将之前的数据更新掉！ Column Family 列族是多个列的集合。一个列族可以动态地灵活定义多个列。表的相关属性大部分都定义在列族上，同一个表里的不同列族可以有完全不同的属性配置，但是同一个列族内的所有列都会有相同的属性。 列族存在的意义是HBase会把相同列族的列尽量放在同一台机器上，所以说，如果想让某几个列被放到一起，你就给他们定义相同的列族。 官方建议一张表的列族定义的越少越好，列族太多会极大程度地降低数据库性能，且目前版本Hbase的架构，容易出BUG。 Column Qualifier Hbase中的列是可以随意定义的，一个行中的列不限名字、不限数量，只限定列族。因此列必须依赖于列族存在！列的名称前必须带着其所属的列族！例如info：name，info：age。 因为HBase中的列全部都是灵活的，可以随便定义的，因此创建表的时候并不需要指定列！列只有在你插入第一条数据的时候才会生成。其他行有没有当前行相同的列是不确定，只有在扫描数据的时候才能得知！ TimeStamp 用于标识数据的不同版本（version）。时间戳默认由系统指定，也可以由用户显式指定。 在读取单元格的数据时，版本号可以省略，如果不指定，Hbase默认会获取最后一个版本的数据返回！ Cell 一个列中可以存储多个版本的数据。而每个版本就称为一个单元格（Cell）。 Cell由{rowkey, column Family：column Qualifier, time Stamp}确定。 Cell中的数据是没有类型的，全部是字节码形式存贮。 Region Region由一个表的若干行组成！在Region中行的排序按照行键（rowkey）字典排序。 Region不能跨RegionSever，且当数据量大的时候，HBase会拆分Region。 Region由RegionServer进程管理。HBase在进行负载均衡的时候，一个Region有可能会从当前RegionServer移动到其他RegionServer上。 Region是基于HDFS的，它的所有数据存取操作都是调用了HDFS的客户端接口来实现的。 HBase基本架构 Region Server RegionServer是一个服务，负责多个Region的管理。其实现类为HRegionServer，主要作用如下: 对于数据的操作：get, put, delete； 对于Region的操作：splitRegion、compactRegion。 客户端从ZooKeeper获取RegionServer的地址，从而调用相应的服务，获取数据。 Master Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下： 对于表的操作：create, delete, alter，这些操作可能需要跨多个ReginServer，因此需要Master来进行协调！ 对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。 即使Master进程宕机，集群依然可以执行数据的读写，只是不能进行表的创建和修改等操作！当然Master也不能宕机太久，有很多必要的操作，比如创建表、修改列族配置，以及更重要的分割和合并都需要它的操作。 Zookeeper RegionServer非常依赖ZooKeeper服务，ZooKeeper管理了HBase所有RegionServer的信息，包括具体的数据段存放在哪个RegionServer上。 客户端每次与HBase连接，其实都是先与ZooKeeper通信，查询出哪个RegionServer需要连接，然后再连接RegionServer。Zookeeper中记录了读取数据所需要的元数据表 hbase:meata,因此关闭Zookeeper后，客户端是无法实现读操作的！ HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。 HDFS HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。","tags":[]},{"title":"Kafka06-Kafka面试题","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/09-Kafka/Kafka06-面试题.html","text":"Kafka中的 ISR(InSyncRepli)、 OSR(OutSyncRepli)、 AR(AllRepli)代表什么？ ISR：与 leader保持同步的follower集合 AR：分区的所有副本 Kafka中的 HW、 LEO等分别代表什么？ LEO：没个副本的最后条消息的offset HW：一个分区中所有副本最小的offset Kafka中是怎么体现消息顺序性的？ 每个分区内，每条消息都有一个offset，故只能保证分区内有序。 Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？ 拦截器 -&gt; 序列化器 -&gt; 分区器 Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？ “消费组中的消费者个数如果超过 topic的分区，那么就会有消费者消费不到数据”这句话是否正确？ 正确 消费者提交消费位移时提交的是当前消费到的最新消息的 offset还是 offset+1 offset+1 有哪些情形会造成重复消费？ 那些情景会造成消息漏消费？ 先提交offset，后消费，有可能造成数据的重复 当你使用kafka-topics.sh 创建（删除）了一个topic 之后，Kafka 背后会执行什么逻辑？ 1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点,如：/brokers/topics/first 2）触发Controller的监听程序 3）kafka Controller 负责topic的创建工作，并更新metadata cache topic 的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？ 可以增加 bin/kafka-topics.sh —zookeeper localhost:2181/kafka —alter —topic topic-config —partitions 3 topic 的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？ 不可以减少，现有的分区数据难以处理。 Kafka 有内部的topic 吗？如果有是什么？有什么所用？ __consumer_offsets,保存消费者offset Kafka 分区分配的概念？ 一个topic多个分区，一个消费者组多个消费者，故需要将分区分配个消费者(roundrobin、range) 简述Kafka 的日志目录结构？ 每个分区对应一个文件夹，文件夹的命名为topic-0，topic-1，内部为.log和.index文件 如果我指定了一个offset，Kafka Controller 怎么查找到对应的消息？ 聊一聊Kafka Controller 的作用？ 负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作 Kafka 中有那些地方需要选举？这些地方的选举策略又有哪些？ partition leader（ISR），controller（先到先得） 失效副本是指什么？有那些应对措施？ 不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入 Kafka 的哪些设计让它有如此高的性能？ 分区，顺序写磁盘，0-copy","tags":[]},{"title":"HBase00-安装","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase00-安装.html","text":"启动Zookeeper 启动Hadoop：hdfs和yarn HBase的解压 tar -zxvf HBase-1.3.1-bin.tar.gz -C /opt/module 修改HBase的配置文件 HBase-env.sh export JAVA_HOME=/opt/module/jdk1.6.0_144 export HBASE_MANAGES_ZK=false HBase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181, hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.10/zkData&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; regionservers: hadoop102 hadoop103 hadoop104 软连接hadoop配置文件到HBase [atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/HBase/conf/core-site.xml [atguigu@hadoop102 module]$ ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/HBase/conf/hdfs-site.xml 分发HBase到其他节点 HBase服务的启动 启动方式1 bin/HBase-daemon.sh start master bin/HBase-daemon.sh start regionserver 提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。 修复提示： a、同步时间服务 b、属性：hbase.master.maxclockskew设置更大的值 &lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;Time difference of regionserver from master&lt;/description&gt; &lt;/property&gt; 启动方式2 bin/start-HBase.sh bin/stop-HBase.sh 查看HBase页面 启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：http://hadoop102:16010","tags":[]},{"title":"HBase03-HBase进阶","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase03-HBase进阶.html","text":"RegionServer 架构 StoreFile 保存实际数据的物理文件，StoreFile以Hfile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。 MemStore 写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。 WAL 由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 每间隔hbase.regionserver.optionallogflushinterval(默认1s)， HBase会把操作从内存写入WAL。 一个RegionServer上的所有Region共享一个WAL实例。 WAL的检查间隔由hbase.regionserver.logroll.period定义，默认值为1小时。检查的内容是把当前WAL中的操作跟实际持久化到HDFS上的操作比较，看哪些操作已经被持久化了，被持久化的操作就会被移动到.oldlogs文件夹内（这个文件夹也是在HDFS上的）。一个WAL实例包含有多个WAL文件。WAL文件的最大数量通过hbase.regionserver.maxlogs（默认是32）参数来定义。 BlockCache 读缓存，每次查询出的数据会缓存在BlockCache中，方便下次查询。 写流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。 与目标Region Server进行通讯； 将数据顺序写入（追加）到WAL； 将数据写入对应的MemStore，数据会在MemStore进行排序； 向客户端发送ack； 等达到MemStore的刷写时机后，将数据刷写到HFile。 读流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。 与目标Region Server进行通讯； 分别在Block Cache（读缓存），MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。 将查询到的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。 将合并后的最终结果返回给客户端。 MemStore Flush MemStore存在的意义是在写入HDFS前，将其中的数据整理有序。 MemStore刷写时机： 当某个memstore的大小达到了hbase.hregion.memstore.flush.size（默认值128M），其所在region的所有memstore都会刷写。 当memstore的大小达到了 hbase.hregion.memstore.flush.size（默认值128M） * hbase.hregion.memstore.block.multiplier（默认值4）时，会阻止继续往该memstore写数据。 当region server中memstore的总大小达到 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值0.95），region会按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下。 当region server中memstore的总大小达到 java_heapsize* hbase.regionserver.global.memstore.size（默认值0.4） 时，会阻止继续往所有的memstore写数据。 到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置hbase.regionserver.optionalcacheflushinterval（默认1小时）。 当WAL文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log以下（该属性名已经废弃，现无需手动设置，最大值为32） StoreFile Compaction由于Hbase依赖HDFS存储，HDFS只支持追加写。所以，当新增一个单元格的时候，HBase在HDFS上新增一条数据。当修改一个单元格的时候，HBase在HDFS又新增一条数据，只是版本号比之前那个大（或者自定义）。当删除一个单元格的时候，HBase还是新增一条数据！只是这条数据没有value，类型为DELETE，也称为墓碑标记（Tombstone） HBase每间隔一段时间都会进行一次合并（Compaction），合并的对象为HFile文件。合并分为两种 minor compaction和major compaction。 在HBase进行major compaction的时候，它会把多个HFile合并成1个HFile，在这个过程中，一旦检测到有被打上墓碑标记的记录，在合并的过程中就忽略这条记录。这样在新产生的HFile中，就没有这条记录了，自然也就相当于被真正地删除了。 由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。 Compaction分为两种，分别是Minor Compaction和Major Compaction。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据。 Region Split默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。 Region Split时机： 当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize，该Region就会进行拆分（0.94版本之前）。 0.94版本之后的切分策略 默认使用IncreasingToUpperBoundRegionSplitPolicy策略切分region，getSizeToCheck()是检查region的大小以判断是否满足切割切割条件 protected long getSizeToCheck(final int tableRegionsCount) { // safety check for 100 to avoid numerical overflow in extreme cases return tableRegionsCount == 0 || tableRegionsCount &gt; 100 ? getDesiredMaxFileSize() : Math.min(getDesiredMaxFileSize(), initialSize * tableRegionsCount * tableRegionsCount * tableRegionsCount); } // tableRegionsCount：为当前Region Server中属于该Table的region的个数。 // getDesiredMaxFileSize() 这个值是hbase.hregion.max.filesize参数值，默认为10GB。 // initialSize的初始化比较复杂，由多个参数决定。 @Override protected void configureForRegion(HRegion region) { super.configureForRegion(region); Configuration conf = getConf(); //默认hbase.increasing.policy.initial.size 没有在配置文件中指定 initialSize = conf.getLong(&quot;hbase.increasing.policy.initial.size&quot;, -1); if (initialSize &gt; 0) { return; } // 获取用户表中自定义的memstoreFlushSize大小，默认也为128M HTableDescriptor desc = region.getTableDesc(); if (desc != null) { initialSize = 2 * desc.getMemStoreFlushSize(); } // 判断用户指定的memstoreFlushSize是否合法，如果不合法，则为hbase.hregion.memstore.flush.size，默认为128. if (initialSize &lt;= 0) { initialSize = 2 * conf.getLong(HConstants.HREGION_MEMSTORE_FLUSH_SIZE, HTableDescriptor.DEFAULT_MEMSTORE_FLUSH_SIZE); } } /* 具体的切分策略为tableRegionsCount在0和100之间，则为 initialSize（默认为2*128） * tableRegionsCount^3,例如： 第一次split：1^3 * 256 = 256MB 第二次split：2^3 * 256 = 2048MB 第三次split：3^3 * 256 = 6912MB 第四次split：4^3 * 256 = 16384MB &gt; 10GB，因此取较小的值10GB 后面每次split的size都是10GB了。 tableRegionsCount超过100个，则超过10GB才会切分region。 */ hbase.regionserver.region.split.policy：","tags":[]},{"title":"HBase02-HBase-shell操作","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase02-HBase-shell操作.html","text":"使用hbase shell可以进入一个shell命令行界面：bin/HBase shell 集群的操作&#39;查看集群状态&#39; 使用status可以查看集群状态，默认为summary，可以选择‘simple’和‘detailed’来查看详情 hbase(main):011:0&gt; status 1 active master, 0 backup masters, 3 servers, 0 dead, 0.6667 average load &#39;查看版本&#39; hbase(main):002:0&gt; version 1.3.1, r930b9a55528fe45d8edce7af42fef2d35e77677a, Thu Apr 6 19:36:54 PDT 2017 &#39;查看操作用户及组信息&#39; hbase(main):003:0&gt; whoami atguigu (auth:SIMPLE) groups: atguigu &#39;查看表操作信息&#39; hbase(main):004:0&gt; table_help Help for table-reference commands. &#39;查看帮助信息&#39; hbase(main):005:0&gt; help HBase Shell, version 1.3.1, r930b9a55528fe45d8edce7af42fef2d35e77677a, Thu Apr 6 19:36:54 PDT 2017 Type &#39;help &quot;COMMAND&quot;&#39;, (e.g. &#39;help &quot;get&quot;&#39; -- the quotes are necessary) for help on a specific command. Commands are grouped. Type &#39;help &quot;COMMAND_GROUP&quot;&#39;, (e.g. &#39;help &quot;general&quot;&#39;) for help on a command group. &#39;查看具体命令的帮助&#39; &#39;注意引号是必须的！&#39; hbase(main):006:0&gt; help &#39;get&#39; Get row or cell contents; pass table name, row, and optionally a dictionary of column(s), timestamp, timerange and versions. Examples: hbase&gt; get &#39;ns1:t1&#39;, &#39;r1&#39; 表的操作 list hbase(main):008:0&gt; list TABLE 0 row(s) in 0.0410 seconds &#39;list后可以使用*等通配符来进行表的过滤！&#39; create 创建表时，需要指定表名和列族名，而且至少需要指定一个列族，没有列族的表是没有任何意义的。 创建表时，还可以指定表的属性，表的属性需要指定在列族上！ 格式： ​ create ‘表名’, { NAME =&gt; ‘列族名1’, 属性名 =&gt; 属性值}, {NAME =&gt; ‘列族名2’, 属性名 =&gt; 属性值}, … 如果你只需要创建列族，而不需要定义列族属性，那么可以采用以下快捷写法： ​ create’表名’,’列族名1’ ,’列族名2’, … HBase(main):002:0&gt; create &#39;student&#39;,&#39;info&#39; desc hbase(main):003:0&gt; describe &#39;person&#39; hbase(main):004:0&gt; desc &#39;person&#39; disable 停用表后，可以防止在对表做一些维护时，客户端依然可以持续写入数据到表。一般在删除表前，必须停用表。 在对表中的列族进行修改时，也需要停用表. disable_all ‘正则表达式’ 可以使用正则来匹配表名。 is_disabled 可以用来判断表是否被停用。 hbase(main):005:0&gt; disable &#39;person&#39; 0 row(s) in 2.4250 seconds enable 和停用表类似。enable ‘表名’用来启用表，is_enabled ‘表名’用来判断一个表是否被启用。 enable_all ‘正则表达式’可以通过正则来过滤表，启用复合条件的表。 exists hbase(main):008:0&gt; exists &#39;person&#39; Table person does exist 0 row(s) in 0.0210 seconds count hbase(main):012:0&gt; count &#39;person&#39; 1 row(s) in 0.0240 seconds =&gt; 1 drop 删除表前，需要先disable表，否则会报错。ERROR: Table xxx is enabled. Disable it first. hbase(main):011:0&gt; drop &#39;person&#39; truncate hbase(main):013:0&gt; truncate &#39;person&#39; Truncating &#39;person&#39; table (it may take a while): - Disabling table... - Truncating table... 0 row(s) in 4.0010 seconds get_split hbase(main):015:0&gt; get_splits &#39;person&#39; Total number of splits = 1 =&gt; [] 获取表所对应的Region个数。每个表在一开始只有一个region，之后记录增多后，region会被自动拆分。 alter alter命令可以修改表的属性，通常是修改某个列族的属性。 ​ alter ‘表名’， ‘delete’ =&gt; ‘列族名’ hbase(main):050:0&gt; alter &#39;myns:t1&#39;,{NAME =&gt; &#39;info&#39;,VERSIONS =&gt; &#39;5&#39;} hbase&gt; alter &#39;ns1:t1&#39;, &#39;delete&#39; =&gt; &#39;f1&#39; 数据的操作 scan scan命令可以按照 rowkey 的字典顺序来遍历指定的表的数据。 scan ‘表名’：默认当前表的所有列族。 scan ‘表名’,{COLUMNS=&gt; [‘列族:列名’],…} ： 遍历表的指定列 scan ‘表名’, { STARTROW =&gt; ‘起始行键’, ENDROW =&gt; ‘结束行键’ }：指定 rowkey 范围。如果不指定，则会从表的开头一直显示到表的结尾。区间为左闭右开。 scan ‘表名’, { LIMIT =&gt;行数量}：指定返回的行的数量 scan ‘表名’, {VERSIONS =&gt; 版本数}：返回 cell 的多个版本 scan ‘表名’, { TIMERANGE =&gt; [最小时间戳,最大时间戳]}：指定时间戳范围 ​ 注意：此区间是一个左闭右开的区间，因此返回的结果包含最小时间戳的记录，但是不包含最大时间戳记录 scan ‘表名’, { RAW =&gt; true, VERSIONS =&gt;版本数} ​ 显示原始单元格记录，在Hbase中，被删掉的记录在HBase被删除掉的记录并不会立即从磁盘上清除，而是先被打上墓碑标记，然后等待下次major compaction的时候再被删除掉。注意RAW参数必须和VERSIONS一起使用，但是不能和COLUMNS参数一起使用。 scan ‘表名’, { FILTER =&gt; “过滤器”} and|or { FILTER =&gt; “过滤器”}: 使用过滤器扫描 HBase(main):008:0&gt; scan &#39;student&#39; HBase(main):009:0&gt; scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;, STOPROW =&gt; &#39;1001&#39;} HBase(main):010:0&gt; scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;} put put可以新增记录还可以为记录设置属性。 put ‘表名’, ‘行键’, ‘列名’, ‘值’ put ‘表名’, ‘行键’, ‘列名’, ‘值’,时间戳 put ‘表名’, ‘行键’, ‘列名’, ‘值’, { ‘属性名’ =&gt; ‘属性值’} put ‘表名’, ‘行键’, ‘列名’, ‘值’,时间戳, { ‘属性名’ =&gt;’属性值’} HBase(main):012:0&gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Nick&#39; HBase(main):003:0&gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:sex&#39;,&#39;male&#39; HBase(main):004:0&gt; put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;18&#39; HBase(main):005:0&gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:name&#39;,&#39;Janna&#39; HBase(main):006:0&gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:sex&#39;,&#39;female&#39; HBase(main):007:0&gt; put &#39;student&#39;,&#39;1002&#39;,&#39;info:age&#39;,&#39;20&#39; get get支持scan所支持的大部分属性，如COLUMNS，TIMERANGE，VERSIONS，FILTER HBase(main):014:0&gt; get &#39;student&#39;,&#39;1001&#39; HBase(main):015:0&gt; get &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39; delete &#39;删除某rowkey的全部数据：&#39; HBase(main):016:0&gt; deleteall &#39;student&#39;,&#39;1001&#39; &#39;删除某rowkey的某一列数据：&#39; HBase(main):017:0&gt; delete &#39;student&#39;,&#39;1002&#39;,&#39;info:sex&#39;","tags":[]},{"title":"HBase05-MR","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase05-HBase-MR.html","text":"MapReduce统计的需要：我们知道HBase的数据都是分布式存储在RegionServer上的，所以对于类似传统关系型数据库的group by操作，扫描器是无能为力的，只有当所有结果都返回到客户端的时候，才能进行统计。这样做一是慢，二是会产生很大的网络开销，所以使用MapReduce在服务器端就进行统计是比较好的方案。 性能的需要：说白了就是“快”！如果遇到较复杂的场景，在扫描器上添加多个过滤器后，扫描的性能很低；或者当数据量很大的时候扫描器也会执行得很慢，原因是扫描器和过滤器内部实现的机制很复杂，虽然使用者调用简单，但是服务器端的性能就不敢保证了 通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。 官方HBase-MapReduce 查看HBase的MapReduce任务的执行 bin/HBase mapredcp 环境变量的导入 让Hadoop加载Hbase的jar包，最简单的就是把HBase的jar包复制到Hadoop的lib里面，或者把HBase的包地址写到Hadoop的环境变量里面 临时生效，执行环境变量的导入（在命令行执行下述操作） $ export HBASE_HOME=/opt/module/HBase-1.3.1 $ export HADOOP_HOME=/opt/module/hadoop-2.7.2 $ export HADOOP_CLASSPATH=\\${HBASE_HOME}/bin/hbase mapredcp 永久生效：在/etc/profile配置 export HBASE_HOME=/opt/module/HBase-1.3.1 export HADOOP_HOME=/opt/module/hadoop-2.7.2 并在hadoop-env.sh中配置：（注意：在for循环之后配） export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/HBase/lib/* 运行官方的MapReduce任务 案例一：统计Student表中有多少行数据 $ /opt/module/hadoop-2.7.2/bin/yarn jar lib/HBase-server-1.3.1.jar rowcounter student 案例二：使用MapReduce将本地数据导入到HBase 1) 在本地创建一个tsv格式的文件：fruit.tsv 1001 Apple Red 1002 Pear Yellow 1003 Pineapple Yellow 2) 创建HBase表 HBase(main):001:0&gt; create &#39;fruit&#39;,&#39;info&#39; 3) 在HDFS中创建input_fruit文件夹并上传fruit.tsv文件 $ /opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/ $ /opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/ 4）执行MapReduce到HBase的fruit表中 $ /opt/module/hadoop-2.7.2/bin/yarn jar lib/HBase-server-1.3.1.jar importtsv \\ -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \\ hdfs://hadoop102:9000/input_fruit 5）使用scan命令查看导入后的结果 HBase(main):001:0&gt; scan ‘fruit’ 自定义HBase-MapReduce1目标：将fruit表中的一部分数据，通过MR迁入到fruit_mr表中 构建ReadFruitMapper类，用于读取fruit表中的数据 构建WriteFruitMRReducer类，用于将读取到的fruit表中的数据写入到fruit_mr表中 构建Fruit2FruitMRRunner extends Configured implements Tool用于组装运行Job任务 主函数中调用运行该Job任务 打包运行任务 import java.io.IOException; import org.apache.hadoop.HBase.Cell; import org.apache.hadoop.HBase.CellUtil; import org.apache.hadoop.HBase.client.Put; import org.apache.hadoop.HBase.client.Result; import org.apache.hadoop.HBase.io.ImmutableBytesWritable; import org.apache.hadoop.HBase.mapreduce.TableMapper; import org.apache.hadoop.HBase.util.Bytes; public class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; { @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException { //将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。 Put put = new Put(key.get()); //遍历添加column行 for(Cell cell: value.rawCells()){ //添加/克隆列族:info if(&quot;info&quot;.equals( Bytes.toString(CellUtil.cloneFamily(cell)))){ //添加/克隆列：name if(&quot;name&quot;.equals( Bytes.toString(CellUtil.cloneQualifier(cell)))){ //将该列cell加入到put对象中 put.add(cell); //添加/克隆列:color }else if(&quot;color&quot;.equals( Bytes.toString(CellUtil.cloneQualifier(cell)))){ //向该列cell加入到put对象中 put.add(cell); } } } //将从fruit读取到的每行数据写入到context中作为map的输出 context.write(key, put); } } import java.io.IOException; import org.apache.hadoop.HBase.client.Put; import org.apache.hadoop.HBase.io.ImmutableBytesWritable; import org.apache.hadoop.HBase.mapreduce.TableReducer; import org.apache.hadoop.io.NullWritable; public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; { @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException { //读出来的每一行数据写入到fruit_mr表中 for(Put put: values){ context.write(NullWritable.get(), put); } } } //组装Job public int run(String[] args) throws Exception { //得到Configuration Configuration conf = this.getConf(); //创建Job任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Fruit2FruitMRRunner.class); //配置Job Scan scan = new Scan(); scan.setCacheBlocks(false); scan.setCaching(500); //设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本 TableMapReduceUtil.initTableMapperJob( &quot;fruit&quot;, //数据源的表名 scan, //scan扫描控制器 ReadFruitMapper.class,//设置Mapper类 ImmutableBytesWritable.class,//设置Mapper输出key类型 Put.class,//设置Mapper输出value值类型 job//设置给哪个JOB ); //设置Reducer TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRReducer.class, job); //设置Reduce数量，最少1个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess){ throw new IOException(&quot;Job running with error&quot;); } return isSuccess ? 0 : 1; } public static void main( String[] args ) throws Exception{ Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args); System.exit(status); } $ /opt/module/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/HBase-0.0.1-SNAPSHOT.jar com.z.HBase.mr1.Fruit2FruitMRRunner 提示：运行任务前，如果待数据导入的表不存在，则需要提前创建。 提示：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin） 自定义HBase-MapReduce2目标：实现将HDFS中的数据写入到HBase表中 构建ReadFruitFromHDFSMapper于读取HDFS中的文件数据 构建WriteFruitMRFromTxtReducer类 创建Txt2FruitRunner组装Job 调用执行Job 打包运行 import java.io.IOException; import org.apache.hadoop.HBase.client.Put; import org.apache.hadoop.HBase.io.ImmutableBytesWritable; import org.apache.hadoop.HBase.util.Bytes; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { //从HDFS中读取的数据 String lineValue = value.toString(); //读取出来的每行数据使用\\t进行分割，存于String数组 String[] values = lineValue.split(&quot;\\t&quot;); //根据数据中值的含义取值 String rowKey = values[0]; String name = values[1]; String color = values[2]; //初始化rowKey ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey)); //初始化put对象 Put put = new Put(Bytes.toBytes(rowKey)); //参数分别:列族、列、值 put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(name)); put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;color&quot;), Bytes.toBytes(color)); context.write(rowKeyWritable, put); } } import java.io.IOException; import org.apache.hadoop.HBase.client.Put; import org.apache.hadoop.HBase.io.ImmutableBytesWritable; import org.apache.hadoop.HBase.mapreduce.TableReducer; import org.apache.hadoop.io.NullWritable; public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; { @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException { //读出来的每一行数据写入到fruit_hdfs表中 for(Put put: values){ context.write(NullWritable.get(), put); } } } public int run(String[] args) throws Exception { //得到Configuration Configuration conf = this.getConf(); //创建Job任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Txt2FruitRunner.class); Path inPath = new Path(&quot;hdfs://hadoop102:9000/input_fruit/fruit.tsv&quot;); FileInputFormat.addInputPath(job, inPath); //设置Mapper job.setMapperClass(ReadFruitFromHDFSMapper.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setMapOutputValueClass(Put.class); //设置Reducer TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRFromTxtReducer.class, job); //设置Reduce数量，最少1个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess){ throw new IOException(&quot;Job running with error&quot;); } return isSuccess ? 0 : 1; } public static void main(String[] args) throws Exception { Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Txt2FruitRunner(), args); System.exit(status); } $ /opt/module/hadoop-2.7.2/bin/yarn jar HBase-0.0.1-SNAPSHOT.jar com.atguigu.HBase.mr2.Txt2FruitRunner","tags":[]},{"title":"HBase06-与Hive集成","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase06-与Hive集成.html","text":"HBase与Hive的对比1.Hive 数据仓库：Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。 用于数据分析、清洗：Hive适用于离线的数据分析和清洗，延迟较高。 基于HDFS、MapReduce：Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。 2.HBase 数据库：是一种面向列存储的非关系型数据库。 用于存储结构化和非结构化的数据：适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。 基于HDFS：数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。 延迟较低，接入在线业务使用：面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。 HBase与Hive集成使用尖叫提示：HBase与Hive的集成在最新的两个版本中无法兼容。所以，只能含着泪勇敢的重新编译：hive-HBase-handler-1.2.2.jar！！ 因为我们后续可能会在操作Hive的同时对HBase也会产生影响，所以Hive需要持有操作HBase的Jar，那么接下来拷贝Hive所依赖的Jar包（或者使用软连接的形式）。 export HBASE_HOME=/opt/module/HBase export HIVE_HOME=/opt/module/hive ln -s $HBASE_HOME/lib/HBase-common-1.3.1.jar $HIVE_HOME/lib/HBase-common-1.3.1.jar ln -s $HBASE_HOME/lib/HBase-server-1.3.1.jar $HIVE_HOME/lib/HBase-server-1.3.1.jar ln -s $HBASE_HOME/lib/HBase-client-1.3.1.jar $HIVE_HOME/lib/HBase-client-1.3.1.jar ln -s $HBASE_HOME/lib/HBase-protocol-1.3.1.jar $HIVE_HOME/lib/HBase-protocol-1.3.1.jar ln -s $HBASE_HOME/lib/HBase-it-1.3.1.jar $HIVE_HOME/lib/HBase-it-1.3.1.jar ln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jar ln -s $HBASE_HOME/lib/HBase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/HBase-hadoop2-compat-1.3.1.jar ln -s $HBASE_HOME/lib/HBase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/HBase-hadoop-compat-1.3.1.jar 同时在hive-site.xml中修改zookeeper的属性，如下: &lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102,hadoop103,hadoop104&lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt; &lt;/property&gt; 案例一: 建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。 1.在Hive中创建表同时关联HBase: CREATE TABLE hive_HBase_emp_table( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;); ​ 提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表 2.在Hive中创建临时中间表，用于load文件中的数据 ​ 提示：不能将数据直接load进Hive所关联HBase的那张表中 CREATE TABLE emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) row format delimited fields terminated by &#39;\\t&#39;; 3.向Hive中间表中load数据 ​ hive&gt; load data local inpath ‘/home/admin/softwares/data/emp.txt’ into table emp; 4.通过insert命令将中间表中的数据导入到Hive关联HBase的那张表中 ​ hive&gt; insert into table hive_hbase_emp_table select * from emp; 5.查看Hive以及关联的HBase表中是否已经成功的同步插入了数据 ​ hive&gt; select * from hive_HBase_emp_table; ​ HBase&gt; scan ‘HBase_emp_table’ 案例二: 目标：在HBase中已经存储了某一张表HBase_emp_table，然后在Hive中创建一个外部表来关联HBase中的HBase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。 注：该案例2紧跟案例1的脚步，所以完成此案例前，请先完成案例1。 1.在Hive中创建外部表 CREATE EXTERNAL TABLE relevance_HBase_emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;HBase_emp_table&quot;); 2.关联后就可以使用Hive函数进行一些分析操作了 hive (default)&gt; select * from relevance_HBase_emp;","tags":[]},{"title":"HBase07-HBase优化","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase07-HBase优化.html","text":"高可用在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。 关闭HBase集群： bin/stop-HBase.sh 在conf目录下创建backup-masters文件： touch conf/backup-masters 在backup-masters文件中配置高可用HMaster节点：echo hadoop103 &gt; conf/backup-masters 分发conf 打开页面测试查看：http://hadooo102:16010 预分区每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。 手动设定预分区 HBase&gt; create &#39;staff1&#39;,&#39;info&#39;,&#39;partition1&#39;,SPLITS =&gt; [&#39;1000&#39;,&#39;2000&#39;,&#39;3000&#39;,&#39;4000&#39;] 生成16进制序列预分区 create &#39;staff2&#39;,&#39;info&#39;,&#39;partition2&#39;,{NUMREGIONS =&gt; 15, SPLITALGO =&gt; &#39;HexStringSplit&#39;} 按照文件中设置的规则预分区 创建splits.txt文件内容如下： aaaa bbbb cccc dddd create &#39;staff3&#39;,&#39;partition3&#39;,SPLITS_FILE =&gt; &#39;splits.txt&#39; 使用JavaAPI创建预分区 //自定义算法，产生一系列Hash散列值存储在二维数组中 byte[][] splitKeys = 某个散列值函数 //创建HBaseAdmin实例 HBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create()); //创建HTableDescriptor实例 HTableDescriptor tableDesc = new HTableDescriptor(tableName); //通过HTableDescriptor实例和散列值二维数组创建带有预分区的HBase表 hAdmin.createTable(tableDesc, splitKeys); RowKey设计一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案. 生成随机数、hash、散列值 原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7 原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd 原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913 在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。 字符串反转 20170524000001转成10000042507102 20170524000002转成20000042507102 这样也可以在一定程度上散列逐步put进来的数据。 字符串拼接 20170524000001_a12e 20170524000001_93i7 内存优化HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。 基础优化 允许在HDFS的文件中追加内容 hdfs-site.xml、HBase-site.xml 属性：dfs.support.append 解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。 优化DataNode允许的最大文件打开数 hdfs-site.xml 属性：dfs.datanode.max.transfer.threads 解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096 优化延迟高的数据操作的等待时间 hdfs-site.xml 属性：dfs.image.transfer.timeout 解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。 优化数据的写入效率 mapred-site.xml 属性： mapreduce.map.output.compress mapreduce.map.output.compress.codec 解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。 设置RPC监听数量 HBase-site.xml 属性：HBase.regionserver.handler.count 解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 优化HStore文件大小 HBase-site.xml 属性：HBase.hregion.max.filesize 解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。 优化HBase客户端缓存 HBase-site.xml 属性：HBase.client.write.buffer 解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。 指定scan.next扫描HBase所获取的行数 HBase-site.xml 属性：HBase.client.scanner.caching 解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。 flush、compact、split机制 当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。 涉及属性： 即：128M就是Memstore的默认阈值 HBase.hregion.memstore.flush.size：134217728 即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。 HBase.regionserver.global.memstore.upperLimit：0.4 HBase.regionserver.global.memstore.lowerLimit：0.38 即：当MemStore使用内存总量达到HBase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit","tags":[]},{"title":"HBase04-API","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase04-HBase-API.html","text":"pom.xml中添加依赖&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; 在对HBase执行增删改查时，只需要引入hbase-client模块即可. 运行MR操作hbase时，需要引入hbase-server。拷贝hdfs-site.xml文件到客户端的类路径下！ 获取Configuration对象Connection代表对集群的连接对象，封装了与实际服务器的低级别单独连接以及与zookeeper的连接。 Connection可以通过ConnectionFactory类实例化。 Connection的生命周期由调用者管理，使用完毕后需要执行close()以释放资源。 Connection是线程安全的，多个Table和Admin可以共用同一个Connection对象。因此一个客户端只需要实例化一个连接即可。 反之，Table和Admin不是线程安全的！因此不建议并缓存或池化这两种对象。 public static Configuration conf; static{ //使用HBaseConfiguration的单例方法实例化 conf = HBaseConfiguration.create(); conf.set(&quot;HBase.zookeeper.quorum&quot;, &quot;192.166.9.102&quot;); conf.set(&quot;HBase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); } 表是否存在Admin为HBase的管理类，可以通过Connection.getAdmin() 获取实例，且在使用完成后调用close()关闭。 Admin可用于创建，删除，列出，启用和禁用以及以其他方式修改表，以及执行其他管理操作。 public static boolean isTableExist(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ //在HBase中管理、访问表需要先创建HBaseAdmin对象 //Connection connection = ConnectionFactory.createConnection(conf); //HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); HBaseAdmin admin = new HBaseAdmin(conf); return admin.tableExists(tableName); } 创建表public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ HBaseAdmin admin = new HBaseAdmin(conf); //判断表是否存在 if(isTableExist(tableName)){ System.out.println(&quot;表&quot; + tableName + &quot;已存在&quot;); //System.exit(0); }else{ //创建表属性对象,表名需要转字节 HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName)); //创建多个列族 for(String cf : columnFamily){ descriptor.addFamily(new HColumnDescriptor(cf)); } //根据对表的配置，创建表 admin.createTable(descriptor); System.out.println(&quot;表&quot; + tableName + &quot;创建成功！&quot;); } } 删除表public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ HBaseAdmin admin = new HBaseAdmin(conf); if(isTableExist(tableName)){ admin.disableTable(tableName); admin.deleteTable(tableName); System.out.println(&quot;表&quot; + tableName + &quot;删除成功！&quot;); }else{ System.out.println(&quot;表&quot; + tableName + &quot;不存在！&quot;); } } 向表中插入数据public static void addRowData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException{ //创建HTable对象 HTable hTable = new HTable(conf, tableName); //向表中插入数据 Put put = new Put(Bytes.toBytes(rowKey)); //向Put对象中组装数据 put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); hTable.put(put); hTable.close(); System.out.println(&quot;插入数据成功&quot;); } 删除多行数据public static void deleteMultiRow(String tableName, String... rows) throws IOException{ HTable hTable = new HTable(conf, tableName); List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); for(String row : rows){ Delete delete = new Delete(Bytes.toBytes(row)); deleteList.add(delete); } hTable.delete(deleteList); hTable.close(); } 获取所有数据public static void getAllRows(String tableName) throws IOException{ HTable hTable = new HTable(conf, tableName); //得到用于扫描region的对象 Scan scan = new Scan(); //使用HTable得到resultcanner实现类的对象 ResultScanner resultScanner = hTable.getScanner(scan); for(Result result : resultScanner){ Cell[] cells = result.rawCells(); for(Cell cell : cells){ //得到rowkey System.out.println(&quot;行键:&quot; + Bytes.toString(CellUtil.cloneRow(cell))); //得到列族 System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); } } } 获取某一行数据public static void getRow(String tableName, String rowKey) throws IOException{ HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); //get.setMaxVersions();显示所有版本 //get.setTimeStamp();显示指定时间戳的版本 Result result = table.get(get); for(Cell cell : result.rawCells()){ System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow())); System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); System.out.println(&quot;时间戳:&quot; + cell.getTimestamp()); } } 获取某一行指定“列族:列”的数据public static void getRowQualifier(String tableName, String rowKey, String family, String qualifier) throws IOException{ HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier)); Result result = table.get(get); for(Cell cell : result.rawCells()){ System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow())); System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); } }","tags":[]},{"title":"HBase08-扩展","date":"2020-08-12T02:00:50.000Z","path":"2020/08/12/10-HBase/HBase08-扩展.html","text":"布隆过滤器在日常生活中，包括在设计计算机软件时，我们经常要判断一个元素是否在一个集合中。比如在字处理软件中，需要检查一个英语单词是否拼写正确（也就是要判断它是否在已知的字典中）；在 FBI，一个嫌疑人的名字是否已经在嫌疑名单上；在网络爬虫里，一个网址是否被访问过等等。最直接的方法就是将集合中全部的元素存在计算机中，遇到一个新元素时，将它和集合中的元素直接比较即可。一般来讲，计算机中的集合是用哈希表（hash table）来存储的。它的好处是快速准确，缺点是费存储空间。当集合比较小时，这个问题不显著，但是当集合巨大时，哈希表存储效率低的问题就显现出来了。比如说，一个像 Yahoo,Hotmail 和 Gmai 那样的公众电子邮件（email）提供商，总是需要过滤来自发送垃圾邮件的人（spamer）的垃圾邮件。一个办法就是记录下那些发垃圾邮件的 email 地址。由于那些发送者不停地在注册新的地址，全世界少说也有几十亿个发垃圾邮件的地址，将他们都存起来则需要大量的网络服务器。如果用哈希表，每存储一亿个 email 地址， 就需要 1.6GB 的内存（用哈希表实现的具体办法是将每一个 email 地址对应成一个八字节的信息指纹googlechinablog.com/2006/08/blog-post.html，然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email 地址需要占用十六个字节。一亿个地址大约要 1.6GB， 即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB 的内存。除非是超级计算机，一般服务器是无法存储的。 布隆过滤器只需要哈希表 1/8 到 1/4 的大小就能解决同样的问题。 Bloom Filter是一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并能判断一个元素是否属于这个集合。Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。 下面我们具体来看Bloom Filter是如何用位数组表示集合的。初始状态时，Bloom Filter是一个包含m位的位数组，每一位都置为0，如下图所示。 为了表达S={x1, x2,…,xn}这样一个n个元素的集合，Bloom Filter使用k个相互独立的哈希函数（Hash Function），它们分别将集合中的每个元素映射到{1,…,m}的范围中。对任意一个元素x，第i个哈希函数映射的位置hi(x)就会被置为1（1≤i≤k）。注意，如果一个位置多次被置为1，那么只有第一次会起作用，后面几次将没有任何效果。如图9-6所示，k=3，且有两个哈希函数选中同一个位置（从左边数第五位）。 在判断y是否属于这个集合时，我们对y应用k次哈希函数，如果所有hi(y)的位置都是1（1≤i≤k），那么我们就认为y是集合中的元素，否则就认为y不是集合中的元素。如图9-7所示y1就不是集合中的元素。y2或者属于这个集合，或者刚好是一个false positive。 为了add一个元素，用k个hash function将它hash得到bloom filter中k个bit位，将这k个bit位置1。 · 为了query一个元素，即判断它是否在集合中，用k个hash function将它hash得到k个bit位。若这k bits全为1，则此元素在集合中；若其中任一位不为1，则此元素比不在集合中（因为如果在，则在add时已经把对应的k个bits位置为1）。 · 不允许remove元素，因为那样的话会把相应的k个bits位置为0，而其中很有可能有其他元素对应的位。因此remove会引入false negative，这是绝对不被允许的。 布隆过滤器决不会漏掉任何一个在黑名单中的可疑地址。但是，它有一条不足之处，也就是它有极小的可能将一个不在黑名单中的电子邮件地址判定为在黑名单中，因为有可能某个好的邮件地址正巧对应一个八个都被设置成一的二进制位。好在这种可能性很小，我们把它称为误识概率。 布隆过滤器的好处在于快速，省空间，但是有一定的误识别率，常见的补救办法是在建立一个小的白名单，存储那些可能个别误判的邮件地址。 布隆过滤器具体算法高级内容，如错误率估计，最优哈希函数个数计算，位数组大小计算，请参见http://blog.csdn.net/jiaomeng/article/details/1495500。","tags":[]},{"title":"Hive安装","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/00-环境/04-Hive.html","text":"Hive安装 解压apache-hive-1.2.1-bin.tar.gz 修改/opt/module/hive/conf 目录下的hive-env.sh.template 名称为hive-env.sh 配置hive-env.sh 文件 配置HADOOP_HOME 路径 ： export HADOOP_HOME=/opt/module/hadoop-2.7.2 配置HIVE_CONF_DIR 路径 ： export HIVE_CONF_DIR=/opt/module/hive/conf 启动hdfs和yarn：sbin/start-dfs.sh、sbin/start-yarn.sh 在 HDFS 上创建/tmp 和/user/hive/warehouse 两个目录并修改他们的同组权限可写。(可不操作，系统会自动创建) [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse Hive基本操作 # 启动hive [atguigu@hadoop102 hive]$ bin/hive # 查看数据库 hive&gt; show databases; # 打开默认数据库 hive&gt; use default; # 显示default 数据库中的表 hive&gt; show tables; # 创建一张表 hive&gt; create table student(id int, name string); # 显示数据库中有几张表 hive&gt; show tables; # 查看表的结构 hive&gt; desc student; # 向表中插入数据 hive&gt; insert into student values(1000,&quot;ss&quot;); # 查询表中数据 hive&gt; select * from student; # 退出hive hive&gt; quit; 本地文件导入Hive将本地/opt/module/data/student.txt 这个目录下的数据导入到 hive 的 student(id int, name string)表中。 # student.txt 注意以tab键间隔 1001 zhangshan 1002 lishi 1003 zhaoliu # 启动hive # 创建student 表, 并声明文件分隔符’\\t’ hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39;; # 加载/opt/module/data/student.txt 文件到 student 数据库表中。 # 这个命令其实就是HDFS的put操作。新建相同格式的txt放到对应的HDFS目录下，同样可以查询到数据 hive&gt; load data local inpath &#39;/opt/module/data/student.txt&#39; into table student; hive&gt; load data inpath &quot;/HDFS/path&quot; into table stu; #这个是从HDFS上mv文件。 # Hive 查询结果 hive&gt; select * from student; OK 1001 zhangshan 1002 lishi 1003 zhaoliu Time taken: 0.266 seconds, Fetched: 3 row(s) 注意：Metastore 默认存储在自带的derby 数据库中，不能同时打开多个hive窗口，但mysql支持。 MySQL安装安装MySQL 查看mysql 是否安装，如果安装了，卸载mysql rpm -qa|grep mysql # 查询 rpm -e --nodeps mysql-libs-XXXXX.x86_64 # 卸载 安装mysql 服务端：rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm 查看产生的随机密码：cat /root/.mysql_secret 查看mysql 状态：service mysql status 启动mysql服务：service mysql start 安装MySql 客户端：rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm 链接mysql：mysql -uroot -pOEXaQuS8IWkG19Xs 修改密码：mysql&gt;SET PASSWORD=PASSWORD(‘000000’); 退出：mysql&gt;exit; MySql 中user 表中主机配置配置只要是root 用户+密码，在任何主机上都能登录MySQL 数据库。 [root@hadoop102 mysql-libs]# mysql -uroot -p000000 mysql&gt;use mysql; mysql&gt;show tables; mysql&gt;select User, Host, Password from user; mysql&gt;update user set host=&#39;%&#39; where host=&#39;localhost&#39;; mysql&gt;delete from user where Host=&#39;hadoop102&#39;; mysql&gt;delete from user where Host=&#39;127.0.0.1&#39;; mysql&gt;delete from user where Host=&#39;::1&#39;; mysql&gt;flush privileges; mysql&gt;quit; 配置Metastore 到MySql 驱动拷贝：cp /opt/software/mysql-libs/mysql-connector-java-5.1.27/mysql-c onnector-java-5.1.27-bin.jar /opt/module/hive/lib/ 配置 hive-site.xml 先创建 hive-site.xml &lt;?xml version=&quot;1.0&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseI fNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;000000&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 配置完毕后，如果启动 hive 异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop 集群） 打开多个窗口，分别启动 hive：bin/hive 启动 hive 后，回到 MySQL 窗口查看数据库，显示增加了 metastore 数据库 mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | metastore | | mysql | | performance_schema | | test | +--------------------+ Hive JDBC访问 启动hiveserver2 服务：[atguigu@hadoop102 hive]$ bin/hiveserver2 启动beeline [atguigu@hadoop102 hive]$ bin/beeline Beeline version 1.2.1 by Apache Hive beeline&gt; 连接hiveserver2 beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车） Connecting to jdbc:hive2://hadoop102:10000 Enter username for jdbc:hive2://hadoop102:10000: atguigu（回车） Enter password for jdbc:hive2://hadoop102:10000: （直接回车） Connected to: Apache Hive (version 1.2.1) Driver: Hive JDBC (version 1.2.1) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://hadoop102:10000&gt; show databases; +----------------+--+ | database_name | +----------------+--+ | default | | hive db2 | +----------------+--+ Hive命令[atguigu@hadoop102 hive]$ bin/hive -help usage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive. commands. e.g. -d A=B or --define A=B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property=value&gt; Use value for given property --hivevar &lt;key=value&gt; Variable subsitution to apply to hive. commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) [atguigu@hadoop102 hive]$ bin/hive -e “select id from student;” [atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql 在 hive cli 命令窗口中如何查看 hdfs 文件系统：hive&gt; dfs -ls /; 在 hive cli 命令窗口中如何查看本地文件系统：hive&gt; ! ls /opt/module/datas; 查看在 hive 中输入的所有历史命令：入到当前用户的根目录/root 或/home/atguigu，查看. hivehistory 文件 Hive 常见属性配置Hive 数据仓库位置配置Default 数据仓库的最原始位置是在hdfs 上的：/user/hive/warehouse 路径下。 在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 数据库，直接在数据仓库目录下创建一个文件夹。 在hive-site.xml 文件中添加以下内容： &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; 查询后信息显示配置在 hive-site.xml 文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。 &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; Hive 运行日志信息配置Hive 的log 默认存放在/tmp/atguigu/hive.log 目录下（当前用户名下） 修改 hive 的 log 存放日志到/opt/module/hive/logs 修改/opt/module/hive/conf/hive-log4j.properties.template 文件名称为hive-log4j.properties 在hive-log4j.properties 文件中修改log 存放位置：hive.log.dir=/opt/module/hive/logs 参数配置方式查看当前所有的配置信息：hive&gt;set; 参数的配置三种方式： 配置文件方式 默认配置文件：hive-default.xml 用户自定义配置文件：hive-site.xml 注意：用户自定义配置会覆盖默认配置。另外，Hive 也会读入Hadoop 的配置，因为Hive 是作为Hadoop 的客户端启动的，Hive 的配置会覆盖Hadoop 的配置。配置文件的设定对本机启动的所有Hive 进程都有效。 命令行参数方式（hive外） 启动Hive 时，可以在命令行添加 -hiveconf param=value 来设定参数。 注意：仅对本次 hive 启动有效 如：bin/hive -hiveconf mapred.reduce.tasks=10; 查看参数设置：hive (default)&gt; set mapred.reduce.tasks; 参数声明方式（hive内） 可以在HQL 中使用SET 关键字设定参数。注意：仅对本次 hive 启动有效。 例如： hive (default)&gt; set mapred.reduce.tasks=100; 查看参数设置 hive (default)&gt; set mapred.reduce.tasks; 上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。 集成Tez引擎Tez是一个Hive的运行引擎，性能优于MR。 用Hive直接编写MR程序，假设有四个有依赖关系的MR作业，上图中，绿色是Reduce Task，云状表示写屏蔽，需要将中间结果持久化写到HDFS。 Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。 安装Tez 将apache-tez-0.9.1-bin.tar.gz上传到HDFS的/tez目录下 本地解压缩apache-tez-0.9.1-bin.tar.gz 在Hive的/opt/module/hive/conf下面创建一个tez-site.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;${fs.defaultFS}/tez/apache-tez-0.9.1-bin.tar.gz&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.use.cluster.hadoop-libs&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.history.logging.service.class&lt;/name&gt; &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 在hive-env.sh文件中添加tez环境变量配置和依赖包环境变量配置 # Set HADOOP_HOME to point to a specific hadoop install directory export HADOOP_HOME=/opt/module/hadoop-2.7.2 # Hive Configuration Directory can be controlled by: export HIVE_CONF_DIR=/opt/module/hive/conf # Folder containing extra libraries required for hive compilation/execution can be controlled by: export TEZ_HOME=/opt/module/tez-0.9.1 #是你的tez的解压目录 export TEZ_JARS=&quot;&quot; for jar in `ls $TEZ_HOME |grep jar`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar done for jar in `ls $TEZ_HOME/lib`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar done export HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS 在hive-site.xml文件中添加如下配置，更改hive计算引擎 &lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;tez&lt;/value&gt; &lt;/property&gt; 测试 启动Hive: bin/hive 创建表 hive (default)&gt; create table student( id int, name string); 向表中插入数据 hive (default)&gt; insert into student values(1,”zhangsan”); 如果没有报错就表示成功了 hive (default)&gt; select * from student; 1 zhangsan 注意事项 运行Tez时检查到用过多内存而被NodeManager杀死进程问题 Caused by: org.apache.tez.dag.api.SessionNotRunning: TezSession has already shutdown. Application application_1546781144082_0005 failed 2 times due to AM Container for appattempt_1546781144082_0005_000002 exited with exitCode: -103 For more detailed output, check application tracking page:http://hadoop103:8088/cluster/app/application_1546781144082_0005Then, click on links to logs of each attempt. Diagnostics: Container [pid=11116,containerID=container_1546781144082_0005_02_000001] is running beyond virtual memory limits. Current usage: 216.3 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container. 这种问题是从机上运行的Container试图使用过多的内存，而被NodeManager kill掉了。 [摘录] The NodeManager is killing your container. It sounds like you are trying to use hadoop streaming which is running as a child process of the map-reduce task. The NodeManager monitors the entire process tree of the task and if it eats up more memory than the maximum set in mapreduce.map.memory.mb or mapreduce.reduce.memory.mb respectively, we would expect the Nodemanager to kill the task, otherwise your task is stealing memory belonging to other containers, which you don’t want. 解决方法： 关掉虚拟内存检查，修改yarn-site.xml， ​ yarn.nodemanager.vmem-check-enabled ​ false 修改后一定要分发，并重新启动hadoop集群","tags":[]},{"title":"Zookeeper安装","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper00-安装.html","text":"官网首页：https://zookeeper.apache.org/ 本地模式安装部署安装 安装JDK 解压Zookeeper安装包 [atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 修改zoo.cfg配置、创建zkData目录 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件，修改dataDir路径： dataDir=/opt/module/zookeeper-3.4.10/zkData 在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹 mkdir zkData 操作Zookeeper 启动Zookeeper：bin/zkServer.sh start 查看进程是否启动 [atguigu@hadoop102 zookeeper-3.4.10]$ jps 4020 Jps 4001 QuorumPeerMain 查看状态 [atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: standalone 启动客户端：bin/zkCli.sh 退出客户端：[zk: localhost:2181(CONNECTED) 0] quit 停止Zookeeper：bin/zkServer.sh stop 配置参数解读Zookeeper中的配置文件zoo.cfg中参数含义解读如下： tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) initLimit =10：Leader 和 Fllower初始通信时限 集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 syncLimit =5：LF同步通信时限 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。 dataDir：数据文件目录+数据持久化路径 主要用于保存Zookeeper中的数据。 clientPort =2181：客户端连接端口 监听客户端连接的端口。 分布式安装部署在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper 解压安装# 解压Zookeeper安装包到/opt/module/目录下 [atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ # 同步/opt/module/zookeeper-3.4.10目录内容到hadoop103、hadoop104 [atguigu@hadoop102 module]$ xsync zookeeper-3.4.10/ 配置服务器编号 在/opt/module/zookeeper-3.4.10/这个目录下创建zkData：mkdir -p zkData 在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件：touch myid 注：添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 在myid文件中添加与server对应的编号：2 可以不从0开始，但是需要是唯一编号！ 拷贝配置好的zookeeper到其他机器上：[atguigu@hadoop102 zkData]$ xsync myid 并分别在hadoop102、hadoop103上修改myid文件中内容为3、4 修改zoo.cfg配置 重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg 修改zoo.cfg配置 dataDir=/opt/module/zookeeper-3.4.10/zkData #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 同步zoo.cfg配置文件 配置参数解读：server.A=B:C:D。 A是一个数字，表示这个是第几号服务器；集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群操作分别启动Zookeeper[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start [atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkServer.sh start [atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkServer.sh start 查看状态[atguigu@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [atguigu@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [atguigu@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper概述","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper01-概述.html","text":"概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 特点 Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。 集群中只要有半数以上(不包括一半)节点存活，Zookeeper集群就能正常服务。 全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 更新请求顺序进行，来自同一个Client的更新请求按其发送顺序依次执行。 数据更新原子性，一次数据更新要么成功，要么失败。 实时性，在一定时间范围内，Client能读到最新数据。 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。 应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper内部原理","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper02-内部原理.html","text":"选举机制相关概念 Serverid：服务器ID 比如有三台服务器，编号分别是1,2,3。 编号越大在选择算法中的权重越大。 Zxid：数据ID 服务器中存放的最大数据ID. 值越大说明数据越新，在选举算法中数据越新权重越大。 Epoch：逻辑时钟 或者叫投票的次数，同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加，然后与接收到的其它服务器返回的投票信息中的数值相比，根据不同的值做出不同的判断。 Server状态：选举状态 LOOKING，竞选状态。 FOLLOWING，随从状态，同步leader状态，参与投票。 OBSERVING，观察状态,同步leader状态，不参与投票。 LEADING，领导者状态。 选举算法 leader的选择机制，zookeeper提供了三种方式： LeaderElection AuthFastLeaderElection FastLeaderElection （最新默认） 默认的算法是FastLeaderElection。 半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。 Zookeeper虽然在配置文件中并没有指定 Master和 Slave。但是，Zookeeper工作时，是有一个节点为 Leader，其他则为 Followe。Leader是通过内部的选举机制临时产生的。 判断规则 (rules judging)：保存的 zxid最大值和 leader Serverid来进行判断的。先看数据 zxid,数据 zxid大者胜出；其次再判断 leader Serverid，leader Serverid大者胜出；然后再将自身最新的选举结果(也就是上面提到的三种数据（leader Serverid，Zxid，Epoch）广播给其他server 选举消息内容 在每轮投票完成后，需要将投票信息发送给集群中的所有服务器，它包含如下内容。 服务器ID 数据ID 逻辑时钟 选举状态 选举流程一、首先开始选举阶段，每个Server读取自身的zxid。 二、发送投票信息 2.1 首先，每个Server第一轮都会投票给自己。 2.2 投票信息包含 ：所选举leader的Serverid、Zxid、Epoch。Epoch会随着选举轮数的增加而递增。 三、接收投票信息 3.1 如果服务器B接收到服务器A的数据（服务器A处于选举状态(LOOKING 状态) ​ a. 首先，判断逻辑时钟值： 如果发送过来的逻辑时钟 Epoch 大于目前的逻辑时钟。首先，更新本逻辑时钟 Epoch，同时清空本轮收到的来自其他 server的选举数据。然后，根据判断规则 判断是否需要更新当前自己的选举 leader Serverid。 如果发送过来的逻辑时钟 Epoch 小于目前的逻辑时钟。说明对方 server 在一个相对较早的 Epoch 中，忽略该 server 的投票，无需修改投票，并将本机的三种数据（leader Serverid，Zxid，Epoch）发送过去就行。 如果发送过来的逻辑时钟 Epoch 等于目前的逻辑时钟。再根据上述判断规则 rules judging 来选举 leader ，然后再将自身最新的选举结果（leader Serverid，Zxid，Epoch）广播给其他server。 ​ b. 其次，判断服务器是不是已经收集到了所有服务器的选举状态：若是，根据选举结果设置自己的角色(FOLLOWING还是LEADER)，退出选举过程就是了。 最后，若没有收到没有收集到所有服务器的选举状态：也可以判断一下根据以上过程之后最新的选举leader是不是得到了超过半数以上服务器的支持,如果是,那么尝试在200ms内接收一下数据,如果没有新的数据到来,说明大家都已经默认了这个结果,同样也设置角色退出选举过程。 3.2 如果所接收服务器A处在其它状态（FOLLOWING或者LEADING）。 ​ a. 逻辑时钟Epoch等于目前的逻辑时钟，将该数据保存到recvset。此时Server已经处于LEADING状态，说明此时这个server已经投票选出结果。若此时这个接收服务器宣称自己是leader, 那么将判断是不是有半数以上的服务器选举它，如果是则设置选举状态退出选举过程。 ​ b. 否则这是一条与当前逻辑时钟不符合的消息，那么说明在另一个选举过程中已经有了选举结果，于是将该选举结果加入到outofelection集合中，再根据outofelection来判断是否可以结束选举,如果可以也是保存逻辑时钟，设置选举状态，退出选举过程。 假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。 服务器1启动，此时只有它一台服务器启动了，它投自己一票，它发出去的报文没有任何响应，且票数未过半，所以它的选举状态一直是LOOKING状态。 服务器2启动，它也投自己一票，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。 服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的Leader。 服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接受当小弟的命了。 服务器5启动，同4一样当小弟。 节点类型 持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除 短暂（Ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久化目录节点：客户端与Zookeeper断开连接后，该节点依旧存在 持久化顺序编号目录节点：客户端与Zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 临时目录节点：客户端与Zookeeper断开连接后，该节点被删除 临时顺序编号目录节点：客户端与Zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 说明：创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护。 注意：在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 Stat结构体 czxid-创建节点的事务zxid 每次修改 ZooKeeper 状态都会收到一个 zxid 形式的时间戳，也就是 ZooKeeper 事务ID。 事务 ID是 ZooKeeper 中所有修改总的次序。每个修改都有唯一的 zxid，如果 zxid1小于zxid2，那么zxid1在zxid2之前发生。 ctime - znode被创建的毫秒数(从1970年开始) mzxid - znode最后更新的事务zxid mtime - znode最后修改的毫秒数(从1970年开始) pZxid-znode最后更新的子节点zxid cversion - znode子节点变化号，znode子节点修改次数 dataversion - znode数据变化号 aclVersion - znode访问控制列表的变化号 ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。 dataLength- znode的数据长度 numChildren - znode子节点数量 监听器原理 监听原理详解 首先要有一个main()线程 在main线程中创建Zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 通过connect线程将注册的监听事件发送给Zookeeper。 在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。 Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。 listener线程内部调用了process()方法。 常见的监听 监听节点数据的变化：get path [watch] 监听子节点增减的变化：ls path [watch] 写数据流程","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper-shell操作","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper03-Shell操作.html","text":"基本语法 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建; -s 含有序列; -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 启动客户端[atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh 显示所有操作命令[zk: localhost:2181(CONNECTED) 1] help 查看当前znode中所包含的内容[zk: localhost:2181(CONNECTED) 0] ls / [zookeeper] 查看当前节点详细数据[zk: localhost:2181(CONNECTED) 1] ls2 / [zookeeper] cZxid = 0x0 ctime = Thu Jan 01 08:00:00 CST 1970 mZxid = 0x0 mtime = Thu Jan 01 08:00:00 CST 1970 pZxid = 0x0 cversion = -1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 分别创建2个普通节点[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;jinlian&quot; Created /sanguo [zk: localhost:2181(CONNECTED) 4] create /sanguo/shuguo &quot;liubei&quot; Created /sanguo/shuguo 获得节点的值[zk: localhost:2181(CONNECTED) 5] get /sanguo jinlian cZxid = 0x100000003 ctime = Wed Aug 29 00:03:23 CST 2018 mZxid = 0x100000003 mtime = Wed Aug 29 00:03:23 CST 2018 pZxid = 0x100000004 cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 7 numChildren = 1 [zk: localhost:2181(CONNECTED) 6] [zk: localhost:2181(CONNECTED) 6] get /sanguo/shuguo liubei cZxid = 0x100000004 ctime = Wed Aug 29 00:04:35 CST 2018 mZxid = 0x100000004 mtime = Wed Aug 29 00:04:35 CST 2018 pZxid = 0x100000004 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 6 numChildren = 0 创建短暂节点[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot; Created /sanguo/wuguo # 在当前客户端是能查看到的 [zk: localhost:2181(CONNECTED) 3] ls /sanguo [wuguo, shuguo] # 退出当前客户端然后再重启客户端 [zk: localhost:2181(CONNECTED) 12] quit [atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh # 再次查看根目录下短暂节点已经删除 [zk: localhost:2181(CONNECTED) 0] ls /sanguo [shuguo] 创建带序号的节点# 先创建一个普通的根节点/sanguo/weiguo [zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot; Created /sanguo/weiguo # 创建带序号的节点 [zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/xiaoqiao &quot;jinlian&quot; Created /sanguo/weiguo/xiaoqiao0000000000 [zk: localhost:2181(CONNECTED) 3] create -s /sanguo/weiguo/daqiao &quot;jinlian&quot; Created /sanguo/weiguo/daqiao0000000001 [zk: localhost:2181(CONNECTED) 4] create -s /sanguo/weiguo/diaocan &quot;jinlian&quot; Created /sanguo/weiguo/diaocan0000000002 # 如果原来没有序号节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推。 修改节点数据值[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo “simayi” 节点的值变化监听# 在hadoop104主机上注册监听/sanguo节点数据变化 # 注意：watch仅能监听一次，节点第二次修改不会有提示，注册一次，监听一次 [zk: localhost:2181(CONNECTED) 8] get /sanguo watch # 在hadoop103主机上修改/sanguo节点的数据 [zk: localhost:2181(CONNECTED) 1] set /sanguo &quot;xisi&quot; # 观察hadoop104主机收到数据变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeDataChanged path:/sanguo 节点的子节点变化监听（路径变化）# 在hadoop104主机上注册监听/sanguo节点的子节点变化 # 注意：watch仅能监听一次，节点第二次修改不会有提示，注册一次，监听一次 [zk: localhost:2181(CONNECTED) 1] ls /sanguo watch [aa0000000001, server101] # 在hadoop103主机/sanguo节点上创建子节点 [zk: localhost:2181(CONNECTED) 2] create /sanguo/jin &quot;simayi&quot; Created /sanguo/jin # 观察hadoop104主机收到子节点变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/sanguo 删除节点[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin 递归删除节点[zk: localhost:2181(CONNECTED) 15] rmr /sanguo/shuguo 查看节点状态[zk: localhost:2181(CONNECTED) 17] stat /sanguo cZxid = 0x100000003 ctime = Wed Aug 29 00:03:23 CST 2018 mZxid = 0x100000011 mtime = Wed Aug 29 00:21:23 CST 2018 pZxid = 0x100000014 cversion = 9 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 4 numChildren = 1","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Zookeeper实战","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/06-Zookeeper/Zookeeper04-实战.html","text":"API应用 创建一个Maven工程 添加pom文件 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 添加log4j.properties文件到项目根目录 需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 创建ZooKeeper客户端 private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void init() throws Exception { zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { // 收到事件通知后的回调函数（用户的业务逻辑） System.out.println(event.getType() + &quot;--&quot; + event.getPath()); // 再次启动监听 try { zkClient.getChildren(&quot;/&quot;, true); } catch (Exception e) { e.printStackTrace(); } } }); } 创建子节点 // 创建子节点 @Test public void create() throws Exception { // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型 String nodeCreated = zkClient.create(&quot;/atguigu&quot;, &quot;jinlian&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); } 获取子节点并监听节点变化 // 获取子节点 @Test public void getChildren() throws Exception { List&lt;String&gt; children = zkClient.getChildren(&quot;/&quot;, true); for (String child : children) { System.out.println(child); } // 延时阻塞 Thread.sleep(Long.MAX_VALUE); } 判断Znode是否存在 // 判断znode是否存在 @Test public void exist() throws Exception { Stat stat = zkClient.exists(&quot;/eclipse&quot;, false); System.out.println(stat == null ? &quot;not exist&quot; : &quot;exist&quot;); } 监听服务器节点动态上下线案例某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。 服务器端向Zookeeper注册代码import java.io.IOException; import org.apache.zookeeper.CreateMode; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher; import org.apache.zookeeper.ZooKeeper; import org.apache.zookeeper.ZooDefs.Ids; public class DistributeServer { private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = &quot;/servers&quot;; // 创建到zk的客户端连接 public void getConnect() throws IOException{ zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { } }); } // 注册服务器 public void registServer(String hostname) throws Exception{ String create = zk.create(parentNode + &quot;/server&quot;, hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +&quot; is online &quot;+ create); } // 业务功能 public void business(String hostname) throws Exception{ System.out.println(hostname+&quot; is working ...&quot;); Thread.sleep(Long.MAX_VALUE); } public static void main(String[] args) throws Exception { // 1获取zk连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 2 利用zk连接注册服务器信息 server.registServer(args[0]); // 3 启动业务功能 server.business(args[0]); } } 客户端代码import java.io.IOException; import java.util.ArrayList; import java.util.List; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher; import org.apache.zookeeper.ZooKeeper; public class DistributeClient { private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = &quot;/servers&quot;; // 创建到zk的客户端连接 public void getConnect() throws IOException { zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { // 再次启动监听 try { getServerList(); } catch (Exception e) { e.printStackTrace(); } } }); } // 获取服务器列表信息 public void getServerList() throws Exception { // 1获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); // 2存储服务器信息列表 ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); // 3遍历所有节点，获取节点中的主机名称信息 for (String child : children) { byte[] data = zk.getData(parentNode + &quot;/&quot; + child, false, null); servers.add(new String(data)); } // 4打印服务器列表信息 System.out.println(servers); } // 业务功能 public void business() throws Exception{ System.out.println(&quot;client is working ...&quot;); Thread.sleep(Long.MAX_VALUE); } public static void main(String[] args) throws Exception { // 1获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 2获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 3业务进程启动 client.business(); } } 企业面试真题 请简述ZooKeeper的选举机制 ZooKeeper的监听原理是什么？ ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？ （1）部署方式单机模式、集群模式 （2）角色：Leader和Follower （3）集群最少需要机器数：3 ZooKeeper的常用命令 ls create get delete set…","tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://mtcai.github.io/tags/Zookeeper/"}]},{"title":"Hive概述","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive01-概述.html","text":"什么是HiveHive：由Facebook 开源用于解决海量结构化日志的数据统计。 Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。 本质是：将HQL 转化成MapReduce 程序 Hive 处理的数据存储在HDFS Hive 分析数据底层的默认实现是MapReduce 执行程序运行在Yarn 上 优缺点优点: 操作接口采用类SQL 语法，提供快速开发的能力（简单、容易上手） 避免了去写MapReduce，减少开发人员的学习成本。 Hive 的执行延迟比较高，因此Hive 常用于数据分析，对实时性要求不高的场合。 Hive 优势在于处理大数据，对于处理小数据没有优势，因为Hive 的执行延迟比较高。 Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 缺点: Hive 的HQL 表达能力有限 迭代式算法无法表达 数据挖掘方面不擅长 Hive 的效率比较低 Hive 自动生成的MapReduce 作业，通常情况下不够智能化 Hive 调优比较困难，粒度较粗 结构原理 用户接口：Client CLI（hive shell）、JDBC/ODBC(java 访问hive)、WEBUI（浏览器访问hive） 元数据：Metastore 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的 derby 数据库中，推荐使用MySQL 存储Metastore Hadoop 使用HDFS 进行存储，使用MapReduce 进行计算。 驱动器：Driver 解析器（SQL Parser）：将SQL 字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误。 编译器（Physical Plan）：将AST编译生成逻辑执行计划。 优化器（Query Optimizer）：对逻辑执行计划进行优化。 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive 来说，就是MR/Spark。 Hive 通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。 Hive和数据库的比较由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本节将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。 优势：易于扩展机器数量；可处理大规模数据 劣势： 修改数据耗时，读多写少； 不适合在线查询； 延时相对高； 相同：SQL语法； 不同： 存储在HDFS（本地文件系统）； 无索引（根据列建立索引），由于MR并行，对大量数据查询缺陷不明显； 执行引擎MR（有自己的的执行引擎） 查询语言由于SQL 被广泛的应用在数据仓库中，因此，专门针对Hive 的特性设计了类SQL 的查询语言HQL。熟悉SQL 开发的开发者可以很方便的使用Hive 进行开发。 数据存储位置Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。 数据更新由于Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的， 因此可以使用 INSERT INTO … VALUES 添加数据， 使用 UPDATE … SET 修改数据。 索引Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。 执行Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。 执行延迟Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive 的并行计算显然能体现出优势。 可扩展性由于Hive 是建立在Hadoop 之上的，因此Hive 的可扩展性是和Hadoop 的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009 年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有100 台左右。 数据规模由于Hive 建立在集群上并可以利用MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。","tags":[]},{"title":"Hive安装","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive02-安装.html","text":"Hive安装 解压apache-hive-1.2.1-bin.tar.gz 修改/opt/module/hive/conf 目录下的hive-env.sh.template 名称为hive-env.sh 配置hive-env.sh 文件 配置HADOOP_HOME 路径 ： export HADOOP_HOME=/opt/module/hadoop-2.7.2 配置HIVE_CONF_DIR 路径 ： export HIVE_CONF_DIR=/opt/module/hive/conf 启动hdfs和yarn：sbin/start-dfs.sh、sbin/start-yarn.sh 在 HDFS 上创建/tmp 和/user/hive/warehouse 两个目录并修改他们的同组权限可写。(可不操作，系统会自动创建) [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse Hive基本操作 # 启动hive [atguigu@hadoop102 hive]$ bin/hive # 查看数据库 hive&gt; show databases; # 打开默认数据库 hive&gt; use default; # 显示default 数据库中的表 hive&gt; show tables; # 创建一张表 hive&gt; create table student(id int, name string); # 显示数据库中有几张表 hive&gt; show tables; # 查看表的结构 hive&gt; desc student; # 向表中插入数据 hive&gt; insert into student values(1000,&quot;ss&quot;); # 查询表中数据 hive&gt; select * from student; # 退出hive hive&gt; quit; 本地文件导入Hive将本地/opt/module/data/student.txt 这个目录下的数据导入到 hive 的 student(id int, name string)表中。 # student.txt 注意以tab键间隔 1001 zhangshan 1002 lishi 1003 zhaoliu # 启动hive # 创建student 表, 并声明文件分隔符’\\t’ hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39;; # 加载/opt/module/data/student.txt 文件到 student 数据库表中。 # 这个命令其实就是HDFS的put操作。新建相同格式的txt放到对应的HDFS目录下，同样可以查询到数据 hive&gt; load data local inpath &#39;/opt/module/data/student.txt&#39; into table student; hive&gt; load data inpath &quot;/HDFS/path&quot; into table stu; #这个是从HDFS上mv文件。 # Hive 查询结果 hive&gt; select * from student; OK 1001 zhangshan 1002 lishi 1003 zhaoliu Time taken: 0.266 seconds, Fetched: 3 row(s) 注意：Metastore 默认存储在自带的derby 数据库中，不能同时打开多个hive窗口，但mysql支持。 MySQL安装安装MySQL 查看mysql 是否安装，如果安装了，卸载mysql rpm -qa|grep mysql # 查询 rpm -e --nodeps mysql-libs-XXXXX.x86_64 # 卸载 安装mysql 服务端：rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm 查看产生的随机密码：cat /root/.mysql_secret 查看mysql 状态：service mysql status 启动mysql服务：service mysql start 安装MySql 客户端：rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm 链接mysql：mysql -uroot -pOEXaQuS8IWkG19Xs 修改密码：mysql&gt;SET PASSWORD=PASSWORD(‘000000’); 退出：mysql&gt;exit; MySql 中user 表中主机配置配置只要是root 用户+密码，在任何主机上都能登录MySQL 数据库。 [root@hadoop102 mysql-libs]# mysql -uroot -p000000 mysql&gt;use mysql; mysql&gt;show tables; mysql&gt;select User, Host, Password from user; mysql&gt;update user set host=&#39;%&#39; where host=&#39;localhost&#39;; mysql&gt;delete from user where Host=&#39;hadoop102&#39;; mysql&gt;delete from user where Host=&#39;127.0.0.1&#39;; mysql&gt;delete from user where Host=&#39;::1&#39;; mysql&gt;flush privileges; mysql&gt;quit; 配置Metastore 到MySql 驱动拷贝：cp /opt/software/mysql-libs/mysql-connector-java-5.1.27/mysql-c onnector-java-5.1.27-bin.jar /opt/module/hive/lib/ 配置 hive-site.xml 先创建 hive-site.xml &lt;?xml version=&quot;1.0&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseI fNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;000000&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 配置完毕后，如果启动 hive 异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop 集群） 打开多个窗口，分别启动 hive：bin/hive 启动 hive 后，回到 MySQL 窗口查看数据库，显示增加了 metastore 数据库 mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | metastore | | mysql | | performance_schema | | test | +--------------------+ Hive JDBC访问 启动hiveserver2 服务：[atguigu@hadoop102 hive]$ bin/hiveserver2 启动beeline [atguigu@hadoop102 hive]$ bin/beeline Beeline version 1.2.1 by Apache Hive beeline&gt; 连接hiveserver2 beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车） Connecting to jdbc:hive2://hadoop102:10000 Enter username for jdbc:hive2://hadoop102:10000: atguigu（回车） Enter password for jdbc:hive2://hadoop102:10000: （直接回车） Connected to: Apache Hive (version 1.2.1) Driver: Hive JDBC (version 1.2.1) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://hadoop102:10000&gt; show databases; +----------------+--+ | database_name | +----------------+--+ | default | | hive db2 | +----------------+--+ Hive命令[atguigu@hadoop102 hive]$ bin/hive -help usage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive. commands. e.g. -d A=B or --define A=B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property=value&gt; Use value for given property --hivevar &lt;key=value&gt; Variable subsitution to apply to hive. commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) [atguigu@hadoop102 hive]$ bin/hive -e “select id from student;” [atguigu@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql 在 hive cli 命令窗口中如何查看 hdfs 文件系统：hive&gt; dfs -ls /; 在 hive cli 命令窗口中如何查看本地文件系统：hive&gt; ! ls /opt/module/datas; 查看在 hive 中输入的所有历史命令：入到当前用户的根目录/root 或/home/atguigu，查看. hivehistory 文件 Hive 常见属性配置Hive 数据仓库位置配置Default 数据仓库的最原始位置是在hdfs 上的：/user/hive/warehouse 路径下。 在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 数据库，直接在数据仓库目录下创建一个文件夹。 在hive-site.xml 文件中添加以下内容： &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; 查询后信息显示配置在 hive-site.xml 文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。 &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; Hive 运行日志信息配置Hive 的log 默认存放在/tmp/atguigu/hive.log 目录下（当前用户名下） 修改 hive 的 log 存放日志到/opt/module/hive/logs 修改/opt/module/hive/conf/hive-log4j.properties.template 文件名称为hive-log4j.properties 在hive-log4j.properties 文件中修改log 存放位置：hive.log.dir=/opt/module/hive/logs 参数配置方式查看当前所有的配置信息：hive&gt;set; 参数的配置三种方式： 配置文件方式 默认配置文件：hive-default.xml 用户自定义配置文件：hive-site.xml 注意：用户自定义配置会覆盖默认配置。另外，Hive 也会读入Hadoop 的配置，因为Hive 是作为Hadoop 的客户端启动的，Hive 的配置会覆盖Hadoop 的配置。配置文件的设定对本机启动的所有Hive 进程都有效。 命令行参数方式（hive外） 启动Hive 时，可以在命令行添加 -hiveconf param=value 来设定参数。 注意：仅对本次 hive 启动有效 如：bin/hive -hiveconf mapred.reduce.tasks=10; 查看参数设置：hive (default)&gt; set mapred.reduce.tasks; 参数声明方式（hive内） 可以在HQL 中使用SET 关键字设定参数。注意：仅对本次 hive 启动有效。 例如： hive (default)&gt; set mapred.reduce.tasks=100; 查看参数设置 hive (default)&gt; set mapred.reduce.tasks; 上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。 集成Tez引擎Tez是一个Hive的运行引擎，性能优于MR。 用Hive直接编写MR程序，假设有四个有依赖关系的MR作业，上图中，绿色是Reduce Task，云状表示写屏蔽，需要将中间结果持久化写到HDFS。 Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。 安装Tez 将apache-tez-0.9.1-bin.tar.gz上传到HDFS的/tez目录下 本地解压缩apache-tez-0.9.1-bin.tar.gz 在Hive的/opt/module/hive/conf下面创建一个tez-site.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;${fs.defaultFS}/tez/apache-tez-0.9.1-bin.tar.gz&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.use.cluster.hadoop-libs&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.history.logging.service.class&lt;/name&gt; &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 在hive-env.sh文件中添加tez环境变量配置和依赖包环境变量配置 # Set HADOOP_HOME to point to a specific hadoop install directory export HADOOP_HOME=/opt/module/hadoop-2.7.2 # Hive Configuration Directory can be controlled by: export HIVE_CONF_DIR=/opt/module/hive/conf # Folder containing extra libraries required for hive compilation/execution can be controlled by: export TEZ_HOME=/opt/module/tez-0.9.1 #是你的tez的解压目录 export TEZ_JARS=&quot;&quot; for jar in `ls $TEZ_HOME |grep jar`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar done for jar in `ls $TEZ_HOME/lib`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar done export HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS 在hive-site.xml文件中添加如下配置，更改hive计算引擎 &lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;tez&lt;/value&gt; &lt;/property&gt; 测试 启动Hive: bin/hive 创建表 hive (default)&gt; create table student( id int, name string); 向表中插入数据 hive (default)&gt; insert into student values(1,”zhangsan”); 如果没有报错就表示成功了 hive (default)&gt; select * from student; 1 zhangsan 注意事项 运行Tez时检查到用过多内存而被NodeManager杀死进程问题 Caused by: org.apache.tez.dag.api.SessionNotRunning: TezSession has already shutdown. Application application_1546781144082_0005 failed 2 times due to AM Container for appattempt_1546781144082_0005_000002 exited with exitCode: -103 For more detailed output, check application tracking page:http://hadoop103:8088/cluster/app/application_1546781144082_0005Then, click on links to logs of each attempt. Diagnostics: Container [pid=11116,containerID=container_1546781144082_0005_02_000001] is running beyond virtual memory limits. Current usage: 216.3 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container. 这种问题是从机上运行的Container试图使用过多的内存，而被NodeManager kill掉了。 [摘录] The NodeManager is killing your container. It sounds like you are trying to use hadoop streaming which is running as a child process of the map-reduce task. The NodeManager monitors the entire process tree of the task and if it eats up more memory than the maximum set in mapreduce.map.memory.mb or mapreduce.reduce.memory.mb respectively, we would expect the Nodemanager to kill the task, otherwise your task is stealing memory belonging to other containers, which you don’t want. 解决方法： 关掉虚拟内存检查，修改yarn-site.xml， ​ yarn.nodemanager.vmem-check-enabled ​ false 修改后一定要分发，并重新启动hadoop集群","tags":[]},{"title":"Hive数据类型","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive03-数据类型.html","text":"基本数据类型 Hive 数据类型 Java 数据类型 长度 例子 TINYINT byte 1byte 有符号整数 20 SMALINT short 2byte 有符号整数 20 INT int 4byte 有符号整数 20 BIGINT long 8byte 有符号整数 20 BOOLEAN boolean 布尔类型，true 或者 false TRUE、FALSE FLOAT float 单精度浮点数 3.14 DOUBLE double 双精度浮点数 3.14 STRING string 字符系列。可以指定字符集。可以使用单引号或者双引号。 ‘now is the time’ “for all good men” TIMESTAMP 时间类型 BINARY 字节数组 对于Hive 的String 类型相当于数据库的varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。 集合数据类型 数据类型 描述 语法示例 STRUCT 和c 语言中的 struct 类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT {first STRING, last STRING},那么第1 个元素可以通过字段.first 来引用。 struct() MAP MAP 是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是 MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素 map() ARRAY 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第 2 个元素可以通过数组名[1]进行引用。 Array() Hive 有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY 和MAP 与Java 中的Array 和Map 类似，而STRUCT 与C 语言中的Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。 案例： { &quot;name&quot;: &quot;songsong&quot;, &quot;friends&quot;: [&quot;bingbing&quot; , &quot;lili&quot;] , //列表 Array, &quot;children&quot;: { //键值 Map, &quot;xiao song&quot;: 18 , &quot;xiaoxiao song&quot;: 19 } &quot;address&quot;: { //结构 Struct, &quot;street&quot;: &quot;hui long guan&quot; , &quot;city&quot;: &quot;beijing&quot; } } 基于上述数据结构，我们在Hive 里创建对应的表，并导入数据。 创建本地测试文件 test.txt songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 注意：MAP，STRUCT 和ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。 Hive 上创建测试表 test: create table test( name string, friends array&lt;string&gt;, children map&lt;string, int&gt;, address struct&lt;street:string, city:string&gt; ) row format delimited fields terminated by &#39;,&#39; # -- 列分隔符 collection items terminated by &#39;_&#39; # --MAP STRUCT 和 ARRAY 的分隔符(数据分割符号) map keys terminated by &#39;:&#39; # -- MAP 中的key 与value 的分隔符 lines terminated by &#39;\\n&#39;; # -- 行分隔符 导入文本数据到测试表: hive (default)&gt; load data local inpath “/opt/module/datas/test.txt” into table test; 访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT 的访问方式： hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from test where name=&quot;songsong&quot;; OK _c0 _c1 city lili 18 beijing Time taken: 0.076 seconds, Fetched: 1 row(s) 类型转化Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式 使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如， 某表达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使 用 CAST 操作。 隐式类型转换规则 任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成INT，INT 可以转换成BIGINT。 所有整数类型、FLOAT 和STRING 类型都可以隐式地转换成DOUBLE TINYINT、SMALLINT、INT 都可以转换为 FLOAT。 BOOLEAN 类型不可以转换为任何其它的类型。 使用 CAST 操作显示进行数据类型转换例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数 1； 如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。 算数运算符 运算符 描述 A+B A 和B 相加 A-B A 减去B A*B A 和B 相乘 A/B A 除以B A%B A 对B 取余 A&amp;B A 和B 按位取与 A \\ B A 和B 按位取或 A^B A 和B 按位取异或 ~A A 按位取反 比较运算符 操作符 支持的数据类型 描述 A=B 基本数据类型 如果A 等于B 则返回TRUE，反之返回FALSE A&lt;=&gt;B 基本数据类型 如果A 和B 都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL 则结果为NULL A&lt;&gt;B,A!=B 基本数据类型 A 或者B 为NULL 则返回NULL；如果A 不等于B，则返回TRUE，反之返回FALSE A&lt;B 基本数据类型 A 或者B 为NULL，则返回NULL；如果A 小于B，则返回TRUE，反之返回FALSE A&lt;=B 基本数据类型 A 或者B 为NULL，则返回NULL；如果A 小于等于B，则返回TRUE，反之返回FALSE A&gt;B 基本数据类型 A 或者B 为NULL，则返回NULL；如果A 大于B，则返回TRUE，反之返回FALSE A&gt;=B 基本数据类型 A 或者B 为NULL，则返回NULL；如果A 大于等于B，则返回TRUE，反之返回FALSE A [NOT] BETWEEN B AND C 基本数据类型 如果A，B 或者C 任一为NULL，则结果为NULL。如果A 的值大于等于B 而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT 关键字则可达到相反的效果。 A IS NULL 基本数据类型 如果A 等于NULL，则返回TRUE，反之返回FALSE A IS NOT NULL 基本数据类型 如果A 不等于NULL，则返回TRUE，反之返回FALSE IN(数值1, 数值2) 基本数据类型 使用 IN 运算显示列表中的值 A [NOT] LIKE B 基本数据类型 B 是一个SQL 下的简单正则表达式，如果A 与其匹配的话，则返回TRUE；反之返回FALSE。B 的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A 必须以字母’x’结尾，而‘%x%’表示A 包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT 关键字则可达到相反的效果。 A RLIKE B, A REGEXP B 基本数据类型 B 是一个正则表达式，如果A 与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK 中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A 相匹配，而不是只需与其字符串匹配。 逻辑运算符 操作符 含义 AND 逻辑与 OR 逻辑或 NOT 逻辑否","tags":[]},{"title":"Hive-DML","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive05-DML.html","text":"数据导入向表中装载数据（Load）hive&gt; load data [local] inpath &#39;/opt/module/datas/student.txt&#39; [overwrite] | into table student [partition (partcol1=val1,…)]; load data:表示加载数据 local:表示从本地加载数据到hive 表；否则从HDFS 加载数据到hive 表 inpath:表示加载数据的路径 overwrite:表示覆盖表中已有数据，否则表示追加 into table:表示加载到哪张表 student:表示具体的表 partition:表示上传到指定分区 hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#39;\\t&#39;; -- 加载本地文件到hive hive (default)&gt; load data local inpath &#39;/opt/module/datas/student.txt&#39; into table default.student; -- 加载HDFS 文件到hive 中 hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive; hive (default)&gt; load data inpath &#39;/user/atguigu/hive/student.txt&#39; into table default.student; -- 加载数据覆盖表中已有的数据 hive (default)&gt; load data inpath &#39;/user/atguigu/hive/student.txt&#39; overwrite into table default.student; 通过查询语句向表中插入数据（Insert）hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by &#39;\\t&#39;; -- 基本插入数据 hive (default)&gt; insert into table student partition(month=&#39;201709&#39;) values(1,&#39;wangwu&#39;); -- 基本模式插入（根据单张表查询结果） hive (default)&gt; insert overwrite table student partition(month=&#39;201708&#39;) select id, name from student where month=&#39;201709&#39;; -- 多插入模式（根据多张表查询结果） hive (default)&gt; from student insert overwrite table student partition(month=&#39;201707&#39;) select id, name where month=&#39;201709&#39; insert overwrite table student partition(month=&#39;201706&#39;) select id, name where month=&#39;201709&#39;; 查询语句中创建表并加载数据（As Select）根据查询结果创建表（查询的结果会添加到新创建的表中） create table if not exists student3 as select id, name from student; 创建表时通过Location 指定加载数据路径-- 创建表，并指定在hdfs 上的位置 hive (default)&gt; create table if not exists student5(id int, name string) row format delimited fields terminated by &#39;\\t&#39; location &#39;/user/hive/warehouse/student5&#39;; -- 上传数据到hdfs 上 hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5; -- 查询数据 hive (default)&gt; select * from student5; Import 数据到指定Hive 表中注意：先用export 导出后，再将数据导入。 可以直接导入不存在的表 hive (default)&gt; import table student2 partition(month=&#39;201709&#39;) from &#39;/user/hive/warehouse/export/student&#39;; 数据导出Insert 导出-- 将查询的结果导出到本地，local directory 这个代表本地文件 hive (default)&gt; insert overwrite local directory &#39;/opt/module/datas/export/student&#39; select * from student; -- 将查询的结果格式化导出到本地 hive(default)&gt;insert overwrite local directory &#39;/opt/module/datas/export/student1&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39; select * from student; -- 将查询的结果导出到HDFS 上(没有local) hive (default)&gt; insert overwrite directory &#39;/user/atguigu/student2&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39; select * from student; Hadoop 命令导出到本地hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/datas/export/student3.txt; Hive Shell 命令导出基本语法：（hive -f/-e 执行语句或者脚本&gt; file） -e 执行语句 -f 执行sql脚本 [atguigu@hadoop102 hive]$ bin/hive -e &#39;select * from default.student;&#39; &gt; /opt/module/datas/export/student4.txt; Export 导出到HDFS 上(defahiveult)&gt; export table default.student to &#39;/user/hive/warehouse/export/student&#39;; 清除表中数据注意：Truncate 只能删除管理表，不能删除外部表中数据。只清除数据，不清除元数据 hive (default)&gt; truncate table student;","tags":[]},{"title":"Hive-DDL","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive04-DDL.html","text":"创建数据库hive&gt; create database if not exists db_hive; -- 默认存储路径是/user/hive/warehouse/*.db。 hive&gt; create database db_hive2 location &#39;/db_hive2.db&#39;; hive&gt; create database db_hive2 location &#39;/hive/db&#39;; -- 指定数据库在HDFS 上存放的位置 -- 路径和数据库名可以不一致 查询数据库-- 显示数据库 hive&gt; show databases; -- 过滤显示查询的数据库 hive&gt; show databases like &#39;db_hive*&#39;; -- 显示数据库信息 hive&gt; desc database db_hive; OK db_hive hdfs://hadoop102:9000/user/hive/warehouse/db_hive.db atguiguUSER -- 显示数据库详细信息 hive&gt; desc database extended db_hive; OK db_hive hdfs://hadoop102:9000/user/hive/warehouse/db_hive.db atguiguUSER 修改数据库用户可以使用ALTER DATABASE 命令为某个数据库的DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。只能修改自定义的属性，其他属性不能修改。 hive&gt; alter database hive set dbproperties(&#39;createtime&#39;=&#39;20170830&#39;); hive&gt; desc database extended db_hive; db_name comment location owner_name owner_type parameters db_hive hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db atguigu USER {createtime=20170830} 删除数据库-- 删除空数据库 hive&gt; drop database db_hive2; -- 如果删除的数据库不存在，最好采用if exists 判断数据库是否存在 hive&gt; drop database if exists db_hive2; -- 如果数据库不为空，可以采用cascade 命令，强制删除 hive&gt; drop database db_hive cascade; 创建表CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 COMMENT：为表和列添加注释。 PARTITIONED BY 创建分区表 CLUSTERED BY 创建分桶表 SORTED BY 不常用 ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 用户在建表的时候可以自定义SerDe 或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive 通过SerDe确定表的具体的列的数据。 SerDe 是Serialize/Deserilize 的简称，目的是用于序列化和反序列化。 STORED AS 指定存储文件类型 常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件） 如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 LOCATION ：指定表在HDFS 上的存储位置。 LIKE 允许用户复制现有的表结构，但是不复制数据 管理表(内部表)默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir (如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive 也会删除这个表中数据。管理表不适合和其他工具共享数据。 -- 普通创建表 create table if not exists student2( id int, name string) row format delimited fields terminated by &#39;\\t&#39; stored as textfile location &#39;/user/hive/warehouse/student2&#39;; -- 根据查询结果创建表（查询的结果会添加到新创建的表中） create table if not exists student3 as select id, name from student; -- 根据已经存在的表结构创建表 create table if not exists student4 like student; -- 查询表的类型 hive (default)&gt; desc formatted student2; Table Type: MANAGED_TABLE 外部表 (元数据+表)因为表是外部表，所以Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。删除后再建立相同的表（即恢复元数据），不导入数据，依然能查到原来的数据。 每天将收集到的网站日志定期流入HDFS 文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT 进入内部表。 create external table if not exists default.dept( deptno int, dname string, loc int ) row format delimited fields terminated by &#39;\\t&#39;; load data local inpath &#39;/opt/module/data/dept.txt&#39; into table default.dept; hive (default)&gt; desc formatted dept; Table Type: EXTERNAL_TABLE 管理表与外部表的互相转换-- 修改内部表student2 为外部表 alter table student2 set tblproperties(&#39;EXTERNAL&#39;=&#39;TRUE&#39;); # 必须大写 -- 修改外部表student2 为内部表 alter table student2 set tblproperties(&#39;EXTERNAL&#39;=&#39;FALSE&#39;); 分区表分区表实际上就是对应一个HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 -- 需要根据日期对日志进行管理 /user/hive/warehouse/log_partition/20170702/20170702.log /user/hive/warehouse/log_partition/20170703/20170703.log /user/hive/warehouse/log_partition/20170704/20170704.log -- 创建分区表 hive (default)&gt; create table dept_partition( deptno int, dname string, loc string ) partitioned by (month string) row format delimited fields terminated by &#39;\\t&#39;; -- 加载数据到分区表中 hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table default.dept_partition partition(month=&#39;201709&#39;); hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table default.dept_partition partition(month=&#39;201708&#39;); hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table default.dept_partition partition(month=&#39;201707’); -- 查询分区表中数据 hive (default)&gt; select * from dept_partition where month=&#39;201709&#39;; -- 多分区联合查询 hive (default)&gt; select * from dept_partition where month=&#39;201709&#39; union select * from dept_partition where month=&#39;201708&#39; union select * from dept_partition where month=&#39;201707&#39;; _u3.deptno _u3.dname _u3.loc _u3.month 10 ACCOUNTING NEW YORK 201707 10 ACCOUNTING NEW YORK 201708 10 ACCOUNTING NEW YORK 201709 20 RESEARCH DALLAS 201707 20 RESEARCH DALLAS 201708 20 RESEARCH DALLAS 201709 30 SALES CHICAGO 201707 30 SALES CHICAGO 201708 30 SALES CHICAGO 201709 40 OPERATIONS BOSTON 201707 40 OPERATIONS BOSTON 201708 40 OPERATIONS BOSTON 201709 -- 增加分区 hive (default)&gt; alter table dept_partition add partition(month=&#39;201706&#39;) ; hive (default)&gt; alter table dept_partition add partition(month=&#39;201705&#39;) partition(month=&#39;201704&#39;); -- 删除分区 hive (default)&gt; alter table dept_partition drop partition (month=&#39;201704&#39;); hive (default)&gt; alter table dept_partition drop partition (month=&#39;201705&#39;), partition (month=&#39;201706&#39;); -- 查看分区表有多少分区 hive&gt; show partitions dept_partition; -- 查看分区表结构 hive&gt; desc formatted dept_partition; -- # Partition Information -- # col_name data_type comment month string -- 创建二级分区表 hive (default)&gt; create table dept_partition2( deptno int, dname string, loc string ) partitioned by (month string, day string) row format delimited fields terminated by &#39;\\t&#39;; -- 加载数据到二级分区表中 hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table default.dept_partition2 partition(month=&#39;201709&#39;, day=&#39;13&#39;); -- 查询分区数据 hive (default)&gt; select * from dept_partition2 where month=&#39;201709&#39; and day=&#39;13&#39;; 把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式： 上传数据后修复 -- 上传数据 hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12; hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; -- 查询数据（查询不到刚上传的数据） hive (default)&gt; select * from dept_partition2 where month=&#39;201709&#39; and day=&#39;12&#39;; -- 执行修复命令 hive&gt; msck repair table dept_partition2; 上传数据后添加分区 -- 上传数据 hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11; hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=11; -- 执行添加分区 hive (default)&gt; alter table dept_partition2 add partition(month=&#39;201709&#39;, day=&#39;11&#39;); 创建文件夹后load 数据到分区 -- 创建目录 hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10; -- 上传数据 hive (default)&gt; load data local inpath &#39;/opt/module/datas/dept.txt&#39; into table dept_partition2 partition(month=&#39;201709&#39;,day=&#39;10&#39;); 修改表-- 重命名 ALTER TABLE table_name RENAME TO new_table_name -- 增加/修改/替换列信息 -- 更新列，可以更改列名和列的数据类型，即使不修改类型也要写上 ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] -- 增加和替换列 -- ADD 是代表新增一字段，字段位置在所有列后面(partition 列前)，REPLACE 则是表示替换表中所有字段 ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) -- alter table xixi replace colums(id int, name string). -- 把所有字段替换为两个字段，相当于至保留前两个字段，修改了元数据，但没有修改HDFS上的文件。如果多加列，数据会显示为null。HDFS上文件没有添加相应字段的数据 hive (default)&gt; alter table dept_partition add columns(deptdesc string); # 添加列 hive (default)&gt; alter table dept_partition change column deptdesc desc int; # 更新列 hive (default)&gt; alter table dept_partition replace columns(deptno string, dname string, loc string); # 替换列 删除表hive (default)&gt; drop table dept_partition;","tags":[]},{"title":"Hive-查询","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive06-查询.html","text":"相关指南：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [ORDER BY col_list] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list] ] [LIMIT number] 基本查询 SQL 语言大小写不敏感。 SQL 可以写在一行或者多行 关键字不能被缩写也不能分行 各子句一般要分行写 使用缩进提高语句的可读性 列别名：便于计算、紧跟列名，也可以在列名和别名之间加入关键字‘AS’ hive (default)&gt; select * from emp; hive (default)&gt; select empno, ename from emp; hive (default)&gt; select ename AS name, deptno dn from emp; -- 求总行数（count）、求工资的最大值（max）、求工资的最小值（min） -- 求工资的总和（sum）、求工资的平均值（avg） hive (default)&gt; select count(*) cnt from emp; hive (default)&gt; select max(sal) max_sal from emp; hive (default)&gt; select min(sal) min_sal from emp; hive (default)&gt; select sum(sal) sum_sal from emp; hive (default)&gt; select avg(sal) avg_sal from emp; -- 典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数 hive (default)&gt; select * from emp limit 5; where语句&amp;Like 使用WHERE 子句，将不满足条件的行过滤掉 WHERE 子句紧随FROM 子句 hive (default)&gt; select * from emp where sal =5000; hive (default)&gt; select * from emp where sal between 500 and 1000; -- between and 的区间不能反 hive (default)&gt; select * from emp where comm is null; hive (default)&gt; select * from emp where sal IN (1500, 5000); hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30; hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30; hive (default)&gt; select * from emp where deptno not IN(30, 20); -- Like 和RLike 使用LIKE 运算选择类似的值 选择条件可以包含字符或数字: % 代表零个或多个字符(任意个字符) _ 代表一个字符 RLIKE 子句是Hive 中这个功能的一个扩展，其可以通过Java 的正则表达式这个更强大的语言来指定匹配条件 hive (default)&gt; select * from emp where sal LIKE &#39;2%&#39;; # 查找以2 开头薪水的员工信息 hive (default)&gt; select * from emp where sal LIKE &#39;_2%&#39;; # 查找第二个数值为2 的薪水的员工信息 hive (default)&gt; select * from emp where sal RLIKE &#39;[2]&#39;; # 查找薪水中含有2 的员工信息 分组（Group By &amp; Having）-- Group By GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。 hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno; # 计算emp 表每个部门的平均工资 hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group byt.deptno, t.job; # 计算emp 每个部门中每个岗位的最高薪水 -- Having 语句 having 与 where 不同点: 1. where 针对表中的列发挥作用，查询数据；having 针对查询结果中的列发挥作用，筛选数据。 2. where 后面不能写聚合函数，而having 后面可以使用聚合函数。 3. having 只用于 group by 分组统计语句。 hive (default)&gt; select deptno, avg(sal) from emp group by deptno; # 求每个部门的平均工资 hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000; # 求每个部门的平均薪水大于2000 的部门 join语句Hive 支持通常的SQL JOIN 语句，但是只支持等值连接，不支持非等值连接。 -- 等值Join hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept d on e.deptno = d.deptno; -- 内连接, 只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno; -- 左外连接, JOIN 操作符左边表中符合WHERE 子句的所有记录将会被返回。 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno; -- 右外连接, JOIN 操作符右边表中符合WHERE 子句的所有记录将会被返回。 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno; -- 满外连接, 将会返回所有表中符合WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL 值替代。 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno; -- 多表连接 注意：连接 n 个表，至少需要 n-1 个连接条件。例如：连接三个表，至少需要两个连接条件。 hive (default)&gt;SELECT e.ename, d.deptno, l.loc_name FROM emp e JOIN dept d ON d.deptno = e.deptno JOIN location l ON d.loc = l.loc; 大多数情况下，Hive 会对每对JOIN 连接对象启动一个MapReduce 任务。本例中会首 先启动一个MapReduce job 对表e 和表d 进行连接操作，然后会再启动一个MapReduce job 将第一个MapReduce job 的输出和表l进行连接操作。 注意：为什么不是表d 和表l 先进行连接操作呢？这是因为Hive 总是按照从左到右的顺序执行的。 -- 笛卡尔积 笛卡尔集会在下面条件下产生 1. 省略连接条件 2. 连接条件无效 3. 所有表中的所有行互相连接 hive (default)&gt; select empno, dname from emp, dept; -- 连接谓词中不支持or hive (default)&gt; select e.empno, e.ename, d.deptno &gt; from emp e &gt; join dept d &gt; on e.deptno=d.deptno or e.ename=d.dname; FAILED: SemanticException [Error 10019]: Line 10:3 OR not supported in JOIN currently &#39;dname&#39; 排序全局排序（Order By）Order By：全局排序，一个Reducer, 无论set几个reducer，order by都使用一个reducer ASC（ascend）: 升序（默认） DESC（descend）: 降序 ORDER BY 子句在 SELECT 语句的结尾 hive (default)&gt; select * from emp order by sal; # 查询员工信息按工资升序排列 hive (default)&gt; select * from emp order by sal desc; # 查询员工信息按工资降序排列 hive (default)&gt; select ename, sal*2 twosal from emp order by twosal; hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal; # 按照部门和工资升序排序 每个MapReduce 内部排序（Sort By）Sort By：每个Reducer 内部进行排序，对全局结果集来说不是排序。 -- 设置reduce 个数 hive (default)&gt; set mapreduce.job.reduces=3; -- 查看设置reduce 个数 hive (default)&gt; set mapreduce.job.reduces; -- 根据部门编号降序查看员工信息 hive (default)&gt; select * from emp sort by empno desc; -- 将查询结果导入到文件中（按照部门编号降序排序） hive (default)&gt; insert overwrite local directory &#39;/opt/module/datas/sortby-result&#39; select * from emp sort by deptno desc; 分区排序（Distribute By）Distribute By：类似MR 中partition，进行分区，结合sort by 使用。 注意，Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。 对于distribute by 进行测试，一定要分配多reduce 进行处理，否则无法看到distribute by的效果。 -- 先按照部门编号分区，再按照员工编号降序排序。 hive (default)&gt; set mapreduce.job.reduces=3; hive (default)&gt; insert overwrite local directory &#39;/opt/module/datas/distribute-result&#39; select * from emp distribute by deptno sort by empno desc; Cluster By当distribute by 和sorts by 字段相同时，可以使用cluster by 方式 cluster by 除了具有distribute by 的功能外还兼具sort by 的功能。但是排序只能是升序排序，不能指定排序规则为ASC 或者DESC。 -- 两种写法等价 hive (default)&gt; select * from emp cluster by deptno; hive (default)&gt; select * from emp distribute by deptno sort by deptno; -- 注意：按照部门编号分区，不一定就是固定死的数值，可以是20 号和30 号部门分到一个分区里面去。 分桶及抽样查询分区针对的是数据的存储路径；分桶针对的是数据文件。 分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。 分桶是将数据集分解成更容易管理的若干部分的另一个技术。 -- 设置属性 hive (default)&gt; set hive.enforce.bucketing=true; hive (default)&gt; set mapreduce.job.reduces=-1; -- 创建分桶表 create table stu_buck(id int, name string) clustered by(id) into 4 buckets row format delimited fields terminated by &#39;\\t&#39;; -- 查看表结构 hive (default)&gt; desc formatted stu_buck; Num Buckets: 4 -- 创建分桶表时，数据通过子查询的方式导入 insert into table stu_buck select id, name from stu; -- 查询分桶的数据 hive (default)&gt; select * from stu_buck; OK stu_buck.id stu_buck.name 1004 ss4 1008 ss8 1012 ss12 1016 ss16 1001 ss1 1005 ss5 1009 ss9 1013 ss13 1002 ss2 1006 ss6 1010 ss10 1014 ss14 1003 ss3 1007 ss7 1011 ss11 1015 ss15 分桶抽样查询 对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive 可以通过对表进行抽样来满足这个需求. -- 查询表stu_buck 中的数据 hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id); -- 注：tablesample 是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。 -- y 必须是 table 总 bucket 数的倍数或者因子。hive 根据 y 的大小，决定抽样的比例。 -- 例如，table 总共分了4 份，当y=2 时，抽取(4/2=)2 个bucket 的数据，当y=8 时，抽取(4/8=)1/2个bucket 的数据. -- 如果抽取 1/2 个桶，则返回相应桶的1/2的数据，也不会抽取第二个桶。 -- x 表示从哪个bucket 开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。 -- 例如，table 总bucket 数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2 个bucket 的数据，抽取第1(x)个和第3(x+y)个bucket 的数据。 -- 注意：x 的值必须小于等于y 的值. 其他查询函数空字段赋值NVL：给值为NULL 的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果string1 为NULL，则NVL 函数返回replace_with 的值，否则返回string1 的值，如果两个参数都为NULL ，则返回NULL。 select nvl(comm,-1) from emp; 时间类-- date_format:格式化时间 hive (default)&gt; select date_format(&#39;2019-06-29&#39;,&#39;yyyy-MM-dd&#39;); OK _c0 2019-06-29 只能是&quot;-&quot;，&quot;/&quot;不能识别，但可以使用函数替换&quot;/&quot;， regexp_replace(&#39;2019/07/03&#39;, &#39;/&#39;, &#39;-&#39;); // 2019-07-03 -- date_add:时间跟天数相加 hive (default)&gt; select date_add(&#39;2019-06-29&#39;,5); OK _c0 2019-07-04 hive (default)&gt; select date_add(&#39;2019-06-29&#39;,-5); OK _c0 2019-06-24 -- date_sub:时间跟天数相减 hive (default)&gt; select date_sub(&#39;2019-06-29&#39;,5); OK _c0 2019-06-24 hive (default)&gt; select date_sub(&#39;2019-06-29 12:12:12&#39;,5); OK _c0 2019-06-24 hive (default)&gt; select date_sub(&#39;2019-06-29&#39;,-5); OK _c0 2019-07-04 -- datediff:两个时间相减 hive (default)&gt; select datediff(&#39;2019-06-29&#39;,&#39;2019-06-24&#39;); OK _c0 5 hive (default)&gt; select datediff(&#39;2019-06-24&#39;,&#39;2019-06-29&#39;); OK _c0 -5 hive (default)&gt; select datediff(&#39;2019-06-24 12:12:12&#39;,&#39;2019-06-29&#39;); OK _c0 -5 hive (default)&gt; select datediff(&#39;2019-06-24 12:12:12&#39;,&#39;2019-06-29 13:13:13&#39;); OK _c0 -5 CASE WHEN数据准备： name dept_id sex 悟空 A 男 大海 A 男 宋宋 B 男 凤姐 A 女 婷姐 B 女 婷婷 B 女 -- 需求：求出不同部门男女各多少人，结果如下 A 2 1 B 1 2 -- 创建hive 表并导入数据 create table emp_sex( name string, dept_id string, sex string) row format delimited fields terminated by &quot;\\t&quot;; load data local inpath &#39;/opt/module/data/emp_sex.txt&#39; into table emp_sex; -- 按需求查询数据 select dept_id, sum(case sex when &#39;男&#39; then 1 else 0 end) male_count, sum(case sex when &#39;女&#39; then 1 else 0 end) female_count from emp_sex group by dept_id; -- case when 可以替换为 sum(if(sex=&#39;男&#39;,1,0)) male_count, sum(if(sex=&#39;女&#39;,1,0)) famale_count 行转列CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串; CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数是剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间; COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array 类型字段。 数据准备： name constellation blood_type 孙悟空 白羊座 A 大海 射手座 A 宋宋 白羊座 B 猪八戒 白羊座 A 凤姐 射手座 A -- 需求：把星座和血型一样的人归类到一起。预期结果如下 射手座,A 大海|凤姐 白羊座,A 孙悟空|猪八戒 白羊座,B 宋宋 -- 创建hive 表并导入数据 create table person_info( name string, constellation string, blood_type string) row format delimited fields terminated by &quot;\\t&quot;; load data local inpath &quot;/opt/module/data/person_info.txt&quot; into table person_info; -- 按需求查询数据 select t1.base, concat_ws(&#39;|&#39;, collect_set(t1.name)) name from (select name, concat(constellation, &quot;,&quot;, blood_type) base from person_info) t1 group by t1.base; 列转行EXPLODE(col)：将hive 一列中复杂的array 或者map 结构拆分成多行。 LATERAL VIEW：用法：LATERAL VIEW udtf(expression) tableAlias(表名) AS columnAlias(udtf的列别名)。解释：用于和split, explode 等UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合 数据准备： movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 《Lie to me》 悬疑,警匪,动作,心理,剧情 《战狼2》 战争,动作,灾难 -- 将电影分类中的数组数据展开。预期结果如下： 《疑犯追踪》 悬疑 《疑犯追踪》 动作 《疑犯追踪》 科幻 《疑犯追踪》 剧情 《Lie to me》 悬疑 《Lie to me》 警匪 《Lie to me》 动作 《Lie to me》 心理 《Lie to me》 剧情 《战狼2》 战争 《战狼2》 动作 《战狼2》 灾难 -- 创建hive 表并导入数据 create table movie_info( movie string, category array&lt;string&gt;) row format delimited fields terminated by &quot;\\t&quot; collection items terminated by &quot;,&quot;; load data local inpath &quot;/opt/module/datas/movie.txt&quot; into table movie_info; -- 按需求查询数据 select movie, category_name from movie_info lateral view explode(category) table_tmp as category_name; 窗口函数OVER()：指定分析函数工作的数据窗口大小(理解为当前行可以看到的视野)，这个数据窗口大小可能会随着行的变化而变化； CURRENT ROW：当前行； n PRECEDING：往前n 行数据； n FOLLOWING：往后n 行数据； UNBOUNDED：起点 UNBOUNDED PRECEDING 表示从前面的起点 UNBOUNDED FOLLOWING 表示到后面的终点； LAG(col,n)：往前第n 行数据； LEAD(col,n)：往后第n 行数据 NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1 开始，对于每一行，NTILE 返回此行所属的组的编号。注意：n 必须为int 类型。 name，orderdate，cost jack,2017-01-01,10 tony,2017-01-02,15 jack,2017-02-03,23 tony,2017-01-04,29 jack,2017-01-05,46 jack,2017-04-06,42 tony,2017-01-07,50 jack,2017-01-08,55 mart,2017-04-08,62 mart,2017-04-09,68 neil,2017-05-10,12 mart,2017-04-11,75 neil,2017-06-12,80 mart,2017-04-13,94 -- 查询在2017 年4 月份购买过的顾客及总人数 select name,count(*) over () from business where substring(orderdate,1,7) = &#39;2017-04&#39; group by name; -- 查询顾客的购买明细及月购买总额 select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business; -- 上述的场景,要将cost 按照日期进行累加 select name,orderdate,cost, sum(cost) over() as sample1,-- 所有行相加 sum(cost) over(partition by name) as sample2,-- 按name 分组，组内数据相加 sum(cost) over(distribute by name sort by orderdate) as sample33， sum(cost) over(partition by name order by orderdate) as sample3, -- 按name 分组，组内数据累加 sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,-- 和sample3 一样,由起点到当前行的聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, -- 当前行和前面一行做聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,-- 当前行和前边一行及后面一行 sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 -- 当前行及后面所有行 from business; -- 查看顾客上次的购买时间 select name,orderdate,cost, lag(orderdate,1,&#39;1900-01-01&#39;) over(partition by name order by orderdate ) as time1, lag(orderdate,2) over (partition by name order by orderdate) as time2 from business; -- 查询前20%时间的订单信息 select * from ( select name,orderdate,cost, ntile(5) over(order by orderdate) sorted from business ) t where sorted = 1; Rank RANK() 排序相同时会重复，总数不会变 DENSE_RANK() 排序相同时会重复，总数会减少 ROW_NUMBER() 会根据顺序计算 select name, subject, score, rank() over(partition by subject order by score desc) rp, dense_rank() over(partition by subject order by score desc) drp, row_number() over(partition by subject order by score desc) rmp from score; name subject score rp drp rmp 孙悟空 数学 95 1 1 1 宋宋 数学 86 2 2 2 婷婷 数学 85 3 3 3 大海 数学 56 4 4 4 宋宋 英语 84 1 1 1 大海 英语 84 1 1 2 婷婷 英语 78 3 2 3 孙悟空 英语 68 4 3 4 大海 语文 94 1 1 1 孙悟空 语文 87 2 2 2 婷婷 语文 65 3 3 3 宋宋 语文 64 4 4 4","tags":[]},{"title":"Hive08-压缩和存储","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive08-压缩和存储.html","text":"Hadoop 源码编译支持Snappy 压缩 hadoop-2.7.2-src.tar.gz jdk-8u144-linux-x64.tar.gz snappy-1.1.3.tar.gz apache-maven-3.0.5-bin.tar.gz protobuf-2.5.0.tar.gz 准备编译环境 yum install svn yum install autoconf automake libtool cmake yum install ncurses-devel yum install openssl-devel yum install gcc* 编译安装snappy cd snappy-1.1.3/ ./configure make make install &#39;查看snappy 库文件 ls -lh /usr/local/lib |grep snappy 编译安装protobuf cd protobuf-2.5.0/ ./configure make make install &#39;查看protobuf 版本以测试是否安装成功 protoc --version 编译hadoop native cd hadoop-2.7.2-src/ mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy 执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/hadoop-2.7.2.tar.gz 即为新生成的支持snappy 压缩的二进制安装包。 Hadoop 压缩配置MR 支持的压缩编码 压缩格式 工具 算法 文件扩展名 是否可切分 DEFAULT 无 DEFAULT .deflate 否 Gzip gzip DEFAULT .gz 否 bzip2 bzip2 bzip2 .bz2 是 LZO lzop LZO .lzo 是 Snappy 无 Snappy .snappy 否 为了支持多种压缩/解压缩算法，Hadoop 引入了编码/解码器 压缩格式 对应的编码/解码器 DEFAULT org.apache.hadoop.io.compress.DefaultCodec Gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较： 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 Gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s 压缩参数配置core-site.xml 参数 默认值 阶段 建议 io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec 输入压缩 Hadoop 使用文件扩展名判断是否支持某种编解码器 mapred-site.xml 参数 默认值 阶段 建议 mapreduce.map.output.compress false mapper 输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.DefaultCodec mapper 输出 使用LZO、LZ4 或snappy 编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress false reducer 输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec org.apache.hadoop.io.compress. DefaultCodec reducer 输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type RECORD reducer 输出 SequenceFile 输出使用的压缩类型：NONE 和BLOCK 开启Map 输出阶段压缩开启map 输出阶段压缩可以减少job 中map 和Reduce task 间数据传输量。具体配置如下： &#39;开启hive 中间传输数据压缩功能 hive (default)&gt;set hive.exec.compress.intermediate=true; &#39;开启mapreduce 中map 输出压缩功能 hive (default)&gt;set mapreduce.map.output.compress=true; &#39;设置mapreduce 中map 输出数据的压缩方式 hive (default)&gt;set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec; &#39;执行查询语句 hive (default)&gt; select count(ename) name from emp; 开启Reduce 输出阶段压缩当Hive 将输出写入到表中时， 输出内容同样可以进行压缩。属性hive.exec.compress.output 控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。 &#39;开启hive 最终输出数据压缩功能 hive (default)&gt;set hive.exec.compress.output=true; &#39;开启mapreduce 最终输出数据压缩 hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true; &#39;设置mapreduce 最终数据输出压缩方式 hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; &#39;设置mapreduce 最终数据输出压缩为块压缩 hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK; &#39;测试一下输出结果是否是压缩文件 hive (default)&gt; insert overwrite local directory &#39;/opt/module/datas/distribute-result&#39; select * from emp distribute by deptno sort by empno desc; 文件存储格式Hive 支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。图中左边为逻辑表，右边第一个为行式存储，第二个为列式存储。 列式存储和行式存储 TEXTFILE 和SEQUENCEFILE 的存储格式都是基于行存储的； ORC 和PARQUET 是基于列式存储的。 行存储的特点 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。 列存储的特点 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。 TextFile 格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2 使用，但使用Gzip 这种方式，hive 不会对数据进行切分，从而无法对数据进行并行操作。 Orc 格式Orc (Optimized Row Columnar)是Hive 0.11 版里引入的新的存储格式。 每个Orc 文件由1 个或多个stripe 组成，每个stripe 250MB 大小，这个Stripe 实际相当于RowGroup 概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe 里有三部分组成，分别是Index Data，Row Data，Stripe Footer： Index Data：一个轻量级的index，默认是每隔1W 行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data 中的offset。 Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream 来存储。 Stripe Footer：存的是各个Stream 的类型，长度等信息。 每个文件有一个File Footer，这里面存的是每个Stripe 的行数，每个Column 的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek 到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe 信息，再读各个Stripe，即从后往前读。 Parquet 格式Parquet 是面向分析型业务的列式存储格式，由Twitter 和Cloudera 合作开发，2015 年5月从Apache 的孵化器里毕业成为Apache 顶级项目。 Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet 格式文件是自解析的。 通常情况下，在存储Parquet 数据的时候会按照Block 大小设置行组的大小，由于一般情况下每一个Mapper 任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper 任务处理，增大任务执行并行度。 上图展示了一个Parquet 文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet 文件，Footer length 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema 信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet 中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet 中还不支持索引页。 存储和压缩结合修改Hadoop 集群具有Snappy 压缩方式&#39;查看hadoop checknative 命令使用&#39; [atguigu@hadoop104 hadoop-2.7.2]$ hadoop checknative [-a|-h] check native hadoop and compression libraries availability &#39;查看hadoop 支持的压缩方式&#39; [atguigu@hadoop104 hadoop-2.7.2]$ hadoop checknative 17/12/24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version 17/12/24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library Native library checking: hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so zlib: true /lib64/libz.so.1 snappy: false lz4: true revision:99 bzip2: false &#39;将编译好的支持Snappy 压缩的hadoop-2.7.2.tar.gz 包导入到hadoop102 的 /opt/software 中&#39; &#39;进入到/opt/software/hadoop-2.7.2/lib/native 路径可以看到支持Snappy 压缩的动态链接库&#39; libsnappy.a libsnappy.la libsnappy.so -&gt; libsnappy.so.1.3.0 libsnappy.so.1 -&gt; libsnappy.so.1.3.0 libsnappy.so.1.3.0 &#39;拷贝/opt/software/hadoop-2.7.2/lib/native 里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native 路径上&#39; &#39;分发集群&#39; &#39;再次查看hadoop 支持的压缩类型&#39; [atguigu@hadoop102 hadoop-2.7.2]$ hadoop checknative 17/12/24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will usepure-Java version 17/12/24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library Native library checking: hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so zlib: true /lib64/libz.so.1 snappy: true /opt/module/hadoop-2.7.2/lib/native/libsnappy.so.1 lz4: true revision:99 bzip2: false &#39;重新启动hadoop 集群和hive&#39; 测试存储和压缩在实际的项目开发当中，hive 表的数据存储格式一般选择：orc 或parquet。压缩方式一般选择snappy，lzo。 &#39;创建一个SNAPPY 压缩的ORC 存储方式: create table log_orc_snappy( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) row format delimited fields terminated by &#39;\\t&#39; stored as orc tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;); &#39;插入数据&#39; &#39;查看插入后数据&#39; hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/; ORC 存储方式的压缩： Key Default Notes orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries (must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)","tags":[]},{"title":"Hive07-函数","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive07-函数.html","text":"系统内置函数查看系统自带的函数：hive&gt; show functions; 显示自带的函数的用法：hive&gt; desc function upper; 详细显示自带的函数的用法：hive&gt; desc function extended upper; 自定义函数 UDF（User-Defined-Function）：一进一出 UDAF（User-Defined Aggregation Function）：聚集函数，多进一出，类似于：count/max/min UDTF（User-Defined Table-Generating Functions）：一进多出，如 lateral view explore() 官方文档地址：https://cwiki.apache.org/confluence/display/Hive/HivePlugins 自定义UDF 函数UDF(user defined functions) 用于处理单行数据，并生成单个数据行。 编程步骤 继承org.apache.hadoop.hive.ql.UDF 需要实现evaluate 函数；evaluate 函数支持重载；回调方法，函数名不可修改 在hive 的命令行窗口创建函数 添加 jar：add jar linux_jar_path 创建 function：create [temporary] function [dbname.]function_name AS class_name; 在hive 的命令行窗口删除函数 Drop [temporary] function [if exists] [dbname.]function_name; UDF 必须要有返回类型，可以返回null，但是返回类型不能为void 实操： 创建一个Maven 工程Hive； 导入依赖 &lt;dependencies&gt; &lt;!--https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 创建自定义UDF类 package com.atguigu.hive; import org.apache.hadoop.hive.ql.exec.UDF; public class Lower extends UDF { public String evaluate (String s) { if (s == null) { return null; } return s.toLowerCase(); } } 打成jar 包上传到服务器/opt/module/datas/udf.jar 将jar 包添加到hive 的classpath：hive (default)&gt; add jar /opt/module/datas/udf.jar; 创建临时函数与开发好的java class 关联：hive (default)&gt; create temporary function mylower as“com.atguigu.hive.Lower”; 在hql 中使用自定义的函数：hive (default)&gt; select ename, mylower(ename) lowername from emp; 自定义UDTF 函数UDTF(user defined Table functions) 用于处理单行数据，并生成多个数据行。 用户定义聚集函数（user-defined aggregate function ）， UDAF 自定义一个UDTF 实现将一个任意分割符的字符串切割成独立的单词 package com.atguigu.udtf; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory; import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory; import java.util.ArrayList; import java.util.List; public class MyUDTF extends GenericUDTF { private ArrayList&lt;String&gt; outList = new ArrayList&lt;&gt;(); @Override public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException { //1.定义输出数据的列名和类型 List&lt;String&gt; fieldNames = new ArrayList&lt;&gt;(); List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;&gt;(); //2.添加输出数据的列名和类型 fieldNames.add(&quot;lineToWord&quot;); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); } @Override public void process(Object[] args) throws HiveException { //1.获取原始数据 String arg = args[0].toString(); //2.获取数据传入的第二个参数，此处为分隔符 String splitKey = args[1].toString(); //3.将原始数据按照传入的分隔符进行切分 String[] fields = arg.split(splitKey); //4.遍历切分后的结果，并写出 for (String field : fields) { //集合为复用的，首先清空集合 outList.clear(); //将每一个单词添加至集合 outList.add(field); //将集合内容写出 forward(outList); } } @Override public void close() throws HiveException { } } 打成jar 包上传到服务器/opt/module/data/udtf.jar 将jar 包添加到hive 的classpath 下：hive (default)&gt; add jar /opt/module/data/udtf.jar; 创建临时函数与开发好的java class 关联：hive (default)&gt; create temporary function myudtf as “com.atguigu.hive.MyUDTF”; 在hql 中使用自定义的函数：hive (default)&gt; select myudtf(line, “,”) word from words;","tags":[]},{"title":"Hive09-调优","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive09-企业级调优.html","text":"Fetch 抓取Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用MapReduce 计算。例如：SELECT * FROM employees;在这种情况下，Hive 可以简单地读取employee 对应的存储目录下的文件，然后输出查询结果到控制台。 在hive-default.xml.template 文件中hive.fetch.task.conversion 默认是more，老版本hive默认是minimal，该属性修改为more 以后，在全局查找、字段查找、limit 查找等都不走mapreduce。 &lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;more&lt;/value&gt; &lt;description&gt; Expects one of [none, minimal, more]. Some select queries can be converted to single FETCH task minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins. 0. none : disable hive.fetch.task.conversion 1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns) &lt;/description&gt; &lt;/property&gt; 本地模式大多数的Hadoop Job 是需要Hadoop 提供的完整的可扩展性来处理大数据集的。不过，有时Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。 用户可以通过设置hive.exec.mode.local.auto 的值为true，来让Hive 在适当的时候自动启动这个优化。 set hive.exec.mode.local.auto=true; //开启本地mr //设置local mr 的最大输入数据量，当输入数据量小于这个值时采用local mr 的方式，默认为134217728，即128M set hive.exec.mode.local.auto.inputbytes.max=50000000; //设置local mr 的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4 set hive.exec.mode.local.auto.input.files.max=10; 表的优化小表、大表Join将key 相对分散，并且数据量小的表放在join 的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join 让小的维度表（1000 条以下的记录条数）先进内存。在map 端完成reduce。 实际测试发现：新版的hive 已经对小表JOIN 大表和大表JOIN 小表进行了优化。小表放在左边和右边已经没有明显区别。 大表 Join 大表空KEY 过滤有时join 超时是因为某些key 对应的数据太多，而相同key 对应的数据都会发送到相同的reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key 对应的数据是异常数据，我们需要在SQL 语句中进行过滤。 insert overwrite table jointableselect n.* from (select * from nullidtable where id is not null )n left join ori o on n.id = o.id; 空key 转换有时虽然某个key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join 的结果中，此时我们可以表a 中key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer 上 设置5 个 reduce 个数：set mapreduce.job.reduces = 5; JOIN 两张表：insert overwrite table jointableselect n.* from nullidtable n left join ori b on n.id = b.id; 出现了数据倾斜，某些reducer 的资源消耗远大于其他reducer。 随机分布空null 值：insert overwrite table jointableselect n.* from nullidtable n full join ori o oncase when n.id is null then concat(‘hive’, rand()) else n.id end= o.id; 消除了数据倾斜，负载均衡reducer 的资源消耗。 MapJoin如果不指定MapJoin 或者不符合MapJoin 的条件，那么Hive 解析器会将Join 操作转换成Common Join，即：在Reduce 阶段完成join。容易发生数据倾斜。可以用MapJoin 把小表全部加载到内存在map 端进行join，避免reducer 处理。 开启MapJoin 参数设置; 设置自动选择MapJoin：set hive.auto.convert.join = true; 默认为true 大表小表的阈值设置（默认25M 一下认为是小表）：set hive.mapjoin.smalltable.filesize=25000000; MapJoin 工作机制： Group By默认情况下，Map 阶段同一Key 数据分发给一个reduce，当一个key 数据过大时就倾斜了。 并不是所有的聚合操作都需要在Reduce 端完成，很多聚合操作都可以先在Map 端进行部分聚合，最后在Reduce 端得出最终结果。 是否在Map 端进行聚合，默认为True：hive.map.aggr = true 在Map 端进行聚合操作的条目数目：hive.groupby.mapaggr.checkinterval = 100000 有数据倾斜的时候进行负载均衡（默认是false）：hive.groupby.skewindata = true 当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job 中，Map 的输出结果会随机分布到Reduce 中，每个Reduce 做部分聚合操作，并输出结果，这样处理的结果是==相同的Group By Key 有可能被分发到不同的Reduce 中==，从而达到负载均衡的目的；第二个MR Job 再根据预处理的数据结果按照Group By Key 分布到Reduce 中（这个过程可以保证相同的Group By Key 被分布到同一个Reduce 中），最后完成最终的聚合操作。 Count(Distinct) 去重统计数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT 操作需要用一个Reduce Task 来完成，这一个Reduce 需要处理的数据量太大，就会导致整个Job 很难完成，一般COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换 select count(distinct id) from bigtable; select count(id) from (select id from bigtable group by id) a; &#39;虽然会多用一个Job 来完成，但在数据量大的情况下，这个绝对是值得的&#39; 笛卡尔积尽量避免笛卡尔积，join 的时候不加on 条件，或者无效的on 条件，Hive 只能使用1个reducer 来完成笛卡尔积。 行列过滤列处理：在SELECT 中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。 行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where 后面，那么就会先全表关联，之后再过滤。 -- 测试先关联两张表，再用where 条件过滤 hive (default)&gt; select o.id from bigtable b join ori o on o.id = b.id where o.id &lt;= 10; -- 通过子查询后，再关联表 select b.id from bigtable b join (select id from ori where id &lt;= 10 ) o on b.id = o.id; 动态分区调整关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive 的动态分区，需要进行相应的配置。 开启动态分区功能（默认true，开启） hive.exec.dynamic.partition=true 设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。） hive.exec.dynamic.partition.mode=nonstrict 在所有执行MR 的节点上，最大一共可以创建多少个动态分区 hive.exec.max.dynamic.partitions=1000 在每个执行MR 的节点上，最大可以创建多少个动态分区 hive.exec.max.dynamic.partitions.pernode=100 整个MR Job 中，最大可以创建多少个HDFS 文件 hive.exec.max.created.files=100000 当有空分区生成时，是否抛出异常。一般不需要设置 hive.error.on.empty.partition=false insert overwrite table ori_partitioned_target partition (p_time) select id, time, uid, keyword, url_rank, click_num, click_url, p_time from ori_partitioned; &#39;select插入的最后一个字段就是分区字段&#39; MR 优化设置 Map 数 通常情况下，作业会通过input 的目录产生一个或者多个map 任务。主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。 map 数不是越多越好。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map 任务来完成，而一个map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map 数是受限的。 比如有一个127m 的文件，正常会用一个map 去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map 处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。 针对上面的问题 2 和 3，我们需要采取两种方式来解决：即减少 map 数和增加 map 数； 小文件进行合并在map 执行前合并小文件，减少map 数：CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。 set hive.input.format =org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 复杂文件增加 Map 数当input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加Map数，来使得每个map 处理的数据量减少，从而提高任务的执行效率。 增加map 的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M 公式，调整maxSize 最大值。让maxSize 最大值低于blocksize 就可以增加map 的个数。 合理设置 Reduce 数 调整reduce 个数方法 &#39;在hadoop 的mapred-default.xml 文件中修改&#39; set mapreduce.job.reduces = 15; &#39;每个Reduce 处理的数据量默认是256MB&#39; hive.exec.reducers.bytes.per.reducer=256000000 &#39;每个任务最大的reduce 数，默认为1009&#39; hive.exec.reducers.max=1009 &#39;计算reducer 数的公式&#39; N=min(参数2，总输入数据量/参数1) reduce 个数并不是越多越好 过多的启动和初始化reduce 也会消耗时间和资源； 另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题； 在设置reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce 数；使单个reduce 任务处理数据量大小要合适； 并行执行Hive 会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce 阶段、抽样阶段、合并阶段、limit 阶段。或者Hive 执行过程中可能需要的其他阶段。默认情况下，Hive 一次只会执行一个阶段。不过，某个特定的job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job 的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job 可能就越快完成。 通过设置参数hive.exec.parallel 值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job 中并行阶段增多，那么集群利用率就会增加。当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。 set hive.exec.parallel=true; //打开任务并行执行 set hive.exec.parallel.thread.number=16; //同一个sql 允许最大并行度，默认为8。 严格模式Hive 提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。 通过设置属性hive.mapred.mode 值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode 值为strict，开启严格模式可以禁止3 种类型的查询。 &lt;property&gt; &lt;name&gt;hive.mapred.mode&lt;/name&gt; &lt;value&gt;strict&lt;/value&gt; &lt;description&gt; The mode in which the Hive operations are being performed. In strict mode, some risky queries are not allowed to run. They include: Cartesian Product. No partition being picked up for a query. Comparing bigints and strings. Comparing bigints and doubles. Orderby without limit. &lt;/description&gt; &lt;/property&gt; 对于分区表，除非where 语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表. 对于使用了order by 语句的查询，要求必须使用limit 语句。因为order by 为了执行排序过程会将所有的结果数据分发到同一个Reducer 中进行处理，强制要求用户增加这个LIMIT 语句可以防止Reducer 额外执行很长一段时间。 限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN 查询的时候不使用ON 语句而是使用where 语句，这样关系数据库的执行优化器就可以高效地将WHERE 语句转化成那个ON 语句。不幸的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。 JVM 重用JVM 重用是Hadoop 调优参数的内容，其对Hive 的性能具有非常大的影响，特别是==对于很难避免小文件的场景或task 特别多的场景，这类场景大多数执行时间都很短==。 Hadoop 的默认配置通常是使用派生JVM 来执行map 和Reduce 任务的。这时JVM 的启动过程可能会造成相当大的开销，尤其是执行的job 包含有成百上千task 任务的情况。JVM重用可以使得JVM 实例在同一个job 中重新使用N 次。N 的值可以在Hadoop 的 mapred-site.xml 文件中进行配置。通常在10-20 之间，具体多少需要根据具体业务场景测试得出。 &lt;property&gt; &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;description&gt;How many tasks to run per jvm. If set to -1, there is no limit.&lt;/description&gt; &lt;/property&gt; 这个功能的缺点是，开启JVM 重用将一直占用使用到的task 插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job 中有某几个reduce task 执行的时间要比其他Reduce task 消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task 都结束了才会释放。 推测执行在分布式集群环境下，因为程序Bug（包括Hadoop 本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop 采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。 设置开启推测执行参数：Hadoop 的mapred-site.xml 文件中进行配置 &lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt; &lt;/property&gt; 不过hive 本身也提供了配置项来控制reduce-side 的推测执行： &lt;property&gt; &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;/description&gt; &lt;/property&gt; 关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map 或者Reduce task 的话，那么启动推测执行造成的浪费是非常巨大大。 执行计划（Explain）语法：EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query hive (default)&gt; explain select * from emp; hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by deptno; &#39;查看详细执行计划&#39; hive (default)&gt; explain extended select * from emp; hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp group by deptno;","tags":[]},{"title":"Hive10-实战","date":"2020-07-03T12:58:30.000Z","path":"2020/07/03/07-Hive/Hive10-实战.html","text":"需求描述统计硅谷影音视频网站的常规指标，各种TopN 指标： 统计视频观看数Top10 统计视频类别热度Top10 统计视频观看数Top20 所属类别以及类别包含的Top20 的视频个数 统计视频观看数Top50 所关联视频的所属类别Rank 统计每个类别中的视频热度Top10 统计每个类别中视频流量Top10 统计上传视频最多的用户Top10 以及他们上传的观看次数在前20 视频 统计每个类别视频观看数Top10 数据结构视频表 字段 备注 详细描述 video id 视频唯一id 11 位字符串 uploader 视频上传者 上传视频的用户名String age 视频年龄 视频在平台上的整数天 category 视频类别 上传视频指定的视频分类 length 视频长度 整形数字标识的视频长度 views 观看次数 视频被浏览的次数 rate 视频评分 满分5 分 ratings 流量 视频的流量，整型数字 conments 评论数 一个视频的整数评论数 related ids 相关视频id 相关视频的id，最多20 个 用户表 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int 答案 统计视频观看数Top10 select videoId, views from gulivideo_orc order by views desc limit 10; dMH0bHeiRNg 42513417 0XxI-hvPRRA 20282464 1dmVU08zVpA 16087899 RB-wUgnyGv0 15712924 QjA5faZF1A8 15256922 -_CSo1gOd48 13199833 49IDp76kjPw 11970018 tYnn51C3X_w 11823701 pv5zWaTEVkI 11672017 D2kJZOfq7zk 11184051 统计视频类别热度Top10 select category_name, count(*) category_count from (select videoId, category_name from gulivideo_orc lateral view explode(category) tmp_category as category_name) t1 group by category_name order by category_count desc limit 10; 统计视频观看数Top20 所属类别以及类别包含的Top20 的视频个数 select category_name, count(*) category_count from (select videoId, category_name from (select videoId, views, category from gulivideo_orc order by views desc limit 20) t1 lateral view explode(category) tmp_category as category_name) t2 group by category_name order by category_count desc; 统计视频观看数Top50 所关联视频的所属类别Rank select category_name, count(*) category_count from (select explode(category) category_name from (select category from (select related_id from (select relatedId, views from gulivideo_orc order by views desc limit 50) t1 lateral view explode(relatedId) tmp_related as related_id group by related_id) t2 join gulivideo_orc orc on t2.related_id=orc.videoId) t3 ) t4 group by category_name order by category_count desc; 统计每个类别中的视频热度Top10 select video.videoId, video.views from (select uploader, videos from gulivideo_user_orc order by videos desc limit 10) t1 join gulivideo_orc video on t1.uploader=video.uploader order by views desc limit 20; 统计每个类别中视频流量Top10 统计上传视频最多的用户Top10 以及他们上传的观看次数在前20 视频 统计每个类别视频观看数Top10 1.给每一种类别根据视频观看数添加rank值(倒序) select categoryId, videoId, views, rank() over(partition by categoryId order by views desc) rk from gulivideo_category; 2.过滤前十 select categoryId, videoId, views from (select categoryId, videoId, views, rank() over(partition by categoryId order by views desc) rk from gulivideo_category) t1 where rk&lt;=10;","tags":[]},{"title":"数据结构02-复杂度分析(下)","date":"2020-06-03T13:40:14.000Z","path":"2020/06/03/01-数据结构/数据结构03-复杂度分析(下).html","text":"复杂度分析(下)浅析最好、最坏、平均、均摊时间复杂度今天会继续讲四个复杂度分析方面的知识点，最好情况时间复杂度（best case time complexity）、最坏情况时间复杂度（worst case time complexity）、平均情况时间复杂度（average case time complexity）、均摊时间复杂度（amortized time complexity）。如果这几个概念你都能掌握，那对你来说，复杂度分析这部分内容就没什么大问题了。 最好、最坏情况时间复杂度用上节教你的分析技巧，自己先试着分析一下这段代码的时间复杂度。 // n 表示数组 array 的长度 int find(int[] array, int n, int x) { int i = 0; int pos = -1; for (; i &lt; n; ++i) { if (array[i] == x) pos = i; } return pos; } 应该可以看出来，这段代码要实现的功能是，在一个无序的数组（array）中，查找变量 x 出现的位置。如果没有找到，就返回 -1。按照上节课讲的分析方法，这段代码的复杂度是 O(n)，其中，n 代表数组的长度。 在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。但是，这段代码写得不够高效。我们可以这样优化一下这段查找代码。 // n 表示数组 array 的长度 int find(int[] array, int n, int x) { int i = 0; int pos = -1; for (; i &lt; n; ++i) { if (array[i] == x) { pos = i; break; } } return pos; } 这个时候，问题就来了。我们优化完之后，这段代码的时间复杂度还是 O(n) 吗？很显然，咱们上一节讲的分析方法，解决不了这个问题。 因为，要查找的变量 x 可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量 x，那就不需要继续遍历剩下的 n-1 个数据了，那时间复杂度就是 O(1)。但如果数组中不存在变量 x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了 O(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。 为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度。 顾名思义，最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。就像我们刚刚讲到的，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。 同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。就像刚举的那个例子，如果数组中没有要查找的变量 x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。 平均情况时间复杂度最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，需要引入另一个概念：平均情况时间复杂度，后面简称为平均时间复杂度。 平均时间复杂度又该怎么分析呢？还是借助刚才查找变量 x 的例子来解释。 要查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0～n-1 位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即： 时间复杂度的大 O 标记法中，可以省略掉系数、低阶、常量，所以，把刚刚这个公式简化之后，得到的平均时间复杂度就是 O(n)。 这个结论虽然是正确的，但是计算过程稍微有点儿问题。究竟是什么问题呢？刚讲的这 n+1 种情况，出现的概率并不是一样的。 要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。 因此，前面的推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样： 这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。 引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。 你可能会说，平均时间复杂度分析好复杂啊，还要涉及概率论的知识。实际上，在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。像上一节课举的那些例子那样，很多时候，使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，才会使用这三种复杂度表示法来区分。 均摊时间复杂度均摊时间复杂度，听起来跟平均时间复杂度有点儿像。对于初学者来说，这两个概念确实非常容易弄混。我前面说了，大部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。 // array 表示一个长度为 n 的数组 // 代码中的 array.length 就等于 n int[] array = new int[n]; int count = 0; void insert(int val) { if (count == array.length) { int sum = 0; for (int i = 0; i &lt; array.length; ++i) { sum = sum + array[i]; } array[0] = sum; count = 1; } array[count] = val; ++count; } 这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的 count == array.length 时，我们用 for 循环遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。 那这段代码的时间复杂度是多少呢？可以先用我们刚讲到的三种时间复杂度的分析方法来分析一下： 最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置就可以了，所以最好情况时间复杂度为 O(1)。 最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 O(n)。 那平均时间复杂度是多少呢？答案是 O(1)。我们还是可以通过前面讲的概率论的方法来分析。假设数组的长度是 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂度是 O(1)。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 O(n)。而且，这 n+1 种情况发生的概率一样，都是 1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是： 至此为止，前面的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是这个例子里的平均复杂度分析其实并不需要这么复杂，不需要引入概率论的知识。这是为什么呢？我们先来对比一下这个 insert() 的例子和前面那个 find() 的例子，你就会发现这两者有很大差别。 首先，find() 函数在极端情况下，复杂度才为 O(1)。但 insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()第一个区别于 find() 的地方我们再来看第二个不同的地方。对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。 所以，针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。 针对这种特殊的场景，我们引入了一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。 那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？ 我们还是继续看在数组中插入数据的这个例子。每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。这就是均摊分析的大致思路。你都理解了吗？ 均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。为了方便你理解、记忆，我这里简单总结一下它们的应用场景。如果你遇到了，知道是怎么回事儿就行了。 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。 尽管很多数据结构和算法书籍都花了很大力气来区分平均时间复杂度和均摊时间复杂度，但其实我个人认为，均摊时间复杂度就是一种特殊的平均时间复杂度，我们没必要花太多精力去区分它们。你最应该掌握的是它的分析方法，摊还分析。至于分析出来的结果是叫平均还是叫均摊，这只是个说法，并不重要。 内容小结今天我们学习了几个复杂度分析相关的概念，分别有：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。之所以引入这几个复杂度概念，是因为，同一段代码，在不同输入的情况下，复杂度量级有可能是不一样的。 在引入这几个概念之后，我们可以更加全面地表示一段代码的执行效率。而且，这几个概念理解起来都不难。最好、最坏情况下的时间复杂度分析起来比较简单，但平均、均摊两个复杂度分析相对比较复杂。如果你觉得理解得还不是很深入，不用担心，在后续具体的数据结构和算法学习中，我们可以继续慢慢实践！ // 全局变量，大小为 10 的数组 array，长度 len，下标 i。 int array[] = new int[10]; int len = 10; int i = 0; // 往数组中添加一个元素 void add(int element) { if (i &gt;= len) { // 数组空间不够了 // 重新申请一个 2 倍大小的数组空间 int new_array[] = new int[len*2]; // 把原来 array 数组中的数据依次 copy 到 new_array for (int j = 0; j &lt; len; ++j) { new_array[j] = array[j]; } // new_array 复制给 array，array 现在大小就是 2 倍 len 了 array = new_array; len = 2 * len; } // 将 element 放到下标为 i 的位置，下标 i 加一 array[i] = element; ++i; } 答：最小是O(1)，最大是O(n)，平均和分摊都是O(1),","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"时间规划","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java00--时间计划.html","text":"","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"IDEA配置","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java00-IDEA配置.html","text":"使用内存设置 安装目录下：idea64.exe.vmoptions 鼠标滚轮更改字体大小 鼠标悬浮提示 自动导包 显示行号和方法间的分隔符 忽略大小写提示 设置取消单行显示tabs的操作 注释颜色 设置超过import个数，改为* 文件头 文件格式-UTF-8 自动编译 模板 第一种不能修改或者添加模板，第二种可以。","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"学习路线","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java01-概述.html","text":"学习路线 Java知识图解 Java技术体系平台 Java SE (Java Standard Edition) 标准版 支持面向桌面级应用（如Windows 下的应用程序）的 Java 平台，提供了完整的 Java 核心 API ，此版本以前称为 J2SE Java EE(Java Enterprise Edition) 企业版 是为开发企业环境下的应用程序提供的一套解决方案。该技术体系中包含的技术如Servlet 、 Jsp 等，主要针对于 Web 应用程序开发。版本以前称为 J2EE Java ME(Java Micro Edition) 小型版支持 支持Java 程序运行在移动终端（手机、 PDA ）上的平台，对 Java API 有所精简，并加入了针对移动终端的支持，此版本以前称为 J2ME Java Card 支持一些Java 小程序（ Applets ）运行在小内存设备（如智能卡）上的平台 JDK 、 JRE 、 JVM 关系 • JDK = JRE + 开发工具集（例如 Javac 编译工具等） • JRE = JVM + Java SE 标准类库","tags":[]},{"title":"关键字&标识符","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java02-关键字&标识符.html","text":"关键字(keyword) 定义： 被Java 语言赋予了特殊含义，用做专门用途的字符串（单词） 特点： 关键字中所有字母都为小写 官方地址： https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html keyword ==用于定义数据类型的关键字== class interface enum byte short int long long double char boolean void ==用于定义流程控制的关键字== if else switch case default while do for break continue return ==用于定义访问权限修饰符的关键字== private protected public ==用于定义类，函数，变量修饰符的关键字== abstract final static synchronized ==用于定义类与类之间关系的关键字== extends implements ==用于定义建立实例及引用实例，判断实例的关键字== new this super instanceof ==用于异常处理的关键字== try catch finally throw throws ==用于包的关键字== package import ==其他修饰符关键字== native strictfp transient volatile assert ==用于定义数据类型值的字面值== true false null Java 保留字 现有 Java 版本尚未使用 但以后版本可能会作为关键字使用。自己命名标识符时要避免使用这些保留字goto 、 const 标识符(Identifier)驼峰命名法、蛇形命名法 标识符： Java 对各种 变量 、 方法 和 类 等要素命名时使用的字符序列称为标识符 技巧：凡是自己可以起名字的地方都叫标识符 。 定义合法标识符规则： 由 26 个英文字母大小写， 0-9，_ 或 $ 组成 数字不可以开头 不可以使用关键字和保留字，但能包含关键字和保留字 Java 中严格区分大小写，长度无限制 标识符不能包含空格 Java 中的名称命名规范： 包名 ：多单词组成时所有字母都小写 xxxyyyzzz 类名、接口名 ：多单词组成时，所有单词的首字母大写 XxxYyyZzz 变量名、方法名 ：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词首字母大写： xxxYyyZzz 常量名 ：所有字母都大写。多单词时每个单词用下划线连接 XXX_YYY_ZZZ 注意 1 : 在起名字时，为了提高阅读性，要尽量有意义，“见名知意”。 注意 2 : java 采用 unicode 字符集，因此标识符也可以使用汉字声明，但是不建议使用。 变量 // 声明long类型，必须以&quot;l&quot;或者&quot;L&quot;结尾。 long a = 123123123L; // Java 的浮点型常量默认为 double 型,声明 float 型常量，须后加‘ f’ 或‘ F’ 。 自动类型转换 ：容量小的类型自动转换为容量大的数据类型。数据类型按容量大小排序为： 有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算。 byte,short,char 之间不会相互转换，他们三者在计算(包含同类计算)时首先转换为 int 类型 。 boolean 类型不能与其它数据类型运算。 当把任何基本数据类型的值和字符串(String)进行连接运算时(+)基本数据类型的值将自动转化为字符串类型(String) 强制类型转换：自动类型转换的逆过程 将容量大的数据类型转换为容量小的数据类型 。 使用时要加上强制转换符()，但可能造成 精度降低或溢出格外要注意 。 通常字符串不能直接转换为基本类型，但通过基本类型对应的包装类则可以实现把字符串转换成基本类型 。 如 String a = “ 43 ”; int i = Integer.parseInt(a); boolean 类型不可以转换为其它的数据类型 。 long l1=213123;//未报错，默认int，自动转long long l2=12312323123123123123123;//过大的整数，int存不下 float f1=12.3;//报错，默认double，无法自动转float 进制对于 整数，有四种表示方式： 二进制 (binary) 0,1：满 2 进 1. 以 0b 或 0B 开头。 十进制 (decimal) 0 9：满 10 进 1 。 八进制 (octal)0 7：满 8 进 1. 以数字 0 开头 表示。 十六进制 (hex) 0 9 及 A F：满 16 进 1. 以 0x 或 0X 开头 表示。此处的 A F 不区分大小写。如：0x21AF +1= 0X21B0","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"运算符","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java03-运算符.html","text":"算数运算符int num1=12; int num2=5; double num3=num1/num2; //2 double num3=(double)num1/num2; //2.4 ++ -- += /= *=符号不会改变本身数据类型。 short s1=10; s1++; 逻辑运算符 // &amp;&amp; 若第一个值为false，第二个值不会判断(第二个值被短路)，整体表达式判定为false // 若第一个值为true，第二个值会继续判断 // &amp; 任何情况都会判断两个值 // | || 同理 boolean b1=false; int num1=10; if (b1 &amp; (num1++ &gt; 0)){ System.out.println(&quot;a&quot;); }else{ System.out.println(&quot;b&quot;); } System.out.println(num1); // num1=11 boolean b2=false; int num2=10; if (b2 &amp;&amp; (num2++ &gt; 0)){ System.out.println(&quot;c&quot;); }else{ System.out.println(&quot;d&quot;); } System.out.println(num2); // num2=10 位运算符 // 交换两个数的值 int num1 = 10; int mun2 = 10; //方法1 不限定数据量类型 int temp = num1; num1 = num2; num2 = temp; //方法2 相加操作可能超出范围，仅用于数值类型 num1 = num1 + num2; num2 = num1 - num2; num1 = num1 - num2; //方法3 仅用于数值类型 num1 = num1 ^ num2; num2 = num1 ^ num2; num1 = num1 ^ num2; 三元运算符(条件表达时)?表达式1：表达式2 true：执行表达式1 false：执行表达式2 运算符优先级","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"流程控制","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java04-流程控制.html","text":"Scanner/* 1. 导包 import java.util.Sacnner; 2. Scanner的实例化， Scanner scan = new Scanner(System.in); 3. 调用相关方法，获取指定变量 */ ifint m=0,n=3; if(m&gt;0) if(n&gt;2) System.out.println(&quot;A&quot;); else System.out.println(&quot;B&quot;); // if下只有一条语句，则不必有{} // else采用就近原则匹配if switch-case 根据switch表达式中的值，依次匹配各个case中的常量。一旦匹配成功，则进入相应的case中，当执行完语句后，则继续向下执行其他case中的语句，直到遇到break或者全部case执行完。 switch中的表达式只能是以下6种类型：byte，short，char， int，枚举类型(JDK5.0)，String(JGK7.0) switch(表达式){ case 常量1: 语句1; //break; case 常量2: 语句2; //break; …… case 常量n: 语句n; //break; default: // (可选), 位置不定 语句; //break; } 如果switch-case结构中的多个case的执行语句一样，则可以考虑合并 //对学生成绩大于 60 分的，输出“合格”。低于 60 分的，输出“不合格” 。 int score = 78; switch(score / 10){ case 0: case 1: case 2: case 3: case 4: case 5: System.out.println(&quot;不及格&quot;) case 6: case 7: case 8: case 9: case 10: System.out.println(&quot;及格&quot;) } 如果判断的具体数值不多，而且符合 byte、short、char、int、String、枚举等几种类型。虽然两个语句都可以使用，建议使用 swtich 语句。因为 效率稍高 。 其他情况：对区间判断，对结果为 boolean 类型判断，使用 if，if 的使用范围更 广。也就是说，使用switch case的，都可以改写为 if else。反之不成立。 forfor(初始化部分，可多个语句;循环条件;迭代部分，可多个语句){ 循环体； } while初始条件; while(循环条件){ 循环体; 迭代部分; } do-while初始化部分; do{ 循环体; 迭代部分; }while(循环条件) break、continuelabel:for(int i=0;i&lt;=4;i++){ for(int j=0;j&lt;=10;j++){ if(j%4==0){ continue;// 结束当次循环，后面不能声明执行语句 break;// 结束当前循环，后面不能声明执行语句 break label;//结束指定层循环 continue label;//结束指定层的某次循环 } System.out.println(j); } System.out.println(); }","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"数组","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java05-数组.html","text":"数组特点 数组本身是==引用数据类型==而数组中的元素可以是任何数据类型 包括基本数据类型和引用数据类型 。 创建数组对象会在内存中开辟==一整块连续的空间==而数组名中引用的是这块连续空间的首地址 。 数组的长度一旦确定就不能修改 。 我们可以直接通过下标 或索引 的方式调用指定位置的元素 速度很快 。 一维数组声明方式： Java 语言中声明数组时不能指定其长度 数组中元素的数 例如： int a[5 ]; //非法 // type var[]; // type[] var; int[] ids; // 静态初始化，数组的初始化和元素的赋值操作同时进行 ids = new int[]{1,2,3,4}; // 动态初始化，数组的初始化和元素的赋值操作分开进行 String[] names = new String[5]; int[] arr = {1,2,3,4};//类型推断 数组是引用类型，它的元素 相当于类的成员变量 ，因此数组一经分配空间，其中的每个元素也被按照成员变量同样的方式被隐式初始化. 对于基本数据类型而言，默认初始化值各有不同 对于引用数据类型而言，默认初始化值为 null( 注意 与 0 不同！) 多维数组对于二维数组的理解，我们可以看成是一维数组array1 又作为另一个一维数组 array2 的元素而存在。其实， 从数组底层的运行机制来看，其实没有多维数组。 格式1：动态初始化 int[][] arr = new int [3][2]; /* 定义了名称为arr的二维数组 二维数组中有3个一维数组 每一个一维数组中有2个元素 一维数组的名称分别为arr [0], arr [1], arr [2] 给第一个一维数组1脚标位赋值为78写法是： arr[0][1] = 78 */ 格式2：动态初始化 int[][] arr = new int [3][]; /* 二维数组中有3个一维数组 每个一维数组都是默认初始化值null (注意：区别于格式 1) 可以对这个三个一维数组分别进行初始化 arr[0] = new int [3]; arr[1] = new int [1]; arr[2] = new int [2]; */ 注： int arr = new int [][3]; //非法 格式3：静态初始化 int[][] arr = new int[][]{{3,8,2},{2,7},{9,0,1,6}}; int[][] arr = {{1,2,3,4},{33,44}};//类型推断 /* 定义一个名称为arr的二维数组，二维数组中有三个一维数组 每一个一维数组中具体元素也都已初始化 第一个一维数组arr[0] = {3,8,2} 第二个一维数组arr[1] = {2,7} 第三个一维数组arr[2] = {9.0,1,6} 第三个一维数组的长度表示方式：arr[2].length */ 注意特殊写法情况： int[] x,y[]; x 是一维数组， y 是二维数组。 Java 中多维数组==不必都是规则矩阵形式==. 初始化方式一：外层元素存储==地址值== 初始化方式二：外层元素存储==null== arr1=arr2; 浅拷贝，指向同一个地址 排序算法排序算法分类：内部排序和外部排序 。 内部排序 ：整个排序过程不需要借助于外部存储器（如磁盘等），所有排序操作都在内存中完成。 外部排序 ：参与排序的数据非常多，数据量非常大，计算机无法把整个排序过程放在内存中完成，必须借助于外部存储器（如磁盘）。外部排序最常见的是多路归并排序。可以认为外部排序是由多次内部排序组成。 各种内部排序方法性能比较 1.从平均时间而言：快速排序最佳。 但在最坏情况下时间性能不如堆排序和归并排序。 2.从算法简单性看 ：由于直接选择排序、直接插入排序和冒泡排序的算法比较简单，将其认为是 简单 算法。 对于 Shell 排序、堆排序、快速排序和归并排序算法，其算法比较复杂，认为是复杂排序。 3.从稳定性看 ：直接插入排序、冒泡排序和归并排序时稳定的；而直接选择排序、快速排序、 Shell 排序和堆排序是不稳定排序 4.从待排序的记录数 n 的大小看： n 较小时，宜采用简单排序；而 n 较大时宜采用改进排序。 排序算法的选择 (1)若 n 较小，如 n≤50，可采用直接插入或直接选择排序 。当记录规模较小时，直接插入排序较好；否则因为直接选择移动的记录数少于直接插入 ，应选直接选择排序为宜 。 (2)若文件初始状态基本有序(指正序)，则应选用直接插入 、冒泡或随机的快速排序为宜 (3)若 n 较大，则应采用时间复杂度为 O( nlgn )的排序方法： 快速排序 、 堆排序或归并排序","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"面向对象02抽象类接口","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java06-面向对象02抽象类接口.html","text":"static 关键字有时候希望无论是否产生了对象或无论产生了多少对象的情况下， 某些特定的数据在内存空间里只有一份。 static可以修饰属性、方法、代码块、内部类。 类属性作为该类各个对象之间共享的变量。 在设计类时分析哪些属性不因对象的不同而改变 ，将这些属性设置为类属性。相应的方法设置为类方法。 如果方法与调用者无关，则这样的方法通常被声明为类方法，由于不需要创建对象就可以调用类方法 ，从而简化了方法的调用。 被static修饰后的成员具备以下特点： 随着类的加载而加载，由于类只会加载一次，则静态变量在内存中也只会存在一份，存放在方法区的静态域中。 优先于对象存在 修饰的成员，被所有对象所共享 访问权限允许时，可不创建对象，直接被类调用 类属性前面省略的是类名，而不是this 被static修饰后的方法： 静态方法中，只能调用静态的方法或属性；非静态方法都可以调用 在静态方法内，补鞥呢使用this关键字、super关键字 static修饰的方法不能被重写 操作静态属性的方法，通常设置为static 单例(Singleton)设计模式所谓类的单例设计模式，就是采取一定的方法保证在整个的软件系统中，对某个类只能存在一个对象实例 ，并且该类只提供一个取得其对象实例的方法。如果我们要让类在一个虚拟机中只能产生一个对象，我们首先必须将类的==构造器的访问权限设置为 private==，这样，就不能用 new 操作符在类的外部产生类的对象了，但在==类内部仍可以产生该类的对象==。因为在类的外部开始还无法得到类的对象，==只能====调用该类的某个静态方法==以返回类内部创建的对象，静态方法只能访问类中的静态成员变量，所以，指向类内部产生的==该类对象的变量也必须定义成静态的==。 // 饿汉式 class Singleton { // 1. 私有化构造器 private Singleton() { } // 2. 内部提供一个当前类的实例 // 4. 此实例也必须静态化 private static Singleton single = new Singleton(); // 3. 提供公共的静态的方法，返回当前类的对象 public static Singleton getInstance() { return single; } } // 饿汉式 class Bank{ private Bank(){} public static final Bank instance = new Bank(); } // 懒汉式 // 懒汉式暂时还存在线程安全问题，讲到多线程时，可修复 class Singleton { // 1. 私有化构造器 private Singleton() { } // 2. 声明当前类对象，没有初始化 // 4. 此实例也必须静态化 private static Singleton single = null; //3. 提供公共的静态的方法，返回当前类的对象 public static Singleton getInstance() { if (single == null ) single = new Singleton(); } return single; } } 懒汉式：好处，延迟对象创建；坏处，目前不安全。 饿汉式：好处，是线程安全的；坏处，对象加载时间过长； 单例模式的优点：由于单例模式只生成一个实例，==减少了系统性能开销==，当一个对象的产生 需要比较 多的资源时，如读取配置、产生其他依赖对象时，则可以通过在应用启动时直接产生一个单例对象，然后永久驻留内存的方式来 解决。 main()方法说明 作为程序的入口 也是一个普通的静态方法 可以作为与控制台交互的方式，参数为String类型 代码块 (初始化块) 代码块或初始化块的作用：对 Java 类或对象进行初始化 一个类中代码块若有修饰符则只能被 static 修饰，称为静态代码块(static block) ；没有使用static 修饰的为非静态代码块。 静态代码块随着类的加载而执行，而且只执行一次，通常用于初始化 static 的属性。 非静态代码块随着类的创建而执行，每次创建均执行。可以对属性初始化。 静态代码块：用 static 修饰的代码块 可以有输出语句。 可以对类的属性、类的声明进行初始化操作。 不可以对非静态的属性初始化。即：不可以调用非静态的属性和方法。 若有多个静态的代码块，那么按照从上到下的顺序依次执行。 静态代码块的执行要先于非静态代码块。 静态代码块随着类的加载而加载，且只执行 一 次。 非静态代码块：没有 static 修饰的代码块 可以有输出语句 。 可以对类的属性 、 类的声明进行初始化操作 。 除了调用非静态的结构外还可以调用静态的变量或方法 。 若有多个非静态的代码块，那么按照从上到下的顺序依次执行 。 每次创建对象的时候都会执行一次 。 且先于构造器执行 。 class Root{ static{ System.out.println(&quot;Root的静态初始化块&quot;); } { System.out.println(&quot;Root的普通初始化块&quot;); } public Root(){ System.out.println(&quot;Root的无参数的构造器&quot;); } } class Mid extends Root{ static{ System.out.println(&quot;Mid的静态初始化块&quot;); } { System.out.println(&quot;Mid的普通初始化块&quot;); } public Mid(){ System.out.println(&quot;Mid的无参数的构造器&quot;); } public Mid(String msg){ //通过this调用同一类中重载的构造器 this(); System.out.println(&quot;Mid的带参数构造器，其参数值：&quot; + msg); } } class Leaf extends Mid{ static{ System.out.println(&quot;Leaf的静态初始化块&quot;); } { System.out.println(&quot;Leaf的普通初始化块&quot;); } public Leaf(){ //通过super调用父类中有一个字符串参数的构造器 super(&quot;尚硅谷&quot;); System.out.println(&quot;Leaf的构造器&quot;); } } public class LeafTest{ public static void main(String[] args){ new Leaf(); System.out.println(); new Leaf(); } } /* Root的静态初始化块 Mid的静态初始化块 Leaf的静态初始化块 Root的普通初始化块 Root的无参数的构造器 Mid的普通初始化块 Mid的无参数的构造器 Mid的带参数构造器，其参数值：尚硅谷 Leaf的普通初始化块 Leaf的构造器 Root的普通初始化块 Root的无参数的构造器 Mid的普通初始化块 Mid的无参数的构造器 Mid的带参数构造器，其参数值：尚硅谷 Leaf的普通初始化块 Leaf的构造器 */ class Father { static { System.out.println(&quot;11111111111&quot;); } { System.out.println(&quot;22222222222&quot;); } public Father() { System.out.println(&quot;33333333333&quot;); } } public class Son extends Father { static { System.out.println(&quot;44444444444&quot;); } { System.out.println(&quot;55555555555&quot;); } public Son() { System.out.println(&quot;66666666666&quot;); } public static void main(String[] args) { // 由父及子 静态先行 System.out.println(&quot;77777777777&quot;); System.out.println(&quot;1************************&quot;); new Son(); System.out.println(&quot;2************************&quot;); new Son(); System.out.println(&quot;3************************&quot;); new Father(); } } /* 11111111111 44444444444 77777777777 1************************ 22222222222 33333333333 55555555555 66666666666 2************************ 22222222222 33333333333 55555555555 66666666666 3************************ 22222222222 33333333333 */ final 关键字可以修饰类、方法、变量 final 标记的类不能被继承 。 提高安全性，提高程序的可读性：String 类 、 System 类 、 StringBuffer 类 final 标记的方法不能被子类重写。比如： Object 类中的 getClass 。 final 标记的变量 (成员变量或局部变量) 即称为常量 。 名称大写且只能被赋值一次：final 标记的成员变量必须在声明时或在每个构造器中或代码块中显式赋值，然后才能使用。 final修饰属性：显示初始化、代码块、构造器；不能在方法中初始化。 final修饰局部变量、形参。final修饰形参时，表明形参是一个常量，调用此方法时，给常量形参赋一个实参，且只能在方法体内调用，补能重新赋值。 static final ：全局常量 public class Something { public int addOne( final int x ) return ++x; // 报错，x无法修改 // return x + 1; // 正确 } } public class Something { public static void main(String[] args ) Other o = new Other(); new Something().addOne(o); } public void addOne( final Other o ) // o = new Other(); // 报错，形参o的地址无法修改，内部属性可以修改 o.i++; // 正确 } } class Other { public int i; } 抽象类与抽象方法 用 abstract 关键字来修饰一个类， 这个类叫做抽象类 。 此类不能实例化 抽象类一定有构造器，便于子类实例化调用 开发中，都会提供抽象类的子类，让子类对象实例化，完成相关操作。 用 abstract 来修饰一个方法， 该方法叫做抽象方法 。 抽象方法：只有方法的声明，没有方法的实现。以分号结束：比如：public abstract void talk() 含有抽象方法的类必须被声明为抽象类。 抽象类不能被实例化。抽象类是用来被继承的，抽象类的子类必须重写父类的抽象方法，并提供方法体。==若没有重写全部的抽象方法，仍为抽象类==。 不能用 abstract 修饰变量、代码块、构造器； 不能用 abstract 修饰 私有方法、静态方法、 final 的方法、 final 的类。 abstract注意点： abstract不能修饰属性、构造器、 abstract私有方法、静态方法、final方法、final的类 是否可以这样理解：抽象类就是比普通类多定义了抽象方法 ，除了不能直接进行类的实例化操作之外，并没有任何的不同？对~ 模板方法设计模式 TemplateMethod抽象类体现的就是一种模板模式的设计，抽象类作为多个子类的通用模板，子类在抽象类的基础上进行扩展、改造，但子类总体上会保留抽象类的行为方式。 解决的问题： 当功能内部一部分实现 是 确定 的 一部分实现是不确定的。这时可以把不确定的部分暴露出去，让子类去 实现 。 换句话说，在软件开发中实现一个算法时，整体步骤很固定、通用，这些步骤已经在父类中写好了。但是某些部分易变，易变部分可以抽象出来，供不同子类实现。这就是一种模板模式。 模板方法设计模式是编程中经常用得到的模式 。 各个框架 、 类库中都有他的影， 比如常见的有： 数据库访问的封装 Junit 单元测试 JavaWeb 的 Servlet 中关于 doGet/doPost 方法调用 Hibernate 中模板程序 Spring 中 JDBCTemlate 、 HibernateTemplate 等 接口一方面 有时 必须从几个类中派生出一个子类 继承它们所有的属性和方法 。 但是 Java 不支持多重继承 。 有了接口就可以得到多重继承的 效果 。 接口就是规范，定义的是一组规则，体现了现实世界中“如果你是 要 则必须能 ……”的思想。 ==继承是一个是不是的关系，而接口实现则是能不能的关系==。 ==接口的本质是契约，标准，规范==，就像我们的法律一样。制定好后大家都要遵守。 JDK7.0 接口 interface 是==抽象方法==和==常量值==定义的集合 。 JDK8.0 还可以定义静态方法，默认方法： 接口中定义的静态方法，只能通过接口来调用。 默认方法使用 default 关键字修饰。可以通过实现类对象来调用。我们在已有的接口中提供新方法的同时，还保持了与旧版本代码的兼容性。重写接口中的默认方法，调用的为重写后的方法。 类优先原则：如果子类（实现类）继承的父类和实现的接口中声明了同名同参数的默认方法，在子类没有重写的情况下，默认调用父类的方法。 接口冲突：如果实现类实现了多个接口，而多个接口存在同名同参数的默认方法，在实现类没有重写的情况下，会出现接口冲突；这就需要必须重写此方法。 调用接口中被重写的方法。 接口的特点： 用 interface 来定义 。接口和类是并列的结构。 接口中的所有成员变量都 默认是由 public static final 修饰的。可以省略不写。 接口中的所有抽象方法 都 默认是由 public abstract 修饰的 。 接口中没有构造器 。 接口采用多继承机制。 定义 Java 类的语法格式： 先写 extends ，后写 implements：class SubClass extends SuperClass implements InterfaceA 一 个类可以实现多个接口，接口也可以继承其它接口 。 实现接口的类中必须提供接口中所有方法的具体实现内容，方可实例化。否则，仍为抽象类。 接口的主要用途就是被实现类实现（面向接口编程） 与继承关系类似，接口与实现类之间存在多态性 接口和类是并列关系 或者可以理解为一种特殊的类 。 从本质上讲，接口是一种特殊的抽象类，这种抽象类中只包含常量和方法的定义(JDK7.0 及之前) 而没有变量和方法的实现。 interface A { int x = 1; } class B { int x = 1; } class C extends B implements A { public void pX() { // System.out.println(x);// 编译不通过 System.out.println(super.x);// 1 System.out.println(A.x);// 0 } public static void main(String[] args ){ new C().pX(); } } interface Playable { void play(); } interface Bounceable { void play(); } interface Rollable extends Playable, Bounceable { Ball ball = new Ball(&quot;PingPang&quot;); } class Ball implements Rollable { private String name; public String getName() { return name; } public Ball(String name ) this.name = name; } public void play() { ball = new Ball(&quot;Football&quot;);// ball 是 static final修饰的，无法重新赋值 System.out.println(ball.getName()); } } 内部类成员内部类： 作为外部类的成员： 调用外部类的结构，Person.this.属性 static修饰 4中权限修饰符 作为一个类： 定义属性、方法、构造器 final修饰 abstract修饰 编译以后生成 OuterClass$InnerClass.class 字节码文件 也适用于局部内部类 实例化成员内部类的对象 // 静态的成员内部类 Person.Dog dog = new Person.Dog(); // 非静态的成员内部类 Persion p = new Person(); Person.Bird bir = p.new Bird(); 注意： 1.非 static 的成员内部类中的成员不能声明为 static 的，只有在外部类或 static 的成员内部类中才可声明 static 成员 。 2.外部类访问成员内部类的成员需要”内部类.成员”或”内部类对象.成员”的方式 3.成员内部类可以直接使用外部类的所有成员 包括私有的数据 4.当想要在外部类的静态成员部分使用内部类时可以考虑内部类声明为静态的 抽象类可以实现接口。抽象类可以继承非抽象的类。 在局部内部类的方法中，如果调用外部类声明的方法中局部变量，则要求此局部变量声明final JDK8.0可以省略final关键字。","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"面向对象01三大特性","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java06-面向对象01三大特性.html","text":"面向对象 类是对一类事物的描述 ，是抽象的、概念上的定义 对象是实际存在的该类事物的每个个体，因而也称为实例 (instance) 面向对象的三大特征 封装 (Encapsulation) 继承 (Inheritance) 多态 (Polymorphism) 万事万物皆对象 在java语言范畴中，将功能、结构等封装到类中，通过实例化调用具体的功能结构 涉及到java语言与前端HTML、后端的额数据库交互时，前后端的结构在java层面交互时，都提现为类、对象。 Java类及类的成员 也可以不定义对象的句柄，而直接调用这个对象的方法。这样的对象叫做匿名对象。如： new Person().shout(); 如果对一个对象只需要进行一次方法调用，那么就可以使用匿名对象。我们经常将匿名对象作为实参传递给一个 方法。 类中属性 语法格式：修饰符数据类型 属性名 = 初始化值 ; 说明1：修饰符：常用的权限修饰符有： private 、缺省、 protected 、 public；其他修饰符： static 、 final ( 暂不考虑） 说明2：数据类型：任何基本数据类型或任何引用数据类型。 说明3：属性名：属于标识符，符合命名规则和规范即可。 成员变量 局部变量 声明的位置 直接声明在类中 方法形参、代码块、构造器 修饰符 private、public、static、final等 可用final 初始化值 有默认初始化值 必须显示赋值 内存加载位置 堆空间 栈空间 类中方法 Java 里的方法不能独立存在，所有的方法必须定义在类里。 修饰符 返回值类型 方法名 (参数类型 形参1, 参数类型 形参2,...){ 方法体程序代码 return 返回值 } 没有具体返回值的情况，返回值类型用关键字 void 表示 ，那么方法体中可以不必使用 return 语句。如果使用，”return;”仅用来结束方法。 定义方法时，方法的结果应该返回给调用者，交由调用者处理 。 方法中只能调用方法或属性，==不可以在方法内部定义方法==。 方法重载在同一个类中，允许存在一个以上的同名方法，只要它们的参数个数或者参数类型不同即可。与权限修饰符、返回值类型、形参无关。 可变形参 声明格式： 方法名 (参数的类型名… 参数名) 可变参数：方法参数部分指定类型的参数个数是可变 多 个： 0 个， 1 个或多个 可变个数形参的方法与本类中同名，形参不同的方法之间彼此构成重载 可变参数方法的使用与本类中同名，形参为数组的方法之间不构成重载，无法共存 方法的参数部分有可变形参，需要放在形参声明的最后 在 一个方法的形参位置，最多只能声明一个可变个数形参 参数值传递Java里方法的参数传递方式只有一种： 值传递 。 即将实际参数值的副本（复制品）传入方法内，而参数本身不受 影响 。 形参是基本数据类型：将实参基本数据类型变量的“数据值”传递给形参 形参是引用数据类型：将实参引用数据类型变量的“地址值”传递给形参(包括数据类型) class Value{ int i=15; } public class TestArray { public static void main(String[] args) { TestArray testArray = new TestArray(); testArray.first(); } public void first(){ int i =5; Value v = new Value(); v.i = 25; second(v, i); System.out.println(v.i); // 20 } public void second(Value v , int i ){ i = 0; v.i=20; // 局部变量指向的内存中类的属性被修改 Value val = new Value(); v=val; // 局部变量指向了新的内存 // ==存的是地址值，改的也是地址值== System.out.println(v.i+&quot; &quot;+i); // 15 0 } } int[] arr1 = new int[]{1,2,3}; char[] arr2 = new char[]{&#39;1&#39;,&#39;2&#39;,&#39;3&#39;}; System.out.println(arr1);// [I@1b6d3586 System.out.println(arr2);// 123 // println()重载 三大特性一：封装把该隐藏的隐藏起来，该暴露的暴露出来 。 这就是封装性的设计思想。 “高内聚，低耦合”：类的内部数据操作细节自己完成，不允许外部干涉；仅对外暴露少量的方法。 封装性的体现： 类的属性私有化(private)，同时提供公共的(public)的方法获取(getXXX)和设置(setXXX)属性； 不对外暴露私有的方法； 单例模式； 四种权限修饰符： 从小到大：private—&gt;缺省—&gt;protected—&gt;public 修饰符 类内部 同一个包 不同包的子类 同一个工程 private yes 缺省 yes yes protected yes yes yes public yes yes yes yes 4种权限可以用来修饰类的内部结构：属性、方法、构造器、内类； 对于class 的权限修饰只可以用 public 和 缺省（default）。public 类可以在任意地方被访问。default 类只可以被同一个包内部的类访问。 构造器构造器的作用：创建对象；给对象进行初始化。 构造器的特点： 它具有与类相同的名称 它不声明返回值类型。（与声明为 void 不同） 不能被 static 、 final 、 synchronized 、 abstract 、 native 修饰，不能有return 语句返回值。 注 意： Java 语言中，每个类都至少有一个构造器 默认构造器的修饰符与所属类的修饰符一致 一旦显式定义了构造器， 则系统不再提供默认构造器 一个类可以创建多个重载的构造器 父类的构造器不可被子类继承 赋值的位置及顺序： 默认初始化；int i; 显式初始化；int i=9; 构造器中初始化； 通过“对象 属性“或“对象 方法”的方式赋值； this、package、import关键字this this可以调用类的属性、方法和构造器 ； 它在方法内部使用，即这个方法所属对象的引用； 它在构造器内部使用，表示该构造器正在初始化的对象。 方法和构造器中形参和属性重名，需使用this区分，不重名则无要求。 class Person{ private int age=99; public void setAge(int age){ //形参和属性重名，需使用this区分，不重名则无要求。 //形参=形参， 赋值失败 age = age; //可以赋值 this.age=age; } } this 可以作为一个 类中构造器相互调用的特殊格式 注意： 可以 在类的构造器中使用 “ 形参列表 的方式，调用本类中重载的其他的构造器！ 明确 ：构造器中不能通过 “形参列表 “的方式调用自身构造器 如果 一个类中声明了 n 个构造器，则最多有 n-1 个构造器中使用了 “ 形参列表 “this 形参列表 必须声明在类的构造器的==首行==！ 在类的一个构造器中，最多只能声明一个 “形参列表” class Person{ private int age; private String name; public Person(){ System.out.println(&quot;新对象实例化&quot;); } public Person(String name){ this(); this.name = name; } public Person(String name, int age){ this(name); this.age = age; } } `使用 this 访问属性和方法时，如果在本类中未找到，会从父类中查找 package package 语句作为 Java 源文件的第一条语句，指明该文件中定义的类所在的包。若缺省该语句，则指定为无名包 。它的格式为：package 顶层包名.子包名; 包对应于文件系统的目录，package语句中，用.来指明包目录的层次； 包通常用小写单词标识。通常使用所在公司域名的倒置： com.atguigu.xxx import 为使用定义在不同包中的 Java 类，需用 import 语句来引入 指定包层次下所需要的类或全部类 。 import 语句告诉编译器到哪里去寻找类。 语法格式：import 包名.类名 注意： 在源文件中使用 import 显式的导入指定包下的类或接口 声明在包的声明和类的声明之间。 如果需要导入多个类或接口，那么就并列显式多个 import 语句即可 举例 ：可以使用 java.util.* 的方式，一次性导入 util 包下所有的类或接口。 如果导入的类或接口是 java.lang 包下的，或者是当前包下的，则可以省略此 import 语句。 如果在代码中使用不同包下的同名的类。那么就需要使用类的==全类名==的方式指明调用的是哪个类。 如果已经导入 java.a 包下的类。那么如果需要使用 a 包的子包下的类的话，仍然需要导入。 import static 组合的使用：调用指定类或接口下的静态的属性或方法。 三大特性二：继承作用： 继承的出现减少了代码冗余，提高了代码的复用性。 继承的出现，更有利于功能的扩展。 继承的出现让类与类之间产生了关系 ，提供了多态的前提 子类继承了父类，就继承了父类的所有属性和方法。特别的，private属性，由于封装性的影响，子类不能直接调用。 在子类中，可以使用父类中定义的方法和属性，也可以创建新的数据和方法。实现功能的扩展。在 Java 中，继承的关键字用的是“ extends ””，即子类不是父类的子集而是对父类的“扩展” 。 Java 只支持==单继承==和多层继承，不允许多重继承：一个子类只能有一个父类；一个父类可以派生出多个子类。 方法重写 定义 ：在子类中可以根据需要对从父类中继承来的方法进行改造也称为方法的重置、覆盖 。在程序执行时，子类的方法将覆盖父类的方法。 要求 子类重写的方法必须和父类被重写的方法具有相同的方法名称、 参数列表 父类被重写的返回值类型为基本数值类型时，子类重写方法的返回值类型必须相同；父类被重写的返回值类型为引用数据类型时(如Object)，子类重写方法的返回值类型只能是相同类型或者其子类(String)。 子类重写的方法使用的访问权限不能小于父类被重写的方法的访问权限子类==不能重写==父类中声明为 ==private 权限==的方法 子 类方法抛出的异常不能大于父类被重写方法的异常，即异常的子类。 注意：子类与父类中同名同参数的方法必须同时声明为非 static 的(即为重写)，或者同时声明为 static 的(不是重写) 。因为 static 方法是属于类的，子类无法覆盖父类的方法。 super关键字super和 this 的用法相像，this 代表本类对象的引用，super 代表父类的内存空间的标识。 在子类的方法或者构造器中，通过使用”super.属性”和”super.方法”，显示地调用父类中声明的属性或方法，一般省略super 特殊情况：当子父类出现同名属性或方法时可以用 super 表明调用的是父类(父类的父类)中的属性或方法，使用this表明调用的是子类中的属性或方法。 super可以调用父类的构造器。在子类的构造器中显式地使用”super(形参列表)”的方式调用父类中指定的构造器。且必须声明在子类构造器的首行。 子类中所有的构造器默认都会访问父类中空参数的构造器，在类的多个构造器汇总，至少有一个调用了”super(形参列表)”. 当父类中没有空参数的构造器时，子类的构造器必须通过 this(参数列表) 或者 super(参数列表) 语句指定调用本类或者父类中相应的构造器 。 同时==只能二选一，且必须放在构造器的首行==。 如果子类构造器中既未显式调用父类或本类的构造器，且父类中又没有无参的构造器，则编译出错。 明确：虽然创建子类时，调用了父类的构造器，但是自始至终就创建过一个对象，即new的对象。 三大特性三：多态使用前提：1. 类的继承关系；2. 方法重写 对象的多态性：父类的引用指向子类的对象 多态的使用：虚拟方法调用 在编译器，只能调用父类中声明的方法，但在运行期，实际执行的是子类重写的父类方法，不能调用子类特有的属性和方法。==编译看左边，运行看右边==。 多态性不适用属性！！！子父类中同名的属性，多态时调用父类的属性。 小结：方法的重载与重写 二者的定义细节： 略 从编译和运行的角度看：重载，是指允许存在多个同名方法，而这些方法的参数不同。 编译器根据方法不同的参数表， 对同名方法的名称做修饰。对于编译器而言，这些同名方法就成 了不同的方法。 它们的调用地址在编译期就绑定了 。 Java 的重载是可以包括父类和子类的，即子类可以重载父类的同名不同参数的方法。所以：对于重载而言，在方法调用之前，编译器就已经确定了所要调用的方法，这称为 “早绑定”或“静态绑定。而对于多态，只有等到方法调用的那一刻 解释运行器才会确定所要调用的具体方法，这称为 “晚绑定”或“动态绑定 。 重载不表现为多态性，重写表现为多态性。 instanceof 、Casting 操作符instanceof有了对象的多态性后，内存中实际上是加载了子类特有的属性和方法，但是由于变量声明为父类，导致编译时只能调用父类中声明的属性和方法，子类特有的属性和方法不能调用。 如何才能调用子类特有的属性和方法？答：使用强制类型转换！但可能会出现异常。 Persion p = new Man(); Man m = (Man)p; // 可以调用Man特有的属性和方法 Woman m = (Woman)p; // 出现异常ClassCastException a instanceof A : 判断对象a是否为类A的实例：如果是，返回true，否则，返回false。 // 为了避免在向下转型时出现ClassCastException异常，所以需要先判断类型 // 编译通过，运行不过 Person p = new Woman(); Man m = (Man)p; Person p = new Person(); Man m = (Man)p; // 编译通过，运行通过 Object obj = new Woman(); Person p = (Person)obj; // 编译不通过 Man m = new Woman(); class Base { int count = 10; public void display() { System.out.println(this.count); } } class Sub extends Base { int count = 20; public void display() { System.out.println(this.count); } } public class FieldMethodTest { public static void main(String[] args){ Sub s = new Sub(); System.out.println(s.count);//20 s.display();//20 Base b = s;//多态 // == 对于引用数据类型，比较的是两个引用数据类型的地址是否相等 System.out.println(b == s);// true System.out.println(b.count);// 10 b.display();//20 } } 面试题：多态是编译时行为还是运行时行为？ import java.util.Random; //面试题：多态是编译时行为还是运行时行为？ //证明如下： class Animal { protected void eat() { System.out.println(&quot;animal eat food&quot;); } } class Cat extends Animal { protected void eat() { System.out.println(&quot;cat eat fish&quot;); } } class Dog extends Animal { public void eat() { System.out.println(&quot;Dog eat bone&quot;); } } class Sheep extends Animal { public void eat() { System.out.println(&quot;Sheep eat grass&quot;); } } public class InterviewTest { public static Animal getInstance(int key) { switch (key) { case 0: return new Cat (); case 1: return new Dog (); default: return new Sheep (); } } public static void main(String[] args) { int key = new Random().nextInt(3); System.out.println(key); Animal animal = getInstance(key); animal.eat(); } } //考查多态的笔试题目： public class InterviewTest1 { public static void main(String[] args) { Base base = new Sub(); base.add(1, 2, 3); // sub_1, 多态 Sub s = (Sub)base; s.add(1,2,3);// sub_2 } } class Base { public void add(int a, int... arr) { System.out.println(&quot;base&quot;); } } class Sub extends Base { public void add(int a, int[] arr) { System.out.println(&quot;sub_1&quot;); } public void add(int a, int b, int c) { System.out.println(&quot;sub_2&quot;); } } Object 类 Object 类是所有 Java 类的根父类 如果在类的声明中未使用 extends 关键字指明其父类，则默认父类为 java.lang.Object 类 数组也作为object类的子类出现，可以调用object中的方法 ==操作符与 equals 方法==： 基本类型比较值：只要两个变量的值相等，即为 true 。 引用类型比较引用 (是否指向同一个 对象)：只有指向同一个对象时，才返回 true 用==进行比较时，符号两边的数据类型必须兼容（可自动转换的基本数据类型除外），否则编译出错 equals()： 是一个方法，而非运算符。只适用于引用数据类型 默认equals()：object类中的equals()的定义：和 == 一致、 public boolean equals(Object obj)}{ return (this == obj); } 特例 ：当用 equals() 方法进行比较时对类 File 、 String 、 Date 及包装类Wrapper Class 来说是比较类型及内容而不考虑引用的是否是同一个对象 当自定义使用 equals() 时，可以重写。用于比较两个对象的内容是否都相等。重写equals()方法的原则： 对称性：如果 x.equals(y) 返回是 true，那么 y.equals(x) 也应该返回是true 。 自反性：x.equals(x) 必须返回是 true 传递性：如果 x.equals(y) 返回是 true，而且 y.equals(z) 返回是 true，那么 z.equals(x) 也应该返回是 true 。 一致性：如果 x.equals(y) 返回是 true，只要 x 和 y 内容一直不变，不管你重复 x.equals(y) 多少次，返回都是 true. 任何情况下 x.equals(null) 永远返回是 false x.equals(和x不同类型的对象), 永远返回是false 。 先用==比较地址是否相同，然后再比较属性值是否全部相同。 面试题：== 和 equals 的区别 1 == 既可以比较基本类型也可以比较引用类型。对于基本类型就是比较值，对于引用类型，就是比较内存地址 2 equals 的话，它是属于 java.lang.Object 类里面的方法，如果该方法没有被重写过默认也是==，我们可以看到 String 等类的 equals 方法是被重写过的，而且 String 类在日常开发中用的比较多，久而久之，形成了 equals 是比较值的错误观点。 3 具体要看自定义类里有没有重写 Object 的 equals 方法来判断。 4 通常情况下，重写 equals 方法，会比较类中的相应属性是否都相等。 toString() 方法 输出一个对象的引用时，实际上是调用当前对象的toString()。”System.out.println(ClassA);” Object类中的toString()方法：返回类名和它的引用地址 public String toString() { return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode()); } String、Data、File、包装类等都重写了Object类中的toString()方法，使得在调用toString方法时，返回“实体内容”信息。进行 String 与其它类型数据的连接操作时，自动调用 toString() 方法 包装类 (Wrapper)和单元测试JUnit单元测试： add libraries —- JUnit 4 创建Java类，进行单元测试：1）Java类是public的，2）此类提供公共的无参构造器 此类中声明单元测试方法：public，void，无形参。 单元测试方法上需要声明注解：@Test，需要导入包：import org.junit.Test 声明好单元测试方法后，在方法体内测试相关代码 包装类 针对八种基本数据类型定义 相应的引用类型包装类（封装类） 有了类的特点，就可以调用类中的方法， Java才是真正的面向对象 基本数据类型 包装类 byte Byte short Short int Integer long Long float Float double Double boolean Boolean char Charactor 基本数据类型转化为包装类：调用构造器即可，可传入数字或字符串。注意，Boolean类型，传入的字符小写化后为“true”即为true，如“true123”为false。 包装类转化为基本数据类型：调用xxxValue()方法 JDK1.5 之后，支持自动装箱，自动拆箱。但类型必须匹配。 int num = 10; Integer num1 = num; int num2 num1; 基本数据类型和包装类转换到String类型：均可以调用String重载的valueOfxxx(); 面试题： Object o1 = true ? new Integer(1) : new Double(2.0); System.out.println(o1);// 1.0 ：前后要求类型一致 Object o2; if (true) o2 = new Integer(1); else o2 = new Double(2.0); System.out.println(o2);//1 public void method() { Integer i = new Integer(1); Integer j = new Integer(1); System.out.println(i == j);// false 比较地址值 Integer m = 1; Integer n = 2; System.out.println(m == n);// true // Integer内部类IntegerCache中定义了Integer[]，保存了-128 -- +127 // 如果使用自动装箱的方法，给Integer赋值的访问在-128 -- +127范围内时，可以直接使用数组中的元素，不需要new， 即地址值一样 Integer x = 128; Integer y = 128; System.out.prinltn(x == y);// false new的对象，地址值不一样 }","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"异常","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java07-异常.html","text":"常见异常 Java 程序的执行过程中如出现异常会生成一个异常类对象该异常对象将被提交给 Java 运行时系统这个过程称为抛出（throw）异常。 异常对象的生成 由虚拟机 自动生成 ：程序运行过程中，虚拟机检测到程序发生了问题，如果在当前代码中没有找到相应的处理程序，就会在后台自动创建一个对应异常类的实例对象并抛出自动抛出 由开发人员 手动创建 Exception exception = new ClassCastException(); 创建好的异常对象不抛出对程序没有任何影响，和创建一个普通对象 一样 异常处理机制一：try-catch-finallytry{ ...... //可能产生异常的代码 } // 如果明确知道产生的是何种异常,可以用该异常类作为 catch 的参数；也可以用其父类作为 catch 的参数。 catch( ExceptionName1 e) ...... //当产生 ExceptionName 1 型异常时的处置措施 } catch( ExceptionName2 e) ...... //当产生 ExceptionName 2 型异常时的处置措施 } finally{// 可选 ...... //无论是否发生异常 都无条件执行的语句 } final 是可选的；不论在 try 代码块中是否发生了异常事件， catch 语句是否执行， catch 语句是否有异常， try / catch 语句中是否有 return， finally 块中的语句都会被执行。 像输入输出流、数据库连接、网络编程Socket等资源、JVM是不能自动回收的，需要手动释放资源，这时需要放到finally。 catch中的异常类型：子类异常必在父类异常之上，否则报错。 getMessage() 获取异常信息，返回字符串 printStackTrace() 获取异常类名和异常信息，以及异常出现在程序中的位置。返回值 void 。 try中声明的变量，出了try结构后，就不能被调用 异常处理机制二：throws如果一个方法 中的语句执行时，可能生成某种异常，但是并不能确定如何处理这种异常，则此方法应显示地声明抛出异常，表明该方法将不对这些异常进行处理而由该方法的调用者负责处理 。 在方法声明中用 throws 语句可以声明抛出异常的列表，throws 后面的异常类型可以是方法中产生的异常类型，也可以是它的父类。（throws后可以跟多个异常，逗号隔开） 异常后续的代码，不会再执行。 public void readFile (String file) throws FileNotFoundException{ …… //读文件的操作可能产生 FileNotFoundException 类型的异常 FileInputStream fis new FileInputStream (file); …… } 重写方法不能抛出比被重写方法范围更大的异常类型 。 在多态的情况下，对 method()A 方法的调用-异常的捕获按父类声明的异常处理 。 public class A { public void methodA () throws IOException …… } } public class B1 extends A { public void methodA () throws FileNotFoundException …… } } public class B2 extends A { public void methodA () throws Exception { //报错 …… } } 两种机制的选择： 父类被重写的方法没有throws抛出异常，则子类重写的方法也不能使用throws，如果子类重写的方法中有异常，必须使用try-catch-finally结构 执行的方法a中，先后调用了递进关系的另外几个方法，建议这几个方法用throws的方式处理，方法a考虑使用try-catch-finally方式处理 手动跑抛出异常-throwJava 异常类对象除在程序执行过程中出现异常时由系统自动生成并抛出也可根据需要使用人工创建并抛出 。 首先要生成异常类对象 然后通过 throw 语句实现抛出操作 提交给 Java 运行环境 。IOException e = new IOException(); throw e; 可以抛出的异常必须是 Throwable 或其子类的实例 。 下面的语句在编译时将会产生语法错误： throw new String(“want to throw”); 用户自定义异常类 用户自定义的异常类必须继承现有的异常类。 一般地，用户自定义异常类都是 RuntimeException 的子类。 自定义异常类通常需要编写几个重载的构造器 。 自定义异常需要提供 serialVersionUID，声明为全局常量：static final long serialVersionUID=XXXL; 自定义的异常通过 throw 抛出 。 自定义异常最重要的是异常类的名字，当异常出现时，可以根据名字判断异常类型。","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"常用类","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java09-常用类.html","text":"字符串相关的类String String 是一个 final 类，代表不可变的字符序列。 字符串是常量 ，用双引号引起来表示。 它们的值在创建之后不能更改。 String 对象的字符内容是存储在一个字符数组 value[] 中的。 String 实现了Serializable, Comparable接口，表明String是支持序列化的，可比较大小的 public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence { /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 结论： 常量与常量的拼接结果在常量池 。 且常量池中不会存在相同内容的常量 。 只要其中有一个是变量，结果就在堆中； 如果拼接的结果调用 intern() 方法，返回值就在常量池中 int length() 返回字符串的长度： return value length char charAt(int index) 返回某索引处的字符 return value[index] boolean isEmpty() 判断是否是空字符串： return value length 0 String toLowerCase() 使用默认语言环境将 String 中的所有字符转换为小写 String toUpperCase() 使用默认语言环境将 String 中的所有字符转换为大写 String trim() 返回字符串的副本,忽略前导空白和尾部空白 boolean equals(Object obj) 比较字符串的 内容 是否相同 boolean equalsIgnoreCase(String anotherString) 与 equals 方法类似,忽略大小写 String concat(String str) 将指定字符串连接到此字符串的结尾 。 等价于用”+” int compareTo(String anotherString) 比较两个字符串的 大小 String substring(int beginIndex) 返回一个新的字符串, 它是此字符串的从beginIndex 开始截取到最后的一个子字符串 。 String substring(int beginIndex, int endIndex) 返回一个新字符串 它是此字符串从 beginIndex 开始截取到 endIndex(不包含) 的一个子字符串 。 boolean endsWith(String suffix) 测试此字符串是否以指定的后缀结束 boolean startsWith(String prefix) 测试此字符串是否以指定的前缀开始 boolean startsWith(String prefix, int toffset) 测试此字符串从指定索引开始的子字符串是否以指定前缀开始 boolean contains(CharSequence s) 当且仅当此字符串包含指定的 char 值序列时，返回 true int indexOf(String str) 返回指定子字符串在此字符串中第一次出现处的索引 int indexOf(String str, int fromIndex) 返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引 开始 int lastIndexOf(String str) 返回指定子字符串在此字符串中最右边出现处的 索引 int lastIndexOf(String str, int fromIndex) 返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索 注：indexOf 和 lastIndexOf 方法如果未找到都是返回 1 String replace(char oldChar, char newChar) 返回 一个新的字符串, 它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的 。 String replace(CharSequence target, CharSequence replacement) 使用指定的字面值替换序列替换此字符串所有匹配字面值目标序列的子字符串。 String replaceAll(String regex, String replacement) 使用 给定的replacement 替换此字符串所有匹配给定的正则表达式的子字符串 。 String replaceFirst(String regex, String replacement) 使用 给定的replacement 替换此字符串匹配给定的正则表达式的第一个子字符串 boolean matches(String regex) 告知此字符串是否匹配给定的正则表达式 String[] split(String regex) 根据给定正则表达式的匹配拆分此字符串 。 String[] split(String regex, int limit) 根据匹配给定的正则表达式来拆分此字符串 最多不超过 limit 个 如果超过了 剩下的全部都放到最后一个元素中 。 String转换 StringBuffer String、StringBuffer、StringBuilder的异同 String：不可变的字符序列；底层使用char[] 存储；最慢 StringBuffer：可变字符序列；线程安全的，效率低；底层使用char[] 存储；较快 StringBuilder：可变字符序列；线程不安全的，效率高；底层使用char[] 存储；最快 源码分析： String str1 = new String();// char[] value = new char[0]; String str2 = new String(&quot;abc&quot;);//char[] value = new char[]{&#39;a&#39;,&#39;b&#39;,&#39;c&#39;}; StringBuffer sb1 = new StringBuffer();//char[] value = new char[16];底层创建长度是16的数组 StringBuffer sb2 = new StringBuffer(&quot;abc&quot;);//char[] value = new char[&quot;abc&quot;.length+16]; sb2.length为3 扩容问题：长度扩容为原来长度*2+2，同时将原有数组赋值到新的数组中 开发中建议：使用 StringBuffer(int capacity)或 StringBuilder(int capacity) 常用方法： StringBufferappend (xxx)：提供了很多的 append() 方法 用于进行字符串拼接 StringBufferdelete (int start,int end)：删除指定位置的内容 StringBufferreplace (int start, int end, String str)：把 [start,end) 位置替换为 str StringBufferinsert (int offset, xxx)：在指定位置插入 xxx StringBufferreverse ()：把当前字符序列逆转 public int indexOf (String str) public Stringsubstring (int start,int end) public intlength() public charcharAt (int n) public voidsetCharAt (int n ,char ch) StringBuilderStringBuilder 和 StringBu ffer 非常类似，均代表可变的字符序列 而且提供相关功能的方法也一样 增 append 删 delete 改 setChar 查 charAt 插 insert 遍历 for + charAt / toString 长度 length 互相转换 String——&gt; StringBuffer、StringBuilder：调用StringBuffer和StringBuilder的构造器 StringBuffer、StringBuilder——&gt;String：调用String的构造器、StringBuffer和StringBuilder的toString方法 日期时间API java.lang.SystemSystem类提供的 public static long currentTimeMillis() 用来返回当前时间与 1970 年 1 月 1 日 0 时 0 分 0 秒之间以==毫秒==为单位的时间差。此方法适于计算时间差。 java.util.Date 类java.util.Date 类 |---java.sql.Date类 // 继承关系 1.两个构造器的使用 Date 使用无参构造器创建的对象可以获取本地当前时间。 Date(long date) 创建指定毫秒数的Date对象 2.两个方法的使用 toString() ：把此 Date 对象转换为以下形式的 String dow mon dd hh:mm:ss zzz yyyy 其中： dow 是一周中的某一天 (Sun, Mon, Tue, Wed, Thu, Fri, Sat) zzz 是时间 标准 。 getTime()：返回自 1970 年 1 月 1 日 00:00:00 GMT 以来此 Date 对象表示的毫秒数。 3.java.sql.Date对应数据库中的日期类型的对象 实例化：java.sql.Date d = new java.sql.Date(23132323123L);println出来为“1999-02-12”格式 java.sql.Date—&gt;java.util.Date：多态 java.util.Date—&gt;java.sql.Date： Date d = new Date(); java.sql.Date d1 = new java.sql.Date(d.getTime()); java.text.SimpleDateFormat 类Date 类的 API 不易于国际化，大部分被废弃了 java.text.SimpleDateFormat类是一个不与语言环境有关的方式来格式化和解析日期的具体 类。 它允许进行格式化：日期—&gt;文本 、 解析：文本—&gt;日期 // 1. 实例化，使用默认构造器 SimpleDateFormat sdf = new SimpleDateFormat(); // 2. 格式化：日期--&gt;字符串 Date date = new Date(); System.out.println(date); String format = sdf.format(date); System.out.println(format); // 3. 解析：字符串--&gt;日期 String str = &quot;19-12-28 上午11:21&quot;; Data data1 = sdf.parse(str); System.out.println(date1); /*按照指定方式格式化和解析*/ // 4. 实例化，使用带参构造器 SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); // 5. 格式化：日期--&gt;字符串 String format = sdf.format(date); System.out.println(format); // 6. 解析：字符串--&gt;日期 Data data1 = sdf.parse(&quot;2020-01-21 12:45:55&quot;);// 格式符合构造器中的格式 System.out.println(date1); java.util.Calendar 日历类（抽象类）// 1.实例化 // 方式一：创建其子类（GregorianCalendar）对象 // 方式二：调用其静态方法getInstance() Calendar calendar = Calendar.getInstance(); // 2. 常用方法 // get() int days = calendar.get(Calendar.DAY_OF_MONTH);// 返回当前时间是这个月的第几天 int days = calendar.get(Calendar.DAY_OF_YEAR);// 返回当前时间是这一年的第几天 // set() calendar.set(Calendar.DAY_OF_MONTH, 22);// 修改当前时间为这个月的第几天，修改了calendar System.out.println(calendar.get(Calendar.DAY_OF_MONTH));// 22 // add() calendar.add(Calendar.DAY_OF_MONTH, 3);// 天数可以为负数 如：-3 System.out.println(calendar.get(Calendar.DAY_OF_MONTH));// 25 // getTime() 日历类---&gt;Date Date date = calendar.getTime(); // setTime() Date---&gt;日历类 Date date = new Date(); calendar.setTime(date); 注意 获取月份 时： 一月 是 0 ，二月是 1 ，以此类推 12 月是 11 获取星期时： 周日是 1 ，周二是 2 。 。。。周六是 7 以下是JDK8新日期时间API可变性：像日期和时间这样的类应该是不可变的。 偏移性：Date 中的年份是从 1900 开始的，而月份都从 0 开始。 格式化：格式化只对Date 有用， Calendar 则不行。此外，它们也不是线程安全的；不能处理闰秒等。 java.time包含值对象的基础包 java.time.chrono提供对不同的日历系统的访问 java.time.format格式化和解析时间和日期 java.time.temporal包括底层框架和扩展特性 java.time.zone包含时区支持的类 说明：大多数开发者只会用到基础包和format 包，也可能会用到 temporal 包。因此，尽管有 68 个新的公开类型，大多数开发者，大概将只会用到其中的三分之一。 LocalDate、LocalTime、LocalDateTime LocalDate 、 LocalTime 、 LocalDateTime 类是其中较重要的几个类，它们的实例是==不可变==的对象。 LocalDate 代表 IOS 格式（ yyyy MM dd ）的日期 可以存储 生日、纪念日等日期。 LocalTime 表示一个时间，而不是日期 。 LocalDateTime 是用来表示日期和时间的， 这是一个最常用的类之一 。 方法 描述 now() /* now(ZoneId zone) 静态方法，根据当前时间创建对象指定时区的对象 of() 静态方法，根据指定日期时间创建对象 getDayOfMonth()/getDayOfYear() 获得月份天数1-31/获得年份天数 1-366 getDayOfWeek() 获得星期几 (返回一个DayOfWeek 枚举值) getMonth() 获得月份 (返回一个 Month 枚举值) getMonthValue() /getYear() 获得月份1-12/获得年份 getHour()/getMinute()/getSecond() 获得当前对象对应的小时、 分钟 、 秒 withDayOfMonth()/withDayOfYear()/withMonth()/withYear() 将月份天数、 年份天数 、 月份 、 年份修改为指定的值并返回新 的对象 plusDays(), plusWeeks(),plusMonths(), plusYears(),plusHours() 向当前对象添加几天 、 几周 、 几个月 、 几年 、 几小时 minusMonths()/minusDays()/minusYears()/minusHours() 从当前对象减去 几 月 、 几周 、 几 天 、 几年 、 几小时 Instantjava.time 包通过值类型 Instant 提供机器视图，不提供处理人类意义上的时间单位。 Instant 表示时间线上的一点，而不需要任何上下文信息，例如，时区。概念上讲， 它只是简单的表示自 1970 年 1 月 1 日 0 时 0 分 0 秒（ UTC ）开始的秒数。 因为 java.time 包是基于纳秒计算的，所以 Instant 的精度可以达到纳秒级。1秒=1000毫秒=10^6微吗秒=10^9纳秒 时间戳是指格林威治时间1970 年 01 月 01 日 00 时 00 分 00 秒 （北京时间 1970 年 01 月 01日 08 时 00 分 00 秒）起至现在的总秒数。 方法 描述 now() 静态方法返回默认 UTC 时区的 Instant 类的对象 ofEpochMilli (long epochMilli) 静态方法返回在 1970-01-01 00:00:00 基础上加上指定毫秒数之后的 Instant 类的对象 atOffset(ZoneOffset offset) 结合即时的偏移来创建一个OffsetDateTime toEpochMilli() 返回1970-01-01 00:00:00 到当前时间的毫秒数, 即为时间戳 java.time.format.DateTimeFormatter 类 预定义的标准格式。如：ISO_LOCAL_DATE_TIME; ISO_LOCAL_DATE; ISO_LOCAL_TIME DateTimeFormatter formatter = DateTimeFormatter.ISO_LOCAL_DATE_TIME; LocalDateTime localDateTime = LocalDateTime.now(); String str = formatter.format(localDateTime); TemporalAccessor parse = formatter.parse(&quot;2019-02-18T15:42:18.797&quot;); System.out.println(parse); 本地化相关的格式。如： ofLocalizedDateTime(FormatStyle.LONG) // 本地化相关的格式。如：ofLocalizedDateTime() //FormatStyle.LONG / FormatStyle.MEDIUM / FormatStyle.SHORT :适用于LocalDateTime DateTimeFormatter formatter1 = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.LONG); //格式化 String str2 = formatter1.format(localDateTime); System.out.println(str2);//2019年2月18日 下午03时47分16秒 // 本地化相关的格式。如：ofLocalizedDate() // FormatStyle.FULL / FormatStyle.LONG / FormatStyle.MEDIUM / FormatStyle.SHORT : 适用于LocalDate 自定义的格式。如： ofPattern(“yyyy MM dd hh:mm:ss”) // 重点： 方式三：自定义的格式。如：ofPattern(“yyyy-MM-dd hh:mm:ss”) DateTimeFormatter formatter3 = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd hh:mm:ss&quot;); //格式化 String str4 = formatter3.format(LocalDateTime.now()); System.out.println(str4);//2019-02-18 03:52:09 //解析 TemporalAccessor accessor = formatter3.parse(&quot;2019-02-18 03:52:09&quot;); System.out.println(accessor); 其他API ZoneId 该类中包含了所有的时区信息 一个时区的 ID 如 Europe/Paris ZonedDateTime 一个在 ISO 8601 日历系统时区的日期时间 如 2007 1203 T 10 15 30 01 00 Europe/Paris 。 其中每个时区都对应着 ID 地区 ID 都为 区域 城市 的格式 例如：Asia/Shanghai 等 Clock 使用时区提供对当前即时 、 日期和时间的访问的时钟 。 持续时间： Duration 用于计算两个 时间 间隔 日期间隔： Period 用于计算两个 日期 间隔 TemporalAdjuster : 时间校正器。有时我们可能需要获取例如：将日期调整到“下一个工作日”等操作。 TemporalAdjusters : 该类通过静态方法 (firstDayOfXxx()/lastDayOfXxx()/ 提供了大量的常用 TemporalAdjuster 的实现 。 Java比较器Java 实现对象排序的方式有两种： 自然排序： java.lang.Comparable 定制排序： java.util.Comparator 两者比较：Comparable接口的方式一旦一定，保证Comparable接口实现类的对象在任何位置都可以比较大小；Comparator接口属于临时比较。 自然排序 java.lang.Comparable像String、包装类等实现了Comparable接口，重写了compareTo()方法。 重写compareTo(obj)方法的规则： 如果当前对象this大于形参对象obj，则返回正整数； 如果当前对象this小于形参对象obj，则返回负整数； 如果当前对象this等于形参对象obj，则返0 对于自定义类来说，如果需要排序，我们可以让==自定义类实现Comparable接口，重写compareTo(obj)方法。在compareTo(obj)方法中指明如何排序==。 实现 Comparable 接口的对象列表（和数组）可以通过 Collections.sort 或Arrays.sort 进行自动排序。实现此接口的对象可以用作有序映射中的键或有序集合中的元素，无需指定比较器。 Comparable 的典型 实现 默认都是从小到大排列的 String ：按照字符串中字符的 Unicode 值进行比较 Character ：按照字符的 Unicode 值来进行比较 数值类型对应的包装类以及 BigInteger 、 BigDecimal ：按照它们对应的数值大小进行比较 Boolean true 对应的包装类实例大于 false 对应的包装类实例 Date 、 Time 等：后面的日期时间比前面的日期时间大 定制排序 java.util.Comparator当元素的类型没有实现 java.lang.Comparable 接口而又不方便修改代码，或者实现了java.lang.Comparable 接口的排序规则不适合当前的操作，那么可以考虑使用 Comparator 的对象来排序强行对多个对象进行 整体 排序的比较。 重写 compare(Object o1,Object o2) 方法，比较 o1 和 o2 的大小： 如果方法返回正整数，则表示 o1 大于 o2 ；如果返回 0 ，表示相等；返回负整数，表示o1 小于 o2 。 可以将 Comparator 传递给 sort 方法（如 Collections.sort 或 Arrays.sort) 从而允许在排序顺序上实现精确控制。 还可以使用 Comparator 来控制某些数据结构（如有序 set 或有序映射）的顺序，或者为那些没有自然顺序的对象 collection 提供排序 。 System类 void exit(int status)该方法的作用是退出程序。其中 status 的值为 0 代表正常退出，非零代表异常退出。 使用该方法可以在图形界面编程中实现程序的退出功能等。 native long currentTimeMillis()该方法的作用是返回当前的计算机时间，时间的表达格式为当前计算机时间和 GMT 时间 格林威治时间 )1970 年 1 月 1 号 0 时 0 分 0 秒所差的毫秒数。 void gc()该方法的作用是请求系统进行垃圾回收。至于系统是否立刻回收，则取决于系统中垃圾回收算法的实现以及系统执行时的情况。 String getProperty(String key)该方法的作用是获得系统中属性名为 key 的属性对应的值。系统中常见的属性名以及属性的作用如下表所示： | 属性名 | 属性说明 || —————— | ————————— || java.version | java运行时环境版本 || java.home | java安装目录 || os.name | 操作系统名称 || os.version | 操作系统版本 || user.name | 用户的账户名称 || user.home | 用户的主目录 || user.dir | 用户的当前工作目录 | Math类java.lang.Math提供了一系列静态方法用于科学计算。其方法的参数和返回值类型一般为 double 型。 abs 绝对值 acos,asin,atan,cos,sin,tan 三角函数 sqrt 平方根 pow(double a,doble b) a的 b 次幂 log 然对数 exp e 为底指数 max(double a,double b) min(double a,double b) random() 返回 0.0 到 1.0 的随机数 long round(double a) double型数据 a 转换为 long 型（四舍五入） toDegrees(double angrad) 弧度—-&gt;角度 toRadians(double angdeg) 角度—-&gt;弧度 BigInteger 与 BigDecimalBigIntegerjava.math 包的 BigInteger 可以表示==不可变==的任意精度的整数 。 BigInteger 提供所有 Java 的基本整数操作符的对应物，并提供 java.lang.Math 的所有相关方法。另外， BigInteger 还提供以下运算：模算术、 GCD 计算、质数测试、素数生成、位操作以及一些其他操作。 构造器： BigInteger (String val）根据字符串构建 BigInteger 对象； 常用方法 public BigInteger abs ()：返回此 BigInteger 的绝对值的BigInteger 。 BigInteger add (BigInteger val) ：返回其值为 (this + val) 的 BigInteger BigInteger subtract (BigInteger val) ：返回其值为 (this - val) 的 BigInteger BigInteger multiply (BigInteger val) ：返回其值为 (this * val) 的 BigInteger BigInteger divide (BigInteger val) ：返回其值为 (this / val) 的 BigInteger 。整数相除只保留整数部分 。 BigInteger remainder (BigInteger val) ：返回其值为 (this % val) 的 BigInteger 。 BigInteger [] divideAndRemainder (BigInteger val)：返回包含 (this / val) 后跟(this % val) 的两个 BigInteger 的数组 。 BigInteger pow (int exponent) ：返回其值为 (this^{exponent} ) 的 BigInteger 。 BigDecimal一般的 Float 类和 Double 类可以用来做科学计算或工程计算，但在商业计算中，要求数字精度比较高，故用到 java.math.BigDecimal 类 。 BigDecimal 类支持不可变的、任意精度的有符号十进制定点数 。 构造器 public BigDecimal(double val) public BigDecimal (String val) 常用方法 public BigDecimal add (BigDecimal augend) public BigDecimal subtract (BigDecimal subtrahend) public BigDecimal multiply (BigDecimal multiplicand) public BigDecimal divide (BigDecimal divisor, int scale(精度，小数位数), int roundingMode)","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"枚举类与注解","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java10-枚举类与注解.html","text":"枚举类类的对象只有有限个，确定的。当需要定义一组常量时，强烈建议使用枚举类。若枚举只有一个对象 , 则可以作为一种单例模式的实现方式。 枚举类的属性 枚举类对象的属性不应允许被改动 , 所以应该使用 private final 修饰 枚举类的使用 private final 修饰的属性应该在构造器中为 其 赋值 若枚举类显式的定义了带参数的构造器 , 则在列出枚举值时也必须对应的传入参数 自定义枚举类 私有化 类的构造器，保证不能在类的外部创建其对象 在类的内部创建枚举类的实例。声明为： public static final 对象如果有实例变量，应该声明为 private final ，并在构造器中初始化 class Season{ private final String SEASONNAME;// 季节的名称 private final String SEASONDESC;// 季节的描述 private Season(String seasonName, String seasonDesc){ this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; } public static final Season SPRING = new Season( &quot;春天&quot;, &quot;春暖花开&quot;); public static final Season SUMMER = new Season( &quot;夏天&quot;, &quot;夏日炎炎&quot;); public static final Season AUTUMN = new Season( &quot;秋天&quot;, &quot;秋高气爽&quot;); public static final Season WINTER = new Season( &quot;冬天&quot;, &quot;白雪皑皑&quot;); } 使用 enum 定义枚举类使用说明 使用 enum 定义的枚举类默认继承 了 java.lang.Enum 类，因此不能再继承其他类 枚举类的构造器只能使用 private 权限修饰符 枚举类的所有实例必须在枚举类中显式列出 (, 分隔 ; 结尾)。列出的实例系统会自动添加 public static final 修饰 必须在枚举类的第一行声明枚举类对象 JDK 1.5 中可以在 switch 表达式中使用 Enum 定义的枚举类的对象作为表达式 , case 子句可以直接使用枚举值的名字 , 无需添加枚举类作为限定。 enum Season{ SPRING( &quot;春天&quot;, &quot;春暖花开&quot;), SUMMER( &quot;夏天&quot;, &quot;夏日炎炎&quot;), AUTUMN( &quot;秋天&quot;, &quot;秋高气爽&quot;), WINTER( &quot;冬天&quot;, &quot;白雪皑皑&quot;); private final String SEASONNAME;// 季节的名称 private final String SEASONDESC;// 季节的描述 private Season(String seasonName, String seasonDesc){ this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; } } Enum 类 的主要方法： values() 方法 ：返回枚举类型的对象数组。该方法可以很方便地遍历所有的枚举值。 valueOf (String str)：可以把一个字符串转为对应的枚举类对象。要求字符串必须是枚举类对象的“名字”。如不是，会有运行时异常：IllegalArgumentException 。 toString()toString()：返回当前枚举类对象常量的名称 和普通 Java 类一样，枚举类可以实现一个或多 个接口 若每个枚举值在调用实现的接口方法呈现相同的行为方式，则只要统一实现该方法即可。 若需要每个枚举值在调用实现的接口方法呈现出不同的行为方式 ,则可以让每个枚举值分别来实现该方法。 interface Info{ show(); } //情况一 enum Season implements Info{ SPRING( &quot;春天&quot;, &quot;春暖花开&quot;), SUMMER( &quot;夏天&quot;, &quot;夏日炎炎&quot;), AUTUMN( &quot;秋天&quot;, &quot;秋高气爽&quot;), WINTER( &quot;冬天&quot;, &quot;白雪皑皑&quot;); private final String SEASONNAME;// 季节的名称 private final String SEASONDESC;// 季节的描述 private Season(String seasonName, String seasonDesc){ this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; } // 所有对象执行相同的行为方式 show(){ System.out.println(&quot;XXXXXXXXXXXX&quot;); } } //情况二 enum Season implements Info{ // 对象执行不同的行为方式 // 在对象内部定义需要重写的接口 SPRING( &quot;春天&quot;, &quot;春暖花开&quot;){ show(){ System.out.println(&quot;11111&quot;); } }, SUMMER( &quot;夏天&quot;, &quot;夏日炎炎&quot;){ show(){ System.out.println(&quot;22222&quot;); }}, AUTUMN( &quot;秋天&quot;, &quot;秋高气爽&quot;){ show(){ System.out.println(&quot;333333&quot;); } }, WINTER( &quot;冬天&quot;, &quot;白雪皑皑&quot;){ show(){ System.out.println(&quot;444444&quot;); } }; private final String SEASONNAME;// 季节的名称 private final String SEASONDESC;// 季节的描述 private Season(String seasonName, String seasonDesc){ this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; } } 注解Annotation 其实就是代码里的 特殊标记 , 这些标记可以在编译 , 类加载 , 运行时被读取 , 并执行相应 的处理。通过 使用 Annotation, 程序员可以在不改变原有逻辑的情况下 , 在源文件中嵌入一些 补充 信息 。 代码分析工具、开发工具和部署工具可以通过这些补充信息进行验证或者进行部署。 Annotation 可以像修饰符一样被使用 , 可用于 修饰包 类 , 构造器 , 方法 , 成员变量 , 参数 , 局部变量的声明 , 这些信息被保存在 Annotation的 “name=value” 对中 。 未来的开发模式都是基于注解的， JPA 是基于注解的， Spring2.5 以上都是基于注解的， Hibernate3.x 以后也是基于注解的，现在的Struts2 有一部分也是基于注解的了，注解是一种趋势 ，一定程度上可以说： ==框架 = 注解 + 反射 + 设计模式==。 使用 Annotation 时要在其前面增加 @ 符号 , 并 把该 Annotation 当成一个修饰符使用。 用于修饰它支持的程序元素 生成文档相关的注解 @author标明开发该类模块的作者，多个作者之间使用分割 @version 标明该类模块的版本 @see 参考转向 也就是相关主题 @since 从哪个版本开始增加的 @param 对方法中某参数的说明，如果没有参数就不能写 @return 对方法返回值的说明，如果方法的返回值类型是 void 就不能写 @exception 对方法可能抛出的异常进行说明，如果方法没有用 throws 显式抛出的异常就不能写 其中 @param @return 和 @exception 这三个标记都是只用于方法的 。 @param 的格式要求： @param 形参名 形参类型 形参说明 @return 的格式要求： @return 返回值类型 返回值说明 @exception 格式要求： @exception 异常类型 异常说明 @param @exception 可以并列多个 在编译时进行格式检查 (JDK 内置的三个基本注解) @override: 限定重写父类方法或实现接口 , 该注解只能用于方法 @Deprecated : 用于表示所修饰的元素(类 , 方法等)已过 时。通常是因为所修饰的结构危险或存在更好的选择 @SuppressWarnings : 抑制编译器警告 跟踪代码依赖性，实现替代配置文件功能 自定义Annotation 定义新的 Annotation 类型使用 @interface 关键字 自定义注解自动继承了 java.lang.annotation.Annotation 接口 Annotation 的成员变量在 Annotation 定义中以无参数方法的形式来声明 。 其方法名和返回值定义了该成员的名字和类型 。 我们称为配置参数 。 类型只能是八种基本数据类型 、 String 类型 、 Class 类型 、 enum 类型 、 Annotation 类型 、以上所有类型的数组 。 可以在定义 Annotation 的成员变量时为其指定初始值，指定成员变量的初始值可使用 default 关键字 如果只有一个参数成员，建议使用参数名为 value 如果定义的注解含有配置参数，那么使用时必须指定参数值，除非它有默认值 。 格式是 “参数名=参数值“，如果只有一个参数成员 且名称为 value，可以省略 ”value=“ 没有成员定义的 Annotation 称为 标记；包含成员变量的 Annotation 称为元数据 Annotation 注意：==自定义注解必须配上注解的信息处理流程才有意义==。 @MyAnnotation( value = &quot;尚硅谷&quot;) public class MyAnnotationTest { public static void main(String[] args ) Class clazz = MyAnnotationTest.class; Annotation a = clazz.getAnnotation(MyAnnotation.class); MyAnnotation m = (MyAnnotation)a; String info = m.value(); System.out.println(info); } } @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @interface MyAnnotation{ String value() default &quot;尚硅谷&quot;; } 元注解JDK 的元 Annotation 用于修饰其他 Annotation 定义 JDK5.0 提供了 4 个标准的 meta-annotation 类型分别是： Retention：指定所修饰Annotation 的生命周期：SOURCE、CLASS(默认)、RUNTIME，只有声明为RUNTIME的注解，才能通过反射获取。 Target：指定被修饰的Annotation能用于修饰那些结构 Documented：用于指定被该元 Annotation 修饰的 Annotation 类将被javadoc 工具提取 成 文档。 默认情况下 javadoc 是不包括注解的 。 Inherited 1.@Retention : 只能用于修饰一个 Annotation 定义 , 用于指定该 Annotation 的生命周期，@Rentention 包含一个 RetentionPolicy 类型的成员变量 , 使用@Rentention 时必须为该 value 成员变量指定值 RetentionPolicy.SOURCE 在源文件中有效（即源文件保留）编译器直接丢弃这种策略的注释 RetentionPolicy.CLASS 在 class 文件中有效（即 class 保留） 当运行 Java 程序时 , JVM不会保留注解。 这是默认值 RetentionPolicy.RUNTIME 在运行时有效（即运行时保留） 当运行 Java 程序时 , JVM 会保留注释。程序可以通过反射获取该注释。 2.@Target : 用于修饰 Annotation 定义 , 用于指定被修饰的 Annotation 能用于修饰哪些程序元素。 @Target 也包含一个名为 value 的成员变量。 CONSTRUCTOR 用于描述构造器 FIELD 用于描述域 LOCAL_VARIABLE 用于描述局部变量 METHOD 用于描述方法 PACKAGE 用于描述包 PARAMETER 用于描述参数 TYPE 用于描述类、接口（包括注解类型）或enum声明 3.@Documented：定义为 Documented 的注解必须设置 Retention 值为 RUNTIME 。 4.@Inherited: 被它修饰的 Annotation 将具有继承性 。如果某个类使用了被@Inherited 修饰的 Annotation, 则其子类将自动具有该注解。 比如：如果 把标有 @Inherited 注解的自定义的注解标注在类级别上，子类则可以继承父类类级别的注解 实际应用中，使用较少 JDK8新特性Java 8对注解处理提供了两点改进： 可重复的注解及可用于类型的注解 。此外，反射也得到了加强，在 Java8 中能够得到方法参数的名称。这会简化标注在方法参数上的注解。 可重复注解：第一步：在MyAnnotation上声明@Repeatable，成员值为MyAnnotations.class；第二步：MyAnnotation的Target和Retention等元注解和MyAnnotationa相同。Inherited也需一致。 类型注解：TYPE_PARAMETER，TYPE_USE。 在 Java 8 之前 注解只能是在声明的地方所使用 Java 8 开始 注解可以应用在任何地方 。 ElementType.TYPE_PARAMETER 表示该注解能写在类型变量的声明语句中，如泛型声明 ElementType.TYPE_USE 表示该注解能写在使用类型的任何语句中。 public class TestTypeDefine&lt;TestTypeDefine&lt;@TypeDefine () U&gt; private U u; public &lt;@TypeDefine() T&gt; void test(T t } } @Target({ ElementType.TYPE_PARAMETER}) @interface TypeDefine{ } @MyAnnotation public class AnnotationTest&lt;U&gt; { @MyAnnotation private String name; public static void main(String[] args){ AnnotationTest&lt;@MyAnnotation String&gt; t = null; int a = (@MyAnnotation int )2L; @MyAnnotation int b = 10; } public static &lt;@MyAnnotation T&gt; void method(T t ) } public static void test(@MyAnnotation String arg ) throws @MyAnnotation Exception{ } } @Target(ElementType.TYPE_USE) @interface MyAnnotation }","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"多线程","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java08-多线程.html","text":"基本概念程序 (program) 是为完成特定任务、用某种语言编写的一组指令的集合 。即指 一段静态的代码，静态对象。 进程 (process) 是程序的一次执行过程，或是正在运行的一个程序。是 一个动态的过程 ：有它自身的产生、存在和消亡的过程 。——生命周期 程序是静态的，进程是动态的 进程作为资源分配的单位， 系统在运行时会为每个进程分配不同的内存区域 线程 (thread)，进程可进一步细化为线程，是一个程序内部的一条执行路径。 若 一个进程同一时间并行执行多 个线程，就是支持多线程的 线程作为调度和执行的单位，每个线程拥有独立的运行栈和程序计数器 ( pc)，线程切换的开销小 一个进程中的多个线程共享相同的内存单元 / 内存地址空间—&gt;它们从同一堆中分配对象，可以访问相同的变量和对象。这就使得线程间通信更简便、高效。但多个线程操作共享的系统资源可能就会带来==安全的隐患==。 一个 Java 应用程序 java.exe ，其实至少有三个线程 main() 主线程， gc()垃圾回收线程，异常处理线程。当然如果发生异常，会影响主线程 。 并行与并发： 并行： 多 个 CPU 同时执行多个任务。比如：多个人同时做不同的事 。 并发： 一 个 CPU (采用时间片) 同时执行多个任务。比如：秒杀、多个人做同一件事。 使用多线程的优点 背景：以单核 CPU 为例， 只使用单个线程先后完成多个任务（调用多个方法），肯定比用多个线程来完成用的时间更短，为何仍需多线程呢？ 多线程程序的优点： 提高应用程序的响应。对图形化界面更有意义，可增强用户体验。 提高计算机系统 CPU 的利用率 改善程序结构。将既长又复杂的进程分为多个线程，独立运行，利于理解和修改 何时需要多线程： 程序需要同时执行两个或多个任务。 程序需要实现一些需要等待的任务时，如用户输入、文件读写操作、网络操作、搜索等。 需要一些后台运行的程序时。 线程的创建和使用java.lang.Thread Thread 类的特性 构造器 Thread() 创建新的Thread 对象 Thread(String threadname) 创建线程并指定线程实例名 Thread(Runnable target) 指定创建线程的目标对象，它实现了Runnable 接口中的run方法 Thread(Runnable target, String name) 创建新的 Thread 对象 每个线程都是通过某个特定 Thread 对象的 run() 方法来完成操作的，经常把 run() 方法的主体称为线程体 通过该 Thread 对象的 start() 方法来启动这个线程，而非直接调用 run() Thread类的有关方法 void start(): 启动线程，并执行对象的 run() 方法 run(): 线程在被调度时执行的操作 String getName(): 返回线程的名称 void setName (String) 设置该线程名称，也可用带参构造器设置线程名字 static Thread currentThread(): 静态方法，返回当前线程 。在Thread子类中就是this ，通常用于主线程和 Runnable 实现类 static void yield()：线程让步；暂停当前正在执行的线程，把执行机会让给优先级相同或更高的线程；若队列中没有同优先级的线程，忽略此方法。 当某个程序执行流（如主线程）中调用其他线程（如子线程）的 join() 方法时，调用线程（主线程）将被阻塞，直到 join() 方法加入的 join 线程（子线程）执行完为止（子线程执行完毕再返回主线程）；低优先级的线程也可以获得执行。 static void sleep(long milli): 指定时间毫秒；令当前活动线程在指定时间段内放弃对 CPU 控制，使其他线程有机会被执行，时间到后重排队。抛出 InterruptedException 异常 stop(): 强制线程生命期结束，不推荐使用 boolean isAlive(): 返回 boolean ，判断线程是否还活着 wait() 令当前线程挂起并放弃 CPU 、 同步资源并等待， 使别的线程可访问并修改共享资源，而当前线程排队等候其他线程调用 notify() 或 notifyAll() 方法唤醒，唤醒后等待重新获得对监视器的所有权后才能继续执行。 创建方式一：继承Thread类 创建继承于Thread的子类 重写run()方法，将此线程执行的操作声明在run()中 创建子类的对象 通过此对象调用start()方法：启动线程，并自动调用run()方法 注意点： 如果 自己手动调用 run() 方法，那么就只是普通方法，没有启动多线程模式。 run 方法由 JVM 调用，什么时候调用，执行的过程控制都有操作系统的 CPU调度决定 。 想要启动多线程，必须调用 start 方法 。 一 个线程对象只能调用一次 start() 方法启动，如果重复调用了，则将抛出的异常“ “IllegalThreadStateException”。 线程的优先级、调度线程的优先级等级： MAX_PRIORITY：10 MIN PRIORITY：1 NORM_PRIORITY：5 默认优先级 getPriority() 返回线程优先值 setPriority(int newPriority) 改变线程的优先级 线程创建时继承父线程的优先级 低优先级只是获得调度的概率低，并非一定是在高优先级线程之后才被调用 调度策略: 时间片 抢占式：高优先级的线程抢占CPU；高优先级的线程高概率的情况下被执行，并不意味高优先级的线程执行完后再执行低优先级的线程。 Java 的调度方法: 同优先级线程组成先进先出队列（先到先服务），使用时间片策略 对高优先级，使用优先调度的抢占式策略 创建方式二：实现 Runnable 接口 定义子类 ，实现 Runnable 接口。 子类中重写 Runnable 接口中的 run 方法。 通过 Thread 类含参构造器创建线程对象。 将 Runnable 接口的子类对象作为实际参数传递给 Thread 类的构造器中 。 调用 Thread 类的 start 方法：开启线程调用，Runnable 子类接口的 run 方法。 比较两种创建方式开发中，优先选择实现Runnable接口的方式； 没有类的单继承的局限性 更适合来处理多个线程有共享数据的情况 两者联系：Thread也是 implements Runnable；都需要重写run方法 补充：线程的分类：Java中的线程分为两类：一种是 守护线程 ，一种是 用户线程 。 它们在几乎每个方面都是相同的，唯一的区别是判断 JVM 何时离开。 守护线程是用来服务用户线程的，通过在 start() 方法前调用thread.setDaemon (true 可以把一个用户线程变成一个守护线程。 Java 垃圾回收就是一个典型的守护线程。 若 JVM 中都是守护线程，当前 JVM 将 退出 。 形象理解： 兔死狗烹，鸟尽弓藏 线程的生命周期要想实现 多 线程 必须在主线程中创建新的线程对象 。 Java 语言使用 Thread 类及其子类的对象来表示线程，在它的一个完整的生命周期中通常要经历如下的五种状态： 新建： 当 一个 Thread 类或其子类的对象被声明并创建时，新生的线程对象处于新建状态 就绪： 处于新建状态的线程被 start() 后，将进入线程队列等待 CPU 时间片，此时它已具备了运行的条件 ，只是没分配到 CPU 资源 运行： 当就绪的线程被调度并获得 CPU 资源时便进入运行状态， run() 方法定义了线程的操作和功能 阻塞： 在某种特殊情况下，被人为挂起或执行输入输出操作时，让出 CPU 并临时中止自己的执行，进入阻塞状态 死亡： 线程完成了它的全部工作或线程被提前强制性地中止或出现异常导致结束 线程的同步当多条语句在操作同一个线程共享数据时，一个线程对多条语句只执行了一部分，还没有执行完，另一个线程参与进来执行。导致共享数据的错误 。 解决办法：对多条操作共享数据的语句，只能让一个线程都执行完，在执行过程中，其他线程不可以参与执行。 方式一：同步代码块synchronized(同步监视器){ // 需要同步的代码，即操作共享数据的代码 } 同步监视器：俗称“锁”。任何一个类的对象都可以充当锁 要求多个线程==必须共用同一把锁== /* 继承Thread类的方式，需要把锁声明为static，因每个子线程均new一个线程对象；或者使用 当前类 的 对象 实现Runnable的方式，每个子线程共用同一个Runnable实现类；或者使用this */ class Window2 extends Thread{ private static int ticket = 100; private static Object obj = new Object(); @Override public void run() { while(true){ //正确的 //synchronized (obj){ synchronized (Window2.class){ // Class clazz = Window2.class,Window2.class只会加载一次 // 类是一个对象 window1.class只加载一次，也是唯一的 // 错误的方式：this代表着t1,t2,t3三个对象 // synchronized (this){ if(ticket &gt; 0){ try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(getName() + &quot;：卖票，票号为：&quot; + ticket); ticket--; }else{ break; } } } } } public class WindowTest2 { public static void main(String[] args) { Window2 t1 = new Window2(); Window2 t2 = new Window2(); Window2 t3 = new Window2(); t1.setName(&quot;窗口1&quot;); t2.setName(&quot;窗口2&quot;); t3.setName(&quot;窗口3&quot;); t1.start(); t2.start(); t3.start(); } } class Window1 implements Runnable{ private int ticket = 100; Object obj = new Object(); @Override public void run() { while(true){ synchronized (this){ // 此时的this:唯一的Window1的对象 { if (ticket &gt; 0) { try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;:卖票，票号为：&quot; + ticket); ticket--; } else { break; } } } } } public class WindowTest1 { public static void main(String[] args) { Window1 w = new Window1(); Thread t1 = new Thread(w); Thread t2 = new Thread(w); Thread t3 = new Thread(w); t1.setName(&quot;窗口1&quot;); t2.setName(&quot;窗口2&quot;); t3.setName(&quot;窗口3&quot;); t1.start(); t2.start(); t3.start(); } } 方式二：同步方法// synchronized 还可以放在方法声明中，表示整个方法为同步方法 public synchronized void show (String name){ … } implements Runnable的方式 class Window3 implements Runnable { private int ticket = 100; @Override public void run() { while (true) { show(); } } private synchronized void show(){ //同步监视器：this if (ticket &gt; 0) { try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;:卖票，票号为：&quot; + ticket); ticket--; } } } 继承Thread的方法 同步方式要求是==静态==的 class Window4 extends Thread { private static int ticket = 100; @Override public void run() { while (true) { show(); } } private static synchronized void show(){//同步监视器：Window4.class //private synchronized void show(){ //同步监视器：t1,t2,t3。此种解决方式是错误的 if (ticket &gt; 0) { try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;：卖票，票号为：&quot; + ticket); ticket--; } } } 同步的方式解决了线程的安全问题；但操作同步代码时，只能有一个线程参与，其他线程等待，相当于一个单线程的过程，效率低. 同步方法仍然涉及同步监视器，只是不需要显式地声明；非静态的同步方法，同步监视器为this；静态的同步方法，同步监视器为 当前类本身 (类.class) 使用同步机制将单例模式中的懒汉式改写为线程安全的: class Bank{ private Bank(){} private static Bank instance = null; public static Bank getInstance(){ //方式一：效率稍差 // synchronized (Bank.class) { // if(instance == null){ // instance = new Bank(); // } // return instance; // } //方式二：效率更高 if(instance == null){ synchronized (Bank.class) { if(instance == null){ instance = new Bank(); } } } return instance; } } 死锁 不同的线程分别占用对方需要的同步资源不放弃，都在等待对方放弃自己需要的同步资源，就形成了线程 的 死锁 出现死锁后，不会出现异常，不会出现提示，只是所有的线程都处于阻塞状态，无法继续 //死锁的演示 class A { public synchronized void foo(B b) { //同步监视器：A类的对象：a System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot; 进入了A实例的foo方法&quot;); // ① // try { // Thread.sleep(200); // } catch (InterruptedException ex) { // ex.printStackTrace(); // } System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot; 企图调用B实例的last方法&quot;); // ③ b.last(); } public synchronized void last() {//同步监视器：A类的对象：a System.out.println(&quot;进入了A类的last方法内部&quot;); } } class B { public synchronized void bar(A a) {//同步监视器：b System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot; 进入了B实例的bar方法&quot;); // ② // try { // Thread.sleep(200); // } catch (InterruptedException ex) { // ex.printStackTrace(); // } System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot; 企图调用A实例的last方法&quot;); // ④ a.last(); } public synchronized void last() {//同步监视器：b System.out.println(&quot;进入了B类的last方法内部&quot;); } } public class DeadLock implements Runnable { A a = new A(); B b = new B(); public void init() { Thread.currentThread().setName(&quot;主线程&quot;); // 调用a对象的foo方法 a.foo(b); System.out.println(&quot;进入了主线程之后&quot;); } public void run() { Thread.currentThread().setName(&quot;副线程&quot;); // 调用b对象的bar方法 b.bar(a); System.out.println(&quot;进入了副线程之后&quot;); } public static void main(String[] args) { DeadLock dl = new DeadLock(); new Thread(dl).start(); dl.init(); } } /* 当前线程名: 主线程 进入了A实例的foo方法 当前线程名: 主线程 企图调用B实例的last方法 进入了B类的last方法内部 进入了主线程之后 当前线程名: 副线程 进入了B实例的bar方法 当前线程名: 副线程 企图调用A实例的last方法 进入了A类的last方法内部 进入了副线程之后 */ // 取消注释后 /* 当前线程名: 主线程 进入了A实例的foo方法 当前线程名: 副线程 进入了B实例的bar方法 当前线程名: 副线程 企图调用A实例的last方法 当前线程名: 主线程 企图调用B实例的last方法 // 这里卡住了 */ Lock (锁）从 JDK 5.0 开始 Java 提供了更强大的线程同步机制，通过显式定义同步锁对象来实现同步。同步锁使用 Lock 对象充当 。 java.util.concurrent.locks.Lock 接口是控制多个线程对共享资源进行访问的工具。 锁提供了对共享资源的独占访问，每次只能有一个线程对 Lock 对象加锁，==线程开始访问共享资源之前应先获得 Lock 对象==。 ReentrantLock 类实现了 Lock ，它拥有与 synchronized 相同的并发性和内存语义， 在实现线程安全的控制中，比较常用的是 ReentrantLock 可以显式加锁、释放锁。对多个线程，lock应是唯一的。 class A { private final ReentrantLock lock = new ReenTrantLock(); // 对多个线程应是唯一的 public void m(){ lock.lock(); try{ //保证线程安全的代码 } finally{ lock.unlock(); } } } // 注意：如果同步代码有异常，要将unlock() 写入 finally 语句块 synchronized 与 Lock 的对比 Lock 是显式锁（手动开启和关闭锁，别忘记关闭锁）， synchronized 是隐式锁，出了作用域自动释放 Lock 只有代码块锁， synchronized 有代码块锁和方法锁 使用 Lock 锁， JVM 将花费较少的时间来调度线程，性能更好。并且具有更好的扩展性（提供更多的子类） 优先使用顺序：① Lock ②同步代码块（已经进入了方法体，分配了相应资源) ③同步方法（在方法体之外） 线程的通信class Number implements Runnable{ private int number = 1; private Object obj = new Object(); @Override public void run() { while(true){ synchronized (obj) { obj.notify(); // notify()唤醒正在排队等待同步资源的线程中优先级最高者结束等待 // notifyAll()唤醒正在排队等待资源的所有线程结束等待 if(number &lt;= 100){ try { Thread.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;:&quot; + number); number++; try { //使得调用如下wait()方法的线程进入阻塞状态 //自动释放锁 obj.wait(); } catch (InterruptedException e) { e.printStackTrace(); } }else{ break; } } } } } public class CommunicationTest { public static void main(String[] args) { Number number = new Number(); Thread t1 = new Thread(number); Thread t2 = new Thread(number); t1.setName(&quot;线程1&quot;); t2.setName(&quot;线程2&quot;); t1.start(); t2.start(); } } wait () 与 notify() 和 notifyAll() 这三个方法只有在 synchronized 方法或 synchronized 代码块中才能使用 这三个方法的调用者必须是同步代码块或同步方法的同步监视器，否则会报java.lang.IllegalMonitorStateException 异常。 因为这三个方法必须由锁对象调用，而任意对象都可以作为 synchronized 的同步锁，因此这三个方法只能在 Object 类中声明 。 sleep()和wait()方法异同 相同点：都可以使当前线程进入阻塞状态 不同： a. 两个方法声明的位置不同，Thread类中声明sleep()方法；Object类中声明wait()方法。 b. 调用范围不同，sleep()可以在任何场景下调用；wait()必须使用在同步代码块或同步方法中 c. sleep不会释放锁，wait释放锁 JDK5.0新增线程的创建方式新增方式一：实现Callable 接口 创建一个实现Callable的实现类 重写call方法，将此线程需要执行的操作声明在call()中 创建Callable接口实现类的对象 将此Callable接口实现类的对象作为传递到FutureTask构造器中，创建FutureTask的对象 将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread对象，并调用start() 获取Callable中call方法的返回值。get()返回值即为FutureTask构造器参数Callable实现类重写的call()的返回值。 //1.创建一个实现Callable的实现类 class NumThread implements Callable{ //2.实现call方法，将此线程需要执行的操作声明在call()中 @Override public Object call() throws Exception { int sum = 0; for (int i = 1; i &lt;= 100; i++) { if(i % 2 == 0){ System.out.println(i); sum += i; } } return sum; } } public class ThreadNew { public static void main(String[] args) { //3.创建Callable接口实现类的对象 NumThread numThread = new NumThread(); //4.将此Callable接口实现类的对象作为传递到FutureTask构造器中，创建FutureTask的对象 FutureTask futureTask = new FutureTask(numThread); //5.将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread对象，并调用start() new Thread(futureTask).start(); try { //6.获取Callable中call方法的返回值 //get()返回值即为FutureTask构造器参数Callable实现类重写的call()的返回值。 Object sum = futureTask.get(); System.out.println(&quot;总和为：&quot; + sum); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } } 如何理解实现Callable接口的方式创建多线程比实现Runnable接口创建多线程方式强大： call()可以有返回值 call()可以抛出异常，被外面的操作捕获，获取异常的信息 Callable是支持泛型的 新增方式二：使用线程池背景： 经常创建和销毁、使用量特别大的资源，比如并发情况下的线程，对性能影响很大 。 思路： 提前创建好多个线程，放入线程池中，使用时直接获取，使用完放回池中。可以避免频繁创建销毁、实现重复 利用。类似生活中的公共交通工具。 好处： 提高响应速度 （减少了创建新线程的时间） 降低资源消耗 （重复利用线程池中线程，不需要每次都创建） 便于线程管理 corePoolSize ：核心池的 大小 maximumPoolSize ：最大线程数 keepAliveTime ：线程没有任务时最多保持多长时间后会终止 JDK 5.0 起提供了线程池相关 API：ExecutorService 和 Executors ExecutorService ：真正的线程池接口。常见子类 ThreadPoolExecutor void execute(Runnable command) ：执行任务 命令，没有返回值，一般用来执行Runnable Future submit(Callable task)：执行任务，有返回值，一般又来执行Callable void shutdown() ：关闭连接池 Executors ：工具类、线程池的工厂类，用于创建并返回不同类型的线程池 Executors.newCachedThreadPool ()：创建一个可根据需要创建新线程的线程池 Executors.newFixedThreadPool(n); 创建一个可重用固定线程数的线程池 Executors.newSingleThreadExecutor () ：创建一个只有一个线程的线程池 Executors.newScheduledThreadPool(n)：创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。 class NumberThread implements Runnable{ @Override public void run() { for(int i = 0;i &lt;= 100;i++){ if(i % 2 == 0){ System.out.println(Thread.currentThread().getName() + &quot;: &quot; + i); } } } } class NumberThread1 implements Runnable{ @Override public void run() { for(int i = 0;i &lt;= 100;i++){ if(i % 2 != 0){ System.out.println(Thread.currentThread().getName() + &quot;: &quot; + i); } } } } public class ThreadPool { public static void main(String[] args) { //1. 提供指定线程数量的线程池 ExecutorService service = Executors.newFixedThreadPool(10); ThreadPoolExecutor service1 = (ThreadPoolExecutor) service; //设置线程池的属性 // System.out.println(service.getClass()); // service1.setCorePoolSize(15); // service1.setKeepAliveTime(); //2.执行指定的线程的操作。需要提供实现Runnable接口或Callable接口实现类的对象 service.execute(new NumberThread());//适合适用于Runnable service.execute(new NumberThread1());//适合适用于Runnable // service.submit(Callable callable);//适合使用于Callable 返回值可以用FutureTask接收 //3.关闭连接池 service.shutdown(); } }","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"集合","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java11-集合.html","text":"数组在内存存储方面的特点： 数组初始化以后，长度就确定了。 数组声明的类型，就决定了进行元素初始化时的类型 数组在存储数据方面的弊端： 数组初始化以后，长度就不可变了，不便于扩展 数组中提供的属性和方法少，不便于进行添加、删除、插入等操作， 且效率不高。同时无法直接获取存储元素的个数 数组存储的数据是有序的、可以重复的。 存储数据的特点单一 Java 集合类可以用于存储数量不等的多个对象 ，还可用于保存具有映射关系的关联数组。 Collection 接口方法Collection 接口：单列数据， 定义了存取一组对象的方法的集合 List 元素有序、可重复的集合；“动态”数组 Set 元素无序、不可重复的集合 Collection 接口是 List 、 Set 和 Queue 接口的父接口，该接口里定义的方法既可用于操作 Set 集合，也可用于操作 List 和 Queue 集合 。 JDK 不提供此接口的任何直接实现，而是提供更具体的子接口 (如: Set 和 List)实现 。 在 Java5 之前， Java 集合会丢失容器中所有对象的数据类型，把所有对象都当成 Object 类型处理； 从 JDK 5.0 增加了泛型以后， Java 集合可以记住容器中对象的数据类型。 接口方法： 添加：add(Object obj)；addAll(Collection coll) 获取有效元素的个数：int size() 清空集合：void clear() 是否是空集合：boolean isEmpty() 是否包含某个元素：boolean contains(Object obj); boolean containsAll(Collection c); 调用的obj.equals()方法，==需要重写equals()方法== 删除：boolean remove(Object obj) 只会删除找到的第一个元素；boolean removeAll(Collection coll) 取当前集合的差集；调用的obj.equals()方法，==需要重写equals()方法== 取两个结合的交集：boolean retainAll(Collection c), 把交集的结果存在当前集合中，不影响 c 集合是否相等：boolean equals(Object obj) 转成对象数组：Object[] toArray() 获取集合对象的哈希值：hashCode() 遍历：iterator() 返回迭代器对象，用于集合遍历 Iterator 迭代器接口Iterator 对象称为迭代器 (设计模式的一种)，主要用于遍历 Collection 集合中的元素。 GOF 给迭代器模式的定义为：提供一种方法访问一个容器 (container) 对象中各个元素，而又不需暴露该对象的内部细节。 迭代器模式，就是为容器而生。 Collection 接口继承了 java.lang.Iterable 接口，该接口有一个 iterator() 方法，那么所有实现了 Collection 接口的集合类都有一个 iterator() 方法，用以返回一个实现了Iterator 接口的对象 。 Iterator 仅用于遍历集合 Iterator 本身并不提供承装对象的能力。如果需要 创建Iterator 对象，则必须有一个被迭代的集合。 集合对象==每次调用== iterator() 方法都得到一个全新的迭代器对象”Iterator iterator = coll.iterator();”，默认游标都在集合的第一个元素之前 。 在调用it.next() 方法之前必须要调用 it.hasNext() 进行检测。若不调用，且下一条记录无效，直接调用 it.next() 会抛出 NoSuchElementException 异常。 注意： Iterator 可以删除集合的元素，但是是遍历过程中通过迭代器对象的 remove 方法，不是集合对象的 remove 方法 。 如果还未调用 next() 或在上一次调用 next 方法之后已经调用了 remove 方法，再调用 remove (调用来两次remove非法)都会报 IllegalStateException 使用 foreach 循环遍历集合元素Java 5.0 提供了 foreach 循环迭代 访问 Collection 和数组。遍历操作不需获取 Collection 或数组的长度，无需使用索引访问元素。==遍历集合的底层调用 Iterator 完成操作==。foreach 还可以用来遍历数组。 Collection 子接口一： List鉴于 Java 中数组用来存储数据 的局限性，我们通常使用 List 替代数组。List 集合类中==元素有序、且可重复==，集合中的每个元素都有其对应的顺序索引。List 容器中的元素都对应一个整数型的序号记载其在容器中的位置，可以根据序号存取容器中的元素。JDK API 中 List 接口的实现类常用的有： ArrayList 、 LinkedList 和 Vector 。 ArrayList 、 LinkedList 和 Vector的异同： 同：三个类都实现了List接口，存储数据特点相同：元素有序、且可重复 不同： ArrayList：作为List接口的主要实现类，效率高，线程不安全；底层使用Object[]存储 LinkedList：对于频繁的插入。删除操作，此类效率比ArrayList高；底层使用双向链表 Vector：作为List接口的古老实现类，效率低，线程安全；底层使用Object[]存储 List 除了从 Collection 集合继承的方法外， List 集合里添加了一些根据索引来操作集合元素的 方法 ： void add( int index, Object ele) 在 index 位置插入 ele 元素 boolean addAll (int index, Collection eles) 从 index 位置开始将 eles 中的所有元素添加进来 Object get( int index): 获取指定 index 位置的元素 int indexOf (Object obj) 返回 obj 在集合中首次出现的位置 int lastIndexOf (Object obj) 返回 obj 在当前集合中末次出现的位置 Object remove( int index): 移除指定 index 位置的元素，并返回此元素 Object set( int index, Object ele) 设置指定 index 位置的元素为 ele List subList (int fromIndex , int toIndex) 返回从 fromIndex 到 toIndex位置的子集合 ArrayListArrayList 的 JDK1.8 之前与之后的实现区别？ JDK1.7 ArrayList 像饿汉式，直接创建一个初始容量为 10 的数组；容量不够时，扩容为原来容量的1.5倍，同时复制数组。 JDK1.8 ArrayList 像懒汉式，一开始创建一个长度为 0 的数组，当添加第一个元素时再创建一个始容量为 10 的 数组，扩容和JDK7一致。 建议：开发中使用带参构造器：ArrayList list = new ArrayList(int initCapacity); 本质上， ArrayList 是对象引用的 一个“变长”数组。 Arrays.asList (…) 方法返回的 List 集合 既不是 ArrayList 实例，也不是Vector 实例。Arrays.asList (…) 返回值是一个固定长度的 List 集合。 LinkedList新增方法： void addFirst (Object obj) void addLast (Object obj) Object getFirst() Object getLast() Object removeFirst() Object removeLast() LinkedList 双向链表内部没有声明数组，而是定义了 Node 类型的 first 和 last用于记录首末元素。同时，定义内部类 Node ，作为 LinkedList 中保存数据的基本结构。 Node 除了保存数据，还定义了两个变量： prev 变量记录前一个元素的位置 next 变量记录下一个元素的 位置 LinkedList内部声明了Node类型的 first 和 last 属性，默认null。 Vector新增方法： void addElement (Object obj) void insertElementAt (Object obj,int index) void setElementAt (Object obj,int index) void removeElement (Object obj) void removeAllElements() 面试题：请问ArrayList/LinkedList/Vector 的 异同 谈谈你的理解？ ArrayList 底层是什么？扩容机制？ Vector 和 ArrayList 的最大区别？ ArrayList 和 LinkedList 的异同：二者都线程不安全，相对线程安全的Vector ，执行效率高。此外，ArrayList 是实现了基于动态数组的数据结构， LinkedList 基于链表的数据结构。对于随机访问 get 和 set ArrayList 绝对优于 LinkedList ，因为 LinkedList 要移动指针。对于新增和删除 操作 add (特指插入) 和 remove，LinkedList 比较占优势，因为 ArrayList 要移动数据。 ArrayList 和 Vector 的区别：Vector和 ArrayList 几乎是完全相同的，唯一的区别在于 Vector 是同步类 ( synchronized)，属于强同步类。因此开销就比 ArrayList 要大，访问要慢。正常情况下 大多数的 Java 程序员使用ArrayList 而不是 Vector, 因为同步完全可以由程序员自己来控制。 Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。 Vector 还有一个子类 Stack 。 @Test public void testListRemove() { List list = new ArrayList(); list.add(1); list.add(2); list.add(3); updateList(list); System.out.println(list);// 1 2 } private static void updateList(List list) list.remove(2); } Collection 子接口二： SetSet 接口是 Collection 的子接口， set 接口没有提供额外的方法；Set 集合不允许包含相同的元素，如果试把两个相同的元素加入同一个Set 集合中，则添加操作失败。Set 判断两个对象是否相同不是使用 == 运算符，而是根据 equals() 方法。 无序性：不等于随机性。存储的数据在底层数组中并非按照数组索引的顺序添加，而是根据数据的哈希值决定 不可重复性：Set 判断两个对象是否相同是根据 equals() 方法，相同的元素只能添加一个。 添加元素的过程： 我们向HashSet中添加元素a，首先调用元素a所在类的hashCode()方法，计算元素a的哈希值，此哈希值接着通过某种算法计算出在HashSet底层数组中的存放位置（即为：索引位置），判断数组此位置上是否已经有元素： 如果此位置上没有其他元素，则元素a添加成功。 —-&gt;情况1 如果此位置上有其他元素b(或以链表形式存在的多个元素），则比较元素a与元素b的hash值： 如果hash值不相同，则元素a添加成功。—-&gt;情况2 如果hash值相同，进而需要调用元素a所在类的equals()方法： ​ equals()返回true,元素a添加失败， ​ equals()返回false,则元素a添加成功。—-&gt;情况3 对于添加成功的情况2和情况3而言：元素a 与已经存在指定索引位置上数据以链表的方式存储。 jdk 7: 元素a放到数组中，指向原来的元素。 jdk 8: 原来的元素在数组中，指向元素a 总结：七上八下 HashSet底层：数组+链表的结构。 HashSet：作为Set接口的主要实现类；线程不安全的；可以存储null值 LinkedHashSet：继承HashSet；遍历内部数据时，可以按照添加的顺序遍历 TreeSet：可以按照添加对象的指定属性排序 HashSet底层也是数组，初始容量为 16，当如果使用率超过 0.75（16*0.75=12），就会扩大容量为原来的 2 倍。 要求：想set中添加的数据，其所在类 一定要重写hashcode()和equals()； 重写hashCode() 方法的基本原则 在程序运行时，同一个对象多次调用 hashCode() 方法应该返回相同的 值。 当两个对象的 equals() 方法比较返回 true 时，这两个对象的 hashCode()方法的返回值也应相等。 对象中用作 equals() 方法比较的 Field ，都应该用来计算 hashCode 值。 重写equals() 方法的基本原则 当一个类有自己特有的“逻辑相等”概念 ，当改写 equals() 的时候 ，总是要改写 hashCode ()，根据一个类的 equals 方法（改写后），两个截然不同的实例有可能在逻辑上是相等的，但是， 根据 Object.hashCode() 方法它们仅仅是两个对象。 因此 ，违反了 “相等的对象必须具有相等的散列码”。 结论 ：复写 equals 方法的时候一般都需要同时复写 hashCode 方法 。 通常参与计算 hashCode 的对象的属性也应该参与到 equals() 中进行计算。 以Eclipse/IDEA 为 例 ，在自定义类中可以调用工具自动 重写 equals 和 hashCode 。问题：为什么 用 E clipse/IDEA 复写 hashCode 方法，有 31 这个数字？ 选择系数的时候要选择尽量大的系数。因为如果计算出来的 hash 地址越大，所谓的“冲突”就越少，查找起来效率也会提高。（减少冲突） 并且 31 只占用 5bits, 相乘造成数据溢出的概率较小。 31 可以 由 i*31== (i&lt;&lt;5) 1 来表示 现在很多虚拟机里面都有做相关 优化 。 （提高算法效率） 31 是一个素数，素数作用就是如果我用一个数字来乘以这个素数，那么最终出来的结果只能被素数本身和被乘数还有 1 来整除 减少冲突 LinkedHashSetLinkedHashSet 是 HashSet 的子类。 LinkedHashSet 根据元素的 hashCode 值来决定元素的存储位置，但它同时使用双向链表维护元素的次序，这使得元素看起来是以插入顺序保存的。 ==LinkedHashSet 插入性能略低于 HashSet==，但在迭代访问 Set 里的全部元素时有很好的性能。 LinkedHashSet 不允许集合元素重复。 TreeSet 向TreeSet中添加的数据，要求是==相同的类型==； 两种排序方式：自然排序(compareTo)和定制排序(Comparator) 自然排序：比较两个对象是否相同的标准为：compareTo()返回0。不再是equal() 定制排序：将比较方法放到TreeSet的带参构造器中。比较两个对象是否相同的标准为：Compara()返回0。不再是equal() 如果试图把一个对象添加到 TreeSet 时，则该对象的类必须实现 Comparable接口。 TreeSet 是 SortedSet 接口的实现类， TreeSet 可以确保集合元素处于排序状态。、 TreeSet 底层使用 红黑树 结构存储数据 新增的方法如下： 了解 Comparator comparator() Object first() Object last() Object lower(Object e) Object higher(Object e) SortedSet subSet (fromElement , toElement) SortedSet headSet (toElement) SortedSet tailSet (fromElement) TreeSet 两种排序方法： 自然排序 和 定制排序 。默认情况下， TreeSet 采用自然排序。 练习：在List 内去除重复数字值，要求尽量简单 //练习：在List内去除重复数字值，要求尽量简单 public static List duplicateList(List list) { HashSet set = new HashSet(); set.addAll(list); return new ArrayList(set); } @Test public void test2(){ List list = new ArrayList(); list.add(new Integer(1)); list.add(new Integer(2)); list.add(new Integer(2)); list.add(new Integer(4)); list.add(new Integer(4)); List list2 = duplicateList(list); for (Object integer : list2) { System.out.println(integer); } } 面试题 public void test3(){ HashSet set = new HashSet(); // 其中Person 类中重写了 hashCode() 和 equal() 方法 Person p1 = new Person(1001,&quot;AA&quot;); Person p2 = new Person(1002,&quot;BB&quot;); set.add(p1); set.add(p2); System.out.println(set);//2个数据 p1.name = &quot;CC&quot;; set.remove(p1);// 删除失败，hashcode不一致 System.out.println(set); set.add(new Person(1001,&quot;CC&quot;));// 3个数据 System.out.println(set); set.add(new Person(1001,&quot;AA&quot;));// 4个数据 System.out.println(set); } Map 接口Map 接口： 双列数据，保存具有映射关系“ key-value对”的集合 |----Map：双列数据 |----HashMap：作为Map的主要实现类；线程不安全，效率高；存储null的key或value |----LinkedHashMap：保证在遍历map元素时，可以按照添加的顺序实现遍历。对于频繁的遍历操作，此类执行效率高于HashMap |----TreeMap：保证按照添加的key-value对进行排序，实现遍历。此时考虑key的自然排序或者定制排序。底层使用红黑树。 |----Hashtable：作为古老的实现类；线程安全，效率低；不能存储null的key或value |----Properties：常用来处理配置文件。key-value都是String类型。 Map 中的 key 和 value 都可以是任何引用类型的数据；Map 中的 key 用 Set 来存放， 不允许重复 ，即同一个 Map 对象所对应的类，须重写 hashCode 和 equals 方法；常用 String 类作为 Map 的“键”。 key无序且不可重复，使用Set存储；key所在类要重写equals()和hashcode() / compareTo方法 value是无序可重复的，使用Collection 存储；value类需要重写equals() entry无序且不可重复，使用set存储。 1.添加 、 删除、修改操作 Object put(Object key,Object value)：将指定 key-value 添加到 (或修改) 当前 map 对象中 void putAll(Map m): 将 m 中的所有 key-value 对存放到当前 map 中 Object remove(Object key)：移除指定 key 的 key-value 对，并返回 value void clear()：==清空当前 map 中的所有数据，size=0；与map = null不同==。 2.元素 查询的操作： Object get(Object key)：获取指定 key 对应的 value boolean containsKey(Object key)：是否包含指定的 key boolean containsValue(Object value)：是否包含指定的 value int size()：返回 map 中 key-value 对的个数 boolean isEmpty()：判断当前 map 是否为空 boolean equals(Object obj)：判断当前 map 和参数对象 obj 是否相等 3.元视图操作的方法： Set keySet()：返回所有 key 构成的 Set 集合 Collection values()：返回所有 value 构成的 Collection 集合 Set entrySet()：返回所有 key-value 对构成的 Set 集合 Set entrySet = map.entrySet(); Iterator iterator1 = entrySet.iterator(); while (iterator1.hasNext()){ Object obj = iterator1.next(); //entrySet集合中的元素都是entry Map.Entry entry = (Map.Entry) obj; System.out.println(entry.getKey() + &quot;----&gt;&quot; + entry.getValue()); } HashMap允许使用 null 键和 null 值，与 HashSet 一样，不保证映射的顺序。 底层实现原理： JDK 7及以前版本： HashMap 是数组 + 链表结构，即为链地址法 JDK 8版本发布以后： HashMap 是数组 + 链表 + 红黑树实现。 JDK 7: HashMap map = new HashMap(); /*在实例化以后，底层创建了长度是16的一维数组Entry[] table。*/ ...可能已经执行过多次put... map.put(key1,value1): /*首先，调用key1所在类的hashCode()计算key1哈希值，此哈希值经过某种算法计算以后，得到在Entry数组中的存放位置。 1. 如果此位置上的数据为空，此时的key1-value1添加成功。 ----情况1 2. 如果此位置上的数据不为空，(意味着此位置上存在一个或多个数据(以链表形式存在)),比较key1和已经存在的一个或多个数据的哈希值： 2.1 如果key1的哈希值与已经存在的数据的哈希值都不相同，此时key1-value1添加成功。----情况2 2.2 如果key1的哈希值和已经存在的某一个数据(key2-value2)的哈希值相同，继续比较：调用key1所在类的equals(key2)方法，比较： 2.2.1 如果equals()返回false:此时key1-value1添加成功。----情况3 2.2.2 如果equals()返回true:使用value1替换value2。----情况4 补充：关于情况2和情况3：此时key1-value1和原来的数据以链表的方式存储。 在不断的添加过程中，会涉及到扩容问题，当超出临界值(且要存放的位置非空)时，扩容。默认的扩容方式：扩容为原来容量的2倍，并将原有的数据复制过来。 JDK 8 相较于jdk7在底层实现方面的不同： 1. new HashMap():底层没有创建一个长度为16的数组 2. jdk 8底层的数组是：Node[],而非Entry[] 3. 首次调用put()方法时，底层创建长度为16的数组 4. jdk7底层结构只有：数组+链表。jdk8中底层结构：数组+链表+红黑树。 4.1 形成链表时，七上八下（jdk7:新的元素指向旧的元素。jdk8：旧的元素指向新的元素） 4.2 当数组的某一个索引位置上的元素以链表形式存在的数据个数 &gt; 8 且当前数组的长度 &gt; 64时，此时此索引位置上的所数据改为使用红黑树存储。 HashMap的put() .png) HashMap源码中的重要常量： DEFAULT_INITIAL_CAPACITY :HashMap 的默认容量， 16 MAXIMUM_CAPACITY： HashMap 的最大支持容量， 2^30 DEFAULT_LOAD_FACTOR： HashMap 的默认加载因子，0.75 TREEIFY_THRESHOLDBucket：Bucket 中链表长度大于该默认值，转化为红黑树；8 UNTREEIFY_THRESHOLD：Bucket 中红黑树存储的 Node 小于该默认值，转化为链表 MIN_TREEIFY_CAPACITY：桶中的 Node 被树化时最小的 hash 表容量。（当桶中 Node 的数量大到需要变红黑树时，若 hash 表容量小于 MIN_TREEIFY_CAPACITY 时，此时应执行resize 扩容操作这个 MIN_TREEIFY_CAPACITY 的值至少是 TREEIFY_THRESHOLD 的 4倍。）64 table： 存储元素的数组，总是 2 的 n 次幂 entrySet：存储具体元素的集 size：HashMap 中存储的键值对的数量 modCount：HashMap 扩容和结构改变的次数。 threshold： 扩容的临界值， =容量*填充因子，12 loadFactor填充因子 关于映射关系的key 是否可以修改? answer ：不要修改 映射关系存储到HashMap 中会存储 key 的 hash 值，这样就不用在每次查找时重新计算每一个 Entry 或 Node (TreeNode ）的 hash 值了，因此如果已经 put 到 Map 中的映射关系，再修改 key 的属性，而这个属性又参与 hashcode 值的计算，那么会导致匹配不上。 面试题：负载因子值的大小，对 HashMap 有什么影响 负载 因子的大小决定了 HashMap 的数据密度。 负载因子越大密度越大，发生碰撞的几率越高，数组中的链表越容易长，造成 查询或插入时的比较次数增多，性能会下降。 负载因子越小，就越容易触发扩容，数据密度也越小，意味着发生碰撞的几率越小，数组中的链表也就越短，查询和插入时比较的次数也越小，性能会更高。但是会浪费一定的内容空间。而且经常扩容也会影响性能，建议初始化预设大一点的空间。 按照其他语言的参考及研究经验，会考虑将负载因子设置为 0.7~0.75 ，此时平均检索长度接近于常数。 LinkedHashMapLinkedHashMap 是 HashMap 的子类； 在 HashMap 存储结构的基础上，使用了一对双向链表来记录添加元素的顺序； 与 LinkedHashSet 类似 LinkedHashMap 可以维护 Map 的迭代顺序：迭代顺序与 Key-Value 对的插入顺序一致。 TreeMapTreeMap 存储 Key-Value 对时， 需要根据 key-value 对进行排序，所以key必须是同一个类创建的对象。TreeMap 可以保证所有的 Key-Value 对处于有序状态 。 TreeSet 底层使用红黑树结构存储数据 TreeMap 的 Key 的排序： 自然排序：TreeMap 的所有的 Key 必须实现 Comparable 接口，而且所有的 Key 应该是同一个类的对象，否则将会抛出 ClasssCastException 定制排序 ：创建 TreeMap 时，传入一个 Comparator 对象，该对象负责对TreeMap 中的所有 key 进行排序。此时不需要 Map 的 Key 实现Comparable 接口 TreeMap 判断两个 key 相等的标准 ：两个 key 通过 compareTo() 方法或者 compare() 方法返回 0 。 HashtableHashtable 实现原理和 HashMap 相同，功能相同。底层都使用哈希表结构，查询速度快，很多情况下可以互用 。 与 HashMap 不同， Hashtable 不允许使用 null 作为 key 和 value 与 HashMap 一样， Hashtable 也不能保证其中 Key-Value 对的顺序 Hashtable 判断两个key 相等、两个value 相等的标准与HashMap 一致。 PropertiesProperties 类是 Hashtable 的子类，该对象用于处理属性文件 由于属性文件里的 key 、 value 都是字符串类型，所以 Properties 里的 key和value 都是字符串类型 存取数据时，建议使用 setProperty (String key,String value) 方法和getProperty (String key) 方法 Properties pros = new Properties(); pros. load(new FileInputStream(FileInputStream(&quot;jdbc.properties&quot;)); String user = pros.getProperty(&quot;user&quot;); System.out.println(user); Collections 工具类Collections 是一个操作 Set 、 List 和 Map 等集合的工具类；操作数组的工具类：Arrays Collections 中提供了一系列静态的方法对集合元素进行排序、查询和修改等操作，还提供了对集合对象设置不可变、对集合对象实现同步控制等方法 排序操作：（均为 static 方法） reverse(List)：反转 List 中元素的顺序 shuffle(List)：对 List 集合元素进行随机排序 sort(List)：根据元素的自然顺序对指定 List 集合元素按升序排序 sort(List，Comparator)：根据指定的 Comparator 产生的顺序对 List 集合元素进行排序 swap(List，int， int)：将指定 list 集合中的 i 处元素和 j 处元素进行交换 查找、替换 Object max(Collection)：根据元素的自然顺序，返回给定集合中的最大元素 Object max(Collection，Comparator)：根据 Comparator 指定的顺序，返回给定集合中的最大元素 Object min(Collection) Object min(Collection，Comparator) int frequency(Collection，Object)：返回指定集合中指定元素的出现次数 void copy(List dest,List src)：将src中的内容复制到dest中 List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(-97); list.add(0); //报异常：IndexOutOfBoundsException(&quot;Source does not fit in dest&quot;) //List dest = new ArrayList(); //Collections.copy(dest,list); //正确的： List dest = Arrays.asList(new Object[list.size()]); System.out.println(dest.size());//list.size(); Collections.copy(dest,list); System.out.println(dest); boolean replaceAll(List list，Object oldVal，Object newVal)：使用新值替换 List 对象的所有旧值 Collections 类中提供了多个 synchronizedXxx() 方法，该方法可使将指定集合包装成线程同步的集合，从而可以解决多线程并发访问集合时的线程安全问题","tags":[]},{"title":"泛型","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java12-泛型.html","text":"在集合中使用泛型集合容器类在设计阶段声明阶段不能确定这个容器到底实际存的是什么类型的对象，所以 在 JDK 1.5 之前只能把元素类型设计为 Object， JDK 1.5 之后使用泛型来解决 。 因为这个时候除了元素的类型不确定，其他的部分是确定的，例如关于这个元素如何保存，如何管理等是确定的，因此此时 把元素的类型设计成一个参数，这个类型参数叫做泛型 。 Collection，List，ArrayList，这个 就是类型参数，即泛型 。 所谓 泛型，就是允许在定义类 、 接口时通过一个标识表示类中某个属性的类型或者是某个方法的返回值及参数类型 。 这个类型参数将在使用时（例如继承或实现这个接口，用这个类型声明变量 、 创建对象时）确定（即传入实际的类型参数，也称为类型实参）。 //在集合中使用泛型之前的情况： @Test public void test1(){ ArrayList list = new ArrayList(); //需求：存放学生的成绩 list.add(78); list.add(76); list.add(89); list.add(88); //问题一：类型不安全 //list.add(&quot;Tom&quot;); for(Object score : list){ //问题二：强转时，可能出现ClassCastException int stuScore = (Integer) score; System.out.println(stuScore); } } //在集合中使用泛型的情况：以ArrayList为例 @Test public void test2(){ ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); list.add(78); list.add(87); list.add(99); list.add(65); //编译时，就会进行类型检查，保证数据的安全 //list.add(&quot;Tom&quot;); //方式一： for(Integer score : list){ //避免了强转操作 int stuScore = score; System.out.println(stuScore); } //方式二 Iterator&lt;Integer&gt; iterator = list.iterator(); while(iterator.hasNext()){ int stuScore = iterator.next(); System.out.println(stuScore); } } ① 集合接口或集合类在jdk5.0时都修改为带泛型的结构。 ② 在实例化集合类时，可以指明具体的泛型类型 ③ 指明完以后，在集合类或接口中凡是定义类或接口时，内部结构（比如：方法、构造器、属性等）使用到类的泛型的位置，都指定为实例化的泛型类型。 比如：add(E e) —-&gt;实例化以后：add(Integer e) ④ 注意点：泛型的类型必须是类，不能是基本数据类型。需要用到基本数据类型的位置，拿包装类替换 ⑤ 如果实例化时，没有指明泛型的类型。默认类型为java.lang.Object类型。 自定义泛型结构 泛型类可能有多个参数，此时应将多个参数一起放在尖括号内 。比如： 泛型类的构造器如下 public GenericClass(){} 。 而下面是错误的 public GenericClass(){} 实例化后，操作原来泛型位置的结构必须与指定的泛型类型一致 。 泛型不同的引用不能相互赋值。尽管在编译时 ArrayList 和 ArrayList 是两种类型，但是，在运行时只有一个 ArrayList 被加载到 JVM 中 。 Arraylist&lt;String&gt; list1 = null; Arraylist&lt;Integer&gt; list2 = null; list1 = list2;//错误 泛 型如果不指定，将被擦除，泛型对应的类型均按照 Object 处理，但不等价于 Object 。 经验： 泛型要使用一路都用。要不用，一路都不要用。 如果 泛型结构 是 一个接口或抽象类，则不可创建泛型类的对象 。 jdk1.7，泛型的简化 操作 ArrayList flist = new ArrayList 泛型的指定中不能使用基本数据类型，可以使用包装类替换。 在 类 接口上声明的泛型，在本类或本接口中即代表某种类型，可以作为非静态属性的类型、非静态方法的参数类型、非静态方法的返回值类型 。但在 静态 方法中不能使用类的泛 型。 异常 类不能是泛型的 不能使用 new E[] 。但是可以 E [] elements = (E[])new Object[capacity]; 参考： ArrayList 源码中声明： Object[] elementData 而非泛型参数类型数组。 父类有泛型，子类可以选择保留泛型也可以选择指定泛型类型： 子类不保留父类的泛型：按需实现 没有类型 擦除 具体类型 子类保留父类的泛型：泛型子类 全部保留 部分保留 class Father&lt;T1, T2&gt; {} //子类不保留父类的泛型 // 1)没有类型 擦除 class Son1 extends Father { // 等价于 class Son extends Father&lt;Object,Object&gt;{} } class Son&lt;A, B &gt; extends Father{ //等价 于 class Son extends Father&lt;Object,Object&gt;{} } // 2)具体类型 class Son2 extends Father&lt;Integer, String&gt; {} class Son2&lt;A, B&gt; extends Father&lt;Integer, String&gt; {} //子类保留父类的泛型 // 1)全部保留 class Son3&lt;T1, T2&gt; extends Father&lt;T1, T2&gt; {} class Son3&lt;T1, T2, A, B&gt; extends Father&lt;T1, T2&gt; {} // 2)部分保留 class Son4&lt;T2&gt; extends Father&lt;Integer, T2&gt; {} class Son4&lt;T2, A, B&gt; extends Father&lt;Integer, T2&gt; {} 结论：子类必须是“富二代”，子类除了指定或保留父类的泛型，还可以增加自己的泛型 泛型类 (接口)：/** * 自定义泛型类 */ public class Order&lt;T&gt; { String orderName; int orderId; //类的内部结构就可以使用类的泛型 T orderT; public Order(){ //编译不通过 //T[] arr = new T[10]; //编译通过 T[] arr = (T[]) new Object[10]; } public Order(String orderName,int orderId,T orderT){ this.orderName = orderName; this.orderId = orderId; this.orderT = orderT; } //如下的三个方法都不是泛型方法 public T getOrderT(){ return orderT; } public void setOrderT(T orderT){ this.orderT = orderT; } @Override public String toString() { return &quot;Order{&quot; + &quot;orderName=&#39;&quot; + orderName + &#39;\\&#39;&#39; + &quot;, orderId=&quot; + orderId + &quot;, orderT=&quot; + orderT + &#39;}&#39;; } //静态方法中不能使用类的泛型。 // public static void show(T orderT){ // System.out.println(orderT); // } public void show(){ //编译不通过 // try{ // // }catch(T t){ // // } } //泛型方法：在方法中出现了泛型的结构，泛型参数与类的泛型参数没有任何关系。 //换句话说，泛型方法所属的类是不是泛型类都没有关系。 //泛型方法，可以声明为静态的。原因：泛型参数是在调用方法时确定的。并非在实例化类时确定。 public static &lt;E&gt; List&lt;E&gt; copyFromArrayToList(E[] arr){ ArrayList&lt;E&gt; list = new ArrayList&lt;&gt;(); for(E e : arr){ list.add(e); } return list; } } 泛型方法：方法，也可以被泛型化，不管此时定义在其中的类是不是 泛型类。 在泛型方法中可以定义泛型参数，此时，参数的类型就是传入数据的 类型 。 格式：[访问权限] &lt;泛型&gt; 返回类型 方法名 [泛型标识 参数名称] 抛出的异常 public &lt;E&gt; E get(int id, E e){ E result = nill; return result; } 泛型方法：在方法中出现了泛型的结构，泛型参数与类的泛型参数没有任何关系。换句话说，泛型方法所属的类是不是泛型类都没有关系。 泛型方法，可以声明为静态的。原因：泛型参数是在调用方法时确定的。并非在实例化类时确定。 泛型在继承上的体现 泛型在继承方面的体现 虽然类A是类B的父类，但是G 和G二者不具备子父类关系，二者是并列关系。 补充：类A是类B的父类，A 是 B 的父类 Object obj = null; String str = null; obj = str; Object[] arr1 = null; String[] arr2 = null; arr1 = arr2; //编译不通过 //Date date = new Date(); //str = date; List&lt;Object&gt; list1 = null; List&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); //此时的list1和list2的类型不具有子父类关系 //编译不通过 //list1 = list2; @Test public void test2(){ AbstractList&lt;String&gt; list1 = null; List&lt;String&gt; list2 = null; ArrayList&lt;String&gt; list3 = null; list1 = list3; list2 = list3; List&lt;String&gt; list4 = new ArrayList&lt;&gt;();//类型推断，后一个泛型省略 } 通配符的使用使用类型通配符：？ List&lt;?&gt;是 List 、 List 等各种泛型 List 的父类。 读取 List&lt;?&gt; 的对象 list 中的元素时，永远是安全的，因为不管 list 的真实类型是什么，它包含的都是 Object。 写入 list 中的元素时，不行。因为我们不知道 c 的元素类型，我们不能向其中添加对象。 将任意元素加入到其中不是类型安全的。唯一的例外是 null ，它是所有类型的成员。 另一方面，我们可以调用 get() 方法并使用其返回值。返回值是一个未知的类型，但是我们知道，它总是一 个 Object 。 注意点 1 ：编译错误：不能用在泛型方法声明上，返回值类型前面 不能使用 注意点 2 ：编译错误：不能用在泛型类的声明上 注意点 3 ：编译错误：不能用在创建对象上，右边属于创建集合对象 有限制的通配符 通配符指定上限：上限extends ：使用时指定的类型必须是继承某个类，或者实现某个接口，即 &lt;=；上边界通配符直接使用add()方法受限 通配符指定下限：下限super ：使用时指定的类型不能小于操作的类，即&gt;=；下边界通配符使用get()方法受限（不使用强转） 举例： &lt;?extends Number&gt; ( 无穷小 ,Number] 只允许泛型为 Number 及 Number 子类的引用调用 &lt;? super Number&gt; [Number , 无穷大] 只允许泛型为 Number 及 Number 父类的引用调用 &lt;? extends Comparable&gt; 只允许泛型为实现 Comparable 接口的实现类的引用调用 @Test public void test4(){ List&lt;? extends Person&gt; list1 = null; List&lt;? super Person&gt; list2 = null; List&lt;Student&gt; list3 = new ArrayList&lt;Student&gt;(); List&lt;Person&gt; list4 = new ArrayList&lt;Person&gt;(); List&lt;Object&gt; list5 = new ArrayList&lt;Object&gt;(); list1 = list3; list1 = list4; //list1 = list5; //list2 = list3; list2 = list4; list2 = list5; //读取数据： list1 = list3; Person p = list1.get(0); //编译不通过 //Student s = list1.get(0); // list1中可能存父类或者子类，故只能用父类接收，用最大接收 list2 = list4; Object obj = list2.get(0); ////编译不通过 //Person obj = list2.get(0);// 同理，用最小的接收 //写入数据： //编译不通过 //list1.add(new Student()); // 可能存放父类，不能add子类数据 //编译通过 list2.add(new Person()); list2.add(new Student()); } }","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"网络编程","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java14-网络编程.html","text":"概述Java 提供的网络类库，可以实现无痛的网络连接，联网的底层细节被隐藏在 Java 的本机安装系统里，由 JVM 进行控制。并且 Java 实现了一个跨平台的网络库， 程序员面对的是一个统一的网络编程环境。 IP 地址分类方式 IPV4：4个字节组成， 4 个0-255 。大概 42 亿， 30 亿都在北美，亚洲 4 亿。 2011 年初已经用尽 。 以 点分十进制表示，如 192.168.0.1 IPV6： 128 位（16 个字节）写成 8 个无符号整数，每个整数用四个十六进制位表示，数之间 用冒号（:）分开，如： 3ffe:3201:1401:1280:c8ff:fe4d:db39:1984 在Java中使用 InetAddress 类代表IP InetAddress inet1 = InetAddress.getByName(&quot;192.168.10.14&quot;); InetAddress inet2 = InetAddress.getByName(&quot;www.atguigu.com&quot;); //获取本地ip InetAddress inet4 = InetAddress.getLocalHost(); // 两个常用方法 1. getHostName() 2. getHostAddress() 本地回环地址 (hostAddress) 127.0.0.1；主机名 (hostName): localhost 端口号端口号标识正在计算机上运行的进程（程序） 不同的进程有不同的端口号 被规定为一个 16 位的整数 0~65535 。 端口分类： 公认端口： 0~1023 。被预先定义的服务通信占用，如： HTTP 占用端口80；FTP 占用端口 21；Telnet 占用端口 23 注册端口： 1024~49151 。分配给用户进程或应用程序。（如： Tomcat 占用端口 8080 ；MySQL 占用 端口3306； Oracle 占用端口 1521 等） 。 动态私有端口： 49152~65535 。 端口号与 IP 地址的组合得出一个网络套接字： Socket 。 网络通信协议计算机网络中实现通信必须有一些约定，即通信协议， 对速率、传输代码、代码结构、传输控制步骤、出错控制等制定标准 。 传输层协议中有两个非常重要的协议： 传输控制协议 TCP(Transmission Control Protocol) 用户数据报协议 UDP(User Datagram Protocol) 。 TCP/IP 以其两个主要协议：传输控制协议 (TCP) 和网络互联协议 (IP) 而得名，实际上是一组协议，包括多个具有不同功能且互为关联的协议。 IP(Internet Protocol) 协议是网络层的主要协议，支持网间互连的数据通信。 TCP/IP 协议模型从更实用的角度出发，形成了高效的四层体系结构，即物理链路层、 IP 层、传输层和应用层 。 TCP 协议： 使用 TCP 协议前，须先建立 TCP 连接，形成传输数据通道 传输前，采用”三次握手”方式 ，点对点通信，是可靠的 TCP 协议进行通信的两个应用进程：客户端、 服务端。 在连接中可进行大数据量的传输 传输完毕，需释放已建立的连接，效率低 UDP 协议： 将数据、源、目的封装成数据包， 不需要建立连接 每个数据报的大小限制在 64K 内 发送不管对方是否准备好，接收方收到也不确认， 故是不可靠的 可以广播发送 发送数据结束时，无需释放资源 ，开销小，速度快 Socket网络上具有唯一标识的 IP 地址和端口号组合在一起才能构成唯一能识别的标识符套接字 。 通信 的两端都要有 Socket ，是两台机器间通信的端点 。 网络通信其实就是 Socket 间的通信。 Socket 允许程序把网络连接当成一个流， 数据在两个 Socket 间通过 IO 传输。 一般主动发起通信的应用程序属 客户端 ，等待通信 请求的为 服务 端。 Socket 分类： 流套接字（ stream socket ）：使用 TCP 提供可依赖的字节流服务 数据报套接字（ datagram socket ）：使用 UDP 提供“尽力而为”的数据报服务 // Socket 类的常用构造器 public Socket(InetAddress address,int port); //创建一个流套接字并将其连接到指定 IP 地址的指定端口号 。 public Socket(String host,int port); // 创建一个流套接字并将其连接到指定主机上的指定端口号 。 // Socket 类的常用方法： public InputStream getInputStream(); // 返回此套接字的输入流。 可以用于接收网络消息 public OutputStream getOutputStream(); // 返回此套接字的输出流。可以用于发送网络消息 public InetAddress getInetAddress(); // 此套接字连接到的 远程IP 地址；如果套接字是未连接的，则返回 null。 public InetAddress getLocalAddress(); // 获取套接字绑定的本地地址。即 本端的IP 地址 public int getPort(); // 此套接字连接到的远程端口号；如果尚未连接套接字，则返回 0 。 public int getLocalPort(); // 返回此套接字绑定到的本地端口。如果尚未绑定套接字，则返回 1。 即本端的端口号 。 public void close(); // 关闭此套接 。套接字被关闭后，便不可在以后的网络连接中使用，即无法重新连接或重新绑定。 需要创建新的套接字对象。 关闭此套接字也将会关闭该套接字的 InputStream 和 OutputStream。 public void shutdownInput(); // 如果在套接字上调用 shutdownInput() 后从套接字输入流读取内容，则流将返回 EOF 文件结束符。即不能在从此套接字的输入流中接收任何数据 。 public void shutdownOutput(); // 禁用此套接字的输出流。 对于 TCP 套接字，任何以前写入的数据都将被发送，并且后跟 TCP 的正常连接终止序列。 如果在套接字上调用 shutdownOutput() 后写入套接字输出流则该流将抛出 IOException。 即不能通过此套接字的输出流发送任何数据 。 TCP网络编程客户端 Socket 的工作过程包含以下四个基本的步骤 创建 Socket 根据指定服务端的 IP 地址或端口号构造 Socket 类对象。若服务器端响应，则建立客户端到服务器的通信线路。若连接失败，会出现异常。 打开连接到 Socket 的输入输出流： 使用 getInputStream 方法获得输入流，使用getOutputStream 方法获得输出流，进行数据传输 按照一定的协议对 Socket 进行读写操作： 通过输入流读取服务器放入线路的信息（但不能读取自己放入线路的信息，通过输出流将信息写入线程。 关闭 Socket 断开客户端到服务器的连接，释放线路 客户端创建Socket 对象 客户端程序可以使用 Socket 类创建对象，创建的同时会自动向服务器方发起连接 。 Socket 的构造器是 Socket(String host,int port) throws UnknownHostException,IOException: 向服务器(域名是host，端口号为 port) 发起 TCP 连接，若成功，则创建 Socket 对象，否则抛出异常。 Socket(InetAddress address,int port) throws IOException: 根据 InetAddress 对象所表示的IP 地址以及端口号 port 发起连接。 客户端建立 socketAtClient 对象的过程就是向服务器发出套接字连接请求 服务器 程序的工作过程包含以下四个基本的步骤： 调用 ServerSocket (int port): 创建 一个服务器端套接字，并绑定到指定端口上 。用于监听客户端的请求。 调用 accept(): 监听连接请求，如果客户端请求连接，则接受连接，返回通信套接字对象 。 调用 该 Socket 类对象的 getOutputStream () 和 getInputStream 获取 输出流和输入流，开始网络数据的发送和接收。 关闭 ServerSocket 和 Socket 对象： 客户端访问结束，关闭通信套接字。 服务器建立ServerSocket 对象 ServerSocket 对象负责等待客户端请求建立套接字连接，类似邮局某个窗口中的业务员。也就是说， 服务器必须事先建立一个等待客户请求建立套接字连接的 ServerSocket 对象。 所谓“接收”客户的套接字请求，就是 accept() 方法会返回一个 Socket 对象 import org.junit.Test; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.InputStream; import java.io.OutputStream; import java.net.InetAddress; import java.net.ServerSocket; import java.net.Socket; /** * 实现TCP的网络编程 * 例子1：客户端发送信息给服务端，服务端将数据显示在控制台上 */ public class TCPTest1 { //客户端 @Test public void client() { Socket socket = null; OutputStream os = null; try { //1.创建Socket对象，指明服务器端的ip和端口号 InetAddress inet = InetAddress.getByName(&quot;192.168.14.100&quot;); socket = new Socket(inet,8899); //2.获取一个输出流，用于输出数据 os = socket.getOutputStream(); //3.写出数据的操作 os.write(&quot;你好，我是客户端mm&quot;.getBytes()); } catch (IOException e) { e.printStackTrace(); } finally { //4.资源的关闭 if(os != null){ try { os.close(); } catch (IOException e) { e.printStackTrace(); } } if(socket != null){ try { socket.close(); } catch (IOException e) { e.printStackTrace(); } } } } //服务端 @Test public void server() { ServerSocket ss = null; Socket socket = null; InputStream is = null; ByteArrayOutputStream baos = null; try { //1.创建服务器端的ServerSocket，指明自己的端口号 ss = new ServerSocket(8899); //2.调用accept()表示接收来自于客户端的socket socket = ss.accept(); //3.获取输入流 is = socket.getInputStream(); //不建议这样写，可能会有乱码 // byte[] buffer = new byte[1024]; // int len; // while((len = is.read(buffer)) != -1){ // String str = new String(buffer,0,len); // System.out.print(str); // } //4.读取输入流中的数据 ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte[] buffer = new byte[5]; int len; while((len = is.read(buffer)) != -1){ baos.write(buffer,0,len); } System.out.println(baos.toString()); System.out.println(&quot;收到了来自于：&quot; + socket.getInetAddress().getHostAddress() + &quot;的数据&quot;); } catch (IOException e) { e.printStackTrace(); } finally { if(baos != null){ //5.关闭资源 try { baos.close(); } catch (IOException e) { e.printStackTrace(); } } if(is != null){ try { is.close(); } catch (IOException e) { e.printStackTrace(); } } if(socket != null){ try { socket.close(); } catch (IOException e) { e.printStackTrace(); } } if(ss != null){ try { ss.close(); } catch (IOException e) { e.printStackTrace(); } } } } } import org.junit.Test; import java.io.*; import java.net.InetAddress; import java.net.ServerSocket; import java.net.Socket; /** * 实现TCP的网络编程 * 例题3：从客户端发送文件给服务端，服务端保存到本地。并返回“发送成功”给客户端。 * 并关闭相应的连接。 */ public class TCPTest3 { /* 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void client() throws IOException { //1. Socket socket = new Socket(InetAddress.getByName(&quot;127.0.0.1&quot;),9090); //2. OutputStream os = socket.getOutputStream(); //3. FileInputStream fis = new FileInputStream(new File(&quot;beauty.jpg&quot;)); //4. byte[] buffer = new byte[1024]; int len; while((len = fis.read(buffer)) != -1){ os.write(buffer,0,len); } //关闭数据的输出 socket.shutdownOutput();//关闭连接,表明传输完成 //5.接收来自于服务器端的数据，并显示到控制台上 InputStream is = socket.getInputStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte[] bufferr = new byte[20]; int len1; while((len1 = is.read(buffer)) != -1){ baos.write(buffer,0,len1); } System.out.println(baos.toString()); //6. fis.close(); os.close(); socket.close(); baos.close(); } /* 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void server() throws IOException { //1. ServerSocket ss = new ServerSocket(9090); //2. Socket socket = ss.accept(); //3. InputStream is = socket.getInputStream(); //4. FileOutputStream fos = new FileOutputStream(new File(&quot;beauty2.jpg&quot;)); //5. byte[] buffer = new byte[1024]; int len; while((len = is.read(buffer)) != -1){ fos.write(buffer,0,len); } System.out.println(&quot;图片传输完成&quot;); //6.服务器端给予客户端反馈 OutputStream os = socket.getOutputStream(); os.write(&quot;你好，美女，照片我已收到，非常漂亮！&quot;.getBytes()); //7. fos.close(); is.close(); socket.close(); ss.close(); os.close(); } } UDP网络编程 类 DatagramSocket 和 DatagramPacket 实现了基于 UDP 协议网络程序。 UDP 数据报通过数据报套接字 DatagramSocket 发送和接收， 系统 不 保证UDP 数据报一定能够安全送到目的地，也不能确定什么时候可以抵达。 DatagramPacket 对象封装了 UDP 数据报，在数据报中包含了发送端的 IP地址和端口号以及接收端的 IP 地址和端口号。 UDP 协议中 每个数据报都给出了完整的地址信息，因此无须建立发送方和接收方 的 连接。 如同发快递包裹一样 。 public DatagramSocket(int port) //创建数据报套接字并将其绑定到本地主机上的指定端口 。 套接字将被绑定到通配符地址,IP 地址由内核来选择 。 public DatagramSocket(int port,InetAddress laddr) //创建数据报套接字, 将其绑定到指定的本地地址。本地端口必须在 0 到 65535 之间(包括两者)。 如果 IP 地址为 0.0.0.0 套接字将被绑定到通配符地址 IP 地址由内核选择 。 public void close() //关闭此数据报套接字 。 public void send(DatagramPacket p) //从此套接字发送数据报包。 DatagramPacket 包含的信息指示：将要发送的数据、 其长度、 远程主机的 IP 地址和远程主机的端口号 。 public void receive(DatagramPacket p) // 从此套接字接收数据报包。 当此方法返回时 DatagramPacket的缓冲区填充了接收的数据。 数据报包也包含发送方的 IP 地址和发送方机器上的端口号。 此方法在接收到数据报前一直阻塞。 数据报包对象的 length 字段包含所接收信息的长度。 如果信息比包的长度长 该信息将被截短。 public InetAddress getLocalAddress() //获取套接字绑定的本地地址 。 public int getLocalPort() //返回此套接字绑定的本地主机上的端口号 。 public InetAddress getInetAddress() //返回此套接字连接的地址。 如果套接字未连接 则返回 null 。 public int getPort() // 返回此套接字的端口。 如果套接字未连接,则返回 1 。 public DatagramPacket(byte[] buf,int length) // 构造 DatagramPacket 用来接收长度为 length 的数据包。 length 参数必须小于等于 buf.length 。 public DatagramPacket(byte[] buf,int length,InetAddress address,int port) //构造数据报包,用来将长度为 length 的包发送到指定主机上的指定端口号 。 length参数必须小于等于 buf.length 。 public InetAddress getAddress() // 返回某台机器的 IP 地址 此数据报将要发往该机器或者是从该机器接收到的 。 public int getPort() // 返回某台远程主机的端口号 此数据报将要发往该主机或者是从该主机接收到的 。 public byte[] getData() // 返回数据缓冲区 。 接收到的或将要发送的数据从缓冲区中的偏移量 offset 处开始 持续 length 长度 。 public int getLength() // 返回将要发送或接收到的数据的长度。 import org.junit.Test; import java.io.IOException; import java.net.DatagramPacket; import java.net.DatagramSocket; import java.net.InetAddress; /** * UDPd协议的网络编程 */ public class UDPTest { //发送端 @Test public void sender() throws IOException { DatagramSocket socket = new DatagramSocket(); String str = &quot;我是UDP方式发送的导弹&quot;; byte[] data = str.getBytes(); InetAddress inet = InetAddress.getLocalHost(); DatagramPacket packet = new DatagramPacket(data,0,data.length,inet,9090); socket.send(packet); socket.close(); } //接收端 @Test public void receiver() throws IOException { DatagramSocket socket = new DatagramSocket(9090); byte[] buffer = new byte[100]; DatagramPacket packet = new DatagramPacket(buffer,0,buffer.length); socket.receive(packet); System.out.println(new String(packet.getData(),0,packet.getLength())); socket.close(); } } URL网络编程URI、 URL 和 URN 的区别 URI 是 uniform resource identifier，==统一资源标识符==，用来唯一的标识一个资源。而 URL 是 uniform resource locator，==统一资源定位符== ，它是一种具体的URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。 URN 是 uniform resource name，==统一资源命名==，是通过名字来标识资源，比如 mailto:java net@java.sun.com 。 也就是说， URI 是以一种抽象的，高层次概念定义统一资源标识，而 URL 和 URN 则是具体的资源标识的方式。 URL和 URN 都是一种 URI。在Java 的 URI 中，一个 URI 实例可以代表绝对的，也可以是相对的，只要它符合 URI 的语法规则。而 URL 类则不仅符合语义，还包含了定位该资源的信息，因此它不能是相对的。 URL(Uniform Resource Locator) Locator)：统一资源定位符，它表示 Internet 上某一资源的地址 URL 的基本结构由 5 部分组成： &lt;传输协议&gt;://&lt;主机名&gt;:&lt;端口号&gt;/&lt;文件名&gt;#片段名?参数列表 例如 :http://192.168.1.100:8080/helloworld/index.jsp#a?username=shkstart&amp;password=123 片段名：即锚点，例如看小说，直接定位到章节 参数列表格式：参数名 参数值 参数名 参数值 为了表示 URL java.net 中实现了类 URL 。我们可以通过下面的构造器来初始化一个 URL 对象： public URL (String spec); //通过一个表示 URL 地址的字符串可以构造一个 URL 对象。例如 URL url = new URL(http://www. atguigu.com/&quot;); public URL(URL context, String spec); //通过基 URL 和相对 URL 构造一个 URL 对象 。例如 URL downloadUrl = new (url , &quot;download.html&quot;); public URL(String protocol, String host, String file); //例如 new URL(&quot;http&quot;,www.atguigu.com &quot;, “download. html&quot;); public URL(String protocol, String host, int port, String file); //例如 : URL gamelan = newURL (&quot;http&quot;, www.atguigu.com &quot;, &quot;80&quot;, “download.html&quot;); URL 类的构造器都声明抛出非运行时异常，必须要对这一异常进行处理，通常是用 try-catch 语句进行捕获。 URL 类常用方法： public String getProtocol(); // 获取该 URL 的协议名 public String getHost(); // 获取该 URL 的主机名 public String getPort(); // 获取该 URL 的端口号 public String getPath(); // 获取该 URL 的文件路径 public String getFile(); // 获取该 URL 的文件名 public String getQuery(); // 获取该 URL 的查询名 import java.io.FileOutputStream; import java.io.IOException; import java.io.InputStream; import java.net.HttpURLConnection; import java.net.URL; public class URLTest1 { public static void main(String[] args) { HttpURLConnection urlConnection = null; InputStream is = null; FileOutputStream fos = null; try { URL url = new URL(&quot;http://localhost:8080/examples/beauty.jpg&quot;); urlConnection = (HttpURLConnection) url.openConnection(); urlConnection.connect(); is = urlConnection.getInputStream(); fos = new FileOutputStream(&quot;day10\\\\beauty3.jpg&quot;); byte[] buffer = new byte[1024]; int len; while((len = is.read(buffer)) != -1){ fos.write(buffer,0,len); } System.out.println(&quot;下载完成&quot;); } catch (IOException e) { e.printStackTrace(); } finally { //关闭资源 if(is != null){ try { is.close(); } catch (IOException e) { e.printStackTrace(); } } if(fos != null){ try { fos.close(); } catch (IOException e) { e.printStackTrace(); } } if(urlConnection != null){ urlConnection.disconnect(); } } } }","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"IO流","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java13-IO流.html","text":"File类的使用java.io.File 类： 文件和文件目录路径的抽象表示形式 ，与平台无关。File的一个对象，代表一个文件或文件目录。 File 能新建、删除、重命名文件和目录，但 File ==不能访问文件内容==本身。如果需要访问文件内容本身，则需要使用输入输出流 。 想要在 Java 程序中表示一个真实存在的文件或目录，那么必须有一个 File 对象 ，但是 Java 程序中的一个 File 对象，可能没有一个真实存在的文件或目录 。 File 对象可以作为参数传递给流的构造器。 // 创建File实例 public File(String pathname);// 以pathname 为路径创建 File 对象，可以是 绝对路径或者相对路径 ，如果pathname 是相对路径，则默认的当前路径在系统属性 user.dir 中存储。 // 方式1 public static final String separator; // File 类提供了一个常量，根据操作系统，动态的提供分隔符。 File file2 = new File(File(&quot; d:&quot;+ File.separator + &quot; atguigu&quot;+ separator + &quot;info.txt&quot;); // 方式2 public File(String parent,String child); //以parent 为父路径， child 为子路径创建 File 对象 。 // 方式3 public File(File parent,String child);//根据一个父File 对象和子文件路径创建 File 对象 常用函数： public String getAbsolutePath(); // 获取绝对路径 public String getPath(); // 获取传入的路径 public String getName(); // 获取文件名称 public String getParent(); // 获取上层文件目录路径。 若无，返回null public long length(); // 获取文件长度,即：字节数。 不能获取目录的长度。 public long lastModified(); // 获取最后一次的修改时间，毫秒值 public String[] list(); // 获取指定目录下的所有文件或者文件目录的名称数组，仅包含当前文件夹下的文件夹和文件 public File[] listFiles(); // 获取指定目录下的所有文件或者文件目录的File数组，仅包含当前文件夹下的文件夹和文件 public boolean renameTo(File dest); // 把文件重命名为指定的文件路径 // file1.renameTo(file2); 需要保证file1存在，file2不存在。若为True，move(非copy)的同时rename public boolean isDirectory(); // 判断是否是文件目录 public boolean isFile(); // 判断是否是文件 public boolean exists(); // 判断是否存在 public boolean canRead(); // 判断是否可读 public boolean canWrite(); // 判断是否可写 public boolean isHidden(); // 判断是否隐藏 /* 文件确实存在硬盘中存在，创建File对象时，各个属性会显式赋值；文件不存在时，除了指定的目录和路劲之外，其余属性取成员变量的默认值。 */ public boolean createNewFile(); // 创建文件。 若文件存在，则不创建，返回false public boolean mkdir(); // 创建文件目录 。 如果此文件目录存在，就不创建了。 public boolean mkdirs(); // 创建文件目录 。 如果上层文件目录不存在一并创建 // 注意事项：如果你创建文件或者文件目录没有写盘符路径 那么默认在项目路径下。 public boolean delete(); // 删除文件或者文件夹，删除文件夹只能删空文件夹！ // 删除注意事项：Java中的删除不走回收站。要删除一个文件目录请注意该文件目录内不能包含文件或者文件目录 IO 流原理及流的分类java.io 包下提供了各种“流”类和接口，用以获取不同种类的数据，并通过 标准的方法 输入或输出数据。 按操作数据单位不同分为： 字节流 (8 bit)，字符流 (16); 按数据流的流向不同分为： 输入流，输出流 按流的角色的不同分为： 节点流(直接从数据源或目的地读写数据)；处理流(不直接连接到数据源或目的地，而是“连接” 在已存在的流（节点流或处理流）之上，通过对数据的处理为程序提供更为强大的读写功能。) (抽象基类) 字节流 字符流 输入流 InputStream Reader 输出流 outputStram Writer Java 的 IO 流共涉及 40 多个类，实际上非常规则，都是从如下 4 个抽象基类派生的。 由这四个类派生出来的子类名称都是以其父类名作为子类名后缀。 FileOutputStream 从文件系统中的某个文件中获得输出字节 。 FileOutputStream用于 写出 非文本 数据之类的原始字节流。 要写出 字符流 需要使用 FileWriter InputStream int read(); // 从输入流中读取数据的下一个字节。返回 0 到 255 范围内的 int 字节值 。 如果因为已经到达流末尾而没有可用的字节 则返回值 1 int read(byte[] b); // 从此输入流中将最多b.length 个字节的数据读入一个 byte 数组中 。 如果因为已经到达流末尾而没有可用的字节, 则返回值 1。 否则以整数形式返回实际读取的字节数 。 int read(byte[] b, int off,int len); // 将输入流中最多len 个数据字节读入 byte 数组 。 尝试读取 len 个字节, 但读取的字节也可能小于该值。 以整数形式返回实际读取的字节数。 如果因为流位于文件末尾而没有可用的字节, 则返回值 1 。 public void close() throws IOException; // 关闭此输入流并释放与该流关联的所有系统资源 。 Reader int read(); // 读取单个字符。作为整数读取的字符, 范围在 0 到 65535 之间 0 x 00-0 xffff) 2 个字节的 Unicode 码, 如果已到达流的末尾, 则返回 1 int read(char[] cbuf); // 将字符读入数组。如果已到达流的末尾, 则返回 1 。 否则返回本次读取的字符数。 int read(char[] cbuf,int off,int len); // 将字符读入数组的某一部分。 存到数组 cbuf 中, 从 off 处开始存储, 最多读 len 个符 。 如果已到达流的末尾, 则返回 1。 否则返回本次读取的字符数。 public void close() throws IOException; // 关闭此输入流并释放与该流关联的所有系统资源 。 OutputStream void write(int b); // 将指定的字节写入此输出流。 write 的常规协定是：向输出流写入一个字节。 要写入的字节是参数 b 的八个低位 。 b 的 24 个高位将被忽略。 即写入 0-255 范围 的。 void write(byte[] b); // 将b length 个字节从指定的 byte 数组写入此输出流 。 write(b) 的常规协定是：应该与调用 write(b, 0, b.length) 的效果完全相同。 void write(byte[] b,int off,int len); // 将指定byte数组中从偏移量 off 开始的 len 个字节写入此输出流 。 public void flush()throws IOException; // 刷新此输出流并强制写出所有缓冲的输出字节 调用此方法指示应将这些字节立即写入它们预期的目标。 public void close() throws IOException; // 关闭此输出流并释放与该流关联的所有系统资源。 Writer void write(int c);// 写入单个字符。 要写入的字符包含在给定整数值的 16 个低位中, 16 高位被忽略。 即写入 0 到 65535 之间的 Unicode 码 。 void write(char[] cbuf); // 写入字符数组。 void write(char[] cbuf,int off,int len); // 写入字符数组的某一部分。 从 off 开始写入 len 个字符 void write(String str); // 写入字符串。 void write(String str,int off,int len); // 写入字符串的某一部分。 void flush(); // 刷新该流的缓冲则立即将它们写入预期目标 。 public void close() throws IOException; // 关闭此输出流并释放与该流关联的所有系统资源。 节点流或文件流FileReader read()：返回读入的一个字符，如果达到文件末尾，返回-1 read(char[] b) 每次读取指定个数的字符，返回读取的字符数，如果达到文件末尾，返回-1。存在易错点： FileReader fr = new FileReader(new File(&quot;Test.txt&quot;)); char[] cbuf = new char[5]; int len; while((len = fr.read(cbuf)) != -1){ //方式一： //错误的写法 //for(int i = 0;i &lt; cbuf.length;i++){ //System.out.print(cbuf[i]); //} //正确的写法 for(int i = 0; i&lt;len; i++){ System.out.print(cbuf[i]); } //方式二： //错误的写法,对应着方式一的错误的写法 //String str = new String(cbuf); //System.out.print(str); //正确的写法 String str = new String(cbuf,0,len); System.out.print(str); 资源关闭异常的处理：为保证资源一定可以执行关闭操作，需要使用try-catch-finally处理 读入的文件一定要存在，否则会报FileNotFoundException FileWriter FileWriter fw = new FileWriter(new File(&quot;Test.txt&quot;)); /*File对应的硬盘中的文件如果不存在，在输出的过程中，会自动创建此文件。 File对应的硬盘中的文件如果存在： 如果流使用的构造器是：FileWriter(file,false) 或 FileWriter(file):对原有文件的覆盖 如果流使用的构造器是：FileWriter(file,true):不会对原有文件覆盖，而是在原有文件基础上追加内容*/ 在写入一个文件时 ，如果使用构造器 FileOutputStream(file)，则目录下有同名文件将被覆盖。 如果使用构造器 FileOutputStream(file,true)，则目录下的同名文件不会被覆盖在文件内容末尾追加内容。 在读取文件时，必须保证该文件已存在 ，否则报异常 。 字节流操作字节，比如：.mp3 ，.avi ，.rmvb，.mp4 ，.jpg，.doc ，.ppt 字符流操作字符，只能操作普通文本文件 。 最常见的文本文件：.txt ，.java ，.c ，.cpp 等语言的源代码。尤其注意 .doc, excel, ppt 这些不是文本文件。 @Test public void testFileReaderFileWriter() { FileReader fr = null; FileWriter fw = null; try { //1.创建File类的对象，指明读入和写出的文件 File srcFile = new File(&quot;hello.txt&quot;); File destFile = new File(&quot;hello2.txt&quot;); //不能使用字符流来处理图片等字节数据 // File srcFile = new File(&quot;爱情与友情.jpg&quot;); // File destFile = new File(&quot;爱情与友情1.jpg&quot;); //2.创建输入流和输出流的对象 fr = new FileReader(srcFile); fw = new FileWriter(destFile); //3.数据的读入和写出操作 char[] cbuf = new char[5]; int len;//记录每次读入到cbuf数组中的字符的个数 while((len = fr.read(cbuf)) != -1){ //每次写出len个字符 fw.write(cbuf,0,len); } } catch (IOException e) { e.printStackTrace(); } finally { //4.关闭流资源 //方式一： // try { // if(fw != null) // fw.close(); // } catch (IOException e) { // e.printStackTrace(); // }finally{ // try { // if(fr != null) // fr.close(); // } catch (IOException e) { // e.printStackTrace(); // } // } //方式二： try { if(fw != null) fw.close(); } catch (IOException e) { e.printStackTrace(); } try { if(fr != null) fr.close(); } catch (IOException e) { e.printStackTrace(); } } } 缓冲流为了提高数据读写的速度 Java API 提供了带缓冲功能的流类，在使用这些流类时，会创建一个内部缓冲区数组，缺省使用 8192 个 字节 (8Kb) 的缓冲区 。 当读取数据时，数据按块读入缓冲区，其后的读操作则直接访问缓冲区。当使用 BufferedInputStream 读取字节文件时 BufferedInputStream 会一次性从文件中读取 8192 个 (8Kb)，存在缓冲区中直到缓冲区装满了， 才重新从文件中读取下一个 8192 个字节数组 。向流中写入字节时不会直接写到文件，先写到缓冲区中，直到缓冲区写满BufferedOutputStream 才会把缓冲区中的数据一次性写到文件里 。==使用方法flush() 可以强制将缓冲区的内容全部写入输出流==。如果是带缓冲区的流对象的 close() 方法，不但会关闭，流 还会在关闭流之前刷新缓冲区，关闭后不能再写出。 缓冲 流要“套接”在相应的节点流之上，根据数据操作单位可以把缓冲流分为： BufferedInputStream 和 BufferedOutputStream BufferedReader 和 BufferedWriter //造流 //1 造节点流 FileInputStream fis = new FileInputStream((srcFile)); FileOutputStream fos = new FileOutputStream(destFile); //2 造缓冲流 bis = new BufferedInputStream(fis); bos = new BufferedOutputStream(fos); //资源关闭 //要求：先关闭外层的流，再关闭内层的流 //说明：关闭外层流的同时，内层流也会自动的进行关闭。关于内层流的关闭，我们可以省略. //创建文件和相应的流 br = new BufferedReader(new FileReader(new File(&quot;dbcp.txt&quot;))); bw = new BufferedWriter(new FileWriter(new File(&quot;dbcp1.txt&quot;))); //读写操作 //方式一：使用char[]数组 //char[] cbuf = new char[1024]; //int len; //while((len = br.read(cbuf)) != -1){ //bw.write(cbuf,0,len); // //bw.flush(); //} //方式二：使用String String data; while((data = br.readLine()) != null){ //方法一： //bw.write(data + &quot;\\n&quot;);//data中不包含换行符 //方法二： bw.write(data);//data中不包含换行符 bw.newLine();//提供换行的操作 } 转换流转换流提供了在字节流和字符流之间的转换 Java API 提供了两个转换流： InputStreamReader ：将 InputStream 转换为 Reader OutputStreamWriter ：将 Writer 转换为 OutputStream 字节流中的数据都是字符时，转成字符流操作更高效 。很多时候我们使用转换流来处理文件乱码问题。实现编码和解码的功能。 InputStreamReader 构造器：需要 和 InputStream “套接”。 public InputStreamReader (InputStream in) public InputSreamReader (InputStream in, String charsetName) OutputStreamWriter: 构造器：需要 和 OutputStream “套接”。 public OutputStreamWriter (OutputStream out) public OutputSreamWriter (OutputStream out,String charsetName) 常见的编码表 ASCII：美国标准信息交换码。用一个字节的 7 位可以表示。 ISO8859-1：拉丁码表。欧洲码表，用一个字节的 8 位表示。 GB2312：中国的中文编码表 。最多两个字节编码所有字符 GBK：中国的中文编码表升级，融合了更多的中文文字符号。最多两个字节编码 Unicode：国际标准码，融合了目前人类使用的所有字符。为每个字符分配唯一的字符码。所有的文字都用两个字节来表示。 UTF-8：变长的编码方式，可用 1-4个字节 来表示一个字符。 GBK 等双字节编码方式，用最高位是 1 或 0 表示两个字节和一个字节 。 标准输入 、 输出流 System.in 和 System.out 分别代表了系统标准的输入和输出设备 默认输入设备：键盘；输出设备：显示器 System.in 的类型是 InputStream System.out 的类型是 PrintStream ，其是 OutputStream 的子类，FilterOutputStream 的子类 重定向：通过 System 类的 setIn，setOut 方法对默认设备进行改变，重新指定输入输出流 public static void setIn (InputStream in) public static void setOut (PrintStream out) System.out.println(&quot;请输入信息,退出输入 e 或 exit):&quot;); //把标准输入流(键盘输入)这个字节流包装成字符流,再包装成缓冲流 BufferedReader br = new BufferedReader( new InputStreamReader(System.in)); String s = null; try{ while ((s = br.readLine()) != null ) { // 读取用户输入的一行数据 ----&gt; 阻塞程序 if(&quot; e&quot;.equalsIngoreCase(s) ) || &quot; exit&quot;.equalsIngoreCase(s)){ System.out.println(&quot;安全退出&quot;); break; } //将读取到的整行字符串转成大写输出 System.out.println(&quot;----&gt;:&quot;+s.toUpperCase()); System.out.println(&quot;继续输入信息&quot;); } } catch (IOException e ) e.printStackTrace(); } finally{ try{ if (br != null ){ br.close (); // 关闭过滤流时 会自动关闭它包装的底层节点流 } }catch (IOException e){ e.printStackTrace(); } } 打印流实现将基本数据类型的数据格式转化为字符串输出。 打印流： PrintStream 和 PrintWriter 提供了一系列重载 的 print() 和 println() 方法 ，用于多种数据类型的输出 PrintStream 和 PrintWriter 的输出不会抛出 IOException 异常 PrintStream 和 PrintWriter 有自动 flush 功能 PrintStream 打印的所有字符都使用平台的默认字符编码转换为字节。在需要写入字符而不是写入字节的情况下，应该使用 PrintWriter 类 。 System.out 返回的是 PrintStream 的实例 @Test public void test2() { PrintStream ps = null; try { FileOutputStream fos = new FileOutputStream(new File(&quot;D:\\\\IO\\\\text.txt&quot;)); // 创建打印输出流,设置为自动刷新模式(写入换行符或字节 &#39;\\n&#39; 时都会刷新输出缓冲区) ps = new PrintStream(fos, true); if (ps != null) {// 把标准输出流(控制台输出)改成输出到文件 System.setOut(ps); } for (int i = 0; i &lt;= 255; i++) { // 输出ASCII字符 System.out.print((char) i); if (i % 50 == 0) { // 每50个数据一行 System.out.println(); // 换行 } } } catch (FileNotFoundException e) { e.printStackTrace(); } finally { if (ps != null) { ps.close(); } } } 数据流为了方便地操作 Java 语言的基本数据类型和 String 的数据，可以使用数据流。 数据流有两个类： DataInputStream 和 DataOutputStream 分别“套接”在 InputStream 和 OutputStream 子类的流 上 DataInputStream 中的方法 boolean readBoolean byte readByte char readChar float readFloat double readDouble short readShort long readLong int readInt String readUTF () void readFully (byte[] b) DataOutputStream 中的方法：将上述的方法的 read 改为相应的 write 即可。 读取数据的顺序需要和写入==顺序一致==！ 对象流ObjectInputStream 和 OjbectOutputSteam：用于存储和读取基本数据类型数据或对象的处理流。它的强大之处就是可以把 Java 中的对象写入到数据源中，也能把对象从数据源中还原回来。 序列化： 用 ObjectOutputStream 类 保存 基本类型数据或对象的机制 反序列化： 用 ObjectInputStream 类 读取 基本类型数据或对象的机制 ObjectOutputStream 和 ObjectInputStream 不能序列化 ==static== 和 ==transient== 修饰的成员变量 除了当前类需要实现Serializable接口之外，还必须保证其内部==所有属性==也必须是可序列化的。（默认情况下，==基本数据类型可序列化==） 对象序列化机制允许把内存中的 Java 对象转换成平台无关的二进制流，从而允许把这种二进制流持久地保存在磁盘上，或通过网络将这种二进制流传输到另一个网络 节点 。 当 其它程序获取了这种二进制流，就可以恢复成原来的 Java 对象 序列化的好处在于可将任何实现了 Serializable 接口的对象转化为字节数据，使其在保存和传输时可被还原 序列化是 RMI （Remote Method Invoke 远程方法调用）过程的参数和返回值都必须实现的机制，而 RMI 是 JavaEE 的基础。因此序列化机制是JavaEE 平台的基础 如果需要让某个对象支持序列化机制，则必须 让对象所属的类及其属性是可序列化的，为了让 某个类是可序列化的，该类 必须实现如下两个接口 之一。否则，会抛出 NotSerializableException 异常 Serializable Externalizable 凡是实现 Serializable 接口的类都有一个表示序列化版本标识符的静态变量： private static final long serialVersionUID; serialVersionUID 用来表明类的不同版本间的兼容性。 简言之，其目的是以序列化对象进行版本控制，有关各版本反序列化时是否兼容。 如果类没有显示定义这个静态常量 ，它的值是 Java 运行时环境根据类的内部细节自动生成的。 若类的实例变量做了修改 serialVersionUID 可能发生变化。 故建议，显式声明。 简单来说， Java 的序列化机制是通过在运行时判断类的 serialVersionUID 来验证版本一致性的。在进行反序列化时， JVM 会把传来的字节流中的serialVersionUID 与本地相应 实体类的 serialVersionUID 进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常。 (InvalidCastException） 谈谈你对java.io.Serializable 接口的理解，我们知道它用于序列化，是空方法接口，还有其它认识吗？ 实现了 Serializable 接口的对象，可将它们转换成一系列字节，并可在以后完全恢复回原来的样子。 这一过程亦可通过网络进行。这意味着序列化机制能自动补偿操作系统间的差异。 换句话说，可以先在 Windows 机器上创建一个对象，对其序列化，然后通过网络发给一台 Unix 机器，然后在那里准确无误地重新“装配”。不必关心数据在不同机器上如何表示，也不必关心字节的顺序或者其他任何细节。 由于大部分作为参数的类如 String 、 Integer 等都实现了java.io.Serializable 的接口，也可以利用多态的性质，作为参数使接口更灵活。 随机存取文件流 RandomAccessFile 声明在 java.io 包下，但直接继承于 java.lang.Object 类。 并且它实现了 DataInput 、 DataOutput 这两个接口，也就意味着这个类既可以读也可以写。 RandomAccessFile 类支持 “随机访问” 的方式，程序可以直接跳到文件的任意地方来读、写文件 支持只访问文件的部分内容 可以向已存在的文件后==追加==内容 RandomAccessFile 对象包含一个记录指针，用以标示当前读写处的位置。RandomAccessFile 类对象可以自由移动记录指针： long getFilePointer 获取文件记录指针的当前位置 void seek(long pos) 将文件记录指针定位到 pos 位置 构造器 public RandomAccessFile(File file , String mode) public RandomAccessFile(String name, String mode) 创建 RandomAccessFile 类实例需要指定一个 mode 参数，该参数指定 RandomAccessFile 的访问模式： r: 以只读方式打开 rw：打开以便读取和写入；则会对原有文件内容进行覆盖。（默认情况下，从头覆盖） rwd: 打开以便读取和写入；同步文件内容的更新 rws: 打开以便读取和写入； 同步文件内容和元数据的更新 如果模式为只读 r 。则不会创建文件，而是会去读取一个已经存在的文件，如果读取的文件不存在则会出现异常。 如果模式为 rw 读写。如果文件不存在则会去创建文件，如果存在则不会创建。 我们可以用RandomAccessFile 这个类，来实现一个 多线程断点下载 的功能，用过下载工具的朋友们都知道，下载前都会建立 两个临时文件 ，一个是与被下载文件大小相同的空文件，另一个是记录文件指针的位置文件，每次暂停的时候，都会保存上一次的指针，然后断点下载的时候，会继续从上一次的地方下载，从而实现断点下载或上传的功能，有兴趣的朋友们可以自己实现下。 NIO.2 中 Path 、 Paths 、Files 类的使用Java API 中提供了两套 NIO 一套是针对标准输入输出 NIO 另一套就是网络编程 NIO 。 |----java.nio.channels.Channel |----FileChannel: 处理本地文件 |----SocketChannel：TCP 网络编程的客户端的 Channel |----ServerSocketChannel:TCP 网络编程的服务器端的 Channel |----DatagramChannel：UDP 网络编程中发送端和接收端的 Channel Path早期 的 Java 只提供了一个 File 类来访问文件系统，但 File 类的功能比较有限，所提供的方法性能也不高。而且， 大多数方法在出错时仅返回失败，并不会提供异常信息。NIO. 2 为了弥补这种不足，引入了 Path 接口，代表一个平台无关的平台路径，描述了目录结构中文件的位置。 Path 可以看成是 File 类的升级版本，实际引用的资源也可以不存在。 Path 常用方法： String toString(); 返回调用 Path 对象的字符串表示形式 boolean startsWith(String path); 判断是否以 path 路径开始 boolean endsWith(String path); 判断是否以 path 路径结束 boolean isAbsolute(); 判断是否是绝对路径 Path getParent(); 返回 Path 对象包含整个路径，不包含 Path 对象指定的文件路径 Path getRoot();返回调用 Path 对象的根路径 Path getFileName(); 返回与调用 Path 对象关联的文件名 int getNameCount(); 返回 Path 根目录后面元素的数量 Path getName(int idx); 返回指定索引位置 idx 的路径名称 Path toAbsolutePath(); 作为绝对路径返回调用 Path 对象 Path resolve(Path p); 合并两个路径，返回合并后的路径对应的 Path 对象 File toFile(); 将 Path 转化为 File 类的对象 Path与File类互换 File file = path.toFile(); Path path = file.toPath(); Paths、Files同时， NIO.2 在 java.nio.file 包下还 提供了 Files 、 Paths 工具类， Files 包含了大量静态的工具方法来操作文件； Paths 则包含了两个返回 Path 的静态工厂方法。 Paths 类提供的静态 get() 方法用来获取 Path 对象： static Path get(String first, String … more); // 用于将多个字符串串连成路径 static Path get(URI uri); // 返回指定 uri 对应的 Path 路径 java.nio.file.Files 用于操作文件或目录的工具类。Files 常用方法： Path copy(Path src, Path dest, CopyOption … how); 文件的复制 Path createDirectory(Path path, FileAttribute&lt;?&gt; … attr); 创建一个目录 Path createFile(Path path, FileAttribute&lt;?&gt; … arr); 创建一个文件 void delete(Path path); 删除一个文件 目录，如果不存在，执行报错 void deleteIfExists(Path path); Path 对应的文件目录如果存在，执行删除 Path move(Path src, Path dest, CopyOption…how); 将 src 移动到 dest 位置 long size(Path path); 返回 path 指定文件的大小 boolean exists(Path path, LinkOption … opts); 判断文件是否存在 boolean isDirectory(Path path, LinkOption … opts); 判断是否是目录 boolean isRegularFile(Path path, LinkOption … opts); 判断是否是文件 boolean isHidden(Path path); 判断是否是隐藏文件 boolean isReadable(Path path); 判断文件是否可读 boolean isWritable(Path path); 判断文件是否可写 boolean notExists(Path path, LinkOption … opts); 判断文件是否不存在 SeekableByteChannel newByteChannel(Path path, OpenOption…how); 获取与指定文件的连 接， how 指定打开方式。 DirectoryStream&lt;Path&gt; newDirectoryStream(Path path); 打开 path 指定的目录 InputStream newInputStream(Path path, OpenOption…how); 获取 InputStream 对象 OutputStream newOutputStream(Path path, OpenOption…how); 获取 OutputStream 对象","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"反射","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java15-反射.html","text":"反射概述Reflection （反射）是被视为 动态语言 的关键，反射机制允许程序在执行期借助于 Reflection API 取得任何类的内部信息，并能直接操作任意对象的内部属性及方法 。 加载完类之后 在堆内存的方法区中就产生了一个 Class 类型的对象 一个类只有一个 Class 对象 这个对象就包含了完整的类的结构信息。 我们可以通过这个对象看到类的结构 。 ==这个对象就像一面镜子透过这个镜子看到类的结构，所以我们形象的称之为： 反射==。 Java不是动态语言 但 Java 可以称之为“准动态语言”。 即 Java 有一定的动态性我们可以利用反射机制 、 字节码操作获得类似动态语言的特性 。Java的动态性让编程的时候更加灵活。 Java 反射机制提供的功能： 在运行时判断任意一个对象所属的类 在运行时构造任意一个类的对象 在运行时判断任意一个类所具有的成员变量和方法 在运行时获取泛型信息 在运行时调用任意一个对象的成员变量和方法 在运行时==处理注解== 生成动态代理 反射相关的主要API： java.lang.Class: 代表一个类 java.lang.reflect.Method: 代表类的方法 java.lang.reflect.Field: 代表类的成员变量 java.lang.reflect.Constructor: 代表类的构造器 问：反射机制与面向对象中的封装性是否矛盾？如何看待这两个技术？ 不矛盾。封装性体现：建议用户使用那些方法和属性，该暴露的暴露，不该暴露的不暴露；反射：私有的方法虽然不建议使用，但仍可通过反射调用。 理解 Class类、获取Class实例关于java.lang.Class类的理解 1.类的加载过程 程序经过javac.exe命令以后，会生成一个或多个字节码文件(.class结尾)。 接着我们使用java.exe命令对某个字节码文件进行解释运行。相当于将某个字节码文件加载到内存中。此过程就称为类的加载。加载到内存中的类，我们就称为运行时类，此运行时类，就作为Class的一个实例。 2.换句话说，Class的实例就对应着一个运行时类。 3.加载到内存中的运行时类，会==缓存一定的时间==。在此时间之内，我们可以通过不同的方式来获取此运行时类。 对象照镜子后可以得到的信息：某个类的属性、方法和构造器、某个类到底实现了哪些接口 。对于每个类而言， JRE 都为其保留一个不变的 Class 类型的对象。一 个 Class 对象包含了特定某个结构 ( class/interface/enum/annotation/primitive type/void/[] )的有关信息 。 Class 本身也是一 个类 ==Class 对象只能由系统建立对象== 一个加载的类 在 JVM 中 只会有一个 Class 实例 一 个 Class 对象对应的是一个加载到 JVM 中的一个 class 文件 每个 类的实例都会记得自己是由 哪个 Class 实例 所 生成 通过 Class 可以完整地得到一个类中的所有被加载的结构 Class 类是 Reflection 的根源，针对任何你想动态加载、运行的类，唯有先获得相应的Class 对象 Class类的常用方法: static Class forName (String name); //返回指定类名 name 的 Class 对象 Object newInstance(); //调用缺省构造函数，返回该Class 对象的一个实例 getName(); //返回此Class 对象所表示的实体（类、接口、数组类、基本类型或 void ）名称 Class getSuperClass(); //返回当前Class 对象的父类的 Class 对象 Class [] getInterfaces (); //获取当前Class 对象的接口 ClassLoader getClassLoader(); //返回该类的类加载器 Class getSuperclass (); //返回表示此Class 所表示的实体的超类的 Class Constructor[] getConstructors(); //返回一个包含某些Constructor 对象的数组 Field[] getDeclaredFields(); //返回Field 对象的一个数组 Method getMethod (String name,Class … paramTypes); //返回一个Method 对象，此对象的形参类型为 paramType 获取Class 类的实例 (四种方法)//获取Class的实例的方式（前三种方式需要掌握） @Test public void test3() throws ClassNotFoundException { //方式一：调用运行时类的属性：.class Class clazz1 = Person.class; System.out.println(clazz1); //方式二：通过运行时类的对象,调用getClass() Person p1 = new Person(); Class clazz2 = p1.getClass(); System.out.println(clazz2); //方式三：调用Class的静态方法：forName(String classPath) // 常用 Class clazz3 = Class.forName(&quot;com.atguigu.java.Person&quot;); //clazz3 = Class.forName(&quot;java.lang.String&quot;); System.out.println(clazz3); System.out.println(clazz1 == clazz2);// True System.out.println(clazz1 == clazz3);// True //方式四：使用类的加载器：ClassLoader (了解) ClassLoader classLoader = ReflectionTest.class.getClassLoader(); //ReflectionTest是当前测试方法所属的类 Class clazz4 = classLoader.loadClass(&quot;com.atguigu.java.Person&quot;); System.out.println(clazz4); System.out.println(clazz1 == clazz4);// True } //Class实例可以是哪些结构的说明： @Test public void test4(){ Class c1 = Object.class; Class c2 = Comparable.class; Class c3 = String[].class; Class c4 = int[][].class; Class c5 = ElementType.class; Class c6 = Override.class; Class c7 = int.class; Class c8 = void.class; Class c9 = Class.class; int[] a = new int[10]; int[] b = new int[100]; Class c10 = a.getClass(); Class c11 = b.getClass(); // 只要数组的元素类型与维度一样，就是同一个Class System.out.println(c10 == c11);// True } 类的加载与 ClassLoader 的理解类的加载过程 加载：将 class 文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后生成一个代表这个类的 java.lang.Class 对象，作为方法区中类数据的访问入口（即引用地址）。所有需要访问和使用类数据只能通过这个 Class 对象。个加载的过程需要类加载器参与 。 链接：将 Java 类的二进制代码合并到 JVM 的运行状态之中的过程。 验证：确保加载的类信息符合 JVM 规范，例如：以 cafe 开头，没有安全方面的问题 准备：正式为类变量（ static ）分配内存并 设置类变量默认初始值的阶段，这些内存都将在方法区中进行分配 。 解析：虚拟机常量池内的符号引用（常量名）替换为直接引用（地址）的 过程 。 初始化： 执行类构造器 ==()== 方法的过程。 ==类构造器 () 方法是由编译期自动收集类中所有类变量的赋值动作和静态代码块中的语句合并产生的==。（类构造器是构造类信息的，不是构造该类对象的构造器） 。 当初始化一个类的时候，如果发现其父类还没有进行初始化，则需要先触发其父类的 初始化 。 虚拟机会保证一个类的 () 方法在多线程环境中被正确加锁和同步 。 什么时候会发生类初始化？ 类的主动引用一定会发生类的初始化 当虚拟机启动，先初始化 main 方法所在的类 new 一个类的对象 调用类的静态成员（除了 final 常量） 和静态方法 使用 java.lang.reflect 包的方法对类进行反射调用 当初始化一个类，如果其父类没有被初始化，则先会初始化它的父类 类的被动引用（不会发生类的初始化） 当访问一个静态域时，只有真正声明这个域的类才会被初始化。当通过子类引用父类的静态变量，不会导致子类初始化 通过数组定义类引用，不会触发此类的初始化 引用常量不会触发此类的初始化，常量在链接阶段就存入调用类的常量池中了 ClassLoader 的理解 类加载器的作用： 类加载的作用： 将 class 文件字节码内容加载到内存中，并将这些静态数据==转换成方法区的运行时数据结构==，然后在堆中生成一个代表这个类的 java.lang.Class 对象作为方法区中类数据的访问入口 。 类缓存： 标准的 JavaSE 类加载器可以按要求查找类，但一旦某个类被加载到类加载器中，它将维持加载缓存一段时间 。 不过 JVM 垃圾回收机制可以回收这些 Class 对象 。 // 1 获取一个系统类加载器 ClassLoader classloader = ClassLoader.getSystemClassLoader(); System.out.println(classloader); // 2 获取系统类加载器的父类加载器，即扩展类加载器 classloader = classloader.getParent(); System.out.println(classloader); // 3 获取扩展类加载器的父类加载器，即引导类加载器 classloader = classloader.getParent(); System.out.println(classloader); // 4 测试当前类由哪个类加载器进行加载 classloader = Class.forName (&quot;exer2.getClassLoaderDemo&quot;).getClassLoader(); System.out.println(classloader); // 5 测试 JDK 提供的 Object 类由哪个类加载器加载 classloader = Class.forName(&quot;java.lang.Object&quot;).getClassLoader(); System.out.println classloader //* 6 关于类加载器的一个主要方法： getResourceAsStream(String str) 获取类路径下的指定文件的输入流 InputStream in = null; in = this.getClass().getClassLoader().getResourceAsStream(&quot;exer2\\\\test.properties&quot;); System.out.println(in); 读取配置文件Properties/* Properties：用来读取配置文件。 */ @Test public void test2() throws Exception { Properties pros = new Properties(); //此时的文件默认在当前的module下。 //读取配置文件的方式一： // FileInputStream fis = new FileInputStream(&quot;jdbc.properties&quot;); // FileInputStream fis = new FileInputStream(&quot;src\\\\jdbc1.properties&quot;); // pros.load(fis); //读取配置文件的方式二：使用ClassLoader //配置文件默认识别为：当前module的src下 ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); InputStream is = classLoader.getResourceAsStream(&quot;jdbc1.properties&quot;); pros.load(is); String user = pros.getProperty(&quot;user&quot;); String password = pros.getProperty(&quot;password&quot;); System.out.println(&quot;user = &quot; + user + &quot;,password = &quot; + password); } 创建运行时类的对象创建类的对象： 调用 Class 对象的 ==newInstance()== 方法 要求： 1 ）类必须有一个无参数的构造器。2）类的构造器的访问权限需要足够。 反射的动态性： //体会反射的动态性 @Test public void test2(){ for(int i = 0;i &lt; 100;i++){ int num = new Random().nextInt(3);//0,1,2 String classPath = &quot;&quot;; switch(num){ case 0: classPath = &quot;java.util.Date&quot;; break; case 1: classPath = &quot;java.lang.Object&quot;; break; case 2: classPath = &quot;com.atguigu.java.Person&quot;; break; } try { Object obj = getInstance(classPath); System.out.println(obj); } catch (Exception e) { e.printStackTrace(); } } } /* 创建一个指定类的对象。 classPath:指定类的全类名 */ public Object getInstance(String classPath) throws Exception { Class clazz = Class.forName(classPath); return clazz.newInstance(); } 获取运行时类的完整结构1.实现的全部接口 public Class&lt;?&gt;[] getInterfaces(); // 确定此对象(当前类，不含父类)所表示的类或接口实现的接口 2.所继承的父类 public Class&lt;? Super T&gt; getSuperclass(); // 返回表示此Class 所表示的实体（类、接口、基本类型）的父类的Class 。 Type getGenericSuperclass(); // 获取运行时类的带泛型的父类 3.全部的构造器 public Constructor&lt;T&gt;[] getConstructors(); // 返回此Class对象所表示的类的所有 public 构造方法。不含父类构造器。 public Constructor&lt;T&gt;[] getDeclaredConstructors(); // 返回此Class 对象表示的类声明的所有构造方法。不含父类构造器。 Constructor 类中： public int getModifiers(); // 取得修饰符 // 0--&gt;默认权限；1--&gt;private；2--&gt;public public String getName(); // 取得方法名称 public Class&lt;?&gt;[] getParameterTypes(); // 取得参数的类型 4.全部的方法 public Method[] getDeclaredMethods(); // 返回此Class 对象所表示的类或接口的全部方法（不包好父类方法） public Method[] getMethods(); // 返回此Class 对象所表示的类及其父类或接口的 public 的方法 Method 类中： public Class&lt;?&gt;[] getName(); // 方法名 public Class&lt;?&gt; getReturnType(); // 取得全部的返回值类型 public Class&lt;?&gt;[] getParameterTypes(); // 取得全部参数的类型 public int getModifiers(); // 取得修饰符 public Class&lt;?&gt;[] getExceptionTypes(); // 取得异常信息 public Class&lt;?&gt;[] Annotations[] getAnnotations(); //取得方法的注解，声明周期要够 5.全部的 Field public Field[] getFields(); // 返回此Class对象所表示的类及其父类或接口的 public 的Field。 public Field[] getDeclaredFields(); //返回此Class对象所表示的类或接口的全部Field。（不包含父类中的属性） Field 方法中： public int getModifiers(); // 以整数形式返回此 Field 的修饰符 public Class&lt;?&gt; getType(); // 得到 Field 的属性类型 public String getName(); // 返回 Field 的名称。 6. Annotation相关 Annotations[] getAnnotations() getDeclaredAnnotations(); 7.泛型相关 Type getGenericSuperclass(); //获取父类泛型类型 ParameterizedType // 泛型类型 getActualTypeArguments(); //获取实际的泛型类型参数数组 /* 获取运行时类的带泛型的父类的泛型 代码：逻辑性代码 vs 功能性代码 */ public void test4(){ Class clazz = Person.class; Type genericSuperclass = clazz.getGenericSuperclass(); ParameterizedType paramType = (ParameterizedType) genericSuperclass; //获取泛型类型 Type[] actualTypeArguments = paramType.getActualTypeArguments(); //System.out.println(actualTypeArguments[0].getTypeName()); System.out.println(((Class)actualTypeArguments[0]).getName()); } 8.类所在的包 Package getPackage() 调用运行时类的指定结构获取、修改指定属性：public Field getField(String name); // 返回此 Class 对象表示的类或接口的指定的public 的 Field。// 要求运行时类中属性声明为public //通常不采用此方法 public Field getDeclaredField(String name); // 返回此 Class 对象表示的类或接口的指定的 Field 通过 Field 类提供的 set() 和 get() 方法就可以完成设置和取得属性内容的操作。 public Object get(Object obj); // 取得指定对象 obj 上此 Field 的属性内容 public void set(Object obj,Object value); // 设置指定对象 obj 上此 Field 的属性内容 /* 如何操作运行时类中的指定的属性 -- 需要掌握 */ @Test public void testField1() throws Exception { Class clazz = Person.class; //创建运行时类的对象 Person p = (Person) clazz.newInstance(); //1. getDeclaredField(String fieldName):获取运行时类中指定变量名的属性 Field name = clazz.getDeclaredField(&quot;name&quot;); //2.保证当前属性是可访问的 name.setAccessible(true); //3.获取、设置指定对象的此属性值 name.set(p,&quot;Tom&quot;); System.out.println(name.get(p)); } 获取指定方法：getMethod(String name, Class …parameterTypes); //方法取得一个 Method 对象，并设置此方法操作时所需要的参数类型。 getDeclaredMethod(String name, Class …parameterTypes); //方法取得一个 Method 对象，并设置此方法操作时所需要的参数类型。 之后使用 Object invoke(Object obj , Object[] args); // 进行调用，并向方法中传递要设置的 obj 对象的参数信息。 说明： 1. Object 对应原方法的返回值，若原方法无返回值，此时返回 null 2. 若原方法若为静态方法，此时形参 Object obj 可为 null 3. 若原方法形参列表为空，则 Object[] args 为 null 4. 若原方法声明为 private, 则需要在调用此 invoke() 方法前，显式调用方法对象的 setAccessible() 方法，将可访问 private 的方法。 关于setAccessible 方法的使用: Method 和 Field 、 Constructor 对象都有 setAccessible() 方法 。 setAccessible 启动和禁用访问安全检查的开关 。 参数值为 true 则指示反射的对象在使用时应该取消 Java 语言访问检查。 提高反射的效率 。 如果代码中必须用反射，而该句代码需要频繁的被调用，那么请设置为 true 。 使得原本无法访问的私有成员也可以访问。 参数值为 false，则指示反射的对象应该实施 Java 语言访问检查 。 /* 如何操作运行时类中的指定的方法 -- 需要掌握 */ @Test public void testMethod() throws Exception { Class clazz = Person.class; //创建运行时类的对象 Person p = (Person) clazz.newInstance(); /* 1.获取指定的某个方法 getDeclaredMethod():参数1 ：指明获取的方法的名称 参数2：指明获取的方法的形参列表 */ Method show = clazz.getDeclaredMethod(&quot;show&quot;, String.class); //2.保证当前方法是可访问的 show.setAccessible(true); /* 3. 调用方法的invoke():参数1：方法的调用者 参数2：给方法形参赋值的实参 invoke()的返回值即为对应类中调用的方法的返回值。 */ Object returnValue = show.invoke(p,&quot;CHN&quot;); //String nation = p.show(&quot;CHN&quot;); System.out.println(returnValue); System.out.println(&quot;*************如何调用静态方法*****************&quot;); // private static void showDesc() Method showDesc = clazz.getDeclaredMethod(&quot;showDesc&quot;); showDesc.setAccessible(true); //如果调用的运行时类中的方法没有返回值，则此invoke()返回null //Object returnVal = showDesc.invoke(null); // 也可以 Object returnVal = showDesc.invoke(Person.class); System.out.println(returnVal);//null } 获取指定构造器：/* 如何调用运行时类中的指定的构造器 */ public void testConstructor() throws Exception { Class clazz = Person.class; //private Person(String name) /* 1.获取指定的构造器 getDeclaredConstructor():参数：指明构造器的参数列表 */ Constructor constructor = clazz.getDeclaredConstructor(String.class); //2.保证此构造器是可访问的 constructor.setAccessible(true); //3.调用此构造器创建运行时类的对象 Person per = (Person) constructor.newInstance(&quot;Tom&quot;); System.out.println(per); } 反射的应用：动态代理代理设计模式的原理 :使用一个代理将对象包装起来 , 然后用该代理对象取代原始对象。任何对原始对象的调用都要通过代理。代理对象决定是否以及何时将方法调用转到原始对象上 。 之前为大家讲解过代理机制的操作，属于静态代理，特征是代理类和目标对象的类都是在编译期间确定下来，不利于程序的扩展。同时，每一个代理类只能为一个接口服务，这样一来程序开发中必然产生过多的代理。 最好可以通过一个代理类完成全部的代理功能 。 静态代理举例： /** * 静态代理举例 * 特点：代理类和被代理类在编译期间，就确定下来了。 */ interface ClothFactory{ void produceCloth(); } //代理类 class ProxyClothFactory implements ClothFactory{ private ClothFactory factory;//用被代理类对象进行实例化 public ProxyClothFactory(ClothFactory factory){ this.factory = factory; } @Override public void produceCloth() { System.out.println(&quot;代理工厂做一些准备工作&quot;); factory.produceCloth(); System.out.println(&quot;代理工厂做一些后续的收尾工作&quot;); } } //被代理类 class NikeClothFactory implements ClothFactory{ @Override public void produceCloth() { System.out.println(&quot;Nike工厂生产一批运动服&quot;); } } public class StaticProxyTest { public static void main(String[] args) { //创建被代理类的对象 ClothFactory nike = new NikeClothFactory(); //创建代理类的对象 ClothFactory proxyClothFactory = new ProxyClothFactory(nike); proxyClothFactory.produceCloth(); } } 动态代理： 动态代理是指客户通过代理类来调用其它对象的方法，并且是在程序运行时根据需要动态创建目标类的代理对象。 动态代理使用场合：1）调试 2）远程方法调用 动态代理相比于静态代理的优点：抽象角色中（接口）声明的所有方法都被转移到调用处理器一个集中的方法中处理，这样，我们可以更加灵活和统一的处理众多的方法。 Proxy ：专门完成代理的操作类，是所有动态代理类的父类。通过此类为一个或多个接口动态地生成实现类。 提供用于创建动态代理类和动态代理对象的静态方法： static Class &lt;?&gt; getProxyClass( ClassLoader loader, Class&lt;?&gt;…interfaces ) 创建一个动态代理类所对应的 Class 对象 static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces,InvocationHandler h ) 直接创建一个动态代理对象 import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; /** * 动态代理的举例 */ interface Human{ String getBelief(); void eat(String food); } //被代理类 class SuperMan implements Human{ @Override public String getBelief() { return &quot;I believe I can fly!&quot;; } @Override public void eat(String food) { System.out.println(&quot;我喜欢吃&quot; + food); } } class HumanUtil{ public void method1(){ System.out.println(&quot;====================通用方法一====================&quot;); } public void method2(){ System.out.println(&quot;====================通用方法二====================&quot;); } } /* 要想实现动态代理，需要解决的问题？ 问题一：如何根据加载到内存中的被代理类，动态的创建一个代理类及其对象。 问题二：当通过代理类的对象调用方法a时，如何动态的去调用被代理类中的同名方法a。 */ class ProxyFactory{ //调用此方法，返回一个代理类的对象。解决问题一 public static Object getProxyInstance(Object obj){//obj:被代理类的对象 MyInvocationHandler handler = new MyInvocationHandler(); handler.bind(obj); return Proxy.newProxyInstance(obj.getClass().getClassLoader(),obj.getClass().getInterfaces(),handler); } } class MyInvocationHandler implements InvocationHandler{ private Object obj;//需要使用被代理类的对象进行赋值 public void bind(Object obj){ this.obj = obj; } //当我们通过代理类的对象，调用方法a时，就会自动的调用如下的方法：invoke() //将被代理类要执行的方法a的功能就声明在invoke()中 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {//参数依次为：代理类对象、要调用的方法、方法调用时所需要的参数 HumanUtil util = new HumanUtil(); util.method1(); //method:即为代理类对象调用的方法，此方法也就作为了被代理类对象要调用的方法 //obj:被代理类的对象 Object returnValue = method.invoke(obj,args); //上述方法的返回值就作为当前类中的invoke()的返回值。 util.method2(); return returnValue; } } public class ProxyTest { public static void main(String[] args) { SuperMan superMan = new SuperMan(); //proxyInstance:代理类的对象 Human proxyInstance = (Human) ProxyFactory.getProxyInstance(superMan); //当通过代理类对象调用方法时，会自动的调用被代理类中同名的方法 String belief = proxyInstance.getBelief(); System.out.println(belief); proxyInstance.eat(&quot;四川麻辣烫&quot;); System.out.println(&quot;*****************************&quot;); NikeClothFactory nikeClothFactory = new NikeClothFactory(); ClothFactory proxyClothFactory = (ClothFactory) ProxyFactory.getProxyInstance(nikeClothFactory); proxyClothFactory.produceCloth(); } }","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"Java8新特性","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/02-Java/Java16-Java8 新特性.html","text":"Java8 新特性概览 Lambda表达式Lambda是一个匿名函数 ，我们可以把 Lambda 表达式理解为是 一段可以传递的代码 （将代码像数据一样进行传递）。使用它可以写出更简洁、更灵活的代码。作为一种更紧凑的代码风格，使 Java 的语言表达能力得到了提升。 Lambda表达式：在 Java 8 语言中引入的一种新的语法元素和操作符。这个操作符为 “-&gt;” ，该操作符被称为 Lambda 操作符或箭头操作符 。它将 Lambda 分为两个部分： 左侧：指定了 Lambda 表达式需要的参数列表 右侧：指定了 Lambda 体是抽象方法的实现逻辑，也即Lambda 表达式要执行的功能。 import org.junit.Test; import java.util.ArrayList; import java.util.Comparator; import java.util.function.Consumer; /** * Lambda表达式的使用：（分为6种情况介绍） */ public class LambdaTest1 { //语法格式一：无参，无返回值 @Test public void test1(){ Runnable r1 = new Runnable() { @Override public void run() { System.out.println(&quot;我爱北京天安门&quot;); } }; r1.run(); System.out.println(&quot;***********************&quot;); Runnable r2 = () -&gt; { System.out.println(&quot;我爱北京故宫&quot;); }; r2.run(); } //语法格式二：Lambda 需要一个参数，但是没有返回值。 @Test public void test2(){ Consumer&lt;String&gt; con = new Consumer&lt;String&gt;() { @Override public void accept(String s) { System.out.println(s); } }; con.accept(&quot;谎言和誓言的区别是什么？&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con1 = (String s) -&gt; { System.out.println(s); }; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); } //语法格式三：数据类型可以省略，因为可由编译器推断得出，称为“类型推断” @Test public void test3(){ Consumer&lt;String&gt; con1 = (String s) -&gt; { System.out.println(s); }; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con2 = (s) -&gt; { System.out.println(s); }; con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); } @Test public void test4(){ ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();//类型推断 int[] arr = {1,2,3};//类型推断 } //语法格式四：Lambda 若只需要一个参数时，参数的小括号可以省略 @Test public void test5(){ Consumer&lt;String&gt; con1 = (s) -&gt; { System.out.println(s); }; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con2 = s -&gt; { System.out.println(s); }; con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); } //语法格式五：Lambda 需要两个或以上的参数，多条执行语句，并且可以有返回值 @Test public void test6(){ Comparator&lt;Integer&gt; com1 = new Comparator&lt;Integer&gt;() { @Override public int compare(Integer o1, Integer o2) { System.out.println(o1); System.out.println(o2); return o1.compareTo(o2); } }; System.out.println(com1.compare(12,21)); System.out.println(&quot;*****************************&quot;); Comparator&lt;Integer&gt; com2 = (o1,o2) -&gt; { System.out.println(o1); System.out.println(o2); return o1.compareTo(o2); }; System.out.println(com2.compare(12,6)); } //语法格式六：当 Lambda 体只有一条语句时，return 与大括号若有，都可以省略 @Test public void test7(){ Comparator&lt;Integer&gt; com1 = (o1,o2) -&gt; { return o1.compareTo(o2); }; System.out.println(com1.compare(12,6)); System.out.println(&quot;*****************************&quot;); Comparator&lt;Integer&gt; com2 = (o1,o2) -&gt; o1.compareTo(o2); System.out.println(com2.compare(12,21)); } @Test public void test8(){ Consumer&lt;String&gt; con1 = s -&gt; { System.out.println(s); }; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*****************************&quot;); Consumer&lt;String&gt; con2 = s -&gt; System.out.println(s); con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); } } 总结： -&gt; 左边：lambda形参列表的参数类型可以省略(类型推断)；如果lambda形参列表只有一个参数，其一对()也可以省略 -&gt; 右边：lambda体应该使用一对{}包裹；如果lambda体只有一条执行语句（可能是return语句），省略这一对{}和return关键字 Lambda表达式的本质：作为函数式接口的实例 所以以前用匿名实现类表示的现在都可以用Lambda表达式来写。 函数式接口如果一个接口中，只声明了一个抽象方法，则此接口就称为函数式接口。我们可以在一个接口上使用 @FunctionalInterface 注解，这样做可以检查它是否是一个函数式接口。同时 javadoc 也会包含一条声明，说明这个接口是一个函数式接口。 在 java.util.function 包下定义 了 Java 8 的丰富的函数式接口。 你可以通过 Lambda 表达式来创建该接口的对象。（若 Lambda 表达式抛出一个受检异常 (即：非运行时异常)，那么该异常需要在目标接口的抽象方法上进行声明。 作为参数传递 Lambda 表达式： // 作为参数传递Lambda 表达式：为了将 Lambda 表达式作为参数传递，接收 Lambda表达式的参数类型必须是与该 Lambda 表达式兼容的函数式接口的类型。 import org.junit.Test; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import java.util.function.Consumer; import java.util.function.Predicate; /** * java内置的4大核心函数式接口 * * 消费型接口 Consumer&lt;T&gt; void accept(T t) * 供给型接口 Supplier&lt;T&gt; T get() * 函数型接口 Function&lt;T,R&gt; R apply(T t) * 断定型接口 Predicate&lt;T&gt; boolean test(T t) */ public class LambdaTest2 { @Test public void test1(){ happyTime(500, new Consumer&lt;Double&gt;() { @Override public void accept(Double aDouble) { System.out.println(&quot;学习太累了，去天上人间买了瓶矿泉水，价格为：&quot; + aDouble); } }); System.out.println(&quot;********************&quot;); happyTime(400,money -&gt; System.out.println(&quot;学习太累了，去天上人间喝了口水，价格为：&quot; + money)); } public void happyTime(double money, Consumer&lt;Double&gt; con){ con.accept(money); } @Test public void test2(){ List&lt;String&gt; list = Arrays.asList(&quot;北京&quot;,&quot;南京&quot;,&quot;天津&quot;,&quot;东京&quot;,&quot;西京&quot;,&quot;普京&quot;); List&lt;String&gt; filterStrs = filterString(list, new Predicate&lt;String&gt;() { @Override public boolean test(String s) { return s.contains(&quot;京&quot;); } }); System.out.println(filterStrs); List&lt;String&gt; filterStrs1 = filterString(list,s -&gt; s.contains(&quot;京&quot;)); System.out.println(filterStrs1); } //根据给定的规则，过滤集合中的字符串。此规则由Predicate的方法决定 public List&lt;String&gt; filterString(List&lt;String&gt; list, Predicate&lt;String&gt; pre){ ArrayList&lt;String&gt; filterList = new ArrayList&lt;&gt;(); for(String s : list){ if(pre.test(s)){ filterList.add(s); } } return filterList; } } 方法引用和构造器引用方法引用当要传递给 Lambda 体的操作，已经有实现的方法了，可以使用方法引用！ 方法引用可以看做是 Lambda 表达式深层次的表达。换句话说，==方法引用就是 Lambda 表达式 ，也就是函数式接口的一个实例==，通过方法的名字来指向一个方法，可以认为是 Lambda 表达式的一个语法糖。 要求：==实现接口的抽象方法 的参数列表和返回值类型，必须与方法引用的方法的参数列表和返回值类型保持一致！== 格式：使用操作符 “::” 将类 (或对象) 与方法名分隔开来： 对象::实例方法名 类::实例方法名 类::静态方法名 import org.junit.Test; import java.io.PrintStream; import java.util.Comparator; import java.util.function.BiPredicate; import java.util.function.Consumer; import java.util.function.Function; import java.util.function.Supplier; /** * 方法引用的使用 * 1.使用情境：当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用！ * 2.方法引用，本质上就是Lambda表达式，而Lambda表达式作为函数式接口的实例。所以 * 方法引用，也是函数式接口的实例。 * 3. 使用格式： 类(或对象) :: 方法名 * 4. 具体分为如下的三种情况： * 情况1 对象 :: 非静态方法 * 情况2 类 :: 静态方法 * 情况3 类 :: 非静态方法 * 5. 方法引用使用的要求：要求接口中的抽象方法的形参列表和返回值类型与方法引用的方法的 * 形参列表和返回值类型相同！（针对于情况1和情况2） */ public class MethodRefTest { // 情况一：对象 :: 实例方法 //Consumer中的void accept(T t) //PrintStream中的void println(T t) @Test public void test1() { Consumer&lt;String&gt; con1 = str -&gt; System.out.println(str); con1.accept(&quot;北京&quot;); System.out.println(&quot;*******************&quot;); PrintStream ps = System.out; Consumer&lt;String&gt; con2 = ps::println; con2.accept(&quot;beijing&quot;); } //Supplier中的T get() //Employee中的String getName() @Test public void test2() { Employee emp = new Employee(1001,&quot;Tom&quot;,23,5600); Supplier&lt;String&gt; sup1 = () -&gt; emp.getName(); System.out.println(sup1.get()); System.out.println(&quot;*******************&quot;); Supplier&lt;String&gt; sup2 = emp::getName; System.out.println(sup2.get()); } // 情况二：类 :: 静态方法 //Comparator中的int compare(T t1,T t2) //Integer中的int compare(T t1,T t2) @Test public void test3() { Comparator&lt;Integer&gt; com1 = (t1,t2) -&gt; Integer.compare(t1,t2); System.out.println(com1.compare(12,21)); System.out.println(&quot;*******************&quot;); Comparator&lt;Integer&gt; com2 = Integer::compare; System.out.println(com2.compare(12,3)); } //Function中的R apply(T t) //Math中的Long round(Double d) @Test public void test4() { Function&lt;Double,Long&gt; func = new Function&lt;Double, Long&gt;() { @Override public Long apply(Double d) { return Math.round(d); } }; System.out.println(&quot;*******************&quot;); Function&lt;Double,Long&gt; func1 = d -&gt; Math.round(d); System.out.println(func1.apply(12.3)); System.out.println(&quot;*******************&quot;); Function&lt;Double,Long&gt; func2 = Math::round; System.out.println(func2.apply(12.6)); } // 情况三：类 :: 实例方法 (有难度) // Comparator中的int comapre(T t1,T t2) // String中的int t1.compareTo(t2) @Test public void test5() { Comparator&lt;String&gt; com1 = (s1,s2) -&gt; s1.compareTo(s2); System.out.println(com1.compare(&quot;abc&quot;,&quot;abd&quot;)); System.out.println(&quot;*******************&quot;); Comparator&lt;String&gt; com2 = String :: compareTo; System.out.println(com2.compare(&quot;abd&quot;,&quot;abm&quot;)); } //BiPredicate中的boolean test(T t1, T t2); //String中的boolean t1.equals(t2) @Test public void test6() { BiPredicate&lt;String,String&gt; pre1 = (s1,s2) -&gt; s1.equals(s2); System.out.println(pre1.test(&quot;abc&quot;,&quot;abc&quot;)); System.out.println(&quot;*******************&quot;); BiPredicate&lt;String,String&gt; pre2 = String :: equals; System.out.println(pre2.test(&quot;abc&quot;,&quot;abd&quot;)); } // Function中的R apply(T t) // Employee中的String getName(); @Test public void test7() { Employee employee = new Employee(1001, &quot;Jerry&quot;, 23, 6000); Function&lt;Employee,String&gt; func1 = e -&gt; e.getName(); System.out.println(func1.apply(employee)); System.out.println(&quot;*******************&quot;); Function&lt;Employee,String&gt; func2 = Employee::getName; System.out.println(func2.apply(employee)); } } 构造器引用&amp;数组引用构造器引用格式：ClassName::new 与函数式接口相结合，自动与函数式接口中方法兼容。可以把构造器引用赋值给定义的方法，要求构造器参数列表要与接口中抽象方法的参数列表一致！且方法的返回值即为构造器对应类的对象。 数组引用格式：type[] :: new package com.atguigu.java2; import org.junit.Test; import java.util.Arrays; import java.util.function.BiFunction; import java.util.function.Function; import java.util.function.Supplier; /** * 一、构造器引用 * 和方法引用类似，函数式接口的抽象方法的形参列表和构造器的形参列表一致。 * 抽象方法的返回值类型即为构造器所属的类的类型 * * 二、数组引用 * 大家可以把数组看做是一个特殊的类，则写法与构造器引用一致。 */ public class ConstructorRefTest { //构造器引用 //Supplier中的T get() //Employee的空参构造器：Employee() @Test public void test1(){ Supplier&lt;Employee&gt; sup = new Supplier&lt;Employee&gt;() { @Override public Employee get() { return new Employee(); } }; System.out.println(&quot;*******************&quot;); Supplier&lt;Employee&gt; sup1 = () -&gt; new Employee(); System.out.println(sup1.get()); System.out.println(&quot;*******************&quot;); Supplier&lt;Employee&gt; sup2 = Employee :: new; System.out.println(sup2.get()); } //Function中的R apply(T t) @Test public void test2(){ Function&lt;Integer,Employee&gt; func1 = id -&gt; new Employee(id); Employee employee = func1.apply(1001); System.out.println(employee); System.out.println(&quot;*******************&quot;); Function&lt;Integer,Employee&gt; func2 = Employee :: new; Employee employee1 = func2.apply(1002); System.out.println(employee1); } //BiFunction中的R apply(T t,U u) @Test public void test3(){ BiFunction&lt;Integer,String,Employee&gt; func1 = (id,name) -&gt; new Employee(id,name); System.out.println(func1.apply(1001,&quot;Tom&quot;)); System.out.println(&quot;*******************&quot;); BiFunction&lt;Integer,String,Employee&gt; func2 = Employee :: new; System.out.println(func2.apply(1002,&quot;Tom&quot;)); } //数组引用 //Function中的R apply(T t) @Test public void test4(){ Function&lt;Integer,String[]&gt; func1 = length -&gt; new String[length]; String[] arr1 = func1.apply(5); System.out.println(Arrays.toString(arr1)); System.out.println(&quot;*******************&quot;); Function&lt;Integer,String[]&gt; func2 = String[] :: new; String[] arr2 = func2.apply(10); System.out.println(Arrays.toString(arr2)); } } Stream APIStream API ( java.util.stream) 把真正的函数式编程风格引入到 Java 中。这是目前为止对 Java 类库最好的补充，因为 Stream API 可以极大提供 Java 程序员的生产力，让程序员写出高效率、干净、简洁的代码。 Stream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。 使用 Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简言之， Stream API 提供了一种高效且易于使用的处理数据的方式。 实际开发中，项目中多数数据源都来自于 Mysql、Oracle 等。但现在数据源可以更多了，有 MongDB、Radis 等，而这些 NoSQL 的数据就需要Java 层面去处理 。 Stream 和 Collection 集合的区别： Collection 是一种静态的内存数据结构，而 Stream 是有关计算的。 前者是主要面向内存，存储在内存中；后者主要是面向 CPU ，通过 CPU 实现计算。 Stream到底是什么呢？ 是数据渠道，用于操作数据源（集合、数组等）所生成的元素序列。“集合讲的是数据 Stream 讲的是计算！” 注意：① Stream 自己不会存储元素。② Stream 不会改变源对象。相反，他们会返回一个持有结果的新 Stream 。③ Stream 操作是==延迟执行==的。这意味着他们会等到需要结果的时候才执行。 Stream 创建方式// 方式一：通过集合 default Stream&lt;E&gt; stream(); // 返回一个顺序流 default Stream&lt;E&gt; parallelStream(); // 返回一个并行流 // 方式二：通过数组 static &lt;T&gt; Stream&lt;T&gt; stream(T[] array); // 返回一个流 // 重载形式，能够处理对应基本类型的数组： public static IntStream stream(int[] array) public static LongStream stream(long[] array) public static DoubleStream stream(double[] array) // 方式三：通过 Stream 的 of() public static&lt;T&gt; Stream&lt;T&gt; of(T... values); // 返回一个流 // 方式四：创建无限流 // 可以使用静态方法Stream.iterate() 和 Stream.generate(),创建无限流。 // 迭代 public static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f) // 生成 public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) public class StreamAPITest { //创建 Stream方式一：通过集合 @Test public void test1(){ List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // default Stream&lt;E&gt; stream() : 返回一个顺序流 Stream&lt;Employee&gt; stream = employees.stream(); // default Stream&lt;E&gt; parallelStream() : 返回一个并行流 Stream&lt;Employee&gt; parallelStream = employees.parallelStream(); } //创建 Stream方式二：通过数组 @Test public void test2(){ int[] arr = new int[]{1,2,3,4,5,6}; //调用Arrays类的static &lt;T&gt; Stream&lt;T&gt; stream(T[] array): 返回一个流 IntStream stream = Arrays.stream(arr); Employee e1 = new Employee(1001,&quot;Tom&quot;); Employee e2 = new Employee(1002,&quot;Jerry&quot;); Employee[] arr1 = new Employee[]{e1,e2}; Stream&lt;Employee&gt; stream1 = Arrays.stream(arr1); } //创建 Stream方式三：通过Stream的of() @Test public void test3(){ Stream&lt;Integer&gt; stream = Stream.of(1, 2, 3, 4, 5, 6); } //创建 Stream方式四：创建无限流 @Test public void test4(){ // 迭代 // public static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f) //遍历前10个偶数 Stream.iterate(0, t -&gt; t + 2).limit(10).forEach(System.out::println); // 生成 // public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) Stream.generate(Math::random).limit(10).forEach(System.out::println); } } 中间操作多个中间操作可以连接起来形成一个流水线，除非流水线上触发终止操作，否则中间操作不会执行任何的处理 ！而在终止操作时一次性全部处理，称为“惰性求值” 。 1-筛选与切片 方法 描述 filter(Predicate p) 接收Lambda，从流中排除某些元素 distinct() 筛选，通过流所生成元素的 hashCode() 和 equals() 去除重复元素 limit(long maxSize) 截断流，使其元素不超过给定数量 skip(long n) 跳过元素，返回一个扔掉了前 n 个元素的流。若流中元素不足 n 个，则返回一个空流。与 limit(n) 互补 2-映射 方法 描述 map(Function f) 接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。 mapToDouble(ToDoubleFunction f) 接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 DoubleStream 。 mapToInt(ToIntFunction f) 接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 IntStream 。 mapToLong(ToLongFunction f) 接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 LongStream 。 flatMap(Function f) 接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流 3-排序 方法 描述 sorted() 产生一个新流，其中按自然顺序排序 sorted(Comparator com) 产生一个新流，其中按比较器顺序排序 产生一个新流，其中按比较器顺序排序 import com.atguigu.java2.Employee; import com.atguigu.java2.EmployeeData; import org.junit.Test; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import java.util.stream.Stream; /** * 测试Stream的中间操作 */ public class StreamAPITest1 { //1-筛选与切片 @Test public void test1(){ List&lt;Employee&gt; list = EmployeeData.getEmployees(); // filter(Predicate p)——接收 Lambda ， 从流中排除某些元素。 Stream&lt;Employee&gt; stream = list.stream(); //练习：查询员工表中薪资大于7000的员工信息 stream.filter(e -&gt; e.getSalary() &gt; 7000).forEach(System.out::println); System.out.println(); // limit(n)——截断流，使其元素不超过给定数量。 list.stream().limit(3).forEach(System.out::println); System.out.println(); // skip(n) —— 跳过元素，返回一个扔掉了前 n 个元素的流。若流中元素不足 n 个，则返回一个空流。与 limit(n) 互补 list.stream().skip(3).forEach(System.out::println); System.out.println(); // distinct()——筛选，通过流所生成元素的 hashCode() 和 equals() 去除重复元素 list.add(new Employee(1010,&quot;刘强东&quot;,40,8000)); list.add(new Employee(1010,&quot;刘强东&quot;,41,8000)); list.add(new Employee(1010,&quot;刘强东&quot;,40,8000)); list.add(new Employee(1010,&quot;刘强东&quot;,40,8000)); list.add(new Employee(1010,&quot;刘强东&quot;,40,8000)); // System.out.println(list); list.stream().distinct().forEach(System.out::println); } //映射 @Test public void test2(){ // map(Function f)——接收一个函数作为参数，将元素转换成其他形式或提取信息，该函数会被应用到每个元素上，并将其映射成一个新的元素。 List&lt;String&gt; list = Arrays.asList(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;); list.stream().map(str -&gt; str.toUpperCase()).forEach(System.out::println); // 练习1：获取员工姓名长度大于3的员工的姓名。 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;String&gt; namesStream = employees.stream().map(Employee::getName); namesStream.filter(name -&gt; name.length() &gt; 3).forEach(System.out::println); System.out.println(); //练习2： Stream&lt;Stream&lt;Character&gt;&gt; streamStream = list.stream().map(StreamAPITest1::fromStringToStream); streamStream.forEach(s -&gt;{ s.forEach(System.out::println); }); System.out.println(); // flatMap(Function f)——接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。 Stream&lt;Character&gt; characterStream = list.stream().flatMap(StreamAPITest1::fromStringToStream); characterStream.forEach(System.out::println); } //将字符串中的多个字符构成的集合转换为对应的Stream的实例 public static Stream&lt;Character&gt; fromStringToStream(String str){//aa ArrayList&lt;Character&gt; list = new ArrayList&lt;&gt;(); for(Character c : str.toCharArray()){ list.add(c); } return list.stream(); } //3-排序 @Test public void test4(){ // sorted()——自然排序 List&lt;Integer&gt; list = Arrays.asList(12, 43, 65, 34, 87, 0, -98, 7); list.stream().sorted().forEach(System.out::println); //抛异常，原因:Employee没有实现Comparable接口 // List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // employees.stream().sorted().forEach(System.out::println); // sorted(Comparator com)——定制排序 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); employees.stream().sorted( (e1,e2) -&gt; { int ageValue = Integer.compare(e1.getAge(),e2.getAge()); if(ageValue != 0){ return ageValue; }else{ return -Double.compare(e1.getSalary(),e2.getSalary()); } }).forEach(System.out::println); } } 终止操作1-匹配与查找 方法 描述 allMatch(Predicate p) 检查是否匹配所有元素 anyMatch(Predicate p) 检查是否至少匹配一个元素 noneMatch(Predicate p) 检查是否没有匹配的元素 findFirst() 返回第一个元素 findAny() 返回当前流中的任意元素 count() 返回流中元素总数 max(Comparator c) 返回流中最大值 min(Comparator c) 返回流中最小值 forEach(Consumer c) 内部迭代 (使用 Collection 接口需要用户去做迭代，称为外部迭代 。相反， Stream API 使用内部迭代，它帮你把迭代做了) 2-归约 方法 描述 reduce(T iden, BinaryOperator b) 可以将流中元素反复结合起来，得到一个值。返回 T reduce(BinaryOperator b) 可以将流中元素反复结合起来，得到一个值。返回 Optional 备注：map 和 reduce 的连接通常称为 map-reduce 模式，因 Google用它来进行网络搜索而出名。 3-收集 方法 描述 collect(Collector c) 将流转换为其他形式。接收一个&lt;Collector接口的实现，用于给 Stream 中元素做汇总的方法 Collector接口中方法的实现决定了如何对流执行收集的操作 (如收集到 List 、 Set 、Map) 。另外，Collectors 实用类提供了很多静态方法，可以方便地创建常见收集器实例，具体方法与实例如下表： import com.atguigu.java2.Employee; import com.atguigu.java2.EmployeeData; import org.junit.Test; import java.util.Arrays; import java.util.List; import java.util.Optional; import java.util.Set; import java.util.stream.Collectors; import java.util.stream.Stream; /** * 测试Stream的终止操作 */ public class StreamAPITest2 { //1-匹配与查找 @Test public void test1(){ List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // allMatch(Predicate p)——检查是否匹配所有元素。 // 练习：是否所有的员工的年龄都大于18 boolean allMatch = employees.stream().allMatch(e -&gt; e.getAge() &gt; 18); System.out.println(allMatch); // anyMatch(Predicate p)——检查是否至少匹配一个元素。 // 练习：是否存在员工的工资大于 10000 boolean anyMatch = employees.stream().anyMatch(e -&gt; e.getSalary() &gt; 10000); System.out.println(anyMatch); // noneMatch(Predicate p)——检查是否没有匹配的元素。 // 练习：是否存在员工姓“雷” boolean noneMatch = employees.stream().noneMatch(e -&gt; e.getName().startsWith(&quot;雷&quot;)); System.out.println(noneMatch); // findFirst——返回第一个元素 Optional&lt;Employee&gt; employee = employees.stream().findFirst(); System.out.println(employee); // findAny——返回当前流中的任意元素 Optional&lt;Employee&gt; employee1 = employees.parallelStream().findAny(); System.out.println(employee1); } @Test public void test2(){ List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // count——返回流中元素的总个数 long count = employees.stream().filter(e -&gt; e.getSalary() &gt; 5000).count(); System.out.println(count); // max(Comparator c)——返回流中最大值 // 练习：返回最高的工资： Stream&lt;Double&gt; salaryStream = employees.stream().map(e -&gt; e.getSalary()); Optional&lt;Double&gt; maxSalary = salaryStream.max(Double::compare); System.out.println(maxSalary); // min(Comparator c)——返回流中最小值 // 练习：返回最低工资的员工 Optional&lt;Employee&gt; employee = employees.stream().min((e1, e2) -&gt; Double.compare(e1.getSalary(), e2.getSalary())); System.out.println(employee); System.out.println(); // forEach(Consumer c)——内部迭代 employees.stream().forEach(System.out::println); //使用集合的遍历操作 employees.forEach(System.out::println); } //2-归约 @Test public void test3(){ // reduce(T identity, BinaryOperator)——可以将流中元素反复结合起来，得到一个值。返回 T // 练习1：计算1-10的自然数的和 List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7,8,9,10); Integer sum = list.stream().reduce(0, Integer::sum); System.out.println(sum); // reduce(BinaryOperator) ——可以将流中元素反复结合起来，得到一个值。返回 Optional&lt;T&gt; // 练习2：计算公司所有员工工资的总和 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Double&gt; salaryStream = employees.stream().map(Employee::getSalary); // Optional&lt;Double&gt; sumMoney = salaryStream.reduce(Double::sum); Optional&lt;Double&gt; sumMoney = salaryStream.reduce((d1,d2) -&gt; d1 + d2); System.out.println(sumMoney.get()); } //3-收集 @Test public void test4(){ // collect(Collector c)——将流转换为其他形式。接收一个 Collector接口的实现，用于给Stream中元素做汇总的方法 // 练习1：查找工资大于6000的员工，结果返回为一个List或Set List&lt;Employee&gt; employees = EmployeeData.getEmployees(); List&lt;Employee&gt; employeeList = employees.stream().filter(e -&gt; e.getSalary() &gt; 6000).collect(Collectors.toList()); employeeList.forEach(System.out::println); System.out.println(); Set&lt;Employee&gt; employeeSet = employees.stream().filter(e -&gt; e.getSalary() &gt; 6000).collect(Collectors.toSet()); employeeSet.forEach(System.out::println); } } Optional类Optional 类 (java.util.Optional) 是一个容器类，它可以保存类型 T 的值， 代表这个值存在 。或者仅仅保存 null ，表示这个值不存在。原来用 null 表示一个值不存在，现在 Optional 可以更好的表达这个概念。并且可以避免空指针异常。 // 创建 Optional 类对象的方法： Optional.of(T t); // 创建一个 Optional 实例， t 必须非空 Optional.empty(); // 创建一个空的 Optional 实例 Optional.ofNullable(T t); // t 可以为 null // 判断 Optional 容器中是否包含对象： boolean isPresent(); // 判断是否包含对象 void ifPresent(Consumer&lt;? super T&gt; consumer); // 如果有值，就执行 Consumer接口的实现代码，并且该值会作为参数传给它。 // 获取 Optional 容器的对象： T get(); // 如果调用对象包含值，返回该值，否则抛异常 T orElse(T other); // 如果有值则将其返回，否则返回指定的 other 对象。 T orElseGet(Supplier&lt;? extends T&gt; other); // 如果有值则将其返回，否则返回由Supplier 接口实现提供的对象。 T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier); // 如果有值则将其返回，否则抛出由 Supplier 接口实现提供的异常 。","tags":[{"name":"Java","slug":"Java","permalink":"https://mtcai.github.io/tags/Java/"}]},{"title":"数据结构01-大纲","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/01-数据结构/数据结构01-大纲.html","text":"以下两个表来自：https://www.bigocheatsheet.com/","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"MySQL安装","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL00-安装.html","text":"MySQL安装安装文件：mysql-5.5.15-winx64.msi 接下来，需要注意：1）Typical 是经典安装，包含服务端和自带的客户端；2）Custom 是自定义安装；3）Complete 是完全安装。这里选择自定义安装。同时修改安装路径。 然后期间会出现mysql的广告，点击下一步即可。然后会继续安装，安装成功后，保证下面是勾选状态（默认也是勾选的），到这里仅是安装好了服务，还没配置。 如果取消了勾选，或配置时中途退出，也可以在安装目录下重新运行配置程序。 D:\\Program Files\\MySQL\\MySQL Server 5.5\\bin\\MySQLInstanceConfig.exe 配置界面有两个选项：1）Detailed XX 是精确配置；2）Standard XXX是标准配置。这里使用精确配置。然后选择服务类型，从上到下依次是开发机、服务器和专用服务器，占用的内存也依次递增。一般选择开发机即可。 接下来选择数据库类型，1）多功能型数据库；2）事务型数据库；3）非事务型数据库。存储引擎有事务性和非事务性，多功能型数据库在两种存储引擎速度都比较快，事务型数据库在事务型引擎较快，非事务型数据库在非事务型引擎速度较快。一般选择多功能型数据库。下一个界面直接下一步。 接下来配置数据库并发连接数：1）策略式，支持20个连接；2）在线式，允许500个连接；3）自定义，自己设定连接数。一般选择第一个即可。然后是配置端口号，开发中一般需要修改，防止别人恶意攻击。学习使用可先不改。 接下来选择字符集：使用第三个，然后下拉选择utf8。下一个界面中起一个服务名，其中绿线是开机自启，红线是添加环境变量。 接下来，设置root账户密码，同时勾选允许远程连接。然后点击“Execute”执行，等待完成，如下图。 图像化界面安装文件：SQLyog-10.0.0-0.exe 激活码： Name: any key: dd987f34-f358-4894-bd0f-21f3f04be9c1 一路下一步即可。然后新建一个连接，输入刚才设置的密码，连接。 连接成功就完成全部配置。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"MySQL概述","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL01-概述.html","text":"数据库的相关概念数据库的好处： 持久化数据到本地 可以实现结构化查询，方便管理 数据库相关概念 DB (database)：数据库，保存一组有组织的数据的容器 DBMS (Database Management System)：数据库管理系统，又称为数据库软件（产品），用于管理DB中的数据 DBMS分为两类： 基于共享文件系统的DBMS （Access） 客户机——服务器的DBMS（MySQL、Oracle、SqlServer） SQL (Structure Query Language)：结构化查询语言，用于和DBMS通信的语言 SQL的优点： 不是某个特定数据库供应商专有的语言，几乎所有DBMS都支持SQL 简单易学 虽然简单，但实际上是一种强有力的语言，灵活使用其语言元素，可以进行非常复杂和高级的数据库操作。 数据库存储数据的特点 将数据放到表中，表再放到库中 一个数据库中可以有多个表，每个表都有一个的名字，用来标识自己。表名具有唯一性。 表具有一些特性，这些特性定义了数据在表中如何存储，类似java中 “类”的设计。 表由列组成，我们也称为字段。所有表都是由一个或多个列组成的，每一列类似java 中的”属性” 表中的数据是按行存储的，每一行类似于java中的“对象”。 初识MySQL配置文件路径：C:\\Program Files\\MySQL\\MySQL Server 5.5\\my.ini [mysql] ：客户端配置 [mysqld]：服务端配置 端口号：port=3306 安装目录：basedir=”C:/Program Files/MySQL/MySQL Server 5.5/“ 文件目录：datadir=”C:/ProgramData/MySQL/MySQL Server 5.5/Data/“ 字符集：character-set-server=utf8 存储引擎：default-storage-engine=INNODB 最大连接数：max_connections=100 启动、终止、登录、退出启动1：右击计算机—管理—服务—启动或停止MySQL服务 启动2：net start mysql服务名 停止：net stop mysql服务名 登录：mysql –h 主机名 -P 端口 –u用户名 –p密码 (-p和密码不能有空格) 退出：exit，Ctrl+C MySQL使用语法规范 不区分大小写，但建议关键字大写，表名、列名小写； 每句话用 ; 或 \\g 结尾； 各子句一般分行写； 关键字不能缩写也不能分行； 用缩进提高语句的可读性。 单行注释：#注释文字 单行注释：— 注释文字(—后有空格) 多行注释：/ 注释文字 / mysql常用命令 show databases; # 查看mysql中有哪些个数据库 use 数据库名称; # 打开指定的库 create database 数据库名; # 新建一个数据库 create table 表名( 列名 列类型, 列名 列类型， ...); # 创建一个表 show tables; # 查看指定的数据库中有哪些数据表 show tables from 数据库名称; # 查看其它库的所有表 desc 表名; # 查看表的结构 drop table 表名; # 删除表 select database(); # 查看在哪个库 select * from 表名; # 查看有哪些数据 select version(); # 登录到mysql服务端，查看服务器的版本 mysql --version 或 mysql --V # 没有登录到mysql服务端，查看服务器的版本 SQL的语言分类DML（Data Manipulation Language)：数据操纵语句，用于添加、删除、修改、查询数据库记录，并检查数据完整性 DDL（Data Definition Language)：数据定义语句，用于库和表的创建、修改、删除。 DCL（Data Control Language)：数据控制语句，用于定义用户的访问权限和安全级别。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"DQL语言","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL02-DQL.html","text":"DQL 基础查询SELECT 要查询的东西 (FROM 表名); # 通过select查询完的结果 ，是一个虚拟的表格，不是真实存在 # 要查询的东西 可以是常量值、可以是表达式、可以是字段、可以是函数 USE myemployees; # 1.查询表中的单个字段 SELECT last_name FROM employees; # 2.查询表中的多个字段 SELECT last_name,salary,email FROM employees; # 3.查询表中的所有字段 #方式一： SELECT `employee_id`, `first_name`, `last_name`, `phone_number`, `last_name`, `job_id`, `phone_number`, `job_id`, `salary`, `commission_pct`, `manager_id`, `department_id`, `hiredate` FROM employees; # `xxx`: `是着重号，用于区分字段和关键字 #方式二： SELECT * FROM employees; # 4.查询常量值 SELECT 100; SELECT &#39;john&#39;; # 5.查询表达式 SELECT 100%98; # 6.查询函数 SELECT VERSION(); # 7.起别名 # ①便于理解 # ②如果要查询的字段有重名的情况，使用别名可以区分开来 #方式一：使用as SELECT 100%98 AS 结果; SELECT last_name AS 姓,first_name AS 名 FROM employees; #方式二：使用空格 SELECT last_name 姓,first_name 名 FROM employees; # 别名有特殊符号或关键字，则需把别名加上双引号 # 案例：查询salary，显示结果为 out put SELECT salary AS &quot;out put&quot; FROM employees; # 8.去重 #案例：查询员工表中涉及到的所有的部门编号 SELECT DISTINCT department_id FROM employees; # 9. + 号的作用 /* java中的+号： ①运算符，两个操作数都为数值型 ②连接符，只要有一个操作数为字符串 mysql中的+号： 仅仅只有一个功能：运算符 select 100+90;（190） 两个操作数都为数值型，则做加法运算 select &#39;123&#39;+90;（113） 只要其中一方为字符型，试图将字符型数值转换成数值型，如果转换成功，则继续做数值的加法运算 select &#39;john+90;（90） 如果转换失败，则将字符型数值转换成0 select null+10;（null） 只要其中一方为null，则结果肯定为null */ #案例：查询员工名和姓连接成一个字段，并显示为 姓名 SELECT CONCAT(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;) AS 结果; SELECT CONCAT(last_name,first_name) AS 姓名 FROM employees; 条件查询# 语法： select 要查询的字段|表达式|常量值|函数 from 表 where 条件; # 分类： /* 一、条件表达式 示例：salary&gt;10000 条件运算符： &gt; &lt; &gt;= &lt;= = != &lt;&gt; */ #案例：查询部门编号不等于90号的员工名和部门编号 SELECT last_name, department_id FROM employees WHERE department_id&lt;&gt;90; /* 二、逻辑表达式 示例：salary&gt;10000 &amp;&amp; salary&lt;20000 逻辑运算符： and（&amp;&amp;）:两个条件如果同时成立，结果为true，否则为false or(||)：两个条件只要有一个成立，结果为true，否则为false not(!)：如果条件成立，则not后为false，否则为true */ #案例：查询部门编号不是在90到110之间，或者工资高于15000的员工信息 SELECT * FROM employees WHERE NOT(department_id&gt;=90 AND department_id&lt;=110) OR salary&gt;15000; /* 三、模糊查询 示例：last_name like &#39;a%&#39; 通配符： % : 任意多个字符,包含0个字符； _ : 任意单个字符； 运算符： like: 一般和通配符搭配使用。 between XXX and XXX: 包含临界值;两个临界值不要调换顺序. in: in列表的值类型必须一致或兼容;in列表中不支持通配符. is null: =或&lt;&gt;不能用于判断null值 is not null */ #案例：查询员工名中第二个字符为_的员工名 SELECT last_name FROM employees WHERE last_name LIKE &#39;_$_%&#39; ESCAPE &#39;$&#39;; # 指定$为转义字符，默认转义字符为&quot;\\&quot; #案例：查询员工编号在100到120之间的员工信息 SELECT * FROM employees WHERE employee_id BETWEEN 120 AND 100; #案例：查询员工的工种编号是 IT_PROG、AD_VP、AD_PRES中的一个员工名和工种编号 SELECT last_name, job_id FROM employees WHERE job_id IN( &#39;IT_PROT&#39; ,&#39;AD_VP&#39;,&#39;AD_PRES&#39;); #案例：查询没有奖金的员工名和奖金率 SELECT last_name, commission_pct FROM employees WHERE commission_pct IS NULL; /* 四、安全等于 &lt;=&gt; */ #案例：查询没有奖金的员工名和奖金率 SELECT last_name, commission_pct FROM employees WHERE commission_pct &lt;=&gt;NULL; #案例：查询工资为12000的员工信息 SELECT last_name, salary FROM employees WHERE salary &lt;=&gt; 12000; # IS NULL 与 &lt;=&gt; # IS NULL:仅仅可以判断NULL值，可读性较高，建议使用 # &lt;=&gt; :既可以判断NULL值，又可以判断普通的数值，可读性较低 SELECT ISNULL(commission_pct) FROM employees; 面试题 问：select * from employees; 和 select * from employees where com mission_pct like ‘%%’ and last_name like ‘%%’;结果是否一样？为什么？ 如果判断字段没有null，结果一致；若字段中含有null，则结果不一样！ 排序查询# 语法： select 要查询的东西 # 执行次序：3 from 表 # 执行次序：1 where 条件 # 执行次序：2 order by 排序的字段|表达式|函数|别名 【asc|desc】 # 执行次序：4 /* 1、asc代表的是升序，可以省略，desc代表的是降序 2、order by子句可以支持 单个字段、别名、表达式、函数、多个字段 3、order by子句在查询语句的最后面，除了limit子句 */ # 按单个字段排序 SELECT * FROM employees ORDER BY salary DESC; # 添加筛选条件再排序 # 案例：查询部门编号&gt;=90的员工信息，并按员工编号降序 SELECT * FROM employees WHERE department_id&gt;=90 ORDER BY employee_id DESC; # 按表达式排序 # 案例：查询员工信息 按年薪降序 SELECT *,salary*12*(1+IFNULL(commission_pct,0)) FROM employees ORDER BY salary*12*(1+IFNULL(commission_pct,0)) DESC; # 按函数排序 # 案例：查询员工名，并且按名字的长度降序 SELECT LENGTH(last_name),last_name FROM employees ORDER BY LENGTH(last_name) DESC; # 按多个字段排序 # 案例：查询员工信息，要求先按工资降序，再按employee_id升序 SELECT * FROM employees ORDER BY salary DESC,employee_id ASC; 常见函数字符函数 concat 拼接 substr 截取子串 upper 转换成大写 lower 转换成小写 trim 去前后指定的空格和字符 ltrim 去左边空格 rtrim 去右边空格 replace 替换 lpad 左填充 rpad 右填充 instr 返回子串第一次出现的索引，如果找不到就返回0 length 获取字节个数 ```mysqlSHOW VARIABLES LIKE ‘%char%’; # 可以查看编码utf-8 ####### 注意：索引从1开始截取从 指定索引处 后面 所有字符SELECT SUBSTR(‘李莫愁爱上了陆展元’,7) out_put; # 陆展元 截取从 指定索引处 指定字符 长度 的字符SELECT SUBSTR(‘李莫愁爱上了陆展元’,1,3) out_put; # 李莫愁 SELECT LENGTH(TRIM(‘ 张翠山 ‘)) AS out_put; # 9SELECT TRIM(‘aa’ FROM ‘aaaaaaaaa张aaaaaaaaaaaa翠山aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa’) AS out_put; # 只能去除两边的，中间不去 lpad 用指定的字符实现左填充指定长度，第二个参数：填充后的长度 SELECT LPAD(‘殷素素’,2,’*’) AS out_put; # 殷素 rpad 用指定的字符实现右填充指定长度 SELECT RPAD(‘殷素素’,12,’ab’) AS out_put; #### 数学函数 - round 四舍五入 - rand 随机数 - floor 向下取整 - ceil 向上取整 - mod 取余 - runcate 截断 - ```mysql #round 四舍五入 SELECT ROUND(-1.55); SELECT ROUND(1.567,2); # 第二个参数：有效数字 #truncate 截断 SELECT TRUNCATE(1.69999,1); # 小数位数，直接截断 日期函数 now 当前系统日期+时间 curdate 当前系统日期 curtime 当前系统时间 str_to_date 将字符转换成日期 date_format 将日期转换成字符 year 返回当前年 month 返回当前月 day 返回当前天 hour 返回当前小时 minute 返回当前分钟 second 返回当前秒 datediff 返回两个日期的差 monthname 以英文形式返回月份 #可以获取指定的部分，年、月、日、小时、分钟、秒 SELECT YEAR(NOW()) 年; SELECT YEAR(&#39;1998-1-1&#39;) 年; SELECT YEAR(hiredate) 年 FROM employees; SELECT MONTH(NOW()) 月; SELECT MONTHNAME(NOW()) 月; # str_to_date 将字符通过指定的格式转换成日期 SELECT STR_TO_DATE(&#39;1998-3-2&#39;,&#39;%Y-%c-%d&#39;) AS out_put; #查询入职日期为1992-4-3的员工信息 SELECT * FROM employees WHERE hiredate = &#39;1992-4-3&#39;; SELECT * FROM employees WHERE hiredate = STR_TO_DATE(&#39;4-3 1992&#39;,&#39;%c-%d %Y&#39;); # date_format 将日期转换成字符 SELECT DATE_FORMAT(NOW(),&#39;%y年%m月%d日&#39;) AS out_put; #查询有奖金的员工名和入职日期(xx月/xx日 xx年) SELECT last_name,DATE_FORMAT(hiredate,&#39;%m月/%d日 %y年&#39;) 入职日期 FROM employees WHERE commission_pct IS NOT NULL; # 查询员工表中的最大入职时间和最小入职时间的相差天数 （DIFFRENCE） SELECT MAX(hiredate) 最大,MIN(hiredate) 最小,(MAX(hiredate)-MIN(hiredate))/1000/3600/24 DIFFRENCE FROM employees; SELECT DATEDIFF(MAX(hiredate),MIN(hiredate)) DIFFRENCE FROM employees; SELECT DATEDIFF(&#39;1995-2-7&#39;,&#39;1995-2-6&#39;); 流程控制函数 if(条件表达时， 表达式1， 表达式2) 处理双分支 case语句 处理多分支 情况1：处理等值判断 情况2：处理条件判断 SELECT IF(10&lt;5,&#39;大&#39;,&#39;小&#39;); SELECT last_name,commission_pct,IF(commission_pct IS NULL,&#39;没奖金，呵呵&#39;,&#39;有奖金，嘻嘻&#39;) 备注 FROM employees; /* java中 switch(变量或表达式){ case 常量1：语句1;break; ... default:语句n;break; } mysql中： case 要判断的字段或表达式 when 常量1 then 要显示的值1或语句1; when 常量2 then 要显示的值2或语句2; else 要显示的值n或语句n; end */ /*案例：查询员工的工资，要求 部门号=30，显示的工资为1.1倍 部门号=40，显示的工资为1.2倍 部门号=50，显示的工资为1.3倍 其他部门，显示的工资为原工资 */ SELECT salary 原始工资,department_id, CASE department_id WHEN 30 THEN salary\\*1.1 WHEN 40 THEN salary\\*1.2 WHEN 50 THEN salary\\*1.3 ELSE salary END AS 新工资 FROM employees; #案例：查询员工的工资的情况 如果工资&gt;20000,显示A级别 如果工资&gt;15000,显示B级别 如果工资&gt;10000，显示C级别 否则，显示D级别 SELECT salary, CASE WHEN salary&gt;20000 THEN &#39;A&#39; WHEN salary&gt;15000 THEN &#39;B&#39; WHEN salary&gt;10000 THEN &#39;C&#39; ELSE &#39;D&#39; END AS 工资级别 FROM employees; 其他函数 version 版本 database 当前库 user 当前连接用户 password(‘字符’) 返回该字符的加密形式 md5(‘字符’) 返回该字符的md5加密形式 分组函数 sum 求和 max 最大值 min 最小值 avg 平均值 count 计数 特点： 1、以上五个分组函数都忽略null值，除了count(*) 2、sum和avg一般用于处理数值型；max、min、count可以处理任何数据类型 3、都可以搭配distinct使用，用于统计去重后的结果 4、count的参数可以支持：字段、*、常量值； count(*) 和count(1)用来结果集的行数；count(字段)。 效率：MYISAM存储引擎下 ，COUNT(*)的效率高INNODB存储引擎下，COUNT(*)和COUNT(1)的效率差不多，比COUNT(字段)要高一些 5、和分组函数一同查询的字段要求是group by后的字段 6、和分组函数一同查询的字段有限制： SELECT AVG(salary),employee_id FROM employees;# “employee_id”无意义 谓词函数 LIKE BETWEEN IS NULL、IS NOT NULL IN EXISTS （链接） # LIKE谓词 – 用于字符串的部分一致查询。部分一致大体可以分为前方一致、中间一致和后方一致三种类型。 SELECT * FROM samplelike WHERE strcol LIKE &#39;ddd%&#39;; # # %是代表“零个或多个任意字符串”的特殊符号， SELECT * FROM samplelike WHERE strcol LIKE &#39;abc__&#39;; # _下划线匹配任意 1 个字符 # BETWEEN谓词 – 用于范围查询 SELECT product_name, sale_price FROM product WHERE sale_price BETWEEN 100 AND 1000; # BETWEEN 的特点就是结果中会包含 100 和 1000 这两个临界值，也就是闭区间。如果不想让结果中包含临界值，那就必须使用 &lt; 和 &gt;。 SELECT product_name, sale_price FROM product WHERE sale_price &gt; 100 AND sale_price &lt; 1000; # IS NULL、 IS NOT NULL – 用于判断是否为NULL # 为了选取出某些值为 NULL 的列的数据，不能使用 =，而只能使用特定的谓词IS NULL。 SELECT product_name, purchase_price FROM product WHERE purchase_price IS NULL; SELECT product_name, purchase_price FROM product WHERE purchase_price IS NOT NULL; # IN谓词 – OR的简便用法 # 多个查询条件取并集时可以选择使用or语句。 SELECT product_name, purchase_price FROM product WHERE purchase_price = 320 OR purchase_price = 500 OR purchase_price = 5000; SELECT product_name, purchase_price FROM product WHERE purchase_price IN (320, 500, 5000); / WHERE purchase_price NOT IN (320, 500, 5000); # 需要注意的是，在使用IN 和 NOT IN 时是无法选取出NULL数据的。实际结果也是如此，上述两组结果中都不包含进货单价为 NULL 的叉子和圆珠笔。 NULL 只能使用 IS NULL 和 IS NOT NULL 来进行判断。 # EXIST谓词的使用方法。谓词的作用就是 “判断是否存在满足某种条件的记录”，判断子查询得到的结果集是否是一个空集，如果不是，则返回 True，如果是，则返回 False。 SELECT product_name, sale_price FROM product AS p WHERE EXISTS (SELECT * FROM shopproduct AS sp WHERE sp.shop_id = &#39;000C&#39; AND sp.product_id = p.product_id); 分组查询/* 语法： select 查询列表 from 表 【where 筛选条件】 group by 分组的字段 【having 分组后的筛选】 【order by 排序的字段】; 特点： 1、和分组函数一同查询的字段必须是group by后出现的字段 2、筛选分为两类：分组前筛选和分组后筛选 针对的表 位置 连接的关键字 分组前筛选 原始表 group by前 where 分组后筛选 group by后的结果集 group by后 having 问题1：分组函数做筛选能不能放在where后面 答：不能 问题2：where——group by——having 一般来讲，能用分组前筛选的，尽量使用分组前筛选，提高效率 3、分组可以按单个字段也可以按多个字段(多个字段用逗号隔开没有顺序要求) 4、可以搭配着排序使用，放在最后 */ #查询每个工种的员工平均工资 SELECT AVG(salary),job_id FROM employees GROUP BY job_id; #查询有奖金的每个领导手下员工的平均工资 SELECT AVG(salary),manager_id FROM employees WHERE commission_pct IS NOT NULL GROUP BY manager_id; #分组后筛选 #案例：查询哪个部门的员工个数&gt;5 SELECT COUNT(*),department_id FROM employees GROUP BY department_id HAVING COUNT(*)&gt;5; #领导编号&gt;102的每个领导手下的最低工资大于5000的领导编号和最低工资 SELECT manager_id,MIN(salary) FROM employees WHERE manager_id&gt;102 GROUP BY manager_id HAVING MIN(salary)&gt;5000; #5.按多个字段分组 #查询每个工种每个部门的最低工资,并按最低工资降序 SELECT MIN(salary),job_id,department_id FROM employees GROUP BY department_id,job_id ORDER BY MIN(salary) DESC; 连接查询/* 含义：又称多表查询，当查询的字段来自于多个表时，就会用到连接查询 笛卡尔乘积现象：表1 有m行，表2有n行，结果=m*n行 发生原因：没有有效的连接条件 如何避免：添加有效的连接条件 分类： 按年代分类： sql92标准:仅仅支持内连接 sql99标准【推荐】：支持内连接+外连接（左外和右外）+交叉连接 按功能分类： 内连接： 等值连接 非等值连接 自连接 外连接： 左外连接 右外连接 全外连接（mysql不支持） 交叉连接 */ sql92标准#一、sql92标准 #1、等值连接 /* ① 多表等值连接的结果为多表的交集部分 ② n表连接，至少需要n-1个连接条件 ③ 多表的顺序没有要求 ④ 一般需要为表起别名 ⑤ 可以搭配前面介绍的所有子句使用，比如排序、分组、筛选 语法格式： select 查询列表 from 表1，表2 where 等值条件 【and 筛选条件】 【group by 分组字段】 【having 分组后的筛选】 【order by 排序字段】 */ #查询员工名和对应的部门名 SELECT last_name,department_name FROM employees,departments WHERE employees.`department_id`=departments.`department_id`; /* ①提高语句的简洁度 ②区分多个重名的字段 注意：如果为表起了别名，则查询的字段就不能使用原来的表名去限定*/ #查询员工名、工种号、工种名 SELECT e.last_name,e.job_id,j.job_title FROM employees e,jobs j WHERE e.`job_id`=j.`job_id`; #案例：查询有奖金的员工名、部门名 SELECT last_name,department_name,commission_pct FROM employees e,departments d WHERE e.`department_id`=d.`department_id` AND e.`commission_pct` IS NOT NULL; #查询有奖金的每个部门的部门名和部门的领导编号和该部门的最低工资 SELECT department_name,d.`manager_id`,MIN(salary) FROM departments d,employees e WHERE d.`department_id`=e.`department_id` AND commission_pct IS NOT NULL GROUP BY department_name,d.`manager_id`; #三表连接：查询员工名、部门名和所在的城市 SELECT last_name,department_name,city FROM employees e,departments d,locations l WHERE e.`department_id`=d.`department_id` AND d.`location_id`=l.`location_id` AND city LIKE &#39;s%&#39; ORDER BY department_name DESC; #2、非等值连接 #查询员工的工资和工资级别 SELECT salary,grade_level FROM employees e,job_grades g WHERE salary BETWEEN g.`lowest_sal` AND g.`highest_sal` AND g.`grade_level`=&#39;A&#39;; #3、自连接 #案例：查询 员工名和上级的名称 SELECT e.employee_id,e.last_name,m.employee_id,m.last_name FROM employees e,employees m WHERE e.`manager_id`=m.`employee_id`; sql99语法#二、sql99语法 /* 语法： select 查询列表 from 表1 别名 【连接类型】 join 表2 别名 on 连接条件 【where 筛选条件】 【group by 分组】 【having 筛选条件】 【order by 排序列表】 分类：(连接类型) 内连接（★）：inner 外连接 左外(★):left 【outer】 右外(★)：right 【outer】 全外：full【outer】 交叉连接：cross */ 内连接： /* 语法： select 查询列表 from 表1 别名 inner join 表2 别名 on 连接条件; 分类： 等值 非等值 自连接 自然连结(NATURAL JOIN) 特点： 1 添加排序、分组、筛选 2 inner可以省略 3 筛选条件放在where后面，连接条件放在on后面，提高分离性，便于阅读 4 inner join连接和sql92语法中的等值连接效果是一样的，都是查询多表的交集 5 SELECT 子句中的列最好按照 表名.列名 的格式来使用. */ #等值连接 #查询员工名、部门名 SELECT last_name,department_name FROM departments d INNER JOIN employees e ON e.`department_id` = d.`department_id`; #查询员工名、部门名、工种名，并按部门名降序（添加三表连接） SELECT last_name,department_name,job_title FROM employees e INNER JOIN departments d ON e.`department_id`=d.`department_id` INNER JOIN jobs j ON e.`job_id` = j.`job_id` ORDER BY department_name DESC; #非等值连接 #查询员工的工资级别 SELECT salary,grade_level FROM employees e INNER JOIN job_grades g ON e.`salary` BETWEEN g.`lowest_sal` AND g.`highest_sal`; #查询工资级别的个数&gt;20的个数，并且按工资级别降序 SELECT COUNT(*),grade_level FROM employees e INNER JOIN job_grades g ON e.`salary` BETWEEN g.`lowest_sal` AND g.`highest_sal` GROUP BY grade_level HAVING COUNT(*)&gt;20 ORDER BY grade_level DESC; #自连接 #查询员工的名字、上级的名字 SELECT e.last_name,m.last_name FROM employees e JOIN employees m ON e.`manager_id`= m.`employee_id`; #查询姓名中包含字符k的员工的名字、上级的名字 SELECT e.last_name,m.last_name FROM employees e JOIN employees m ON e.`manager_id`= m.`employee_id` WHERE e.`last_name` LIKE &#39;%k%&#39;; # 自然连结并不是区别于内连结和外连结的第三种连结, 它其实是内连结的一种特例–当两个表进行自然连结时, 会按照两个表中都包含的列名来进行等值内连结, 此时无需使用 ON 来指定连接条件. # 把两个表的公共列(这里是 product_id, 可以有多个公共列)放在第一列, 然后按照两个表的顺序和表中列的顺序, 将两个表中的其他列都罗列出来。 SELECT * FROM shopproduct NATURAL JOIN Product 外连接 /* 应用场景：用于查询一个表中有，另一个表没有的记录 特点： 1、外连接的查询结果为主表中的所有记录 如果从表中有和它匹配的，则显示匹配的值 如果从表中没有和它匹配的，则显示null 外连接查询结果=内连接结果+主表中有而从表没有的记录 2、左外连接，left join左边的是主表 右外连接，right join右边的是主表 3、左外和右外交换两个表的顺序，可以实现同样的效果 4、全外连接=内连接的结果+表1中有但表2没有的+表2中有但表1没有的 */ #引入：查询男朋友 不在男神表的的女神名 #左外连接 SELECT b.*,bo.* FROM boys bo LEFT OUTER JOIN beauty b ON b.`boyfriend_id` = bo.`id` WHERE bo.`id` IS NULL; #查询哪个部门没有员工 #左外 SELECT d.*,e.employee_id FROM departments d LEFT OUTER JOIN employees e ON d.`department_id` = e.`department_id` WHERE e.`employee_id` IS NULL; #右外 SELECT d.*,e.employee_id FROM employees e RIGHT OUTER JOIN departments d ON d.`department_id` = e.`department_id` WHERE e.`employee_id` IS NULL; #全外 USE girls; SELECT b.*,bo.* FROM beauty b FULL OUTER JOIN boys bo ON b.`boyfriend_id` = bo.id; #交叉连接，实现效果和笛卡尔乘积一致 SELECT b.*,bo.* FROM beauty b CROSS JOIN boys bo; #sql92 和 sql99 /* 功能：sql99支持的较多 可读性：sql99实现连接条件和筛选条件的分离，可读性较高 */ 结合 WHERE 子句使用左连结 使用外连结从ShopProduct表和Product表中找出那些在某个商店库存少于50的商品及对应的商店.希望得到如下结果. 注意高压锅和圆珠笔两种商品在所有商店都无货, 所以也应该包括在内。按照”结合WHERE子句使用内连结”的思路, 我们很自然会写出如下代码 SELECT P.product_id ,P.product_name ,P.sale_price ,SP.shop_id ,SP.shop_name ,SP.quantity FROM Product AS P LEFT OUTER JOIN ShopProduct AS SP ON SP.product_id = P.product_id WHERE quantity&lt; 50 然而不幸的是, 得到的却是如下的结果: null值不能比较！ SELECT P.product_id ,P.product_name ,P.sale_price ,SP.shop_id ,SP.shop_name ,SP.quantity FROM Product AS P LEFT OUTER JOIN-- 先筛选quantity&lt;50的商品 (SELECT * FROM ShopProduct WHERE quantity &lt; 50 ) AS SP ON SP.product_id = P.product_id 子查询含义： 一条查询语句中又嵌套了另一条完整的select语句，其中被嵌套的select语句，称为子查询或内查询，在外面的查询语句，称为主查询或外查询。 分类： 按子查询出现的位置： select后面：仅仅支持标量子查询 from后面：支持表子查询 where或having后面：标量子查询（单行）、列子查询（多行）、行子查询 exists后面（相关子查询）：表子查询 按结果集的行列数不同： 标量子查询（结果集只有一行一列，单行子查询） 列子查询（结果集只有一列多行，多行子查询） 行子查询（结果集有一行多列，多列多行） 表子查询（结果集一般为多行多列） 特点： 子查询都放在小括号内 子查询一般放在条件的右侧 子查询可以放在from后面、select后面、where后面、having后面、exists后面，但一般放在条件的右侧 子查询优先于主查询执行，主查询使用了子查询的执行结果 子查询搭配： 单行子查询 (标量子查询) 结果集只有一行 一般搭配单行操作符使用：&gt; &lt; = &lt;&gt; &gt;= &lt;= 非法使用子查询的情况：a、子查询的结果为一组值b、子查询的结果为空 多行子查询 (列子查询) 结果集有多行 一般搭配多行操作符使用：any、all、in、not inin： 属于子查询结果中的任意一个就行 any和all往往可以用其他查询代替 子查询就是将用来定义视图的 SELECT 语句直接用于 FROM 子句当中。其中AS studentSum可以看作是子查询的名称，而且由于子查询是一次性的，所以子查询不会像视图那样保存在存储介质中， 而是在 SELECT 语句执行之后就消失了。 标量子查询#案例 谁的工资比 Abel 高? SELECT * FROM employees WHERE salary&gt;( SELECT salary FROM employees WHERE last_name = &#39;Abel&#39; ); #案例 返回job_id与141号员工相同，salary比143号员工多的员工 姓名，job_id 和工资 SELECT last_name,job_id,salary FROM employees WHERE job_id = ( SELECT job_id FROM employees WHERE employee_id = 141 ) AND salary&gt;( SELECT salary FROM employees WHERE employee_id = 143 ); #案例 查询最低工资大于50号部门最低工资的部门id和其最低工资 SELECT MIN(salary),department_id FROM employees GROUP BY department_id HAVING MIN(salary)&gt;( SELECT MIN(salary) FROM employees WHERE department_id = 50 ); #案例 查询每个部门的员工个数 SELECT d.*,( SELECT COUNT(*) FROM employees e WHERE e.department_id = d.`department_id` ) 个数 FROM departments d; #案例 查询员工号=102的部门名 /* select后面，仅仅支持标量子查询 */ SELECT ( SELECT department_name,e.department_id FROM departments d INNER JOIN employees e ON d.department_id=e.department_id WHERE e.employee_id=102 ) 部门名; 列子查询（多行子查询）#案例 返回location_id是1400或1700的部门中的所有员工姓名 SELECT last_name FROM employees WHERE department_id &lt;&gt;ALL( SELECT DISTINCT department_id FROM departments WHERE location_id IN(1400,1700) ); 行子查询（结果集一行多列或多行多列）#案例 查询员工编号最小并且工资最高的员工信息 #方法1 SELECT * FROM employees WHERE employee_id=( SELECT MIN(employee_id) FROM employees )AND salary=( SELECT MAX(salary) FROM employees ); #方法2 SELECT * FROM employees WHERE (employee_id,salary)=( SELECT MIN(employee_id),MAX(salary) FROM employees ); 表子查询/* from后面 将子查询结果充当一张表，要求必须起别名 */ #案例：查询每个部门的平均工资的工资等级 SELECT ag_dep.*,g.`grade_level` FROM ( SELECT AVG(salary) ag,department_id FROM employees GROUP BY department_id ) ag_dep INNER JOIN job_grades g ON ag_dep.ag BETWEEN lowest_sal AND highest_sal; ####################################### exists关键字： 语法： exists(完整的查询语句) 结果： 1或0 ####################################### #案例 查询有员工的部门名 #in SELECT department_name FROM departments d WHERE d.`department_id` IN( SELECT department_id FROM employees ); #exists SELECT department_name FROM departments d WHERE EXISTS( SELECT * FROM employees e WHERE d.`department_id`=e.`department_id` ); #案例 查询没有女朋友的男神信息 #in SELECT bo.* FROM boys bo WHERE bo.id NOT IN( SELECT boyfriend_id FROM beauty ); #exists SELECT bo.* FROM boys bo WHERE NOT EXISTS( SELECT boyfriend_id FROM beauty b WHERE bo.`id`=b.`boyfriend_id` ); 关联子查询关联子查询就是通过一些标志将内外两层的查询连接起来起到过滤数据的目的. SELECT product_type, product_name, sale_price FROM product AS p1 WHERE sale_price &gt; (SELECT AVG(sale_price) FROM product AS p2 WHERE p1.product_type = p2.product_type GROUP BY product_type); 关联子查询博客 首先执行不带WHERE的主查询 从主查询的product _type先取第一个值=‘衣服’，通过WHERE P1.product_type = P2.product_type传入子查询， 从子查询得到的结果AVG(sale_price)=2500，返回主查询 将子查询结果再与主查询结合执行完整的SQL语句 product _type取第二个值，得到整个语句的第二结果，依次类推 分页查询/* 应用场景：当要显示的数据，一页显示不全，需要分页提交sql请求 语法： select 查询列表 from 表 【join type join 表2 on 连接条件 where 筛选条件 group by 分组字段 having 分组后的筛选 order by 排序的字段】 limit 【offset,】size; offset要显示条目的起始索引（起始索引从0开始） size 要显示的条目个数 特点： ①limit语句放在查询语句的最后 ②公式 要显示的页数 page，每页的条目数size select 查询列表 from 表 limit (page-1)*size,size; size=10 page 1 0 2 10 3 20 */ #案例 查询第11条——第25条 SELECT * FROM employees LIMIT 10,15; #案例 有奖金的员工信息，并且工资较高的前10名显示出来 SELECT * FROM employees WHERE commission_pct IS NOT NULL ORDER BY salary DESC LIMIT 10; 联合查询/* union：将多条查询语句的结果合并成一个结果 语法： select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】 select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】 select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】 ..... select 字段|常量|表达式|函数 【from 表】 【where 条件】 应用场景： 要查询的结果来自于多个表，且多个表没有直接的连接关系，但查询的信息一致时。 对于同一个表的两个不同的筛选结果集, 使用 UNION 对两个结果集取并集, 和把两个子查询的筛选条件用 OR 谓词连接, 会得到相同的结果, 但倘若要将两个不同的表中的结果合并在一起, 就不得不使用 UNION 了. 而且, 即便是对于同一张表, 有时也会出于查询效率方面的因素来使用 UNION. 特点：★ 1、要求多条查询语句的查询列数是一致的！ 2、要求多条查询语句的查询的每一列的类型和顺序最好一致 3、union关键字默认去重，如果使用union all 可以包含重复项 */ # 引入的案例：查询部门编号&gt;90或邮箱包含a的员工信息 SELECT * FROM employees WHERE email LIKE &#39;%a%&#39; UNION SELECT * FROM employees WHERE department_id&gt;90; #案例 查询中国用户中男性的信息以及外国用户中年男性的用户信息 SELECT id,cname FROM t_ca WHERE csex=&#39;男&#39; UNION ALL SELECT t_id,tname FROM t_ua WHERE tGender=&#39;male&#39;;","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"DML语言","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL03-DML.html","text":"DML语言数据操作语言： 插入：insert 修改：update 删除：delete 插入经典的插入 /* 语法： insert into 表名(列名,...) values(值1,...); */ #1.插入的值的类型要与列的类型一致或兼容 INSERT INTO beauty(id,NAME,sex,borndate,phone,photo,boyfriend_id) VALUES(13,&#39;唐艺昕&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2); #2.不可以为null的列必须插入值。可以为null的列如何插入值？ #方式一： INSERT INTO beauty(id,NAME,sex,borndate,phone,photo,boyfriend_id) VALUES(13,&#39;唐艺昕&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2); #方式二： INSERT INTO beauty(id,NAME,sex,phone) VALUES(15,&#39;娜扎&#39;,&#39;女&#39;,&#39;1388888888&#39;); #3.列的顺序是否可以调换，对应即可 INSERT INTO beauty(NAME,sex,id,phone) VALUES(&#39;蒋欣&#39;,&#39;女&#39;,16,&#39;110&#39;); #4.列数和值的个数必须一致 INSERT INTO beauty(NAME,sex,id,phone) VALUES(&#39;关晓彤&#39;,&#39;女&#39;,17,&#39;110&#39;); #5.可以省略列名，默认所有列，而且列的顺序和表中列的顺序一致 INSERT INTO beauty VALUES(18,&#39;张飞&#39;,&#39;男&#39;,NULL,&#39;119&#39;,NULL,NULL); 方式二：插入 /* 语法： insert into 表名 set 列名=值,列名=值,... */ INSERT INTO beauty SET id=19,NAME=&#39;刘涛&#39;,phone=&#39;999&#39;; 两种方式比较： #1、方式一支持插入多行,方式二不支持 INSERT INTO beauty VALUES(23,&#39;唐艺昕1&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2) ,(24,&#39;唐艺昕2&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2) ,(25,&#39;唐艺昕3&#39;,&#39;女&#39;,&#39;1990-4-23&#39;,&#39;1898888888&#39;,NULL,2); #2、方式一支持子查询，方式二不支持 INSERT INTO beauty(id,NAME,phone) SELECT 26,&#39;宋茜&#39;,&#39;11809866&#39;; # 将查询后的结果插入到表中 INSERT INTO beauty(id,NAME,phone) SELECT id,boyname,&#39;1234567&#39; FROM boys WHERE id&lt;3; 修改/* 1.修改单表的记录★ 语法： update 表名 set 列=新值,列=新值,... where 筛选条件; 2.修改多表的记录【补充】 语法： sql92语法： update 表1 别名,表2 别名 set 列=值,... where 连接条件 and 筛选条件; sql99语法： update 表1 别名 inner|left|right join 表2 别名 on 连接条件 set 列=值,... where 筛选条件; */ #1.修改单表的记录 #案例 修改beauty表中姓唐的女神的电话为13899888899 UPDATE beauty SET phone = &#39;13899888899&#39; WHERE NAME LIKE &#39;唐%&#39;; #案例 修改boys表中id好为2的名称为张飞，魅力值 10 UPDATE boys SET boyname=&#39;张飞&#39;,usercp=10 WHERE id=2; #2.修改多表的记录 #案例 修改张无忌的女朋友的手机号为114 UPDATE boys bo INNER JOIN beauty b ON bo.`id`=b.`boyfriend_id` SET b.`phone`=&#39;119&#39;,bo.`userCP`=1000 WHERE bo.`boyName`=&#39;张无忌&#39;; #案例 修改没有男朋友的女神的男朋友编号都为2号 UPDATE boys bo RIGHT JOIN beauty b ON bo.`id`=b.`boyfriend_id` SET b.`boyfriend_id`=2 WHERE bo.`id` IS NULL; 删除/* 方式一：delete 语法： 1、单表的删除【★】 delete from 表名 where 筛选条件 2、多表的删除【补充】 2.1 sql92语法： delete 表1的别名,表2的别名 from 表1 别名,表2 别名 where 连接条件 and 筛选条件; 2.2 sql99语法： delete 表1的别名,表2的别名 from 表1 别名 inner|left|right join 表2 别名 on 连接条件 where 筛选条件; 方式二：truncate 语法：truncate table 表名; # 删除整个表 */ #方式一：delete #1.单表的删除 #案例：删除手机号以9结尾的女神信息 DELETE FROM beauty WHERE phone LIKE &#39;%9&#39;; #2.多表的删除 #案例：删除张无忌的女朋友的信息 DELETE b FROM beauty b INNER JOIN boys bo ON b.`boyfriend_id` = bo.`id` WHERE bo.`boyName`=&#39;张无忌&#39;; #案例：删除黄晓明的信息以及他女朋友的信息 DELETE b,bo FROM beauty b INNER JOIN boys bo ON b.`boyfriend_id`=bo.`id` WHERE bo.`boyName`=&#39;黄晓明&#39;; #方式二：truncate语句 #案例：将魅力值&gt;100的男神信息删除 TRUNCATE TABLE boys ; # delete 和 truncate【面试题★】 /* 1.delete 可以加where 条件，truncate不能加 2.truncate删除，效率高一丢丢 3.假如要删除的表中有自增长列， 如果用delete删除后，再插入数据，自增长列的值从断点开始， 而truncate删除后，再插入数据，自增长列的值从1开始。 4.truncate删除没有返回值，delete删除有返回值 5.truncate删除不能回滚，delete删除可以回滚. 相比drop``/``delete，truncate用来清除数据时，速度最快。 */ DELETE FROM boys; TRUNCATE TABLE boys; INSERT INTO boys (boyname,usercp) VALUES(&#39;张飞&#39;,100),(&#39;刘备&#39;,100),(&#39;关云长&#39;,100);","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"DDL语言","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL04-DDL.html","text":"DDL语言库和表的管理 一、库的管理：创建、修改、删除 二、表的管理：创建、修改、删除 创建： create；修改： alter；删除： drop。 库的管理 #1、库的创建 /* 语法： create database [if not exists] 库名; */ #案例：创建库Books CREATE DATABASE IF NOT EXISTS books ; #2、库的修改 RENAME DATABASE books TO 新库名; #更改库的字符集 ALTER DATABASE books CHARACTER SET gbk; #3、库的删除 DROP DATABASE IF EXISTS books; 表的管理 #1.表的创建 ★ /* 语法： create table 表名( 列名 列的类型【(长度) 约束】, 列名 列的类型【(长度) 约束】, 列名 列的类型【(长度) 约束】, ... 列名 列的类型【(长度) 约束】 ) */ #案例：创建表Book CREATE TABLE book( id INT,#编号 bName VARCHAR(20),#图书名 price DOUBLE,#价格 authorId INT,#作者编号 publishDate DATETIME#出版日期 ); #案例：创建表author CREATE TABLE IF NOT EXISTS author( id INT, au_name VARCHAR(20), nation VARCHAR(10) ) #2.表的修改 /* 语法 alter table 表名 add|drop|modify|change column 列名 【列类型 约束】; */ #①修改列名 ALTER TABLE book CHANGE COLUMN publishdate pubDate DATETIME; #②修改列的类型或约束 ALTER TABLE book MODIFY COLUMN pubdate TIMESTAMP; #③添加新列 ALTER TABLE author ADD COLUMN annual DOUBLE; #④删除列 ALTER TABLE book_author DROP COLUMN annual; #⑤修改表名 ALTER TABLE author RENAME TO book_author; #3.表的删除 DROP TABLE IF EXISTS book_author; SHOW TABLES; #通用的写法： DROP DATABASE IF EXISTS 旧库名; CREATE DATABASE 新库名; DROP TABLE IF EXISTS 旧表名; CREATE TABLE 表名(); #4.表的复制 INSERT INTO author VALUES (1,&#39;村上春树&#39;,&#39;日本&#39;), (2,&#39;莫言&#39;,&#39;中国&#39;), (3,&#39;冯唐&#39;,&#39;中国&#39;), (4,&#39;金庸&#39;,&#39;中国&#39;); #1.仅仅复制表的结构，没有数据 CREATE TABLE copy LIKE author; #2.复制表的结构+数据 CREATE TABLE copy2 SELECT * FROM author; #只复制部分数据 CREATE TABLE copy3 SELECT id,au_name FROM author WHERE nation=&#39;中国&#39;; #仅仅复制某些字段，不复制数据 CREATE TABLE copy4 SELECT id,au_name FROM author WHERE 0; MySQL数据类型 数值型： 整型 小数：定点数、浮点数 字符型： 较短的文本：char、varchar 较长的文本：text、blob（较长的二进制数据） 日期型： # 一、整型 /* 分类： tinyint、smallint、mediumint、int/integer、bigint 1 2 3 4 8 特点： ① 如果不设置无符号还是有符号，默认是有符号，如果想设置无符号，需要添加unsigned关键字 ② 如果插入的数值超出了整型的范围,会报out of range异常，然后会插入临界值 ③ 如果不设置长度，会有默认的长度，长度代表了显示的默认宽度，如果不够会用0在左边填充，但必须搭配zerofill使用，强制为无符号整型！如果显示长度大于该值，但不会截断。 */ # 1.如何设置无符号和有符号 DROP TABLE IF EXISTS tab_int; CREATE TABLE tab_int( t1 INT(7) ZEROFILL, t2 INT(7) ZEROFILL ); DESC tab_int; INSERT INTO tab_int VALUES(-123456); INSERT INTO tab_int VALUES(-123456,-123456); INSERT INTO tab_int VALUES(2147483648,4294967296); INSERT INTO tab_int VALUES(123,123); SELECT * FROM tab_int; # 二、小数 /* 分类： 1.浮点型 float(M,D) double(M,D) 2.定点型 dec(M，D)或decimal(M,D) 特点： ① M：整数部位+小数部位 D：小数部位 如果超过范围，则插入临界值 ② M和D都可以省略 如果是decimal，则M默认为10，D默认为0 如果是float和double，则会根据插入的数值的精度来决定精度 ③ 定点型的精确度较高，如果要求插入数值的精度较高如货币运算等则考虑使用 */ #测试M和D DROP TABLE tab_float; CREATE TABLE tab_float( f1 FLOAT, f2 DOUBLE, f3 DECIMAL ); SELECT * FROM tab_float; DESC tab_float; INSERT INTO tab_float VALUES(123.4523,123.4523,123.4523); INSERT INTO tab_float VALUES(123.456,123.456,123.456); INSERT INTO tab_float VALUES(123.4,123.4,123.4); INSERT INTO tab_float VALUES(1523.4,1523.4,1523.4); #原则： /* 所选择的类型越简单越好，能保存数值的类型越小越好 */ #三、字符型 /* 较短的文本： char varchar 其他： binary和varbinary 用于保存较短的二进制 enum 用于保存枚举 set 用于保存集合 较长的文本： text blob(较大的二进制) 特点： 写法 M的意思 特点 空间的耗费 效率 char char(M) 最大的字符数，可以省略，默认为1 固定长度的字符 比较耗费 高 varchar varchar(M) 最大的字符数，不可以省略 可变长度的字符 比较节省 低 */ CREATE TABLE tab_char( c1 ENUM(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;) ); INSERT INTO tab_char VALUES(&#39;a&#39;); INSERT INTO tab_char VALUES(&#39;b&#39;); INSERT INTO tab_char VALUES(&#39;c&#39;); INSERT INTO tab_char VALUES(&#39;m&#39;); # 失败,内容为空 INSERT INTO tab_char VALUES(&#39;A&#39;); SELECT * FROM tab_set; CREATE TABLE tab_set( s1 SET(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;) ); INSERT INTO tab_set VALUES(&#39;a&#39;); INSERT INTO tab_set VALUES(&#39;A,B&#39;); INSERT INTO tab_set VALUES(&#39;a,c,d&#39;); # 四、日期型 /* 分类： date 只保存日期 time 只保存时间 year 只保存年 datetime 保存日期+时间 timestamp 保存日期+时间 特点： 字节 范围 时区等的影响 datetime 8 1000——9999 不受 timestamp 4 1970-2038 受 */ CREATE TABLE tab_date( t1 DATETIME, t2 TIMESTAMP # 当前时区时间 ); INSERT INTO tab_date VALUES(NOW(),NOW()); SELECT * FROM tab_date; SHOW VARIABLES LIKE &#39;time_zone&#39;; SET time_zone=&#39;+9:00&#39;; 常见约束含义：一种限制，用于限制表中的数据，为了保证表中的数据的准确和可靠性 分类：六大约束 NOT NULL：非空，用于保证该字段的值不能为空，比如姓名、学号等 DEFAULT：默认，用于保证该字段有默认值，比如性别 PRIMARY KEY：主键，用于保证该字段的值具有唯一性，并且非空，比如学号、员工编号等 UNIQUE：唯一，用于保证该字段的值具有唯一性，可以为空，比如座位号 CHECK：检查约束【mysql中不支持】，比如年龄、性别 FOREIGN KEY：外键，用于限制两个表的关系，用于保证该字段的值必须来自于主表的关联列的值。在从表添加外键约束，用于引用主表中某列的值，比如学生表的专业编号，员工表的部门编号，员工表的工种编号。 添加约束的时机：1.创建表时；2.修改表时 约束的添加分类： 列级约束：六大约束语法上都支持，但外键约束没有效果 表级约束：除了非空、默认，其他的都支持 CREATE TABLE 表名( 字段名 字段类型 列级约束, 字段名 字段类型, 表级约束 ); 主键和唯一的对比： ​ 保证唯一性 是否允许为空 一个表中可以有多少个 是否允许组合 主键 √ × 至多有1个 √，但不推荐 唯一 √ √ 可以有多个 √，但不推荐 外键： 要求在从表设置外键关系 从表的外键列的类型和主表的关联列的类型要求一致或兼容，名称无要求 ==主表的关联列必须是一个key（一般是主键或唯一）== 插入数据时，先插入主表，再插入从表；删除数据时，先删除从表，再删除主表 #一、创建表时添加约束 #1.添加列级约束 /* 语法： 直接在字段名和类型后面追加 约束类型即可。 只支持：默认、非空、主键、唯一 */ CREATE TABLE major( id INT PRIMARY KEY, majorName VARCHAR(20) ); USE students; DROP TABLE stuinfo; CREATE TABLE stuinfo( id INT PRIMARY KEY,#主键 stuName VARCHAR(20) NOT NULL UNIQUE,#非空 gender CHAR(1) CHECK(gender=&#39;男&#39; OR gender =&#39;女&#39;),#检查 seat INT UNIQUE,#唯一 age INT DEFAULT 18,#默认约束 majorId INT REFERENCES major(id)#外键 ); #查看stuinfo中的所有索引，包括主键、外键、唯一 SHOW INDEX FROM stuinfo; #2.添加表级约束 /* 语法：在各个字段的最下面 【constraint 约束名】 约束类型(字段名) */ DROP TABLE IF EXISTS stuinfo; CREATE TABLE stuinfo( id INT, stuname VARCHAR(20), gender CHAR(1), seat INT, age INT, majorid INT, CONSTRAINT pk PRIMARY KEY(id),#主键 CONSTRAINT uq UNIQUE(seat),#唯一键 CONSTRAINT ck CHECK(gender =&#39;男&#39; OR gender = &#39;女&#39;),#检查 CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id)#外键 ); SHOW INDEX FROM stuinfo; # 通用的写法：★ CREATE TABLE IF NOT EXISTS stuinfo( id INT PRIMARY KEY, stuname VARCHAR(20) NOT NULL, sex CHAR(1), age INT DEFAULT 18, seat INT UNIQUE, majorid INT, CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id) ); # 二、修改表时添加约束 /* 1、添加列级约束 alter table 表名 modify column 字段名 字段类型 新约束; 2、添加表级约束 alter table 表名 add 【constraint 约束名】 约束类型(字段名) 【外键的引用】; */ DROP TABLE IF EXISTS stuinfo; CREATE TABLE stuinfo( id INT, stuname VARCHAR(20), gender CHAR(1), seat INT, age INT, majorid INT ) DESC stuinfo; #1.添加非空约束 ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NOT NULL; #2.添加默认约束 ALTER TABLE stuinfo MODIFY COLUMN age INT DEFAULT 18; #3.添加主键 #①列级约束 ALTER TABLE stuinfo MODIFY COLUMN id INT PRIMARY KEY; #②表级约束 ALTER TABLE stuinfo ADD PRIMARY KEY(id); #4.添加唯一 #①列级约束 ALTER TABLE stuinfo MODIFY COLUMN seat INT UNIQUE; #②表级约束 ALTER TABLE stuinfo ADD UNIQUE(seat); #5.添加外键 ALTER TABLE stuinfo ADD CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id); #三、修改表时删除约束 #1.删除非空约束 ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NULL; #2.删除默认约束 ALTER TABLE stuinfo MODIFY COLUMN age INT ; #3.删除主键 ALTER TABLE stuinfo DROP PRIMARY KEY; #4.删除唯一 ALTER TABLE stuinfo DROP INDEX seat; #5.删除外键 ALTER TABLE stuinfo DROP FOREIGN KEY fk_stuinfo_major; SHOW INDEX FROM stuinfo; # 二、修改表时添加约束 /* 1、添加列级约束 alter table 表名 modify column 字段名 字段类型 新约束; 2、添加表级约束 alter table 表名 add 【constraint 约束名】 约束类型(字段名) 【外键的引用】; */ DROP TABLE IF EXISTS stuinfo; CREATE TABLE stuinfo( id INT, stuname VARCHAR(20), gender CHAR(1), seat INT, age INT, majorid INT ) DESC stuinfo; #1.添加非空约束 ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NOT NULL; #2.添加默认约束 ALTER TABLE stuinfo MODIFY COLUMN age INT DEFAULT 18; #3.添加主键 #①列级约束写法 ALTER TABLE stuinfo MODIFY COLUMN id INT PRIMARY KEY; #②表级约束写法 ALTER TABLE stuinfo ADD PRIMARY KEY(id); #4.添加唯一 #①列级约束 ALTER TABLE stuinfo MODIFY COLUMN seat INT UNIQUE; #②表级约束 ALTER TABLE stuinfo ADD UNIQUE(seat); #5.添加外键 ALTER TABLE stuinfo ADD CONSTRAINT fk_stuinfo_major FOREIGN KEY(majorid) REFERENCES major(id); #三、修改表时删除约束 #1.删除非空约束 ALTER TABLE stuinfo MODIFY COLUMN stuname VARCHAR(20) NULL; #2.删除默认约束 ALTER TABLE stuinfo MODIFY COLUMN age INT ; #3.删除主键 ALTER TABLE stuinfo DROP PRIMARY KEY; #4.删除唯一 ALTER TABLE stuinfo DROP INDEX seat; #5.删除外键 ALTER TABLE stuinfo DROP FOREIGN KEY fk_stuinfo_major; SHOW INDEX FROM stuinfo; /* 位置 支持的约束类型 是否可以起约束名 列级约束： 列的后面 语法都支持，但外键没有效果 不可以 表级约束： 所有列的下面 默认和非空不支持，其他支持 可以（主键没有效果） */ 标识列又称为自增长列，含义：可以不用手动的插入值，系统提供默认的序列值。 特点： 标识列必须和主键搭配吗？不一定，但要求是一个key 一个表可以有几个标识列？至多一个！ 标识列的类型只能是数值型 标识列可以通过 SET auto_increment_increment=3, 设置步长。可以通过手动插入第一个值，设置起始值. # 创建表时设置标识列 DROP TABLE IF EXISTS tab_identity; CREATE TABLE tab_identity( id INT , NAME FLOAT UNIQUE AUTO_INCREMENT, seat INT ); TRUNCATE TABLE tab_identity; INSERT INTO tab_identity(id,NAME) VALUES(NULL,&#39;john&#39;); INSERT INTO tab_identity(NAME) VALUES(&#39;lucy&#39;); SELECT * FROM tab_identity; SHOW VARIABLES LIKE &#39;%auto_increment%&#39;; SET auto_increment_increment=3;","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"TCL语言","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL05-TCL.html","text":"TCL语言Transaction Control Language 事务控制语言 事务：一个或一组sql语句组成一个执行单元，这个执行单元要么全部执行，要么全部不执行。 事务的特性：ACID 原子性：一个事务不可再分割，要么都执行要么都不执行 一致性：一个事务执行会使数据从一个一致状态切换到另外一个一致状态 隔离性：一个事务的执行不受其他事务的干扰 持久性：一个事务一旦提交，则会永久的改变数据库的数据. 事务的创建 隐式事务：事务没有明显的开启和结束的标记比如insert、update、delete语句 显式事务：事务具有明显的开启和结束的标记前提：必须先设置自动提交功能为禁用：set autocommit=0; 步骤1：开启事务 set autocommit=0; start transaction;可选的 步骤2：编写事务中的sql语句(select insert update delete) ​ 语句1; ​ 语句2; ​ … 步骤3：结束事务 commit;提交事务 或 rollback;回滚事务 savepoint 节点名;设置保存点 DML语句是insert ,update, delete —-&gt;这个是要COMMIT才能存储在ORACLE数据库里 DDL语句是系统自动提交的，不需要手动COMMIT。这是修改表结构，执行完成，就生效了。 脏读: 对于两个事务T1, T2, T1 读取了已经被T2 更新但还没有被提交的字段. 之后, 若T2 回滚, T1读取的内容就是临时且无效的. 不可重复读: 对于两个事务T1, T2, T1 读取了一个字段, 然后T2 更新了该字段. 之后, T1再次读取同一个字段, 值就不同了. 幻读: 对于两个事务T1, T2, T1 从一个表中读取了一个字段, 然后T2 在该表中插入了一些新的行. 之后, 如果T1 再次读取同一个表, 就会多出几行. 事务的隔离级别： ​ 脏读 不可重复读 幻读 read uncommitted：√ √ √ read committed： × √ √ repeatable read： × × √ serializable × × × 每启动一个mysql 程序, 就会获得一个单独的数据库连接. 每个数据库连接都有一个全局变量@@tx_isolation, 表示当前的事务隔离级别. 查看当前的隔离级别: SELECT @@tx_isolation; 设置当前mySQL 连接的隔离级别:set transaction isolation level read committed; 设置数据库系统的全局的隔离级别:set globaltransaction isolation level read committed; #开启事务 SET autocommit=0; START TRANSACTION; #编写一组事务的语句 UPDATE account SET balance = 1000 WHERE username=&#39;张无忌&#39;; UPDATE account SET balance = 1000 WHERE username=&#39;赵敏&#39;; #结束事务 ROLLBACK; #commit; SELECT * FROM account; #2.演示事务对于delete和truncate的处理的区别 SET autocommit=0; START TRANSACTION; DELETE FROM account; ROLLBACK; #3.演示savepoint 的使用 SET autocommit=0; START TRANSACTION; DELETE FROM account WHERE id=25; SAVEPOINT a;#设置保存点 DELETE FROM account WHERE id=28; ROLLBACK TO a;#回滚到保存点 SELECT * FROM account;","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"JDBC概述&数据库连接方式","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/04-JDBC/JDBC-01 概览&连接数据库方式.html","text":"web技术概览 第1章：JDBC概述1.1 数据的持久化 持久化(persistence)：把数据保存到可掉电式存储设备中以供之后使用。大多数情况下，特别是企业级应用，数据持久化意味着将内存中的数据保存到硬盘上加以”固化”，而持久化的实现过程大多通过各种关系数据库来完成。 持久化的主要应用是将内存中的数据存储在关系型数据库中，当然也可以存储在磁盘文件、XML数据文件中。 1.2 Java中的数据存储技术 在Java中，数据库存取技术可分为如下几类： JDBC直接访问数据库 JDO (Java Data Object )技术 第三方O/R工具，如Hibernate, Mybatis 等 JDBC是java访问数据库的基石，JDO、Hibernate、MyBatis等只是更好的封装了JDBC。 1.3 JDBC介绍 JDBC(Java Database Connectivity)是一个独立于特定数据库管理系统、通用的SQL数据库存取和操作的公共接口（一组API），定义了用来访问数据库的标准Java类库，（java.sql,javax.sql）使用这些类库可以以一种标准的方法、方便地访问数据库资源。 JDBC为访问不同的数据库提供了一种统一的途径，为开发者屏蔽了一些细节问题。 JDBC的目标是使Java程序员使用JDBC可以连接任何提供了JDBC驱动程序的数据库系统，这样就使得程序员无需对特定的数据库系统的特点有过多的了解，从而大大简化和加快了开发过程。 如果没有JDBC，那么Java程序访问数据库时是这样的： 有了JDBC，Java程序访问数据库时是这样的： 总结如下： 1.4 JDBC体系结构 JDBC接口（API）包括两个层次： 面向应用的API：Java API，抽象接口，供应用程序开发人员使用（连接数据库，执行SQL语句，获得结果）。 面向数据库的API：Java Driver API，供开发商开发数据库驱动程序用。 JDBC是sun公司提供一套用于数据库操作的接口，java程序员只需要面向这套接口编程即可。 不同的数据库厂商，需要针对这套接口，提供不同实现。不同的实现的集合，即为不同数据库的驱动。 ————面向接口编程 1.5 JDBC程序编写步骤 补充：ODBC(Open Database Connectivity，开放式数据库连接)，是微软在Windows平台下推出的。使用者在程序中只需要调用ODBC API，由 ODBC 驱动程序将调用转换成为对特定的数据库的调用请求。 第2章：获取数据库连接2.1 要素一：Driver接口实现类2.1.1 Driver接口介绍 java.sql.Driver 接口是所有 JDBC 驱动程序需要实现的接口。这个接口是提供给数据库厂商使用的，不同数据库厂商提供不同的实现。 在程序中不需要直接去访问实现了 Driver 接口的类，而是由驱动程序管理器类(java.sql.DriverManager)去调用这些Driver实现。 Oracle的驱动：oracle.jdbc.driver.OracleDriver mySql的驱动： com.mysql.jdbc.Driver 将上述jar包拷贝到Java工程的一个目录中，习惯上新建一个lib文件夹。 在驱动jar上右键—&gt;Build Path—&gt;Add to Build Path 注意：如果是Dynamic Web Project（动态的web项目）话，则是把驱动jar放到WebContent（有的开发工具叫WebRoot）目录中的WEB-INF目录中的lib目录下即可 2.1.2 加载与注册JDBC驱动 加载驱动：加载 JDBC 驱动需调用 Class 类的静态方法 forName()，向其传递要加载的 JDBC 驱动的类名 Class.forName(“com.mysql.jdbc.Driver”); 注册驱动：DriverManager 类是驱动程序管理器类，负责管理驱动程序 使用DriverManager.registerDriver(com.mysql.jdbc.Driver)来注册驱动 通常不用显式调用 DriverManager 类的 registerDriver() 方法来注册驱动程序类的实例，因为 Driver 接口的驱动程序类都包含了静态代码块，在这个静态代码块中，会调用 DriverManager.registerDriver() 方法来注册自身的一个实例。下图是MySQL的Driver实现类的源码： 2.2 要素二：URL JDBC URL 用于标识一个被注册的驱动程序，驱动程序管理器通过这个 URL 选择正确的驱动程序，从而建立到数据库的连接。 JDBC URL的标准由三部分组成，各部分间用冒号分隔。 jdbc:子协议:子名称 协议：JDBC URL中的协议总是jdbc 子协议：子协议用于标识一个数据库驱动程序 子名称：一种标识数据库的方法。子名称可以依不同的子协议而变化，用子名称的目的是为了定位数据库提供足够的信息。包含主机名(对应服务端的ip地址)，端口号，数据库名 举例： 几种常用数据库的 JDBC URL MySQL的连接URL编写方式： jdbc:mysql://主机名称:mysql服务端口号/数据库名称?参数=值&amp;参数=值 jdbc:mysql://localhost:3306/atguigu jdbc:mysql://localhost:3306/atguigu?useUnicode=true&amp;characterEncoding=utf8（如果JDBC程序与服务器端的字符集不一致，会导致乱码，那么可以通过参数指定服务器端的字符集） jdbc:mysql://localhost:3306/atguigu?user=root&amp;password=123456 Oracle 9i的连接URL编写方式： jdbc:oracle:thin:@主机名称:oracle服务端口号:数据库名称 jdbc:oracle:thin:@localhost:1521:atguigu SQLServer的连接URL编写方式： jdbc:sqlserver://主机名称:sqlserver服务端口号:DatabaseName=数据库名称 jdbc:sqlserver://localhost:1433:DatabaseName=atguigu 2.3 要素三：用户名和密码 user,password可以用“属性名=属性值”方式告诉数据库 可以调用 DriverManager 类的 getConnection() 方法建立到数据库的连接 2.4 数据库连接方式举例2.4.1 连接方式一 @Test public void testConnection1() { try { //1.提供java.sql.Driver接口实现类的对象 Driver driver = null; driver = new com.mysql.jdbc.Driver(); //2.提供url，指明具体操作的数据 String url = &quot;jdbc:mysql://localhost:3306/test&quot;; //3.提供Properties的对象，指明用户名和密码 Properties info = new Properties(); info.setProperty(&quot;user&quot;, &quot;root&quot;); info.setProperty(&quot;password&quot;, &quot;abc123&quot;); //4.调用driver的connect()，获取连接 Connection conn = driver.connect(url, info); System.out.println(conn); } catch (SQLException e) { e.printStackTrace(); } } 说明：上述代码中显式出现了第三方数据库的API 2.4.2 连接方式二 @Test public void testConnection2() { try { //1.实例化Driver String className = &quot;com.mysql.jdbc.Driver&quot;; Class clazz = Class.forName(className); Driver driver = (Driver) clazz.newInstance(); //2.提供url，指明具体操作的数据 String url = &quot;jdbc:mysql://localhost:3306/test&quot;; //3.提供Properties的对象，指明用户名和密码 Properties info = new Properties(); info.setProperty(&quot;user&quot;, &quot;root&quot;); info.setProperty(&quot;password&quot;, &quot;abc123&quot;); //4.调用driver的connect()，获取连接 Connection conn = driver.connect(url, info); System.out.println(conn); } catch (Exception e) { e.printStackTrace(); } } 说明：相较于方式一，这里使用反射实例化Driver，不在代码中体现第三方数据库的API。体现了面向接口编程思想。 2.4.3 连接方式三 @Test public void testConnection3() { try { //1.数据库连接的4个基本要素： String url = &quot;jdbc:mysql://localhost:3306/test&quot;; String user = &quot;root&quot;; String password = &quot;abc123&quot;; String driverName = &quot;com.mysql.jdbc.Driver&quot;; //2.实例化Driver Class clazz = Class.forName(driverName); Driver driver = (Driver) clazz.newInstance(); //3.注册驱动 DriverManager.registerDriver(driver); //4.获取连接 Connection conn = DriverManager.getConnection(url, user, password); System.out.println(conn); } catch (Exception e) { e.printStackTrace(); } } 说明：使用DriverManager实现数据库的连接。体会获取连接必要的4个基本要素。 2.4.4 连接方式四 @Test public void testConnection4() { try { //1.数据库连接的4个基本要素： String url = &quot;jdbc:mysql://localhost:3306/test&quot;; String user = &quot;root&quot;; String password = &quot;abc123&quot;; String driverName = &quot;com.mysql.jdbc.Driver&quot;; //2.加载驱动 （①实例化Driver ②注册驱动） Class.forName(driverName); //Driver driver = (Driver) clazz.newInstance(); //3.注册驱动 //DriverManager.registerDriver(driver); /* 可以注释掉上述代码的原因，是因为在mysql的Driver类中声明有： static { try { DriverManager.registerDriver(new Driver()); } catch (SQLException var1) { throw new RuntimeException(&quot;Can&#39;t register driver!&quot;); } } */ //3.获取连接 Connection conn = DriverManager.getConnection(url, user, password); System.out.println(conn); } catch (Exception e) { e.printStackTrace(); } } 说明：不必显式的注册驱动了。因为在DriverManager的源码中已经存在静态代码块，实现了驱动的注册。 2.4.5 连接方式五(最终版) @Test public void testConnection5() throws Exception { //1.加载配置文件 InputStream is = ConnectionTest.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties pros = new Properties(); pros.load(is); //2.读取配置信息 String user = pros.getProperty(&quot;user&quot;); String password = pros.getProperty(&quot;password&quot;); String url = pros.getProperty(&quot;url&quot;); String driverClass = pros.getProperty(&quot;driverClass&quot;); //3.加载驱动 Class.forName(driverClass); //4.获取连接 Connection conn = DriverManager.getConnection(url,user,password); System.out.println(conn); } 其中，配置文件声明在工程的src目录下：【jdbc.properties】 user=root password=abc123 url=jdbc:mysql://localhost:3306/test driverClass=com.mysql.jdbc.Driver 说明：使用配置文件的方式保存配置信息，在代码中加载配置文件 使用配置文件的好处： ①实现了代码和数据的分离，如果需要修改配置信息，直接在配置文件中修改，不需要深入代码②如果修改了配置信息，省去重新编译的过程。","tags":[{"name":"JDBC","slug":"JDBC","permalink":"https://mtcai.github.io/tags/JDBC/"}]},{"title":"CRUD","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/04-JDBC/JDBC-02 CRUD.html","text":"使用PreparedStatement实现CRUD操作操作和访问数据库 数据库连接被用于向数据库服务器发送命令和 SQL 语句，并接受数据库服务器返回的结果。其实一个数据库连接就是一个Socket连接。 在 java.sql 包中有 3 个接口分别定义了对数据库的调用的不同方式： Statement：用于执行静态 SQL 语句并返回它所生成结果的对象。 PrepatedStatement：SQL 语句被预编译并存储在此对象中，可以使用此对象多次高效地执行该语句。 CallableStatement：用于执行 SQL 存储过程 使用Statement操作数据表的弊端 通过调用 Connection 对象的 createStatement() 方法创建该对象。该对象用于执行静态的 SQL 语句，并且返回执行结果。 Statement 接口中定义了下列方法用于执行 SQL 语句： int excuteUpdate(String sql)：执行更新操作INSERT、UPDATE、DELETE ResultSet executeQuery(String sql)：执行查询操作SELECT 但是使用Statement操作数据表存在弊端： 问题一：存在拼串操作，繁琐 问题二：存在SQL注入问题 SQL 注入是利用某些系统没有对用户输入的数据进行充分的检查，而在用户输入数据中注入非法的 SQL 语句段或命令(如：SELECT user, password FROM user_table WHERE user=’a’ OR 1 = ‘ AND password = ‘ OR ‘1’ = ‘1’) ，从而利用系统的 SQL 引擎完成恶意行为的做法。 对于 Java 而言，要防范 SQL 注入，只要用 PreparedStatement(从Statement扩展而来) 取代 Statement 就可以了。 代码演示： public class StatementTest { // 使用Statement的弊端：需要拼写sql语句，并且存在SQL注入的问题 @Test public void testLogin() { Scanner scan = new Scanner(System.in); System.out.print(&quot;用户名：&quot;); String userName = scan.nextLine(); System.out.print(&quot;密 码：&quot;); String password = scan.nextLine(); // SELECT user,password FROM user_table WHERE USER = &#39;1&#39; or &#39; AND PASSWORD = &#39;=&#39;1&#39; or &#39;1&#39; = &#39;1&#39;; String sql = &quot;SELECT user,password FROM user_table WHERE USER = &#39;&quot; + userName + &quot;&#39; AND PASSWORD = &#39;&quot; + password + &quot;&#39;&quot;; User user = get(sql, User.class); if (user != null) { System.out.println(&quot;登陆成功!&quot;); } else { System.out.println(&quot;用户名或密码错误！&quot;); } } // 使用Statement实现对数据表的查询操作 public &lt;T&gt; T get(String sql, Class&lt;T&gt; clazz) { T t = null; Connection conn = null; Statement st = null; ResultSet rs = null; try { // 1.加载配置文件 InputStream is = StatementTest.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties pros = new Properties(); pros.load(is); // 2.读取配置信息 String user = pros.getProperty(&quot;user&quot;); String password = pros.getProperty(&quot;password&quot;); String url = pros.getProperty(&quot;url&quot;); String driverClass = pros.getProperty(&quot;driverClass&quot;); // 3.加载驱动 Class.forName(driverClass); // 4.获取连接 conn = DriverManager.getConnection(url, user, password); st = conn.createStatement(); rs = st.executeQuery(sql); // 获取结果集的元数据 ResultSetMetaData rsmd = rs.getMetaData(); // 获取结果集的列数 int columnCount = rsmd.getColumnCount(); if (rs.next()) { t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) { // //1. 获取列的名称 // String columnName = rsmd.getColumnName(i+1); // 1. 获取列的别名 String columnName = rsmd.getColumnLabel(i + 1); // 2. 根据列名获取对应数据表中的数据 Object columnVal = rs.getObject(columnName); // 3. 将数据表中得到的数据，封装进对象 Field field = clazz.getDeclaredField(columnName); field.setAccessible(true); field.set(t, columnVal); } return t; } } catch (Exception e) { e.printStackTrace(); } finally { // 关闭资源 if (rs != null) { try { rs.close(); } catch (SQLException e) { e.printStackTrace(); } } if (st != null) { try { st.close(); } catch (SQLException e) { e.printStackTrace(); } } if (conn != null) { try { conn.close(); } catch (SQLException e) { e.printStackTrace(); } } } return null; } } 综上： PreparedStatement的使用PreparedStatement介绍 可以通过调用 Connection 对象的 preparedStatement(String sql) 方法获取 PreparedStatement 对象 PreparedStatement 接口是 Statement 的子接口，它表示一条预编译过的 SQL 语句 PreparedStatement 对象所代表的 SQL 语句中的参数用问号(?)来表示，调用 PreparedStatement 对象的 setXxx() 方法来设置这些参数. setXxx() 方法有两个参数，第一个参数是要设置的 SQL 语句中的参数的索引(从 1 开始)，第二个是设置的 SQL 语句中的参数的值 PreparedStatement vs Statement 代码的可读性和可维护性。 PreparedStatement 能最大可能提高性能： DBServer会对预编译语句提供性能优化。因为预编译语句有可能被重复调用，所以语句在被DBServer的编译器编译后的执行代码被缓存下来，那么下次调用时只要是相同的预编译语句就不需要编译，只要将参数直接传入编译过的语句执行代码中就会得到执行。 在statement语句中,即使是相同操作但因为数据内容不一样,所以整个语句本身不能匹配,没有缓存语句的意义.事实是没有数据库会对普通语句编译后的执行代码缓存。这样每执行一次都要对传入的语句编译一次。 (语法检查，语义检查，翻译成二进制命令，缓存) PreparedStatement 可以防止 SQL 注入 Java与SQL对应数据类型转换表 Java类型 SQL类型 boolean BIT byte TINYINT short SMALLINT int INTEGER long BIGINT String CHAR,VARCHAR,LONGVARCHAR byte array BINARY , VAR BINARY java.sql.Date DATE java.sql.Time TIME java.sql.Timestamp TIMESTAMP 使用PreparedStatement实现增、删、改操作/*除了解决Statement的拼串、sql问题之外，PreparedStatement还有哪些好处呢？ 1.PreparedStatement操作Blob的数据，而Statement做不到。 2.PreparedStatement可以实现更高效的批量操作。*/ //通用的增、删、改操作（体现一：增、删、改 ； 体现二：针对于不同的表） public void update(String sql,Object ... args){ Connection conn = null; PreparedStatement ps = null; try { //1.获取数据库的连接 conn = JDBCUtils.getConnection(); //2.获取PreparedStatement的实例 (或：预编译sql语句) ps = conn.prepareStatement(sql); //3.填充占位符 for(int i = 0;i &lt; args.length;i++){ ps.setObject(i + 1, args[i]); } //4.执行sql语句 ps.execute(); // ps.executeUpdate() 返回影响的行数 // ps.executeUpdate(); 说明：返回值为影响的行数 } catch (Exception e) { e.printStackTrace(); }finally{ //5.关闭资源 JDBCUtils.closeResource(conn, ps); } } JDBCUtils.java import java.io.InputStream; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.Properties; /** * @Description 操作数据库的工具类 * @version * @date 上午9:10:02 */ public class JDBCUtils { /** * @Description 获取数据库的连接 * @return * @throws Exception */ public static Connection getConnection() throws Exception { // 1.读取配置文件中的4个基本信息 InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties pros = new Properties(); pros.load(is); String user = pros.getProperty(&quot;user&quot;); String password = pros.getProperty(&quot;password&quot;); String url = pros.getProperty(&quot;url&quot;); String driverClass = pros.getProperty(&quot;driverClass&quot;); // 2.加载驱动 Class.forName(driverClass); // 3.获取连接 Connection conn = DriverManager.getConnection(url, user, password); return conn; } /** * @Description 关闭连接和Statement的操作 * @param conn * @param ps */ public static void closeResource(Connection conn,Statement ps){ try { if(ps != null) ps.close(); } catch (SQLException e) { e.printStackTrace(); } try { if(conn != null) conn.close(); } catch (SQLException e) { e.printStackTrace(); } } /** * @Description 关闭资源操作 * @param conn * @param ps * @param rs */ public static void closeResource(Connection conn,Statement ps,ResultSet rs){ try { if(ps != null) ps.close(); } catch (SQLException e) { e.printStackTrace(); } try { if(conn != null) conn.close(); } catch (SQLException e) { e.printStackTrace(); } try { if(rs != null) rs.close(); } catch (SQLException e) { e.printStackTrace(); } } } 使用PreparedStatement实现查询操作/* 针对于表的字段名与类的属性名不相同的情况： 1. 必须声明sql时，使用类的属性名来命名字段的别名 2. 使用ResultSetMetaData时，需要使用getColumnLabel()来替换getColumnName(),获取列的别名。 说明：如果sql中没有给字段其别名，getColumnLabel()获取的就是列名 */ // 通用的针对于不同表的查询:返回一个对象 (version 1.0) public &lt;T&gt; T getInstance(Class&lt;T&gt; clazz, String sql, Object... args) { Connection conn = null; PreparedStatement ps = null; ResultSet rs = null; try { // 1.获取数据库连接 conn = JDBCUtils.getConnection(); // 2.预编译sql语句，得到PreparedStatement对象 ps = conn.prepareStatement(sql); // 3.填充占位符 for (int i = 0; i &lt; args.length; i++) { ps.setObject(i + 1, args[i]); } // 4.执行executeQuery(),得到结果集：ResultSet rs = ps.executeQuery(); // 5.得到结果集的元数据：ResultSetMetaData ResultSetMetaData rsmd = rs.getMetaData(); // 6.1通过ResultSetMetaData得到columnCount,columnLabel；通过ResultSet得到列值 int columnCount = rsmd.getColumnCount(); if (rs.next()) { T t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) {// 遍历每一个列 // 获取列值 Object columnVal = rs.getObject(i + 1); //获取列的列名：getColumnName() --不推荐使用 // 获取列的别名:列的别名，使用类的属性名充当 String columnLabel = rsmd.getColumnLabel(i + 1); // 6.2使用反射，给对象的相应属性赋值 Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columnVal); } return t; } } catch (Exception e) { e.printStackTrace(); } finally { // 7.关闭资源 JDBCUtils.closeResource(conn, ps, rs); } return null; } // 查询一组数据 public &lt;T&gt; List&lt;T&gt; getForList(Class&lt;T&gt; clazz,String sql, Object... args){ Connection conn = null; PreparedStatement ps = null; ResultSet rs = null; try { conn = JDBCUtils.getConnection(); ps = conn.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) { ps.setObject(i + 1, args[i]); } rs = ps.executeQuery(); // 获取结果集的元数据 :ResultSetMetaData ResultSetMetaData rsmd = rs.getMetaData(); // 通过ResultSetMetaData获取结果集中的列数 int columnCount = rsmd.getColumnCount(); //创建集合对象 ArrayList&lt;T&gt; list = new ArrayList&lt;T&gt;(); while (rs.next()) { T t = clazz.newInstance(); // 处理结果集一行数据中的每一个列:给t对象指定的属性赋值 for (int i = 0; i &lt; columnCount; i++) { // 获取列值 Object columValue = rs.getObject(i + 1); // 获取每个列的列名 // String columnName = rsmd.getColumnName(i + 1); String columnLabel = rsmd.getColumnLabel(i + 1); // 给t对象指定的columnName属性，赋值为columValue：通过反射 Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columValue); } list.add(t); } return list; } catch (Exception e) { e.printStackTrace(); } finally { JDBCUtils.closeResource(conn, ps, rs); } return null; } 说明：使用PreparedStatement实现的查询操作可以替换Statement实现的查询操作，解决Statement拼串和SQL注入问题。 ResultSet与ResultSetMetaDataResultSet 查询需要调用PreparedStatement 的 executeQuery() 方法，查询结果是一个ResultSet 对象 ResultSet 对象以逻辑表格的形式封装了执行数据库操作的结果集，ResultSet 接口由数据库厂商提供实现 ResultSet 返回的实际上就是一张数据表。有一个指针指向数据表的第一条记录的前面。 ResultSet 对象维护了一个指向当前数据行的游标，初始的时候，游标在第一行之前，可以通过 ResultSet 对象的 next() 方法移动到下一行。调用 next()方法检测下一行是否有效。若有效，该方法返回 true，且指针下移。相当于Iterator对象的 hasNext() 和 next() 方法的结合体。 当指针指向一行时, 可以通过调用 getXxx(int index) 或 getXxx(int columnName) 获取每一列的值。 例如: getInt(1), getString(“name”) 注意：Java与数据库交互涉及到的相关Java API中的索引都从1开始。 ResultSet 接口的常用方法： boolean next() getString() … ResultSetMetaData 可用于获取关于 ResultSet 对象中列的类型和属性信息的对象 ResultSetMetaData meta = rs.getMetaData(); getColumnName(int column)：获取指定列的名称 getColumnLabel(int column)：获取指定列的别名 getColumnCount()：返回当前 ResultSet 对象中的列数。 getColumnTypeName(int column)：检索指定列的数据库特定的类型名称。 getColumnDisplaySize(int column)：指示指定列的最大标准宽度，以字符为单位。 isNullable(int column)：指示指定列中的值是否可以为 null。 isAutoIncrement(int column)：指示是否自动为指定列进行编号，这样这些列仍然是只读的。 问题1：得到结果集后, 如何知道该结果集中有哪些列 ？ 列名是什么？ 需要使用一个描述 ResultSet 的对象， 即 ResultSetMetaData 问题2：关于ResultSetMetaData 如何获取 ResultSetMetaData： 调用 ResultSet 的 getMetaData() 方法即可 获取 ResultSet 中有多少列：调用 ResultSetMetaData 的 getColumnCount() 方法 获取 ResultSet 每一列的列的别名是什么：调用 ResultSetMetaData 的getColumnLabel() 方法 资源的释放 释放ResultSet, Statement,Connection。 数据库连接（Connection）是非常稀有的资源，用完后必须马上释放，如果Connection不能及时正确的关闭将导致系统宕机。Connection的使用原则是尽量晚创建，尽量早的释放。 可以在finally中关闭，保证及时其他代码出现异常，资源也一定能被关闭。 JDBC API小结 两种思想 面向接口编程的思想 ORM思想(object relational mapping) 一个数据表对应一个java类 表中的一条记录对应java类的一个对象 表中的一个字段对应java类的一个属性 sql是需要结合列名和表的属性名来写。注意起别名。 两种技术 JDBC结果集的元数据：ResultSetMetaData 获取列数：getColumnCount() 获取列的别名：getColumnLabel() 通过反射，创建指定类的对象，获取指定的属性并赋值 章节练习练习题1：从控制台向数据库的表customers中插入一条数据，表结构如下： 练习题2：创立数据库表 examstudent，表结构如下： 向数据表中添加如下数据： 代码实现1：插入一个新的student 信息 请输入考生的详细信息 Type:IDCard:ExamCard:StudentName:Location:Grade: 信息录入成功! 代码实现2：在 eclipse中建立 java 程序：输入身份证号或准考证号可以查询到学生的基本信息。结果如下： 代码实现3：完成学生信息的删除功能 操作BLOB类型字段MySQL BLOB类型 MySQL中，BLOB是一个二进制大型对象，是一个可以存储大量数据的容器，它能容纳不同大小的数据。 插入BLOB类型的数据必须使用PreparedStatement，因为BLOB类型的数据无法使用字符串拼接写的。 MySQL的四种BLOB类型(除了在存储的最大信息量上不同外，他们是等同的) 实际使用中根据需要存入的数据大小定义不同的BLOB类型。 需要注意的是：如果存储的文件过大，数据库的性能会下降。 如果在指定了相关的Blob类型以后，还报错：xxx too large，那么在mysql的安装目录下，找my.ini文件加上如下的配置参数： max_allowed_packet=16M。同时注意：修改了my.ini文件之后，需要重新启动mysql服务。 向数据表中插入大数据类型//获取连接 Connection conn = JDBCUtils.getConnection(); String sql = &quot;insert into customers(name,email,birth,photo)values(?,?,?,?)&quot;; PreparedStatement ps = conn.prepareStatement(sql); // 填充占位符 ps.setString(1, &quot;徐海强&quot;); ps.setString(2, &quot;xhq@126.com&quot;); ps.setDate(3, new Date(new java.util.Date().getTime())); // 操作Blob类型的变量 FileInputStream fis = new FileInputStream(&quot;xhq.png&quot;); ps.setBlob(4, fis); //执行 ps.execute(); fis.close(); JDBCUtils.closeResource(conn, ps); 修改数据表中的Blob类型字段Connection conn = JDBCUtils.getConnection(); String sql = &quot;update customers set photo = ? where id = ?&quot;; PreparedStatement ps = conn.prepareStatement(sql); // 填充占位符 // 操作Blob类型的变量 FileInputStream fis = new FileInputStream(&quot;coffee.png&quot;); ps.setBlob(1, fis); ps.setInt(2, 25); ps.execute(); fis.close(); JDBCUtils.closeResource(conn, ps); 从数据表中读取大数据类型String sql = &quot;SELECT id, name, email, birth, photo FROM customer WHERE id = ?&quot;; conn = getConnection(); ps = conn.prepareStatement(sql); ps.setInt(1, 8); rs = ps.executeQuery(); if(rs.next()){ // 方式1 //Integer id = rs.getInt(1); //String name = rs.getString(2); //String email = rs.getString(3); //Date birth = rs.getDate(4); // 方式2 Integer id = rs.getInt(&quot;id&quot;); String name = rs.getString(&quot;name&quot;); String email = rs.getString(&quot;email&quot;); Date birth = rs.getDate(&quot;birth&quot;); Customer cust = new Customer(id, name, email, birth); System.out.println(cust); //读取Blob类型的字段 //Blob photo = rs.getBlob(5); Blob photo = rs.getBlob(&quot;photo&quot;); InputStream is = photo.getBinaryStream(); OutputStream os = new FileOutputStream(&quot;c.jpg&quot;); byte [] buffer = new byte[1024]; int len = 0; while((len = is.read(buffer)) != -1){ os.write(buffer, 0, len); } JDBCUtils.closeResource(conn, ps, rs); if(is != null){ is.close(); } if(os != null){ os.close(); } } 批量插入批量执行SQL语句当需要成批插入或者更新记录时，可以采用Java的批量更新机制，这一机制允许多条语句一次性提交给数据库批量处理。通常情况下比单独提交处理更有效率 JDBC的批量处理语句包括下面三个方法： addBatch(String)：添加需要批量处理的SQL语句或是参数； executeBatch()：执行批量处理语句； clearBatch():清空缓存的数据 通常我们会遇到两种批量执行SQL语句的情况： 多条SQL语句的批量处理； 一个SQL语句的批量传参； 高效的批量插入举例：向数据表中插入20000条数据 数据库中提供一个goods表。创建如下： CREATE TABLE goods( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(20) ); 实现层次一：使用StatementConnection conn = JDBCUtils.getConnection(); Statement st = conn.createStatement(); for(int i = 1;i &lt;= 20000;i++){ String sql = &quot;insert into goods(name) values(&#39;name_&#39; + &quot;+ i +&quot;)&quot;; st.executeUpdate(sql); } 实现层次二：使用PreparedStatementlong start = System.currentTimeMillis(); Connection conn = JDBCUtils.getConnection(); String sql = &quot;insert into goods(name)values(?)&quot;; PreparedStatement ps = conn.prepareStatement(sql); for(int i = 1;i &lt;= 20000;i++){ ps.setString(1, &quot;name_&quot; + i); ps.executeUpdate(); } long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start));//82340 JDBCUtils.closeResource(conn, ps); 实现层次三/* * 修改1： 使用 addBatch() / executeBatch() / clearBatch() * 修改2：mysql服务器默认是关闭批处理的，我们需要通过一个参数，让mysql开启批处理的支持。 * ?rewriteBatchedStatements=true 写在配置文件的url后面 * 修改3：使用更新的mysql 驱动：mysql-connector-java-5.1.37-bin.jar * */ @Test public void testInsert1() throws Exception{ long start = System.currentTimeMillis(); Connection conn = JDBCUtils.getConnection(); String sql = &quot;insert into goods(name)values(?)&quot;; PreparedStatement ps = conn.prepareStatement(sql); for(int i = 1;i &lt;= 1000000;i++){ ps.setString(1, &quot;name_&quot; + i); //1.“攒”sql ps.addBatch(); if(i % 500 == 0){ //2.执行 ps.executeBatch(); //3.清空 ps.clearBatch(); } } long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start));//20000条：625 //1000000条:14733 JDBCUtils.closeResource(conn, ps); } 实现层次四/* * 层次四：在层次三的基础上操作 * 使用Connection 的 setAutoCommit(false) / commit() */ @Test public void testInsert2() throws Exception{ long start = System.currentTimeMillis(); Connection conn = JDBCUtils.getConnection(); //1.设置为不自动提交数据 conn.setAutoCommit(false); String sql = &quot;insert into goods(name)values(?)&quot;; PreparedStatement ps = conn.prepareStatement(sql); for(int i = 1;i &lt;= 1000000;i++){ ps.setString(1, &quot;name_&quot; + i); //1.“攒”sql ps.addBatch(); if(i % 500 == 0){ //2.执行 ps.executeBatch(); //3.清空 ps.clearBatch(); } } //2.提交数据 conn.commit(); long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start));//1000000条:4978 JDBCUtils.closeResource(conn, ps); }","tags":[{"name":"JDBC","slug":"JDBC","permalink":"https://mtcai.github.io/tags/JDBC/"}]},{"title":"数据库事务&DAO","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/04-JDBC/JDBC-03 数据库事务&DAO.html","text":"第6章： 数据库事务6.1 数据库事务介绍 事务：一组逻辑操作单元,使数据从一种状态变换到另一种状态。 事务处理（事务操作）：保证所有事务都作为一个工作单元来执行，即使出现了故障，都不能改变这种执行方式。当在一个事务中执行多个操作时，要么所有的事务都被提交(commit)，那么这些修改就永久地保存下来；要么数据库管理系统将放弃所作的所有修改，整个事务回滚(rollback)到最初状态。 为确保数据库中数据的一致性，数据的操纵应当是离散的成组的逻辑单元：当它全部完成时，数据的一致性可以保持，而当这个单元中的一部分操作失败，整个事务应全部视为错误，所有从起始点以后的操作应全部回退到开始状态。 6.2 JDBC事务处理 数据一旦提交，就不可回滚。 哪些操作会导致数据的自动提交？ DDL操作一旦执行，都会自动提交。 set autocommit = false 对DDL操作失效 DML默认情况下，一旦执行，就会自动提交。 我们可以通过set autocommit = false的方式取消DML操作的自动提交。 默认在关闭连接时，会自动的提交数据 数据什么时候意味着提交？ 当一个连接对象被创建时，默认情况下是自动提交事务：每次执行一个 SQL 语句时，如果执行成功，就会向数据库自动提交，而不能回滚。 关闭数据库连接，数据就会自动的提交。如果多个操作，每个操作使用的是自己单独的连接，则无法保证事务。即同一个事务的多个操作必须在同一个连接下。 JDBC程序中为了让多个 SQL 语句作为一个事务执行： 调用 Connection 对象的 setAutoCommit(false); 以取消自动提交事务 在所有的 SQL 语句都成功执行后，调用 commit(); 方法提交事务 在出现异常时，调用 rollback(); 方法回滚事务 若此时 Connection 没有被关闭，还可能被重复使用，则需要恢复其自动提交状态 setAutoCommit(true)。尤其是在使用数据库连接池技术时，执行close()方法前，建议恢复自动提交状态。 【案例：用户AA向用户BB转账100】 public void testJDBCTransaction() { Connection conn = null; try { // 1.获取数据库连接 conn = JDBCUtils.getConnection(); // 2.开启事务 conn.setAutoCommit(false); // 3.进行数据库操作 String sql1 = &quot;update user_table set balance = balance - 100 where user = ?&quot;; update(conn, sql1, &quot;AA&quot;); // 模拟网络异常 //System.out.println(10 / 0); String sql2 = &quot;update user_table set balance = balance + 100 where user = ?&quot;; update(conn, sql2, &quot;BB&quot;); // 4.若没有异常，则提交事务 conn.commit(); } catch (Exception e) { e.printStackTrace(); // 5.若有异常，则回滚事务 try { conn.rollback(); } catch (SQLException e1) { e1.printStackTrace(); } } finally { try { //6.恢复每次DML操作的自动提交功能 conn.setAutoCommit(true); } catch (SQLException e) { e.printStackTrace(); } //7.关闭连接 JDBCUtils.closeResource(conn, null, null); } } 其中，对数据库操作的方法为： //使用事务以后的通用的增删改操作（version 2.0） public void update(Connection conn ,String sql, Object... args) { PreparedStatement ps = null; try { // 1.获取PreparedStatement的实例 (或：预编译sql语句) ps = conn.prepareStatement(sql); // 2.填充占位符 for (int i = 0; i &lt; args.length; i++) { ps.setObject(i + 1, args[i]); } // 3.执行sql语句 ps.execute(); } catch (Exception e) { e.printStackTrace(); } finally { // 4.关闭资源 JDBCUtils.closeResource(null, ps); } } 6.3 事务的ACID属性 原子性（Atomicity） 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency） 事务必须使数据库从一个一致性状态变换到另外一个一致性状态。 隔离性（Isolation） 事务的隔离性是指一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 持久性（Durability） 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来的其他操作和数据库故障不应该对其有任何影响。 6.3.1 数据库的并发问题 对于同时运行的多个事务, 当这些事务访问数据库中相同的数据时, 如果没有采取必要的隔离机制, 就会导致各种并发问题: 脏读: 对于两个事务 T1, T2, T1 读取了已经被 T2 更新但还没有被提交的字段。之后, 若 T2 回滚, T1读取的内容就是临时且无效的。 不可重复读: 对于两个事务T1, T2, T1 读取了一个字段, 然后 T2 更新了该字段。之后, T1再次读取同一个字段, 值就不同了。 幻读: 对于两个事务T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入了一些新的行。之后, 如果 T1 再次读取同一个表, 就会多出几行。 数据库事务的隔离性: 数据库系统必须具有隔离并发运行各个事务的能力, 使它们不会相互影响, 避免各种并发问题。 一个事务与其他事务隔离的程度称为隔离级别。数据库规定了多种事务隔离级别, 不同隔离级别对应不同的干扰程度, 隔离级别越高, 数据一致性就越好, 但并发性越弱。 6.3.2 四种隔离级别 数据库提供的4种事务隔离级别： Oracle 支持的 2 种事务隔离级别：READ COMMITED, SERIALIZABLE。 Oracle 默认的事务隔离级别为: READ COMMITED 。 Mysql 支持 4 种事务隔离级别。Mysql 默认的事务隔离级别为: REPEATABLE READ。 6.3.3 在MySql中设置隔离级别 每启动一个 mysql 程序, 就会获得一个单独的数据库连接. 每个数据库连接都有一个全局变量 @@tx_isolation, 表示当前的事务隔离级别。 查看当前的隔离级别: SELECT @@tx_isolation; 设置当前 mySQL 连接的隔离级别: set transaction isolation level read committed; 设置数据库系统的全局的隔离级别: set global transaction isolation level read committed; 补充操作： 创建mysql数据库用户： create user tom identified by &#39;abc123&#39;; 授予权限 #授予通过网络方式登录的tom用户，对所有库所有表的全部权限，密码设为abc123. grant all privileges on *.* to tom@&#39;%&#39; identified by &#39;abc123&#39;; #给tom用户使用本地命令行方式，授予atguigudb这个库下的所有表的插删改查的权限。 grant select,insert,delete,update on atguigudb.* to tom@localhost identified by &#39;abc123&#39;; // ***************************************************** @Test public void testTransactionSelect() throws Exception{ Connection conn = JDBCUtils.getConnection(); //获取当前连接的隔离级别 System.out.println(conn.getTransactionIsolation()); //设置数据库的隔离级别： conn.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED); //取消自动提交数据 conn.setAutoCommit(false); String sql = &quot;select user,password,balance from user_table where user = ?&quot;; User user = getInstance(conn, User.class, sql, &quot;CC&quot;); System.out.println(user); } @Test public void testTransactionUpdate() throws Exception{ Connection conn = JDBCUtils.getConnection(); //取消自动提交数据 conn.setAutoCommit(false); String sql = &quot;update user_table set balance = ? where user = ?&quot;; update(conn, sql, 5000,&quot;CC&quot;); Thread.sleep(15000); System.out.println(&quot;修改结束&quot;); } //通用的查询操作，用于返回数据表中的一条记录（version 2.0：考虑上事务） public &lt;T&gt; T getInstance(Connection conn,Class&lt;T&gt; clazz,String sql, Object... args) { PreparedStatement ps = null; ResultSet rs = null; try { ps = conn.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) { ps.setObject(i + 1, args[i]); } rs = ps.executeQuery(); // 获取结果集的元数据 :ResultSetMetaData ResultSetMetaData rsmd = rs.getMetaData(); // 通过ResultSetMetaData获取结果集中的列数 int columnCount = rsmd.getColumnCount(); if (rs.next()) { T t = clazz.newInstance(); // 处理结果集一行数据中的每一个列 for (int i = 0; i &lt; columnCount; i++) { // 获取列值 Object columValue = rs.getObject(i + 1); // 获取每个列的列名 // String columnName = rsmd.getColumnName(i + 1); String columnLabel = rsmd.getColumnLabel(i + 1); // 给t对象指定的columnName属性，赋值为columValue：通过反射 Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columValue); } return t; } } catch (Exception e) { e.printStackTrace(); } finally { JDBCUtils.closeResource(null, ps, rs); } return null; } 第7章：DAO及相关实现类 DAO：Data Access Object访问数据信息的类和接口，包括了对数据的CRUD（Create、Retrival、Update、Delete），而不包含任何业务相关的信息。有时也称作：BaseDAO 作用：为了实现功能的模块化，更有利于代码的维护和升级。 下面是尚硅谷JavaWeb阶段书城项目中DAO使用的体现： 层次结构： 【BaseDAO.java】package com.atguigu.bookstore.dao; import java.lang.reflect.ParameterizedType; import java.lang.reflect.Type; import java.sql.Connection; import java.sql.SQLException; import java.util.List; import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.handlers.BeanHandler; import org.apache.commons.dbutils.handlers.BeanListHandler; import org.apache.commons.dbutils.handlers.ScalarHandler; /** * 定义一个用来被继承的对数据库进行基本操作的Dao * @author HanYanBing * @param &lt;T&gt; */ public abstract class BaseDao&lt;T&gt; { private QueryRunner queryRunner = new QueryRunner(); // 定义一个变量来接收泛型的类型 private Class&lt;T&gt; type; // 获取T的Class对象，获取泛型的类型，泛型是在被子类继承时才确定 public BaseDao() { // 获取子类的类型 Class clazz = this.getClass(); // 获取父类的类型 // getGenericSuperclass()用来获取当前类的父类的类型 // ParameterizedType表示的是带泛型的类型 ParameterizedType parameterizedType = (ParameterizedType) clazz.getGenericSuperclass(); // 获取具体的泛型类型 getActualTypeArguments获取具体的泛型的类型 // 这个方法会返回一个Type的数组 Type[] types = parameterizedType.getActualTypeArguments(); // 获取具体的泛型的类型· this.type = (Class&lt;T&gt;) types[0]; } /** * 通用的增删改操作 * * @param sql * @param params * @return */ public int update(Connection conn,String sql, Object... params) { int count = 0; try { count = queryRunner.update(conn, sql, params); } catch (SQLException e) { e.printStackTrace(); } return count; } /** * 获取一个对象 * * @param sql * @param params * @return */ public T getBean(Connection conn,String sql, Object... params) { T t = null; try { t = queryRunner.query(conn, sql, new BeanHandler&lt;T&gt;(type), params); } catch (SQLException e) { e.printStackTrace(); } return t; } /** * 获取所有对象 * * @param sql * @param params * @return */ public List&lt;T&gt; getBeanList(Connection conn,String sql, Object... params) { List&lt;T&gt; list = null; try { list = queryRunner.query(conn, sql, new BeanListHandler&lt;T&gt;(type), params); } catch (SQLException e) { e.printStackTrace(); } return list; } /** * 获取一个但一值得方法，专门用来执行像 select count(*)...这样的sql语句 * * @param sql * @param params * @return */ public Object getValue(Connection conn,String sql, Object... params) { Object count = null; try { // 调用queryRunner的query方法获取一个单一的值 count = queryRunner.query(conn, sql, new ScalarHandler&lt;&gt;(), params); } catch (SQLException e) { e.printStackTrace(); } return count; } } 【BookDAO.java】package com.atguigu.bookstore.dao; import java.sql.Connection; import java.util.List; import com.atguigu.bookstore.beans.Book; import com.atguigu.bookstore.beans.Page; public interface BookDao { /** * 从数据库中查询出所有的记录 * * @return */ List&lt;Book&gt; getBooks(Connection conn); /** * 向数据库中插入一条记录 * * @param book */ void saveBook(Connection conn,Book book); /** * 从数据库中根据图书的id删除一条记录 * * @param bookId */ void deleteBookById(Connection conn,String bookId); /** * 根据图书的id从数据库中查询出一条记录 * * @param bookId * @return */ Book getBookById(Connection conn,String bookId); /** * 根据图书的id从数据库中更新一条记录 * * @param book */ void updateBook(Connection conn,Book book); /** * 获取带分页的图书信息 * * @param page：是只包含了用户输入的pageNo属性的page对象 * @return 返回的Page对象是包含了所有属性的Page对象 */ Page&lt;Book&gt; getPageBooks(Connection conn,Page&lt;Book&gt; page); /** * 获取带分页和价格范围的图书信息 * * @param page：是只包含了用户输入的pageNo属性的page对象 * @return 返回的Page对象是包含了所有属性的Page对象 */ Page&lt;Book&gt; getPageBooksByPrice(Connection conn,Page&lt;Book&gt; page, double minPrice, double maxPrice); } 【UserDAO.java】package com.atguigu.bookstore.dao; import java.sql.Connection; import com.atguigu.bookstore.beans.User; public interface UserDao { /** * 根据User对象中的用户名和密码从数据库中获取一条记录 * * @param user * @return User 数据库中有记录 null 数据库中无此记录 */ User getUser(Connection conn,User user); /** * 根据User对象中的用户名从数据库中获取一条记录 * * @param user * @return true 数据库中有记录 false 数据库中无此记录 */ boolean checkUsername(Connection conn,User user); /** * 向数据库中插入User对象 * * @param user */ void saveUser(Connection conn,User user); } 【BookDaoImpl.java】package com.atguigu.bookstore.dao.impl; import java.sql.Connection; import java.util.List; import com.atguigu.bookstore.beans.Book; import com.atguigu.bookstore.beans.Page; import com.atguigu.bookstore.dao.BaseDao; import com.atguigu.bookstore.dao.BookDao; public class BookDaoImpl extends BaseDao&lt;Book&gt; implements BookDao { @Override public List&lt;Book&gt; getBooks(Connection conn) { // 调用BaseDao中得到一个List的方法 List&lt;Book&gt; beanList = null; // 写sql语句 String sql = &quot;select id,title,author,price,sales,stock,img_path imgPath from books&quot;; beanList = getBeanList(conn,sql); return beanList; } @Override public void saveBook(Connection conn,Book book) { // 写sql语句 String sql = &quot;insert into books(title,author,price,sales,stock,img_path) values(?,?,?,?,?,?)&quot;; // 调用BaseDao中通用的增删改的方法 update(conn,sql, book.getTitle(), book.getAuthor(), book.getPrice(), book.getSales(), book.getStock(),book.getImgPath()); } @Override public void deleteBookById(Connection conn,String bookId) { // 写sql语句 String sql = &quot;DELETE FROM books WHERE id = ?&quot;; // 调用BaseDao中通用增删改的方法 update(conn,sql, bookId); } @Override public Book getBookById(Connection conn,String bookId) { // 调用BaseDao中获取一个对象的方法 Book book = null; // 写sql语句 String sql = &quot;select id,title,author,price,sales,stock,img_path imgPath from books where id = ?&quot;; book = getBean(conn,sql, bookId); return book; } @Override public void updateBook(Connection conn,Book book) { // 写sql语句 String sql = &quot;update books set title = ? , author = ? , price = ? , sales = ? , stock = ? where id = ?&quot;; // 调用BaseDao中通用的增删改的方法 update(conn,sql, book.getTitle(), book.getAuthor(), book.getPrice(), book.getSales(), book.getStock(), book.getId()); } @Override public Page&lt;Book&gt; getPageBooks(Connection conn,Page&lt;Book&gt; page) { // 获取数据库中图书的总记录数 String sql = &quot;select count(*) from books&quot;; // 调用BaseDao中获取一个单一值的方法 long totalRecord = (long) getValue(conn,sql); // 将总记录数设置都page对象中 page.setTotalRecord((int) totalRecord); // 获取当前页中的记录存放的List String sql2 = &quot;select id,title,author,price,sales,stock,img_path imgPath from books limit ?,?&quot;; // 调用BaseDao中获取一个集合的方法 List&lt;Book&gt; beanList = getBeanList(conn,sql2, (page.getPageNo() - 1) * Page.PAGE_SIZE, Page.PAGE_SIZE); // 将这个List设置到page对象中 page.setList(beanList); return page; } @Override public Page&lt;Book&gt; getPageBooksByPrice(Connection conn,Page&lt;Book&gt; page, double minPrice, double maxPrice) { // 获取数据库中图书的总记录数 String sql = &quot;select count(*) from books where price between ? and ?&quot;; // 调用BaseDao中获取一个单一值的方法 long totalRecord = (long) getValue(conn,sql,minPrice,maxPrice); // 将总记录数设置都page对象中 page.setTotalRecord((int) totalRecord); // 获取当前页中的记录存放的List String sql2 = &quot;select id,title,author,price,sales,stock,img_path imgPath from books where price between ? and ? limit ?,?&quot;; // 调用BaseDao中获取一个集合的方法 List&lt;Book&gt; beanList = getBeanList(conn,sql2, minPrice , maxPrice , (page.getPageNo() - 1) * Page.PAGE_SIZE, Page.PAGE_SIZE); // 将这个List设置到page对象中 page.setList(beanList); return page; } } 【UserDaoImpl.java】package com.atguigu.bookstore.dao.impl; import java.sql.Connection; import com.atguigu.bookstore.beans.User; import com.atguigu.bookstore.dao.BaseDao; import com.atguigu.bookstore.dao.UserDao; public class UserDaoImpl extends BaseDao&lt;User&gt; implements UserDao { @Override public User getUser(Connection conn,User user) { // 调用BaseDao中获取一个对象的方法 User bean = null; // 写sql语句 String sql = &quot;select id,username,password,email from users where username = ? and password = ?&quot;; bean = getBean(conn,sql, user.getUsername(), user.getPassword()); return bean; } @Override public boolean checkUsername(Connection conn,User user) { // 调用BaseDao中获取一个对象的方法 User bean = null; // 写sql语句 String sql = &quot;select id,username,password,email from users where username = ?&quot;; bean = getBean(conn,sql, user.getUsername()); return bean != null; } @Override public void saveUser(Connection conn,User user) { //写sql语句 String sql = &quot;insert into users(username,password,email) values(?,?,?)&quot;; //调用BaseDao中通用的增删改的方法 update(conn,sql, user.getUsername(),user.getPassword(),user.getEmail()); } } 【Book.java】package com.atguigu.bookstore.beans; /** * 图书类 * @author songhongkang * */ public class Book { private Integer id; private String title; // 书名 private String author; // 作者 private double price; // 价格 private Integer sales; // 销量 private Integer stock; // 库存 private String imgPath = &quot;static/img/default.jpg&quot;; // 封面图片的路径 //构造器，get()，set()，toString()方法略 } 【Page.java】package com.atguigu.bookstore.beans; import java.util.List; /** * 页码类 * @author songhongkang * */ public class Page&lt;T&gt; { private List&lt;T&gt; list; // 每页查到的记录存放的集合 public static final int PAGE_SIZE = 4; // 每页显示的记录数 private int pageNo; // 当前页 // private int totalPageNo; // 总页数，通过计算得到 private int totalRecord; // 总记录数，通过查询数据库得到 【User.java】package com.atguigu.bookstore.beans; /** * 用户类 * @author songhongkang * */ public class User { private Integer id; private String username; private String password; private String email;","tags":[{"name":"JDBC","slug":"JDBC","permalink":"https://mtcai.github.io/tags/JDBC/"}]},{"title":"数据库连接池&DBUtils","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/04-JDBC/JDBC-04 数据库连接池&DBUtils.html","text":"第8章：数据库连接池8.1 JDBC数据库连接池的必要性 在使用开发基于数据库的web程序时，传统的模式基本是按以下步骤： 在主程序（如servlet、beans）中建立数据库连接 进行sql操作 断开数据库连接 这种模式开发，存在的问题: 普通的JDBC数据库连接使用 DriverManager 来获取，每次向数据库建立连接的时候都要将 Connection 加载到内存中，再验证用户名和密码(得花费0.05s～1s的时间)。需要数据库连接的时候，就向数据库要求一个，执行完成后再断开连接。这样的方式将会消耗大量的资源和时间。数据库的连接资源并没有得到很好的重复利用。若同时有几百人甚至几千人在线，频繁的进行数据库连接操作将占用很多的系统资源，严重的甚至会造成服务器的崩溃。 对于每一次数据库连接，使用完后都得断开。否则，如果程序出现异常而未能关闭，将会导致数据库系统中的内存泄漏，最终将导致重启数据库。（回忆：何为Java的内存泄漏？） 这种开发不能控制被创建的连接对象数，系统资源会被毫无顾及的分配出去，如连接过多，也可能导致内存泄漏，服务器崩溃。 8.2 数据库连接池技术 为解决传统开发中的数据库连接问题，可以采用数据库连接池技术。 数据库连接池的基本思想：就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。 数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。 数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。 工作原理： 数据库连接池技术的优点 1. 资源重用 由于数据库连接得以重用，避免了频繁创建，释放连接引起的大量性能开销。在减少系统消耗的基础上，另一方面也增加了系统运行环境的平稳性。 2. 更快的系统反应速度 数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于连接池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接，避免了数据库连接初始化和释放过程的时间开销，从而减少了系统的响应时间 3. 新的资源分配手段 对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接池的配置，实现某一应用最大可用数据库连接数的限制，避免某一应用独占所有的数据库资源 4. 统一的连接管理，避免数据库连接泄漏 在较为完善的数据库连接池实现中，可根据预先的占用超时设定，强制回收被占用连接，从而避免了常规数据库连接操作中可能出现的资源泄露 8.3 多种开源的数据库连接池 JDBC 的数据库连接池使用 javax.sql.DataSource 来表示，DataSource 只是一个接口，该接口通常由服务器(Weblogic, WebSphere, Tomcat)提供实现，也有一些开源组织提供实现： DBCP 是Apache提供的数据库连接池。tomcat 服务器自带dbcp数据库连接池。速度相对c3p0较快，但因自身存在BUG，Hibernate3已不再提供支持。 C3P0 是一个开源组织提供的一个数据库连接池，速度相对较慢，稳定性还可以。hibernate官方推荐使用 Proxool 是sourceforge下的一个开源项目数据库连接池，有监控连接池状态的功能，稳定性较c3p0差一点 BoneCP 是一个开源组织提供的数据库连接池，速度快 Druid 是阿里提供的数据库连接池，据说是集DBCP 、C3P0 、Proxool 优点于一身的数据库连接池，但是速度不确定是否有BoneCP快 DataSource 通常被称为数据源，它包含连接池和连接池管理两个部分，习惯上也经常把 DataSource 称为连接池 DataSource用来取代DriverManager来获取Connection，获取速度快，同时可以大幅度提高数据库访问速度。 特别注意： 数据源和数据库连接不同，数据源无需创建多个，它是产生数据库连接的工厂，因此整个应用只需要一个数据源即可。 当数据库访问结束后，程序还是像以前一样关闭数据库连接：conn.close(); 但conn.close()并没有关闭数据库的物理连接，它仅仅把数据库连接释放，归还给了数据库连接池。 8.3.1 C3P0数据库连接池 获取连接方式一 //使用C3P0数据库连接池的方式，获取数据库的连接：不推荐 public static Connection getConnection1() throws Exception{ ComboPooledDataSource cpds = new ComboPooledDataSource(); cpds.setDriverClass(&quot;com.mysql.jdbc.Driver&quot;); cpds.setJdbcUrl(&quot;jdbc:mysql://localhost:3306/test&quot;); cpds.setUser(&quot;root&quot;); cpds.setPassword(&quot;abc123&quot;); cpds.setInitiaPoolAize(10);// 设置初始时数据库连接池中的连接数 // cpds.setMaxPoolSize(100); Connection conn = cpds.getConnection(); return conn; // 销毁c3p0数据库连接池 // DataSources.destory(cpds); } 获取连接方式二 //使用C3P0数据库连接池的配置文件方式，获取数据库的连接：推荐 private static DataSource cpds = new ComboPooledDataSource(&quot;helloc3p0&quot;); public static Connection getConnection2() throws SQLException{ Connection conn = cpds.getConnection(); return conn; } 其中，src下的配置文件为：【c3p0-config.xml】 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;c3p0-config&gt; &lt;named-config name=&quot;helloc3p0&quot;&gt; &lt;!-- 获取连接的4个基本信息 --&gt; &lt;property name=&quot;user&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;password&quot;&gt;abc123&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot;&gt;jdbc:mysql:///test&lt;/property&gt; &lt;property name=&quot;driverClass&quot;&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;!-- 涉及到数据库连接池的管理的相关属性的设置 --&gt; &lt;!-- 若数据库中连接数不足时, 一次向数据库服务器申请多少个连接 --&gt; &lt;property name=&quot;acquireIncrement&quot;&gt;5&lt;/property&gt; &lt;!-- 初始化数据库连接池时连接的数量 --&gt; &lt;property name=&quot;initialPoolSize&quot;&gt;5&lt;/property&gt; &lt;!-- 数据库连接池中的最小的数据库连接数 --&gt; &lt;property name=&quot;minPoolSize&quot;&gt;5&lt;/property&gt; &lt;!-- 数据库连接池中的最大的数据库连接数 --&gt; &lt;property name=&quot;maxPoolSize&quot;&gt;10&lt;/property&gt; &lt;!-- C3P0 数据库连接池可以维护的 Statement 的个数 --&gt; &lt;property name=&quot;maxStatements&quot;&gt;20&lt;/property&gt; &lt;!-- 每个连接同时可以使用的 Statement 对象的个数 --&gt; &lt;property name=&quot;maxStatementsPerConnection&quot;&gt;5&lt;/property&gt; &lt;/named-config&gt; &lt;/c3p0-config&gt; 8.3.2 DBCP数据库连接池 DBCP 是 Apache 软件基金组织下的开源连接池实现，该连接池依赖该组织下的另一个开源系统：Common-pool。如需使用该连接池实现，应在系统中增加如下两个 jar 文件： Commons-dbcp.jar：连接池的实现 Commons-pool.jar：连接池实现的依赖库 Tomcat 的连接池正是采用该连接池来实现的。该数据库连接池既可以与应用服务器整合使用，也可由应用程序独立使用。 数据源和数据库连接不同，数据源无需创建多个，它是产生数据库连接的工厂，因此整个应用只需要一个数据源即可。 当数据库访问结束后，程序还是像以前一样关闭数据库连接：conn.close(); 但上面的代码并没有关闭数据库的物理连接，它仅仅把数据库连接释放，归还给了数据库连接池。 配置属性说明 属性 默认值 说明 initialSize 0 连接池启动时创建的初始化连接数量 maxActive 8 连接池中可同时连接的最大的连接数 maxIdle 8 连接池中最大的空闲的连接数，超过的空闲连接将被释放，如果设置为负数表示不限制 minIdle 0 连接池中最小的空闲的连接数，低于这个数量会被创建新的连接。该参数越接近maxIdle，性能越好，因为连接的创建和销毁，都是需要消耗资源的；但是不能太大。 maxWait 无限制 最大等待时间，当没有可用连接时，连接池等待连接释放的最大时间，超过该时间限制会抛出异常，如果设置-1表示无限等待 poolPreparedStatements false 开启池的Statement是否prepared maxOpenPreparedStatements 无限制 开启池的prepared 后的同时最大连接数 minEvictableIdleTimeMillis 连接池中连接，在时间段内一直空闲， 被逐出连接池的时间 removeAbandonedTimeout 300 超过时间限制，回收没有用(废弃)的连接 removeAbandoned false 超过removeAbandonedTimeout时间后，是否进 行没用连接（废弃）的回收 获取连接方式一： public static Connection getConnection3() throws Exception { BasicDataSource source = new BasicDataSource(); source.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); source.setUrl(&quot;jdbc:mysql:///test&quot;); source.setUsername(&quot;root&quot;); source.setPassword(&quot;abc123&quot;); // source.setInitialSize(10); Connection conn = source.getConnection(); return conn; } 获取连接方式二： //使用dbcp数据库连接池的配置文件方式，获取数据库的连接：推荐 private static DataSource source = null; static{ try { Properties pros = new Properties(); InputStream is = DBCPTest.class.getClassLoader().getResourceAsStream(&quot;dbcp.properties&quot;); pros.load(is); //根据提供的BasicDataSourceFactory创建对应的DataSource对象 source = BasicDataSourceFactory.createDataSource(pros); } catch (Exception e) { e.printStackTrace(); } } public static Connection getConnection4() throws Exception { Connection conn = source.getConnection(); return conn; } 其中，src下的配置文件为：【dbcp.properties】 driverClassName=com.mysql.jdbc.Driver url=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=true&amp;useServerPrepStmts=false username=root password=abc123 initialSize=10 #... 8.3.3 Druid（德鲁伊）数据库连接池Druid是阿里巴巴开源平台上一个数据库连接池实现，它结合了C3P0、DBCP、Proxool等DB池的优点，同时加入了日志监控，可以很好的监控DB池连接和SQL的执行情况，可以说是针对监控而生的DB连接池，可以说是目前最好的连接池之一。 package com.atguigu.druid; import java.sql.Connection; import java.util.Properties; import javax.sql.DataSource; import com.alibaba.druid.pool.DruidDataSourceFactory; public class TestDruid { public static void main(String[] args) throws Exception { // 可以放到静态代码块 Properties pro = new Properties(); pro.load(TestDruid.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;)); DataSource ds = DruidDataSourceFactory.createDataSource(pro); Connection conn = ds.getConnection(); System.out.println(conn); } } 其中，src下的配置文件为：【druid.properties】 url=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=true username=root password=123456 driverClassName=com.mysql.jdbc.Driver initialSize=10 maxActive=20 maxWait=1000 filters=wall 详细配置参数： 配置 缺省 说明 name 配置这个属性的意义在于，如果存在多个数据源，监控的时候可以通过名字来区分开来。 如果没有配置，将会生成一个名字，格式是：”DataSource-” + System.identityHashCode(this) url 连接数据库的url，不同数据库不一样。例如：mysql : jdbc:mysql://10.20.153.104:3306/druid2 oracle : jdbc:oracle:thin:@10.20.149.85:1521:ocnauto username 连接数据库的用户名 password 连接数据库的密码。如果你不希望密码直接写在配置文件中，可以使用ConfigFilter。详细看这里：https://github.com/alibaba/druid/wiki/%E4%BD%BF%E7%94%A8ConfigFilter driverClassName 根据url自动识别 这一项可配可不配，如果不配置druid会根据url自动识别dbType，然后选择相应的driverClassName(建议配置下) initialSize 0 初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时 maxActive 8 最大连接池数量 maxIdle 8 已经不再使用，配置了也没效果 minIdle 最小连接池数量 maxWait 获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。 poolPreparedStatements false 是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql下建议关闭。 maxOpenPreparedStatements -1 要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题，可以把这个数值配置大一些，比如说100 validationQuery 用来检测连接是否有效的sql，要求是一个查询语句。如果validationQuery为null，testOnBorrow、testOnReturn、testWhileIdle都不会其作用。 testOnBorrow true 申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 testOnReturn false 归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能 testWhileIdle false 建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。 timeBetweenEvictionRunsMillis 有两个含义： 1)Destroy线程会检测连接的间隔时间2)testWhileIdle的判断依据，详细看testWhileIdle属性的说明 numTestsPerEvictionRun 不再使用，一个DruidDataSource只支持一个EvictionRun minEvictableIdleTimeMillis connectionInitSqls 物理连接初始化的时候执行的sql exceptionSorter 根据dbType自动识别 当数据库抛出一些不可恢复的异常时，抛弃连接 filters 属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有： 监控统计用的filter:stat日志用的filter:log4j防御sql注入的filter:wall proxyFilters 类型是List，如果同时配置了filters和proxyFilters，是组合关系，并非替换关系 第9章：Apache-DBUtils实现CRUD操作9.1 Apache-DBUtils简介 commons-dbutils 是 Apache 组织提供的一个开源 JDBC工具类库，它是对JDBC的简单封装，学习成本极低，并且使用dbutils能极大简化jdbc编码的工作量，同时也不会影响程序的性能。 API介绍： org.apache.commons.dbutils.QueryRunner org.apache.commons.dbutils.ResultSetHandler 工具类：org.apache.commons.dbutils.DbUtils API包说明： 9.2 主要API的使用9.2.1 DbUtils DbUtils ：提供如关闭连接、装载JDBC驱动程序等常规工作的工具类，里面的所有方法都是静态的。主要方法如下： public static void close(…) throws java.sql.SQLException： DbUtils类提供了三个重载的关闭方法。这些方法检查所提供的参数是不是NULL，如果不是的话，它们就关闭Connection、Statement和ResultSet。 public static void closeQuietly(…): 这一类方法不仅能在Connection、Statement和ResultSet为NULL情况下避免关闭，还能隐藏一些在程序中抛出的SQLEeception。 public static void commitAndClose(Connection conn)throws SQLException： 用来提交连接的事务，然后关闭连接 public static void commitAndCloseQuietly(Connection conn)： 用来提交连接，然后关闭连接，并且在关闭连接时不抛出SQL异常。 public static void rollback(Connection conn)throws SQLException：允许conn为null，因为方法内部做了判断 public static void rollbackAndClose(Connection conn)throws SQLException rollbackAndCloseQuietly(Connection) public static boolean loadDriver(java.lang.String driverClassName)：这一方装载并注册JDBC驱动程序，如果成功就返回true。使用该方法，你不需要捕捉这个异常ClassNotFoundException。 9.2.2 QueryRunner类 该类简单化了SQL查询，它与ResultSetHandler组合在一起使用可以完成大部分的数据库操作，能够大大减少编码量。 QueryRunner类提供了两个构造器： 默认的构造器 需要一个 javax.sql.DataSource 来作参数的构造器 QueryRunner类的主要方法： 更新 public int update(Connection conn, String sql, Object… params) throws SQLException:用来执行一个更新（插入、更新或删除）操作。 …… 插入 public T insert(Connection conn,String sql,ResultSetHandler rsh, Object… params) throws SQLException：只支持INSERT语句，其中 rsh - The handler used to create the result object from the ResultSet of auto-generated keys. 返回值: An object generated by the handler.即自动生成的键值 …. 批处理 public int[] batch(Connection conn,String sql,Object[][] params)throws SQLException： INSERT, UPDATE, or DELETE语句 public T insertBatch(Connection conn,String sql,ResultSetHandler rsh,Object[][] params)throws SQLException：只支持INSERT语句 ….. 查询 public Object query(Connection conn, String sql, ResultSetHandler rsh,Object… params) throws SQLException：执行一个查询操作，在这个查询中，对象数组中的每个元素值被用来作为查询语句的置换参数。该方法会自行处理 PreparedStatement 和 ResultSet 的创建和关闭。 …… 测试 // 测试添加 @Test public void testInsert() throws Exception { QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3();//Druid连接方式 String sql = &quot;insert into customers(name,email,birth)values(?,?,?)&quot;; int count = runner.update(conn, sql, &quot;何成飞&quot;, &quot;he@qq.com&quot;, &quot;1992-09-08&quot;); System.out.println(&quot;添加了&quot; + count + &quot;条记录&quot;); JDBCUtils.closeResource(conn, null); } /** * 使用Druid数据库连接池技术 */ private static DataSource source1; static{ try { Properties pros = new Properties(); InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;druid.properties&quot;); pros.load(is); source1 = DruidDataSourceFactory.createDataSource(pros); } catch (Exception e) { e.printStackTrace(); } } public static Connection getConnection3() throws SQLException{ Connection conn = source1.getConnection(); return conn; } // 测试删除 @Test public void testDelete() throws Exception { QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); String sql = &quot;delete from customers where id &lt; ?&quot;; int count = runner.update(conn, sql,3); System.out.println(&quot;删除了&quot; + count + &quot;条记录&quot;); JDBCUtils.closeResource(conn, null); } 9.2.3 ResultSetHandler接口及实现类 该接口用于处理 java.sql.ResultSet，将数据按要求转换为另一种形式。 ResultSetHandler 接口提供了一个单独的方法：Object handle (java.sql.ResultSet .rs)。 接口的主要实现类： ArrayHandler：把结果集中的第一行数据转成对象数组。 ArrayListHandler：把结果集中的每一行数据都转成一个数组，再存放到List中。 BeanHandler：将结果集中的第一行数据封装到一个对应的JavaBean实例中。 BeanListHandler：将结果集中的每一行数据都封装到一个对应的JavaBean实例中，存放到List里。 ColumnListHandler：将结果集中某一列的数据存放到List中。 KeyedHandler(name)：将结果集中的每一行数据都封装到一个Map里，再把这些map再存到一个map里，其key为指定的key。 MapHandler：将结果集中的第一行数据封装到一个Map里，key是列名，value就是对应的值。 MapListHandler：将结果集中的每一行数据都封装到一个Map里，然后再存放到List ScalarHandler：查询单个值对象 测试 /* * 测试查询:查询一条记录 * 使用ResultSetHandler的实现类：BeanHandler */ @Test public void testQueryInstance() throws Exception{ QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); String sql = &quot;select id,name,email,birth from customers where id = ?&quot;; // BeanHandler&lt;Customer&gt; handler = new BeanHandler&lt;&gt;(Customer.class); Customer customer = runner.query(conn, sql, handler, 23); System.out.println(customer); JDBCUtils.closeResource(conn, null); } /* * 测试查询:查询多条记录构成的集合 * 使用ResultSetHandler的实现类：BeanListHandler */ @Test public void testQueryList() throws Exception{ QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); String sql = &quot;select id,name,email,birth from customers where id &lt; ?&quot;; // BeanListHandler&lt;Customer&gt; handler = new BeanListHandler&lt;&gt;(Customer.class); List&lt;Customer&gt; list = runner.query(conn, sql, handler, 23); list.forEach(System.out::println); JDBCUtils.closeResource(conn, null); } /* * 自定义ResultSetHandler的实现类 */ @Test public void testQueryInstance1() throws Exception{ QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); String sql = &quot;select id,name,email,birth from customers where id = ?&quot;; ResultSetHandler&lt;Customer&gt; handler = new ResultSetHandler&lt;Customer&gt;() { @Override public Customer handle(ResultSet rs) throws SQLException { System.out.println(&quot;handle&quot;); // return new Customer(1,&quot;Tom&quot;,&quot;tom@126.com&quot;,new Date(123323432L)); if(rs.next()){ int id = rs.getInt(&quot;id&quot;); String name = rs.getString(&quot;name&quot;); String email = rs.getString(&quot;email&quot;); Date birth = rs.getDate(&quot;birth&quot;); return new Customer(id, name, email, birth); } return null; } }; Customer customer = runner.query(conn, sql, handler, 23); System.out.println(customer); JDBCUtils.closeResource(conn, null); } /* * 如何查询类似于最大的，最小的，平均的，总和，个数相关的数据， * 使用ScalarHandler * */ @Test public void testQueryValue() throws Exception{ QueryRunner runner = new QueryRunner(); Connection conn = JDBCUtils.getConnection3(); //测试一： // String sql = &quot;select count(*) from customers where id &lt; ?&quot;; // ScalarHandler handler = new ScalarHandler(); // long count = (long) runner.query(conn, sql, handler, 20); // System.out.println(count); //测试二： String sql = &quot;select max(birth) from customers&quot;; ScalarHandler handler = new ScalarHandler(); Date birth = (Date) runner.query(conn, sql, handler); System.out.println(birth); JDBCUtils.closeResource(conn, null); } JDBC总结总结 @Test public void testUpdateWithTx() { Connection conn = null; try { //1.获取连接的操作（ //① 手写的连接：JDBCUtils.getConnection(); //② 使用数据库连接池：C3P0;DBCP;Druid //2.对数据表进行一系列CRUD操作 //① 使用PreparedStatement实现通用的增删改、查询操作（version 1.0 \\ version 2.0) //version2.0的增删改public void update(Connection conn,String sql,Object ... args){} //version2.0的查询 public &lt;T&gt; T getInstance(Connection conn,Class&lt;T&gt; clazz,String sql,Object ... args){} //② 使用dbutils提供的jar包中提供的QueryRunner类 //提交数据 conn.commit(); } catch (Exception e) { e.printStackTrace(); try { //回滚数据 conn.rollback(); } catch (SQLException e1) { e1.printStackTrace(); } }finally{ //3.关闭连接等操作 //① JDBCUtils.closeResource(); //② 使用dbutils提供的jar包中提供的DbUtils类提供了关闭的相关操作 } }","tags":[{"name":"JDBC","slug":"JDBC","permalink":"https://mtcai.github.io/tags/JDBC/"}]},{"title":"概述&安装与源码编译","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop01-概述、运行模式&源码编译.html","text":"大数据概述 Hadoop三大发行版本：Apache、Cloudera、Hortonworks。 Apache版本最原始（最基础）的版本，对于入门学习最好。 官网地址：http://hadoop.apache.org/releases.html 下载地址：https://archive.apache.org/dist/hadoop/common/ Cloudera在大型互联网企业中用的较多。 官网地址：https://www.cloudera.com/downloads/cdh/5-10-0.html 下载地址：http://archive-primary.cloudera.com/cdh5/cdh/5/ CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强 Hortonworks文档较好。 官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform Hadoop组成 HDFS（Hadoop Distributed File System）的架构概述 YARN (Yet Another Resource Negotiator) 架构概述 RM 作用： 处理客户端请求 监控 NodeManager 启动或监控 ApplicationMaster 资源的分配与调度 NodeManger 作用： 管理单个节点的资源 处理来自RM的命令 处理来自AM的命令 ApplicationMaster 作用： 负责数据的切分 为应用程序申请资源，并分配给内部的任务 任务的监控与容错 Container 作用： Container 是Yarn中资源的抽象，它封装了某个节点上的多维度资源，如内存，cpu，磁盘，网络等 MapReduce架构概述MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据 2）Reduce阶段对Map结果进行汇总 HDFS、YARN、MapReduce 三者关系 大数据技术生态体系 图中涉及的技术名词解释如下： Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。 Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 支持通过Kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。 Storm：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。 Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。 Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多 Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。 Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。 Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。 R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。 Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库。 ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 推荐系统框架图 Hadoop目录结构 drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 bin drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 etc drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 include drwxr-xr-x. 3 atguigu atguigu 4096 5月 22 2017 lib drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 libexec -rw-r—r—. 1 atguigu atguigu 15429 5月 22 2017 LICENSE.txt -rw-r—r—. 1 atguigu atguigu 101 5月 22 2017 NOTICE.txt -rw-r—r—. 1 atguigu atguigu 1366 5月 22 2017 README.txt drwxr-xr-x. 2 atguigu atguigu 4096 5月 22 2017 sbin drwxr-xr-x. 4 atguigu atguigu 4096 5月 22 2017 share bin目录：存放对Hadoop相关服务（HDFS,YARN,MapRed）进行操作的脚本 etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能） sbin目录：存放启动或停止Hadoop相关服务的脚本 share目录：存放Hadoop的依赖jar包、文档、和官方案例 伪分布式运行模式启动HDFS并运行MapReduce程序1 执行步骤 （a）配置：hadoop-env.sh Linux系统中获取JDK的安装路径： [atguigu@ hadoop101 ~]# echo $JAVA_HOME/opt/module/jdk1.8.0_144 修改JAVA_HOME 路径： export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置：core-site.xml &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; （c）配置：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 2 启动集群 （a）格式化NameNode（第一次启动时格式化，以后就不要总格式化） [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs namenode -format （b）启动NameNode [atguigu@hadoop1 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode （c）启动DataNode [atguigu@hadoop1 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode 3 查看集群 a）查看是否启动成功 [atguigu@hadoop1 hadoop-2.7.2]$ jps 13586 NameNode 13668 DataNode 13786 Jps 注意：jps是JDK中的命令，不是Linux命令。不安装JDK不能使用jps （b）web端查看HDFS文件系统 http://hadoop1:50070/dfshealth.html#tab-overview 注意：如果不能查看，看如下帖子处理：http://www.cnblogs.com/zlslch/p/6604189.html （c）查看产生的Log日志 说明：在企业中遇到Bug时，经常根据日志提示信息去分析问题、解决Bug。 当前目录：/opt/module/hadoop-2.7.2/logs [atguigu@hadoop101 logs]$ ls hadoop-atguigu-datanode-hadoop.atguigu.com.log hadoop-atguigu-datanode-hadoop.atguigu.com.out hadoop-atguigu-namenode-hadoop.atguigu.com.log hadoop-atguigu-namenode-hadoop.atguigu.com.out SecurityAuth-root.audit [atguigu@hadoop101 logs]# cat hadoop-atguigu-datanode-hadoop101.log （d）思考：为什么不能一直格式化NameNode，格式化NameNode，要注意什么？ [atguigu@hadoop1 hadoop-2.7.2]$ cd data/tmp/dfs/name/current/ [atguigu@hadoop1 current]$ cat VERSION clusterID=CID-f0330a58-36fa-4a2a-a65f-2688269b5837** [atguigu@hadoop1 hadoop-2.7.2]$ cd data/tmp/dfs/data/current/clusterID=CID-f0330a58-36fa-4a2a-a65f-2688269b5837 注意：格式化NameNode，会产生新的集群id,导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。 4 操作集群 ​ （a）在HDFS文件系统上创建一个input文件夹 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/atguigu/input ​ （b）将测试文件内容上传到文件系统上 [atguigu@hadoop1 hadoop-2.7.2]$bin/hdfs dfs -put wcinput/wc.input /user/atguigu/input/ ​ （c）查看上传的文件是否正确 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -ls /user/atguigu/input/ [atguigu@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/atguigu/ input/wc.input ​ （d）运行MapReduce程序 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input/ /user/atguigu/output ​ （e）查看输出结果 命令行查看： [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/atguigu/output/* 浏览器查看，如图 ​ （f）将测试文件内容下载到本地 [atguigu@hadoop1 hadoop-2.7.2]$ hdfs dfs -get /user/atguigu/output/part-r-00000 ./wcoutput/ （g）删除输出结果 [atguigu@hadoop1 hadoop-2.7.2]$ hdfs dfs -rm -r /user/atguigu/output 启动YARN并运行MapReduce程序1 配置集群 （a）配置yarn-env.sh 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置yarn-site.xml &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;/property&gt; （c）配置：mapred-env.sh 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 （d）配置： (对mapred-site.xml.template重新命名为) mapred-site.xml [atguigu@hadoop1 hadoop]$ mv mapred-site.xml.template mapred-site.xml [atguigu@hadoop1 hadoop]$ vi mapred-site.xml &lt;!-- 指定MR运行在YARN上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 2 启动集群 （a）启动前必须保证NameNode和DataNode已经启动 （b）启动ResourceManager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager （c）启动NodeManager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager 3 集群操作 （a）YARN的浏览器页面查看，如图所示 http://hadoop101:8088/cluster （b）删除文件系统上的output文件 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/atguigu/output （c）执行MapReduce程序 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input /user/atguigu/output （d）查看运行结果，如图2-36所示 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/atguigu/output/* 配置历史服务器为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下： 配置mapred-site.xml [atguigu@hadoop1 hadoop]$ vi mapred-site.xml 在该文件里面增加如下配置。 &lt;!-- 历史服务器端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop1:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop1:19888&lt;/value&gt; &lt;/property&gt; 启动历史服务器 [atguigu@hadoop1 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver 查看历史服务器是否启动 [atguigu@hadoop1 hadoop-2.7.2]$ jps 查看JobHistory http://hadoop1:19888/jobhistory 配置日志的聚集日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。 日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。 注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager。 开启日志聚集功能具体步骤如下： 配置yarn-site.xml [atguigu@hadoop1 hadoop]$ vi yarn-site.xml 在该文件里面增加如下配置。 &lt;!-- 日志聚集功能使能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置日志聚集服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;!-- 日志保留时间设置7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt; 关闭NodeManager 、ResourceManager和HistoryManager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh stop historyserver 启动NodeManager 、ResourceManager和HistoryManager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager [atguigu@hadoop1 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver 删除HDFS上已经存在的输出文件 [atguigu@hadoop1 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/atguigu/output 执行WordCount程序 [atguigu@hadoop11 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input /user/atguigu/output 查看日志，如图所示 http://hadoop1:19888/jobhistory 配置文件说明Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。 （1）默认配置文件： 要获取的默认文件 文件存放在Hadoop的jar包中的位置 [core-default.xml] hadoop-common-2.7.2.jar/ core-default.xml [hdfs-default.xml] hadoop-hdfs-2.7.2.jar/ hdfs-default.xml [yarn-default.xml] hadoop-yarn-common-2.7.2.jar/ yarn-default.xml [mapred-default.xml] hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml （2）自定义配置文件： ​ core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置 完全分布式运行模式rsync 远程同步工具rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。 （1）基本语法 rsync -rvl \\$pdir/​\\$fname ​\\$user@hadoop\\$host:\\$pdir/​\\$fname 命令 选项参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 ​ 选项参数说明 选项 功能 -r 递归 -v 显示复制过程 -l 拷贝符号连接 xsync集群分发脚本（1）需求：循环复制文件到所有节点的相同目录下 （2）需求分析： rsync命令原始拷贝： rsync -rvl /opt/module root@hadoop103:/opt/ 期望脚本： xsync 要同步的文件名称 说明：在/home/atguigu/bin这个目录下存放的脚本，atguigu用户可以在系统任何地方直接执行。 （3）脚本实现 在/home/atguigu目录下创建bin目录，并在bin目录下xsync创建文件，文件内容如下： [atguigu@hadoop102 ~]$ mkdir bin [atguigu@hadoop102 ~]$ cd bin/ [atguigu@hadoop102 bin]$ touch xsync [atguigu@hadoop102 bin]$ vi xsync ​ 在该文件中编写如下代码 #!/bin/bash #1 获取输入参数个数，如果没有参数，直接退出 pcount=$# if((pcount==0)); then echo no args; exit; fi #2 获取文件名称 p1=$1 fname=`basename $p1` echo fname=$fname #3 获取上级目录到绝对路径 pdir=`cd -P $(dirname $p1); pwd` echo pdir=$pdir #4 获取当前用户名称 user=`whoami` #5 循环 for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdir done 修改脚本 xsync 具有执行权限 [atguigu@hadoop2 bin]$ chmod 777 xsync 调用脚本形式：xsync 文件名称 [atguigu@hadoop2 bin]$ xsync /home/atguigu/bin 注意：如果将xsync放到/home/atguigu/bin目录下仍然不能实现全局使用，可以将xsync移动到/usr/local/bin目录下。 集群配置➢ ➢ 集群部署规划 hadoop102 —-&gt;hadoop1 hadoop103 —-&gt;hadoop2 hadoop104 —-&gt;hadoop3 hadoop102 hadoop103 hadoop104 HDFS NameNodeDataNode DataNode SecondaryNameNodeDataNode YARN NodeManager ResourceManagerNodeManager NodeManager NameNode和SecondaryNameNode使用相同大小的内存，故一般不放在同一个服务器上 ResourceManager也比较耗内存，故不和NameNode、SecondaryNameNode放在同一个服务器 配置集群（1）核心配置文件 配置core-site.xml [atguigu@hadoop102 hadoop]$ vi core-site.xml 在该文件中编写如下配置 &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- Hadoop3.x 配置 HDFS 网页登录使用的静态用户为 atguigu --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt; &lt;/property&gt; （2）HDFS配置文件 配置hadoop-env.sh [atguigu@hadoop102 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置hdfs-site.xml [atguigu@hadoop102 hadoop]$ vi hdfs-site.xml 在该文件中编写如下配置 &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- nn web 端访问地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt; &lt;!-- 2 nn web 端访问地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt; &lt;/property&gt; （3）YARN配置文件 配置yarn-env.sh [atguigu@hadoop102 hadoop]$ vi yarn-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置yarn-site.xml [atguigu@hadoop102 hadoop]$ vi yarn-site.xml 在该文件中增加如下配置 &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!-- 日志聚集功能使能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置日志聚集服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;!-- 日志保留时间设置7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt; &lt;!-- 环境变量的继承 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE, HADOOP_YARN_HOME, HADOOP_MAPRED_HOME &lt;/value&gt; &lt;/property&gt; （4）MapReduce配置文件 配置mapred-env.sh [atguigu@hadoop102 hadoop]$ vi mapred-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 配置mapred-site.xml [atguigu@hadoop102 hadoop]$ cp mapred-site.xml.template mapred-site.xml [atguigu@hadoop102 hadoop]$ vi mapred-site.xml 在该文件中增加如下配置 &lt;!-- 指定MR运行在Yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop102:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器 web 端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;19888&lt;/value&gt; &lt;/property&gt; 在集群上分发配置好的Hadoop配置文件 [atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/ 查看文件分发情况 [atguigu@hadoop103 hadoop]$ cat /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml 集群单点启动（1）如果集群是第一次启动，需要格式化NameNode，==启动前要关闭所有服务== [atguigu@hadoop102 hadoop-2.7.2]$ hadoop namenode -format （2）在hadoop102上启动NameNode [atguigu@hadoop102 hadoop-2.7.2]$ hadoop-daemon.sh start namenode [atguigu@hadoop102 hadoop-2.7.2]$ jps 3461 NameNode （3）在hadoop102、hadoop103以及hadoop104上分别启动DataNode [atguigu@hadoop102 hadoop-2.7.2]$ hadoop-daemon.sh start datanode [atguigu@hadoop102 hadoop-2.7.2]$ jps 3461 NameNode 3608 Jps 3561 DataNode [atguigu@hadoop103 hadoop-2.7.2]$ hadoop-daemon.sh start datanode [atguigu@hadoop103 hadoop-2.7.2]$ jps 3190 DataNode 3279 Jps [atguigu@hadoop104 hadoop-2.7.2]$ hadoop-daemon.sh start datanode [atguigu@hadoop104 hadoop-2.7.2]$ jps 3237 Jps 3163 DataNode （4）思考：每次都一个一个节点启动，如果节点数增加到1000个怎么办？ ​ 早上来了开始一个一个节点启动，到晚上下班刚好完成，下班？ SSH无密登录配置 生成公钥和私钥 [atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免密登录的目标机器上 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104 注意： 还需要在hadoop102上采用 root 账号，配置一下无密登录到hadoop102、hadoop103、hadoop104； 还需要在hadoop103上采用 atguigu 账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。 还需要在hadoop104上采用 atguigu 账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。 .ssh文件夹下（~/.ssh）的文件功能解释 known_hosts 记录ssh访问过计算机的公钥(public key) id_rsa 生成的私钥 id_rsa.pub 生成的公钥 authorized_keys 存放授权过得无密登录服务器公钥 群起集群配置slaves/workershadoop2.x修改 /opt/module/hadoop-2.7.2/etc/hadoop/slaves hadoop3.x修改 /opt/module/hadoop-3.1.3/etc/hadoop/workers [atguigu@hadoop102 hadoop]$ vi slaves 在该文件中增加如下内容：注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。 hadoop102 hadoop103 hadoop104 同步所有节点配置文件: [atguigu@hadoop102 hadoop]$ xsync slaves 启动集群 如果集群是第一次启动，需要格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据） 注意：格式化 NameNode 会产生新的集群 id，导致 NameNode 和 DataNode的集群 id不一致，集群找不到已往数据。 如果集群在运行过程中报错，需要重新格式化 NameNode的话， 一定要 先停止 namenode和 datanode进程， 并且要 删除 所有机器的 data和 logs目录，然后再进行格式化。 启动HDFS [atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [atguigu@hadoop102 hadoop-2.7.2]$ jps 4166 NameNode 4482 Jps 4263 DataNode [atguigu@hadoop103 hadoop-2.7.2]$ jps 3218 DataNode 3288 Jps [atguigu@hadoop104 hadoop-2.7.2]$ jps 3221 DataNode 3283 SecondaryNameNode 3364 Jps 启动YARN [atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh 注意：NameNode和ResourceManger如果不是同一台机器，不能在NameNode上启动 YARN，应该在ResouceManager所在的机器上启动YARN。 Web端查看SecondaryNameNode 浏览器中输入：http://hadoop104:50090/status.html 常用命令start hadoop_home=/usr/local/hadoop $hadoop_home/sbin/hadoop-daemon.sh start namenode $hadoop_home/sbin/hadoop-daemon.sh start datanode $hadoop_home/sbin/yarn-daemon.sh start resourcemanager $hadoop_home/sbin/yarn-daemon.sh start nodemanager $hadoop_home/sbin/yarn-daemon.sh start historyserver $hadoop_home/sbin/mr-jobhistory-daemon.sh start historyserver stop hadoop_home=/usr/local/hadoop $hadoop_home/sbin/hadoop-daemon.sh stop namenode $hadoop_home/sbin/hadoop-daemon.sh stop datanode $hadoop_home/sbin/yarn-daemon.sh stop resourcemanager $hadoop_home/sbin/yarn-daemon.sh stop nodemanager $hadoop_home/sbin/yarn-daemon.sh stop historyserver $hadoop_home/sbin/mr-jobhistory-daemon.sh stop historyserver hdfs --daemon start/stop namenode/datanode/secondarynamenode yarn --daemon start/stop resourcemanager/nodemanager bin/mapred --daemon start/stop historyserver #各个模块分开启动/停止（配置ssh是前提） #整体启动/停止HDFS start-dfs.sh / stop-dfs.sh #整体启动/停止YARN start-yarn.sh / stop-yarn.sh 集群时间同步 时间服务器配置（必须root用户) 检查 ntp 是否安装 [root@hadoop102 桌面]# rpm -qa|grep ntp ntp-4.2.6p5-10.el6.centos.x86_64 fontpackages-filesystem-1.41-1.1.el6.noarch ntpdate-4.2.6p5-10.el6.centos.x86_64 查看 所有节点 ntpd服务 状态 和 开机自启动 状态 sudo systemctl status ntpd sudo systemctl start ntpd sudo systemctl is-enabled ntpd 修改ntp配置文件 [root@hadoop102 桌面]# vi /etc/ntp.conf 修改内容如下 a）修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间） #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap 为 restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap b）修改2（集群在局域网中，不使用其他互联网上的时间） server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst为 #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步） server 127.127.1.0 fudge 127.127.1.0 stratum 10 修改/etc/sysconfig/ntpd 文件 root@hadoop102 桌面]# vim /etc/sysconfig/ntpd 增加内容如下（让硬件时间与系统时间一起同步） SYNC_HWCLOCK=yes 重新启动ntpd服务 [root@hadoop102 桌面]# service ntpd start 设置ntpd服务开机启动 [root@hadoop102 桌面]# chkconfig ntpd on 其他机器配置（必须root用户） 在其他机器配置10分钟与时间服务器同步一次 [root@hadoop103桌面]# crontab -e 编写定时任务如下： /10 * /usr/sbin/ntpdate hadoop102 关闭 所有节点上 ntp服务和自启动 sudo systemctl stop ntpd sudo systemctl disable ntpd 修改任意机器时间 [root@hadoop103桌面]# date -s “2017-9-11 11:11:11” 十分钟后查看机器是否与时间服务器同步 [root@hadoop103桌面]# date Hadoop编译源码（面试重点）准备工作 （1）系统联网，或者有yum源 （2）hadoop-2.7.2-src.tar.gz 进入hadoop-2.7.2-src文件夹，查看BUILDING.txt cd hadoop-2.7.2-src more BUILDING.txt 可以看到编译所需的库或者工具 （3）jdk-8u144-linux-x64.tar.gz （4）apache-ant-1.9.9-bin.tar.gz（build工具，打包用的） （5）apache-maven-3.0.5-bin.tar.gz （6）protobuf-2.5.0.tar.gz（序列化的框架） （7）apache-tomcat-6.0.44.tar.gz 配置jdk验证命令：java -version 配置Maven [root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml ==== ​ ==nexus-aliyun== ​ ==central== ​ ==Nexus aliyun== ​ ==[http://maven.aliyun.com/nexus/content/groups/public](http://maven.aliyun.com/nexus/content/groups/public)== ​ ==== [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile MAVEN_HOMEexport MAVEN_HOME=/opt/module/apache-maven-3.0.5 export PATH=\\$PATH:\\$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn -version 配置ant [root@hadoop101 apache-ant-1.9.9]# vi /etc/profile ANT_HOMEexport ANT_HOME=/opt/module/apache-ant-1.9.9 export PATH=\\$PATH:\\$ANT_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：ant -version 安装 g++、make、cmake等库 [root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers [root@hadoop101 apache-ant-1.9.9]# yum -y install svn ncurses-devel gcc* [root@hadoop101 apache-ant-1.9.9]# yum install make [root@hadoop101 apache-ant-1.9.9]# yum install cmake [root@hadoop101 apache-ant-1.9.9]# yum -y install lzo-devel zlib-devel autoconf automake libtool cmake openssl-devel 安装protobuf [root@hadoop101 protobuf-2.5.0]#./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make check [root@hadoop101 protobuf-2.5.0]# make install [root@hadoop101 protobuf-2.5.0]# ldconfig [root@hadoop101 hadoop-dist]# vi /etc/profile LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0 export PATH=\\$PATH:\\$LD_LIBRARY_PATH [root@hadoop101 software]#source /etc/profile 验证命令：protoc —version 安装findbugs 解压：tar -zxvf findbugs-3.0.1.tar.gz -C /opt/moudles/ 配置环境变量: 在 /etc/profile 文件末尾添加： export FINDBUGS_HOME=/opt/findbugs-3.0.1 export PATH=\\$PATH:\\$FINDBUGS_HOME/bin 保存退出，并使更改生效。 验证命令：findbugs -version 编译源码 1.进入到源码目录 [root@hadoop101 hadoop-2.7.2-src]# pwd /opt/hadoop-2.7.2-src 2.通过maven执行编译命令 [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 编译过程中会下载 apache-tomcat-6.0.44.tar.gz，速度非常慢，把提前下载好的文件放到如下目录： 注：编译前这两个目录并不存在，编译过程中及时中断，然后复制文件 hadoop-2.7.2-src/hadoop-common-project/hadoop-kms/downloads/ hadoop-2.7.2-src/hadoop-hdfs-project/hadoop-hdfs-httpfs/downloads 等待时间30分钟左右，最终成功是全部SUCCESS，如图 成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下 编译源码过程中常见的问题及解决方案（1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） （2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar （3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐2.7.0版本的问题汇总帖子 http://www.tuicool.com/articles/IBn63qf","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"HDFS概述","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop02-HDFS概述、shell&客户端操作.html","text":"HDFS概述HDFS（Hadoop Distributed File System）是一种分布式文件管理系统。通过目录树定位文件；其次有很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 应用场景：适合一次写入，多次读出的场景，且不支持文件的修改，适合用来做数据分析，不适合用来做网盘应用。 优缺点优点： 高容错性 数据自动保存多个副本，通过增加副本的形式，提高容错性 某一个副本丢失后，他可以自动恢复 适合处理大数据 数据规模：能够处理数据规模达到GB，TB，甚至PB级别的数据 文件规模：能够处理百万规模以上的文件数量 可构建在廉价的机器上，通过多副本的机制，提高可靠性 缺点： 不适合低延时数据访问，比如毫秒级别的存储数据 无法高效地对大量小文件进行存储 存储大量小文件的话，会占用 NameNode 大量的内存来存储文件目录和块信息 小文件的存储的寻址时间会超过读取时间，违反了HDFS的设计目标 不支持文件并发写入，随机修改 一个文件只能有一个写，不允许多个线程同时写 仅支持数据 append，不支持文件的随机修改 组成架构 文件块大小（面试重点）HDFS中的文件在物理上是分块存储(block)，块的大小可以通过配置参数（dfs.blocksize）规定，yarn集群的默认大小在Hadoop2.x/3.x中是128M，Hadoop1.x 是64M，在本地运行时32M。 使用存储块的好处 假如上传的一个文件非常大，没有任何一块磁盘能够存储，这样这个文件就没法上传了，如果使用块的概念，会把文件分割成许多块，这样这个文件可以使用集群中的任意节点进行存储。 数据存储要考虑容灾备份，以块为单位非常有利于进行备份，HDFS默认每个块备份3份，这样如果这个块上或这个节点坏掉，可以直接找其他节点上的备份块。还有就是，有的时候需要将备份数量提高， 这样能够分散机群的读取负载，因为可以在多个节点中寻找到目标数据，减少单个节点读取。 问：为什么块的大小不能太大，也不能太小？ 如果块设置过大， 从磁盘传输数据的时间会明显大于寻址时间，导致程序在处理这块数据时，变得非常慢； mapreduce中的map任务通常一次只处理一个块中的数据，如果块过大运行速度也会很慢。 在数据读写计算的时候,需要进行网络传输。如果block过大会导致网络传输时间增长，程序卡顿/超时/无响应. 任务执行的过程中拉取其他节点的block或者失败重试的成本会过高. namenode监管容易判断数据节点死亡.导致集群频繁产生/移除副本, 占用cpu,网络,内存资源. 如果块设置过小， 存放大量小文件会占用NameNode中大量内存来存储元数据，而NameNode的物理内存是有限的； 文件块过小，寻址时间增大，导致程序一直在找block的开始位置。 操作系统对目录中的小文件处理存在性能问题.比如同一个目录下文件数量操作100万,执行”fs -l “之类的命令会卡死. 则会频繁的进行文件传输,对严重占用网络/CPU资源. 主要取决于磁盘/网络的传输速率。[其实就是CPU,磁盘,网卡之间的协同效率 即 跨物理机/机架之间文件传输速率] 为什么分片大小需要与HDFS数据块（分块）大小一致 hadoop将mapReduce的输入数据划分为等长的小数据块，称为输入分片或者分片，hadoop为每个分片构建一个map任务。 hadoop在存储有输入数据（HDFS中的数据）的节点上运行map任务，可以获得高性能，这就是所谓的数据本地化。所以最佳分片的大小应该与HDFS上的块大小一样，因为如果分片跨越2个数据块，对于任何一个HDFS节点（基本不肯能同时存储这2个数据块），分片中的另外一块数据就需要通过网络传输到map任务节点，与使用本地数据运行map任务相比，效率则更低！ 为什么不能远大于64MB或者128MB或256MB？ 这里主要从上层的MapReduce框架来讨论 （1）Map崩溃问题：系统需要重新启动，启动过程需要重新加载数据，数据块越大，数据加载时间越长，系统恢复过程越长。 （2）监管时间问题：主节点监管其他节点的情况，每个节点会周期性的把完成的工作和状态的更新报告回来。如果一个节点保持沉默超过一个预设的时间间隔，主节点记录下这个节点状态为死亡，并把分配给这个节点的数据发到别的节点。对于这个“预设的时间间隔”，这是从数据块的角度大概估算的。假如是对于64MB的数据块，我可以假设你10分钟之内无论如何也能解决了吧， 超过10分钟也没反应，那就是死了。可对于640MB或是1G以上的数据，我应该要估算个多长的时间呢？估算的时间短了，那就误判死亡了，更坏的情况是所有节点都会被判死亡。 估算的时间长了，那等待的时间就过长了。所以对于过大的数据块，这个“预设的时间间隔”不好估算。 （3）Map任务上：因为MapReducer中一般一个map处理一个块上的数据，如果块很大，任务数会很少(少于集群中的节点个数)这样执行效率会明显降低。 HDFS的Shell操作 bin/hadoop fs XXXX bin/hdfs dfs XXXX dfs是fs的实现类 全部命令[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop fs ** [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] #追加一个文件到已经存在的文件末尾 ** [-cat [-ignoreCrc] &lt;src&gt; ...] #显示文件内容 [-checksum &lt;src&gt; ...] * [-chgrp [-R] GROUP PATH...] #和Linux文件系统中的用法一样，修改文件所属权限 * [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] * [-chown [-R] [OWNER][:[GROUP]] PATH...] * [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] #从本地文件系统中 拷贝 文件到 HDFS路径去 * [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] #从 HDFS拷贝到本地 [-count [-q] &lt;path&gt; ...] ** [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;] #从 HDFS的一个路径拷贝到 HDFS的另一个路径 [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] ** [-du [-s] [-h] &lt;path&gt; ...] #统计文件夹的大小信息 [-expunge] ** [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] #等同于 copyToLocal [-getfacl [-R] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] ** [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] #显示目录信息 ** [-mkdir [-p] &lt;path&gt; ...] #创建路径 * [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] #从本地 剪切 粘贴到 HDFS [-moveToLocal &lt;src&gt; &lt;localdst&gt;] ** [-mv &lt;src&gt; ... &lt;dst&gt;] #在 HDFS目录中移动文件 ** [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] #等同于 copyFromLocal [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] ** [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] #删除文件或文件夹 [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] * [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] #设置 HDFS中文件的副本数量 [-stat [format] &lt;path&gt; ...] * [-tail [-f] &lt;file&gt;] #显示一个文件的末尾 1kb的数据 [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]] hadoop fs -help rm; -help：输出这个命令参数 hadoop fs -ls /; -ls: 显示目录信息 hadoop fs -mkdir -p /sanguo/shuguo; -mkdir：在HDFS上创建目录 hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo; -moveFromLocal：从本地剪切粘贴到HDFS hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt; -appendToFile：追加一个文件到已经存在的文件末尾 hadoop fs -cat /sanguo/shuguo/kongming.txt; -cat：显示文件内容 adoop fs -chmod 666 /sanguo/shuguo/kongming.txt; -chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限 hadoop fs -copyFromLocal README.txt /; -copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去 hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./; -copyToLocal：从HDFS拷贝到本地 hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt; -cp ：从HDFS的一个路径拷贝到HDFS的另一个路径 hadoop fs -mv /zhuge.txt /sanguo/shuguo/; -mv：在HDFS目录中移动文件 hadoop fs -get /sanguo/shuguo/kongming.txt ./; -get：等同于copyToLocal，就是从HDFS下载文件到本地 hadoop fs -getmerge /user/atguigu/test/* ./zaiyiqi.txt; -getmerge：合并下载多个文件 hadoop fs -put ./zaiyiqi.txt /user/atguigu/test/; -put：等同于copyFromLocal hadoop fs -tail /sanguo/shuguo/kongming.txt; -tail：显示一个文件的末尾 hadoop fs -rm /user/atguigu/test/jinlian2.txt; -rm：删除文件或文件夹 hadoop fs -rmdir /test; -rmdir：删除空目录 hadoop fs -du -s -h /user/atguigu/test; -du统计文件夹的大小信息 &gt; 2.7 K /user/atguigu/test hadoop fs -du -h /user/atguigu/test &gt; 1.3 K /user/atguigu/test/README.txt &gt; 15 /user/atguigu/test/jinlian.txt &gt; 1.4 K /user/atguigu/test/zaiyiqi.txt hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt; -setrep：设置HDFS中文件的副本数量 HDFS的客户端操作客户端环境准备配置HADOOP_HOME环境变量；配置Path环境变量 创建一个Maven工程HdfsClientDemo，在pom.xml添加依赖： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 注意：如果Eclipse/Idea打印不出日志，在控制台上只显示 1.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). 2.log4j:WARN Please initialize the log4j system properly. 3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 创建HdfsClient类 public class HdfsClient{ @Test public void testMkdirs() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 // configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); // FileSystem fs = FileSystem.get(configuration); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 创建目录 fs.mkdirs(new Path(&quot;/1108/daxian/banzhang&quot;)); // 3 关闭资源 fs.close(); } } 由于需要连接集群操作，所以需要在idea中设置连接集群的账户名： HDFS的API操作HDFS文件上传（测试参数优先级）@Test public void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException { // 1 获取文件系统 Configuration configuration = new Configuration(); configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 上传文件 fs.copyFromLocalFile(new Path(&quot;e:/banzhang.txt&quot;), new Path(&quot;/banzhang.txt&quot;)); // 3 关闭资源 fs.close(); System.out.println(&quot;over&quot;); } 将hdfs-site.xml拷贝到项目的根目录下 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 参数优先级 参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置 HDFS文件下载@Test public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 执行下载操作 // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件校验 fs.copyToLocalFile(false, new Path(&quot;/banzhang.txt&quot;), new Path(&quot;e:/banhua.txt&quot;), true); // 3 关闭资源 fs.close(); } HDFS文件夹删除@Test public void testDelete() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 执行删除 fs.delete(new Path(&quot;/0508/&quot;), true); // 3 关闭资源 fs.close(); } HDFS文件名更改@Test public void testRename() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 修改文件名称 fs.rename(new Path(&quot;/banzhang.txt&quot;), new Path(&quot;/banhua.txt&quot;)); // 3 关闭资源 fs.close(); } HDFS文件详情查看查看文件名称、权限、长度、块信息 @Test public void testListFiles() throws IOException, InterruptedException, URISyntaxException{ // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while(listFiles.hasNext()){ LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 System.out.println(status.getPath().getName()); // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 分组 System.out.println(status.getGroup()); System.out.println(status.getOwner()); System.out.println(status.getModificationTime()); System.out.println(status.getReplication()); System.out.println(status.getBlockSize()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) { // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) { System.out.println(host); } } System.out.println(&quot;-----------分割线----------&quot;); } // 3 关闭资源 fs.close(); } HDFS文件和文件夹判断@Test public void testListStatus() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件配置信息 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 判断是文件还是文件夹 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); for (FileStatus fileStatus : listStatus) { // 如果是文件 if (fileStatus.isFile()) { System.out.println(&quot;f:&quot;+fileStatus.getPath().getName()); }else { System.out.println(&quot;d:&quot;+fileStatus.getPath().getName()); } } // 3 关闭资源 fs.close(); } HDFS的I/O流操作HDFS文件上传需求：把本地e盘上的banhua.txt文件上传到HDFS根目录 @Test public void putFileToHDFS() throws IOException, InterruptedException, URISyntaxException { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 创建输入流 FileInputStream fis = new FileInputStream(new File(&quot;e:/banhua.txt&quot;)); // 3 获取输出流 FSDataOutputStream fos = fs.create(new Path(&quot;/banhua.txt&quot;)); // 4 流对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); } HDFS文件下载需求：从HDFS上下载banhua.txt文件到本地e盘上 // 文件下载 @Test public void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/banhua.txt&quot;)); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/banhua.txt&quot;)); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); } 定位文件读取需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz // 下载第一块 @Test public void readFileSeek1() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;)); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++){ fis.read(buf); fos.write(buf); } // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close(); } // 下载第二块 @Test public void readFileSeek2() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;)); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } 在Window命令窗口中进入到目录E:\\，然后执行如下命令，对数据进行合并 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"HDFS读写流程","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop03-HDFS读写流程&NN和2NN.html","text":"HDFS数据流HDFS写数据流程剖析文件写入 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 NameNode返回是否可以上传。 客户端请求第一个 Block上传到哪几个DataNode服务器上。 NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 dn1、dn2、dn3逐级应答客户端。 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步） 网络拓扑-节点距离计算节点距离：两个节点到达最近的共同祖先的距离总和。 机架感知（副本存储节点选择） HDFS读数据流程 客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode地址。 挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。 DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。 客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。 NN和2NNNN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？ 首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。==因此产生在磁盘中备份元数据的FsImage。== 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。==因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。==这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。 但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。==因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。== NN和2NN工作机制： Fsimage和Edits解析 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求。 （3）NameNode记录操作日志，更新滚动日志。 （4）NameNode在内存中对数据进行增删改。 第二阶段：Secondary NameNode工作 （1）Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode是否检查结果。 （2）Secondary NameNode 请求执行 CheckPoint。 （3）NameNode 滚动正在写的 Edits日志。 （4）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。 （5）Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。 （6）生成新的镜像文件 fsimage.chkpoint。 （7）拷贝 fsimage.chkpoint 到 NameNode。 （8）NameNode将 fsimage.chkpoint 重新命名成 fsimage。 ==NN和2NN工作机制详解== Fsimage：NameNode内存中元数据序列化后形成的文件。HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。 Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。 seen_txid：文件保存的是一个数字，就是最后一个edits_的数字 每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。 NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。 由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。 SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。 oiv 查看 Fsimage文件 hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径 [atguigu@hadoop102 current]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current [atguigu@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml [atguigu@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml # 将显示的xml文件内容拷贝到Eclipse中创建的xml文件中，并格式化。部分显示结果如下。 &lt;inode&gt; &lt;id&gt;16386&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;user&lt;/name&gt; &lt;mtime&gt;1512722284477&lt;/mtime&gt; &lt;permission&gt;atguigu:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt; &lt;/inode&gt; &lt;inode&gt; &lt;id&gt;16387&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;atguigu&lt;/name&gt; &lt;mtime&gt;1512790549080&lt;/mtime&gt; &lt;permission&gt;atguigu:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt; &lt;/inode&gt; &lt;inode&gt; &lt;id&gt;16389&lt;/id&gt; &lt;type&gt;FILE&lt;/type&gt; &lt;name&gt;wc.input&lt;/name&gt; &lt;replication&gt;3&lt;/replication&gt; &lt;mtime&gt;1512722322219&lt;/mtime&gt; &lt;atime&gt;1512722321610&lt;/atime&gt; &lt;perferredBlockSize&gt;134217728&lt;/perferredBlockSize&gt; &lt;permission&gt;atguigu:supergroup:rw-r--r--&lt;/permission&gt; &lt;blocks&gt; &lt;block&gt; &lt;id&gt;1073741825&lt;/id&gt; &lt;genstamp&gt;1001&lt;/genstamp&gt; &lt;numBytes&gt;59&lt;/numBytes&gt; &lt;/block&gt; &lt;/blocks&gt; &lt;/inode &gt; 思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？ 在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。 oev 查看Edits文件 hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 [atguigu@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml [atguigu@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml # 将显示的xml文件内容拷贝到Eclipse中创建的xml文件中，并格式化。显示结果如下 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;EDITS&gt; &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;129&lt;/TXID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;130&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;16407&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943607866&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_-1544295051_1&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;192.168.1.5&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;atguigu&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;RPC_CLIENTID&gt;908eafd4-9aec-4288-96f1-e8011d181561&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ALLOCATE_BLOCK_ID&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;131&lt;/TXID&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_SET_GENSTAMP_V2&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;132&lt;/TXID&gt; &lt;GENSTAMPV2&gt;1016&lt;/GENSTAMPV2&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD_BLOCK&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;133&lt;/TXID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;0&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;RPC_CLIENTID&gt;&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;-2&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_CLOSE&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;134&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;0&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943608761&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;false&lt;/OVERWRITE&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;25&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;atguigu&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;/EDITS &gt; 思考：NameNode如何确定下次开机启动的时候合并哪些Edits？ CheckPoint时间设置 通常情况下，SecondaryNameNode每隔一小时执行一次 [hdfs-default.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt; &lt;/property&gt; 一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt; &lt;/property &gt; NameNode故障处理NameNode故障后，可以采用如下两种方法恢复数据。 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录； kill -9 NameNode进程 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） 拷贝SecondaryNameNode中数据到原NameNode存储数据目录 重新启动NameNode [atguigu@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* [atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/ [atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中 修改hdfs-site.xml中的 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; kill -9 NameNode进程 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [atguigu@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 [atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./ [atguigu@hadoop102 namesecondary]$ rm -rf in_use.lock [atguigu@hadoop102 dfs]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs [atguigu@hadoop102 dfs]$ ls data name namesecondary 导入检查点数据（等待一会ctrl+c结束掉） [atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint 启动NameNode [atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 集群安全模式 集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。 bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态） NameNode多目录配置NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性 具体配置如下: 在hdfs-site.xml文件中增加如下内容 &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///${hadoop.tmp.dir}/dfs/name1,file:///${hadoop.tmp.dir}/dfs/name2&lt;/value&gt; &lt;/property&gt; 停止集群，删除data和logs中所有数据 格式化集群并启动 查看结果 [atguigu@hadoop102 dfs]$ ll 总用量 12 drwx------. 3 atguigu atguigu 4096 12月 11 08:03 data drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1 drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"视图&存储过程&流程控制","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/03-MySQL/MySQL06-视图&存储过程&流程控制.html","text":"视图视图：MySQL从5.0.1版本开始提供视图功能。一种虚拟存在的表，行和列的数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的，只保存了sql逻辑，不保存查询结果。 《sql础教程第2版》用一句话非常凝练的概括了视图与表的区别—“==是否保存了实际的数据==”。所以视图并不是数据库真实存储的数据表，它可以看作是一个窗口，通过这个窗口我们可以看到数据库表中真实存在的数据。所以我们要区别视图和数据表的本质，即视图是基于真实表的一张虚拟的表，其数据来源均建立在真实表的基础上。 那既然已经有数据表了，为什么还需要视图呢？主要有以下几点原因： 通过定义视图可以将频繁使用的SELECT语句保存以提高效率。 通过定义视图可以使用户看到的数据更加清晰。 通过定义视图可以不对外公开数据表全部字段，增强数据的保密性。 通过定义视图可以降低数据的冗余。 需要注意的是视图名在数据库中需要是唯一的，不能与其他视图和表重名。视图不仅可以基于真实表，我们也可以在视图的基础上继续创建视图。 创建视图/*语法： create view 视图名 as 查询语句; */ USE myemployees; #查询姓名中包含a字符的员工名、部门名和工种信息 #①创建 CREATE VIEW myv1 AS SELECT last_name,department_name,job_title FROM employees e JOIN departments d ON e.department_id = d.department_id JOIN jobs j ON j.job_id = e.job_id; #②使用 SELECT * FROM myv1 WHERE last_name LIKE &#39;%a%&#39;; #查询各部门的平均工资级别 #①创建视图查看每个部门的平均工资 CREATE VIEW myv2 AS SELECT AVG(salary) ag,department_id FROM employees GROUP BY department_id; #②使用 SELECT myv2.`ag`,g.grade_level FROM myv2 JOIN job_grades g ON myv2.`ag` BETWEEN g.`lowest_sal` AND g.`highest_sal`; #查询平均工资最低的部门信息 SELECT * FROM myv2 ORDER BY ag LIMIT 1; #查询平均工资最低的部门名和工资 CREATE VIEW myv3 AS SELECT * FROM myv2 ORDER BY ag LIMIT 1; SELECT d.*,m.ag FROM myv3 m JOIN departments d ON m.`department_id`=d.`department_id`; 视图修改、删除、查看#方式一： /* create or replace view 视图名 as 查询语句; */ SELECT * FROM myv3 CREATE OR REPLACE VIEW myv3 AS SELECT AVG(salary),job_id FROM employees GROUP BY job_id; #方式二： /* 语法： alter view 视图名 as 查询语句; */ ALTER VIEW myv3 AS SELECT * FROM employees; # 删除视图 /* 语法：drop view 视图名,视图名,...; */ DROP VIEW emp_v1,emp_v2,myv3; # 查看视图 DESC myv3; SHOW CREATE VIEW myv3; 视图的更新因为视图是一个虚拟表，所以对视图的操作就是对底层基础表的操作，所以在修改时只有满足底层基本表的定义才能成功修改。 对于一个视图来说，如果包含以下结构的任意一种都是不可以被更新的： 聚合函数 SUM()、MIN()、MAX()、COUNT() 等。 DISTINCT 关键字。 GROUP BY 子句。 HAVING 子句。 UNION 或 UNION ALL 运算符。 FROM 子句中包含多个表。 CREATE OR REPLACE VIEW myv1 AS SELECT last_name,email FROM employees; #插入、会影响原始表 INSERT INTO myv1 VALUES(&#39;张飞&#39;,&#39;zf@qq.com&#39;); #修改、会影响原始表 UPDATE myv1 SET last_name = &#39;张无忌&#39; WHERE last_name=&#39;张飞&#39;; #删除、会影响原始表 DELETE FROM myv1 WHERE last_name = &#39;张无忌&#39;; /* 视图的可更新性和视图中查询的定义有关系，以下类型的视图是不能更新的。 • 包含以下关键字的sql语句：分组函数、distinct、group by、having、union或者union all • 常量视图 • Select中包含子查询 • join • from一个不能更新的视图 • where子句的子查询引用了from子句中的表 */ ​ 创建语法的关键字 是否实际占用物理空间 使用 视图 create view 只是保存了sql逻辑 增删改查，只是一般不能增删改 表 create table 保存了数据 增删改查 变量、存储过程和函数变量系统变量：全局变量、会话变量 自定义变量：用户变量、局部变量 系统变量：说明：变量由系统定义，不是用户定义，属于服务器层面；注意：全局变量需要添加global关键字，会话变量需要添加session关键字，如果不写，==默认会话级别==；全局变量作用域：针对于所有会话（连接）有效，但不能跨重启；会话变量作用域：针对于当前会话（连接）有效。 1、查看所有系统变量show global|【session】variables; 2、查看满足条件的部分系统变量show global|【session】 variables like ‘%char%’; 3、查看指定的系统变量的值select @@global|【session】系统变量名; 4、为某个系统变量赋值；方式一：set global|【session】系统变量名=值；方式二：set @@global|【session】系统变量名=值; 自定义变量：说明：变量由用户自定义，而不是系统提供的；使用步骤：1、声明2、赋值3、使用（查看、比较、运算等）； 用户变量：用户变量作用域：针对于当前会话（连接）有效，作用域同于会话变量。 赋值操作符：=或 := ①声明并初始化: SET @变量名=值;SET @变量名:=值;SELECT @变量名:=值; ②赋值（更新变量的值） 方式一：SET @变量名=值; SET @变量名:=值; SELECT @变量名:=值; 方式二：SELECT 字段 INTO @变量名 FROM 表; ③使用（查看变量的值）：SELECT @变量名; 局部变量：作用域：仅仅在定义它的begin end块中有效，应用==在 begin end中的第一句话== ①声明：DECLARE 变量名 类型； DECLARE 变量名 类型 【DEFAULT 值】; ②赋值（更新变量的值） 方式一：SET 局部变量名=值;SET 局部变量名:=值; SELECT 局部变量名:=值; 方式二：SELECT 字段 INTO 具备变量名 FROM 表; ③使用（查看变量的值）SELECT 局部变量名; 用户变量和局部变量的对比： ​ 作用域 定义位置 语法 用户变量 当前会话 会话的任何地方 加@符号，不用指定类型 局部变量 定义它的BEGIN END中 BEGIN END的第一句话 一般不用加@,需要指定类型 存储过程存储过程和函数：类似于java中的方法 含义：一组预先编译好的SQL语句的集合，理解成批处理语句 提高代码的重用性 简化操作 减少了编译次数并且减少了和数据库服务器的连接次数，提高了效率 #创建语法 CREATE PROCEDURE 存储过程名(参数列表) BEGIN 存储过程体（一组合法的SQL语句） END /*参数列表包含三部分 参数模式 参数名 参数类型 参数模式： in：该参数可以作为输入，也就是该参数需要调用方传入值 out：该参数可以作为输出，也就是该参数可以作为返回值 inout：该参数既可以作为输入又可以作为输出，也就是该参数既需要传入值，又可以返回值 举例： in stuname varchar(20) 如果存储过程体仅仅只有一句话，begin end可以省略 存储过程体中的每条sql语句的结尾要求必须加分号。 存储过程的结尾可以使用 delimiter 重新设置 语法： delimiter 结束标记 案例： delimiter $ */ #调用语法 CALL 存储过程名(实参列表); #案例 创建存储过程实现，用户是否登录成功 CREATE PROCEDURE myp4(IN username VARCHAR(20),IN PASSWORD VARCHAR(20)) BEGIN DECLARE result INT DEFAULT 0;#声明并初始化 SELECT COUNT(*) INTO result#赋值 FROM admin WHERE admin.username = username AND admin.password = PASSWORD; SELECT IF(result&gt;0,&#39;成功&#39;,&#39;失败&#39;);#使用 END $ #调用 CALL myp3(&#39;张飞&#39;,&#39;8888&#39;)$ #案例 根据输入的女神名，返回对应的男神名和魅力值 CREATE PROCEDURE myp7(IN beautyName VARCHAR(20),OUT boyName VARCHAR(20),OUT usercp INT) BEGIN SELECT boys.boyname, boys.usercp INTO boyname,usercp FROM boys RIGHT JOIN beauty b ON b.boyfriend_id = boys.id WHERE b.name=beautyName ; END $ #调用 CALL myp7(&#39;小昭&#39;,@name,@cp)$ SELECT @name,@cp$ #存储过程的删除，不能修改存储过程 #语法：drop procedure 存储过程名，一次只能删除一个 DROP PROCEDURE p1; DROP PROCEDURE p2,p3;#× #存储过程的查看 DESC myp2;× SHOW CREATE PROCEDURE myp2; 函数含义：一组预先编译好的SQL语句的集合，理解成批处理语句 提高代码的重用性 简化操作 减少了编译次数并且减少了和数据库服务器的连接次数，提高了效率 区别： 存储过程：可以有0个返回，也可以有多个返回，适合做批量插入、批量更新 函数：有且仅有1 个返回，适合做处理数据后返回一个结果 #创建语法 CREATE FUNCTION 函数名(参数列表) RETURNS 返回类型 BEGIN 函数体 END /*注意： 1.参数列表 包含两部分： 参数名 参数类型 2.函数体：肯定会有return语句，如果没有会报错 如果return语句没有放在函数体的最后也不报错，但不建议 return 值; 3.函数体中仅有一句话，则可以省略begin end 4.使用 delimiter语句设置结束标记*/ #调用语法 SELECT 函数名(参数列表) #案例：返回公司的员工个数 CREATE FUNCTION myf1() RETURNS INT BEGIN DECLARE c INT DEFAULT 0;#定义局部变量 SELECT COUNT(*) INTO c#赋值 FROM employees; RETURN c; END $ SELECT myf1()$ #案例 根据部门名，返回该部门的平均工资 CREATE FUNCTION myf3(deptName VARCHAR(20)) RETURNS DOUBLE BEGIN DECLARE sal DOUBLE ; SELECT AVG(salary) INTO sal FROM employees e JOIN departments d ON e.department_id = d.department_id WHERE d.department_name=deptName; RETURN sal; END $ SELECT myf3(&#39;IT&#39;)$ #查看和删除 SHOW CREATE FUNCTION myf3; DROP FUNCTION myf3; 流程控制结构分支结构if函数 语法：if(条件,值1，值2) 功能：实现双分支，应用任何地方。 if结构 语法： if 条件1 then 语句1; elseif 条件2 then 语句2; …. else 语句n; end if; 功能：类似于多重if 只能应用在begin end 中 #案例 创建函数，实现传入成绩，如果成绩&gt;90,返回A，如果成绩&gt;80,返回B，如果成绩&gt;60,返回C，否则返回D CREATE FUNCTION test_if(score FLOAT) RETURNS CHAR BEGIN DECLARE ch CHAR DEFAULT &#39;A&#39;; IF score&gt;90 THEN SET ch=&#39;A&#39;; ELSEIF score&gt;80 THEN SET ch=&#39;B&#39;; ELSEIF score&gt;60 THEN SET ch=&#39;C&#39;; ELSE SET ch=&#39;D&#39;; END IF; RETURN ch; END $ SELECT test_if(87)$ #案例 创建存储过程，如果工资&lt;2000,则删除，如果5000&gt;工资&gt;2000,则涨工资1000，否则涨工资500 CREATE PROCEDURE test_if_pro(IN sal DOUBLE) BEGIN IF sal&lt;2000 THEN DELETE FROM employees WHERE employees.salary=sal; ELSEIF sal&gt;=2000 AND sal&lt;5000 THEN UPDATE employees SET salary=salary+1000 WHERE employees.`salary`=sal; ELSE UPDATE employees SET salary=salary+500 WHERE employees.`salary`=sal; END IF; END $ CALL test_if_pro(2100)$ case结构 语法： 情况1：类似于switch case 变量 | 表达式 | 字段 when 值1 then 语句1; when 值2 then 语句2; … else 语句n或要返回的值n; end 情况2： case when 条件1 then 语句1; when 条件2 then 语句2; … else 语句n或要返回的值n; end 作为独立的语句只能在begin end 中或外面；作为表达式，嵌套在其他语句中使用，可以用在任何地方。 else可以省略，若else省略，并when的所有条件都不满足，则返回null。 #案例 创建函数，实现传入成绩，如果成绩&gt;90,返回A，如果成绩&gt;80,返回B，如果成绩&gt;60,返回C，否则返回D CREATE FUNCTION test_case(score FLOAT) RETURNS CHAR BEGIN DECLARE ch CHAR DEFAULT &#39;A&#39;; CASE WHEN score&gt;90 THEN SET ch=&#39;A&#39;; WHEN score&gt;80 THEN SET ch=&#39;B&#39;; WHEN score&gt;60 THEN SET ch=&#39;C&#39;; ELSE SET ch=&#39;D&#39;; END CASE; RETURN ch; END $ SELECT test_case(56)$ 循环结构分类：while、loop、repeat 循环控制：iterate类似于 continue，继续，结束本次循环，继续下一次；leave 类似于 break，跳出，结束当前所在的循环。 while 语法： 【标签:】while 循环条件 do ​ 循环体; end while【 标签】; #案例：批量插入，根据次数插入到admin表中多条记录 DROP PROCEDURE pro_while1$ CREATE PROCEDURE pro_while1(IN insertCount INT) BEGIN DECLARE i INT DEFAULT 1; WHILE i&lt;=insertCount DO INSERT INTO admin(username,`password`) VALUES(CONCAT(&#39;Rose&#39;,i),&#39;666&#39;); SET i=i+1; END WHILE; END $ CALL pro_while1(100)$ #添加leave语句 #案例 批量插入，根据次数插入到admin表中多条记录，如果次数&gt;20则停止 TRUNCATE TABLE admin$ DROP PROCEDURE test_while1$ CREATE PROCEDURE test_while1(IN insertCount INT) BEGIN DECLARE i INT DEFAULT 1; a:WHILE i&lt;=insertCount DO INSERT INTO admin(username,`password`) VALUES(CONCAT(&#39;xiaohua&#39;,i),&#39;0000&#39;); IF i&gt;=20 THEN LEAVE a; END IF; SET i=i+1; END WHILE a; END $ CALL test_while1(100)$ loop 语法： 【标签:】loop ​ 循环体; end loop 【标签】; 可以用来模拟简单的死循环 repeat 语法：使用 leave 跳出循环。 【标签：】repeat ​ 循环体; until 结束循环的条件 end repeat 【标签】; /*已知表stringcontent 其中字段： id 自增长 content varchar(20) 向该表插入指定个数的，随机的字符串 */ DROP TABLE IF EXISTS stringcontent; CREATE TABLE stringcontent( id INT PRIMARY KEY AUTO_INCREMENT, content VARCHAR(20) ); DELIMITER $ CREATE PROCEDURE test_randstr_insert(IN insertCount INT) BEGIN DECLARE i INT DEFAULT 1; DECLARE str VARCHAR(26) DEFAULT &#39;abcdefghijklmnopqrstuvwxyz&#39;; DECLARE startIndex INT;#代表初始索引 DECLARE len INT;#代表截取的字符长度 WHILE i&lt;=insertcount DO SET startIndex=FLOOR(RAND()*26+1);#代表初始索引，随机范围1-26 SET len=FLOOR(RAND()*(20-startIndex+1)+1);#代表截取长度，随机范围1-（20-startIndex+1） INSERT INTO stringcontent(content) VALUES(SUBSTR(str,startIndex,len)); SET i=i+1; END WHILE; END $ CALL test_randstr_insert(10)$","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://mtcai.github.io/tags/MySQL/"}]},{"title":"HDFS-DataNode","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop04-HDFS-DataNode.html","text":"DataNode工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。 &lt;!-- DN 向 NN 汇报当前解读信息的时间间隔，默认6 小时 --&gt; &lt;property&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; &lt;value&gt;21600000&lt;/value&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt; &lt;/property&gt; &lt;!-- DN 扫描自己节点块信息列表的时间，默认6 小时 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.directoryscan.interval&lt;/name&gt; &lt;value&gt;21600s&lt;/value&gt; &lt;description&gt;Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.&lt;/description&gt; &lt;/property&gt; 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。 集群运行中可以安全加入和退出一些机器。 数据完整性如下是DataNode节点保证数据完整性的方法： 当DataNode读取Block的时候，它会计算CheckSum。 如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。 Client读取其他DataNode上的Block。 DataNode在其文件创建后周期验证CheckSum，如图3-16所示 掉线时限参数设置 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为==毫秒==，dfs.heartbeat.interval的单位为==秒==。 &lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; 服役新数据节点 直接启动DataNode，即可关联到集群 [atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode [atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager 如果数据不均衡，可以用命令实现集群的再平衡 [atguigu@hadoop102 sbin]$ ./start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 退役旧数据节点添加白名单添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。 配置白名单的具体步骤如下： 在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件 [atguigu@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [atguigu@hadoop102 hadoop]$ touch dfs.hosts [atguigu@hadoop102 hadoop]$ vi dfs.hosts # 添加如下主机名称（不添加hadoop105） hadoop102 hadoop103 hadoop104 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性 &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt; &lt;/property&gt; 配置文件分发 [atguigu@hadoop102 hadoop]$ xsync hdfs-site.xml 刷新NameNode [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes 更新ResourceManager节点 [atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 如果数据不均衡，可以用命令实现集群的再平衡 [atguigu@hadoop102 sbin]$ ./start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 黑名单退役在黑名单上面的主机都会被强制退出。 在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件 [atguigu@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [atguigu@hadoop102 hadoop]$ touch dfs.hosts.exclude [atguigu@hadoop102 hadoop]$ vi dfs.hosts.exclude # 添加如下主机名称（要退役的节点） hadoop105 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 &lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt; &lt;/property&gt; 刷新NameNode、刷新ResourceManager [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点，如图所示 等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役，如图所示 [atguigu@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode stopping datanode [atguigu@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager stopping nodemanager 如果数据不均衡，可以用命令实现集群的再平衡 [atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-atguigu-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 注意：不允许白名单和黑名单中同时出现同一个主机名称。 Datanode多目录配置DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2&lt;/value&gt; &lt;/property&gt;","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"HDFS2.x新特性与HA","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop05-HDFS2.X新特性和高可用(HA).html","text":"HDFS 2.X新特性集群间数据拷贝采用distcp命令实现两个Hadoop集群之间的递归数据复制： [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp hdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt 小文件存档 每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。 HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存。 案例： 需要启动YARN进程：start-yarn.sh 归档文件 把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。 [atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName input.har –p /user/atguigu/input /user/atguigu/output 查看归档 [atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/atguigu/output/input.har [atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/atguigu/output/input.har 解归档文件 [atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/atguigu/output/input.har/* /user/atguigu 回收站开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。 开启回收站功能参数说明： 默认值fs.trash.interval=0，0表示禁用回收站;其他值表示设置文件的存活时间。 默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。 要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。 回收站工作机制： 启用回收站 修改core-site.xml，配置垃圾回收时间为1分钟。 &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 查看回收站 回收站在集群中的路径：/user/atguigu/.Trash/…. 修改访问垃圾回收站用户名称 进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户. 修改core-site.xml &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt; &lt;/property&gt; 通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 Trash trash = New Trash(conf); trash.moveToTrash(path); 恢复回收站数据 [atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/atguigu/.Trash/Current/user/atguigu/input /user/atguigu/input 清空回收站 [atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -expunge 快照管理快照相当于对目录做一个备份。并不会立即复制所有文件，而是记录文件变化。 参数描述： hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能） hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用） hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照） hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照） hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照） hdfs lsSnapshottableDir （功能描述：列出当前用户所有可快照目录） hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处） hdfs dfs -deleteSnapshot （功能描述：删除快照） 开启/禁用指定目录的快照功能 [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/atguigu/input [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/atguigu/input 对目录创建快照 [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/input # 通过web访问hdfs://hadoop102:50070/user/atguigu/input/.snapshot/s…..// 快照和源文件使用相同数据 [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/atguigu/input/.snapshot/ 指定名称创建快照 [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/atguigu/input miao170508 重命名快照 [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/atguigu/input/ miao170508 atguigu170508 列出当前用户所有可快照目录 [atguigu@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir 比较两个快照目录的不同之处 [atguigu@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/atguigu/input/ . .snapshot/atguigu170508 恢复快照 atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/atguigu/input/.snapshot/s20170708-134303.027 /user HDFS HA高可用HA概述所谓HA（High Available），即高可用（7*24小时不中断服务）。实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。 Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。NameNode主要在以下两个方面影响HDFS集群： NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启 NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用 HDFS HA功能通过配置Active/Standby两个NameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。 HDFS-HA工作机制通过双NameNode消除单点故障 工作要点 元数据管理方式需要改变 内存中各自保存一份元数据； Edits日志只有Active状态的NameNode节点可以做写操作； 两个NameNode都可以读取Edits； 共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）； 需要一个状态管理功能模块 实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在NameNode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。 必须保证两个NameNode之间能够ssh无密码登录 隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务 自动故障转移工作机制前面学习了使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程，如图所示。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能： 故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。 现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。 ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责： 健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。 ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。 基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为Active状态。 HDFS-HA集群配置规划集群 hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager NodeManager NodeManager NodeManager 配置HDFS-HA集群 复制已有的 hadoop-2.7.2 到新目录下； 配置core-site.xml &lt;configuration&gt; &lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/module/HA/hadoop-2.7.2/data/tmp/jn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 配置hdfs-site.xml &lt;configuration&gt; &lt;!-- 完全分布式集群名称 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群中NameNode节点都有哪些 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/atguigu/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 声明journalnode服务器存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/jn&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭权限检查--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 拷贝配置好的hadoop环境到其他节点 注意：这些参数读取后为K-V对儿，参数写错文件也没事。 启动HDFS-HA集群 三台机器分别启动zookeeper。 在各个JournalNode节点上，输入以下命令启动journalnode服务 sbin/hadoop-daemon.sh start journalnode 在[nn1]上，对其进行格式化，并启动： bin/hdfs namenode -format sbin/hadoop-daemon.sh start namenode 在[nn2]上，同步nn1的元数据信息，并启动 [nn2] bin/hdfs namenode -bootstrapStandby sbin/hadoop-daemon.sh start namenode 查看web页面显示 在[nn1]上，启动所有datanode sbin/hadoop-daemons.sh start datanode 将[nn1]切换为Active bin/hdfs haadmin -transitionToActive nn1 查看是否Active bin/hdfs haadmin -getServiceState nn1 为了防止脑裂，手动模式下两个NameNode必须都起来才能切换NameNode。自动故障转移会自动切换NameNode，因为zkfc会kill -9 原先的NameNode进程，防止脑裂。 配置HDFS-HA自动故障转移 增加 hdfs-site.xml 配置 &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 增加 core-site.xml 配置 &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; 启动 # 关闭所有HDFS服务：NameNode、DataNode、journalnode、zkfc sbin/stop-dfs.sh # 在各个机器启动Zookeeper集群 bin/zkServer.sh start # 初始化HA在Zookeeper中状态，然后在Zookeeper根目录下有集群节点 bin/hdfs zkfc -formatZK # 启动HDFS服务 sbin/start-dfs.sh # 在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode sbin/hadoop-daemin. sh start zkfc YARN-HA配置YARN-HA工作机制 配置YARN-HA集群 hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager ResourceManager NodeManager NodeManager NodeManager 配置 yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--启用resourcemanager ha--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--声明两台resourcemanager的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster-yarn1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop102&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper集群的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 同步更新其他节点的配置信息 启动hdfs # 已经格式化NameNode了，这里不需要再格式化 # 启动HFDS sbin/start-dfs.sh 启动YARN # 在hadoop102中执行 sbin/start-yarn.sh # 在hadoop103中执行 sbin/yarn-daemon.sh start resourcemanager 查看服务状态 注意：执行start-yarn.sh并不能启动103的RM，需要单独在103启动RM，此时102为active。访问103:8088会自动跳转到102:8088。如果kill -9 102RM，无法访问102:8088，103RM自动提升为active，103:8088不再跳转。 HDFS Federation结构设计 NameNode架构的局限性 Namespace（命名空间）的限制 由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB。 隔离问题 由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。 性能的瓶颈 由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量。 HDFS Federation架构设计 能不能有多个NameNode | NameNode | NameNode | NameNode || ———— | ———— | ————————- || 元数据 | 元数据 | 元数据 || Log | machine | 电商数据/话单数据 | HDFS Federation应用思考 不同应用可以使用不同NameNode进行数据管理 图片业务、爬虫业务、日志审计业务 Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"MapReduce概述与序列化","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop06-MapReduce概述&序列化.html","text":"MapReduce概述定义MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。 MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。 优点 MapReduce 易于编程：它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 良好的扩展性：当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力 高容错性：MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。 适合PB级以上海量数据的离线处理：可以实现上千台服务器集群并发工作，提供数据处理能力。 缺点 不擅长实时计算：MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果 不擅长式计算：流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 不擅长DAG（有向图）计算：多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 核心思想 分布式的运算程序往往需要分成至少2个阶段。 第一个阶段的MapTask并发实例，完全并行运行，互不相干。 第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。 MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。 总结：分析WordCount数据流走向深入理解MapReduce核心思想。 MapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调。 MapTask：负责Map阶段的整个数据处理流程 ReduceTask：负责Reduce阶段的整个数据处理流程。 常用数据序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable MapReduce编程规范用户编写的程序分成三个部分：Mapper、Reducer和Driver。 Mapper阶段 用户自定义的Mapper要继承自己的父类 Mapper的输入数据是KV对的形式（KV的类型可自定义） Mapper中的业务逻辑写在map()方法中 Mapper的输出数据是KV对的形式（KV的类型可自定义） map()方法（MapTask进程）对每一个调用一次 Reducer阶段 用户自定义的Reducer要继承自己的父类 Reducer的输入数据类型对应Mapper的输出数据类型，也是KV Reducer的业务逻辑写在reduce()方法中 ReduceTask进程对每一组相同k的组调用一次reduce()方法 Driver阶段相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象。 实现WordCount需求在给定的文本文件中统计输出每一个单词出现的总次数。 # 输入 atguigu atguigu ss ss cls cls jiao banzhang xue hadoop # 输出 atguigu 2 banzhang 1 cls 2 hadoop 1 jiao 1 ss 2 xue 1 环境准备创建maven工程，在pom.xml文件中添加如下依赖：（可选：utf-8编码，maven镜像源） &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入：（目的是打印日志） log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 写Mapper类package com.atguigu.mapreduce; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{ Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { // 1 获取一行 String line = value.toString(); // 2 切割 String[] words = line.split(&quot; &quot;); // 3 输出 for (String word : words) { k.set(word); context.write(k, v); } } } 写Reduce类package com.atguigu.mapreduce.wordcount; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ int sum; IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException { // 1 累加求和 sum = 0; for (IntWritable count : values) { sum += count.get(); } // 2 输出 v.set(sum); context.write(key,v); } } 写Driver驱动类package com.atguigu.mapreduce.wordcount; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class WordcountDriver { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { // 1 获取配置信息以及封装任务 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 关联map和reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); } } 测试1 本地测试： 编译Hadoop，在Windows环境上配置HADOOP_HOME环境变量。直接run 2 集群上测试：需要先生成jar包，然后cp到集群运行 pom.xml添加： &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin &lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!-- Driver类的全名 --&gt; &lt;mainClass&gt;com.atguigu.mr.WordcountDriver&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 运行 maven install。等待编译完成就会在项目的target文件夹中生成jar包。修改不带依赖的jar包名称为wc.jar，并拷贝该jar包到Hadoop集群。 [atguigu@hadoop102 software]$ hadoop jar wc.jar com.atguigu.wordcount.WordcountDriver /user/atguigu/input /user/atguigu/output Hadoop序列化为什么需要序列化序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。 一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。 问：为什么不用Java的序列化 Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）。 Hadoop序列化特点： 紧凑 ：高效使用存储空间。 快速：读写数据的额外开销小。 可扩展：随着通信协议的升级而可升级 互操作：支持多语言的交互 自定义bean对象实现序列化自定义序列化要求在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。 必须实现Writable接口：重写序列化方法和重写反序列化方法 // 重写序列化方法 @Override public void write(DataOutput out) throws IOException { out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); } // 重写反序列化方法 @Override public void readFields(DataInput in) throws IOException { upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong(); } 注意反序列化的顺序和序列化的顺序完全一致 反序列化时，需要反射调用空参构造函数，所以必须有空参构造 public FlowBean() { super(); } 要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。 如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。详见后面排序案例。 @Override public int compareTo(FlowBean o) { // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1; } 案例需求：统计每一个手机号耗费的总上行流量、下行流量、总流量。 key为手机号，values为上行，下行流量；则基本数据类型不够用，需要传输对象，计算中存在集群间拷贝数据，还需要序列化对象。 // 输入数据 1 13736230513 192.196.100.1 www.atguigu.com 2481 24681 200 2 13846544121 192.196.100.2 264 0 200 3 13956435636 192.196.100.3 132 1512 200 4 13966251146 192.168.100.1 240 0 404 5 18271575951 192.168.100.2 www.atguigu.com 1527 2106 200 6 84188413 192.168.100.3 www.atguigu.com 4116 1432 200 7 13590439668 192.168.100.4 1116 954 200 8 15910133277 192.168.100.5 www.hao123.com 3156 2936 200 9 13729199489 192.168.100.6 240 0 200 10 13630577991 192.168.100.7 www.shouhu.com 6960 690 200 11 15043685818 192.168.100.8 www.baidu.com 3659 3538 200 12 15959002129 192.168.100.9 www.atguigu.com 1938 180 500 13 13560439638 192.168.100.10 918 4938 200 14 13470253144 192.168.100.11 180 180 200 15 13682846555 192.168.100.12 www.qq.com 1938 2910 200 16 13992314666 192.168.100.13 www.gaga.com 3008 3720 200 17 13509468723 192.168.100.14 www.qinghua.com 7335 110349 404 18 18390173782 192.168.100.15 www.sogou.com 9531 2412 200 19 13975057813 192.168.100.16 www.baidu.com 11058 48243 200 20 13768778790 192.168.100.17 120 120 200 21 13568436656 192.168.100.18 www.alibaba.com 2481 24681 200 22 13568436656 192.168.100.19 1116 954 200 // 期望输出格式 13560436666 1116 954 2070 手机号码 上行流量 下行流量 总流量 流量统计的Bean对象package com.atguigu.mapreduce.flowsum; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import org.apache.hadoop.io.Writable; // 1 实现writable接口 public class FlowBean implements Writable{ private long upFlow; private long downFlow; private long sumFlow; //2 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() { super(); } public FlowBean(long upFlow, long downFlow) { super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; } //3 写序列化方法 @Override public void write(DataOutput out) throws IOException { out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); } //4 反序列化方法 //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException { this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); } // 6 编写toString方法，方便后续打印到文本 @Override public String toString() { return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; } public long getUpFlow() { return upFlow; } public void setUpFlow(long upFlow) { this.upFlow = upFlow; } public long getDownFlow() { return downFlow; } public void setDownFlow(long downFlow) { this.downFlow = downFlow; } public long getSumFlow() { return sumFlow; } public void setSumFlow(long sumFlow) { this.sumFlow = sumFlow; } } 写Mapper类package com.atguigu.mapreduce.flowsum; import java.io.IOException; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;{ FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); k.set(phoneNum); v.set(downFlow, upFlow); // 4 写出 context.write(k, v); } } 写Reducer类package com.atguigu.mapreduce.flowsum; import java.io.IOException; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; { @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context)throws IOException, InterruptedException { long sum_upFlow = 0; long sum_downFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) { sum_upFlow += flowBean.getUpFlow(); sum_downFlow += flowBean.getDownFlow(); } // 2 封装对象 FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow); // 3 写出 context.write(key, resultBean); } } 写Driver驱动类package com.atguigu.mapreduce.flowsum; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class FlowsumDriver { public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException { // 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[] { &quot;e:/input/inputflow&quot;, &quot;e:/output1&quot; }; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); } }","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"Hadoop数据压缩","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop08-Hadoop数据压缩.html","text":"概述压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、 Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。 鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。 压缩是提高Hadoop运行效率的一种优化策略。通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度。注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能。 运算密集型的job，少用压缩 IO密集型的job，多用压缩 MR支持的压缩编码压缩编码 压缩格式 hadoop自带 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFLATE 是，直接使用 DEFLATE .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFLATE .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s 压缩方式选择Gzip： 优点：压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便。 缺点：不支持Split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。 Bzip2： 优点：支持Split；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便。 缺点：压缩/解压速度慢。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split，而且兼容之前的应用程序的情况。 Lzo： 优点：压缩/解压速度也比较快，合理的压缩率；支持Split，是Hadoop中最流行的压缩格式；可以在Linux系统下安装lzop命令，使用方便。 缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理（为了支持Split需要建索引，还需要指定InputFormat为Lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显。 Snappy： 优点：高速压缩速度和合理的压缩率。 缺点：不支持Split；压缩率比Gzip要低；Hadoop本身不支持，需要安装。 应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入。 压缩位置压缩可以在MapReduce作用的任意阶段启用。 压缩参数为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 要在Hadoop中启用压缩，可以配置如下参数： 参数 默认值 阶段 建议 io.compression.codecs（在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress（在mapred-site.xml中配置） false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec mapper输出 企业多使用LZO或Snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置） false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置） RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 案例Map输出端采用压缩、Reduce输出端采用压缩即使你的MapReduce的输入输出文件都是未压缩的文件，仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可。 只修改驱动类即可，无需修改map和reduce类。压缩map端输出不会影响最终输出格式，reduce读取时会自动解压；压缩reduce段输出会改变最终输出格式，内容不变。 import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.io.compress.BZip2Codec; import org.apache.hadoop.io.compress.CompressionCodec; import org.apache.hadoop.io.compress.GzipCodec; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class WordCountDriver { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { Configuration configuration = new Configuration(); // 开启map端输出压缩 configuration.setBoolean(&quot;mapreduce.map.output.compress&quot;, true); // 设置map端输出压缩方式 configuration.setClass(&quot;mapreduce.map.output.compress.codec&quot;, BZip2Codec.class, CompressionCodec.class); Job job = Job.getInstance(configuration); job.setJarByClass(WordCountDriver.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 设置reduce端输出压缩开启 FileOutputFormat.setCompressOutput(job, true); // 设置压缩的方式 FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); // FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); // FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 1 : 0); } }","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"Yarn资源调度","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop09-Yarn资源调度.html","text":"Yarn基本架构Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。 Yarn工作机制 MR程序提交到客户端所在的节点。 YarnRunner 向 ResourceManager 申请一个 Application。 RM 将该应用程序的资源路径返回给 YarnRunner。 该程序将运行所需资源提交到 HDFS 上。 程序资源提交完毕后，申请运行 mrAppMaster。 RM 将用户的请求初始化成一个 Task。 其中一个NodeManager 领取到Task任务。 该 NodeManager 创建容器 Container，并产生 MRAppmaster。 Container 从 HDFS 上拷贝资源到本地。 MRAppmaster 向 RM 申请运行 MapTask 资源。 RM 将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 NodeManager 分别启动 MapTask，MapTask 对数据分区排序。 MrAppMaster 等待所有 MapTask 运行完毕后，向 RM 申请容器，运行 ReduceTask。 ReduceTask 向 MapTask 获取相应分区的数据。 程序运行完毕后，MRAppmaster 会向 RM 申请注销自己。 作业提交过程作业提交过程之HDFS&amp;MapReduce 作业提交过程之YARN （1）作业提交 第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。 第2步：Client向 RM 申请一个作业 id。 第3步：RM 给 Client 返回该 job 资源的提交路径和作业 id。 第4步：Client 提交 jar 包、切片信息和配置文件到指定的资源提交路径。 第5步：Client 提交完资源后，向 RM 申请运行 MrAppMaster。 （2）作业初始化 第6步：当 RM 收到 Client 的请求后，将该 job 添加到容量调度器中。 第7步：某一个空闲的 NM 领取到该 Job。 第8步：该 NM 创建 Container，并产生 MRAppmaster。 第9步：下载 Client 提交的资源到本地。 （3）任务分配 第10步：MrAppMaster 向 RM 申请运行多个 MapTask 任务资源。 第11步：RM 将运行 MapTask 任务分配给另外两 个NodeManager，另两个 NodeManager 分别领取任务并创建容器。 （4）任务运行 第12步：MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 NodeManager 分别启动 MapTask，MapTask 对数据分区排序。 第13步：MrAppMaster 等待所有 MapTask 运行完毕后，向 RM 申请容器，运行 ReduceTask。 第14步：ReduceTask 向 MapTask 获取相应分区的数据。 第15步：程序运行完毕后，MR 会向 RM 申请注销自己。 （5）进度和状态更新 YARN 中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。 （6）作业完成 除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。 资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2/3.1.3 默认的资源调度器是Capacity Scheduler。CDH框架默认调度器是Fair Scheduler。 具体设置详见：yarn-default.xml文件 &lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt; &lt;/property&gt; 公平调度器并发高，要求集群配置高 先进先出调度器并发最低，要求集群配置低 容量调度器并发和配置中等 先进先出调度器（FIFO） 优点：简单易懂； 缺点：不支持多队列，生产环境很少使用； 容量调度器（Capacity Scheduler）Capacity Scheduler 是Yahoo 开发的多用户调度器 公平调度器（Fair Scheduler）Fair Schedulere 是Facebook 开发的多用户调度器。 任务的推测执行作业完成时间取决于最慢的任务完成时间 一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。 思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。 执行推测任务的前提条件： 每个Task只能有一个备份任务; 当前Job已完成的Task必须不小于0.05（5%）； 开启推测执行参数设置。mapred-site.xml文件中默认是打开的。 &lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt; &lt;/property&gt; 不能启用推测执行机制情况 任务间存在严重的负载倾斜；(如Task执行10亿数据，Task2执行5条数据，Task2执行完后，不能再给Task1开备份任务。) 特殊任务，比如任务向数据库中写数据。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"生产调优手册","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop10-生产调优手册.html","text":"HDFS核心参数NameNode内存生产配置每个文件块大概占用 150byte，一台服务器 128G内存为例 128 * 1024 * 1024 * 1024 / 150Byte = 9.1亿 hadoop-env.sh hadoop2.x：NameNode内存默认 2000m，如果服务器内存 4G， NameNode内存可以配置 3G。 HADOOP_NAMENODE_OPTS=-Xmx 3072 m hadoop3.x：hadoop-env.sh中描述 Hadoop的内存是动态分配的，[ jmap -heap 线程号] 可以查看使用的最大内存。 export HDFS_NAMENODE_OPTS=”-Dhadoop.security.logger=INFO,RFAS -Xmx 1024 m” export HDFS_DATANODE_OPTS=”-Dhadoop.security.logger=ERROR,RFAS -Xmx 1024 m NameNode心跳并发配置 hdfs-site.xml NameNode 有一个工作线程池，用来处理不同 DataNode 的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大该参数 。 默认值是 10。 &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;21&lt;/value&gt; &lt;/property&gt; \\text{dfs.namenode.handler.count}=20*log_e^{Cluster\\_size}开启回收站配置 core-site.xml &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;!-- 单位是分钟 --&gt; &lt;/property&gt; &lt;!-- 1. 默认值 fs.trash.interval = 0, 0表示禁用回收站 其他值表示设置文件的存活时间。 2. 默认值 fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为 0，则该值设置和 fs.trash.interval的参数值相等。 3. 要求 fs.trash.checkpoint.interval &lt;= fs.trash.interval --&gt; 回收站目录在 HDFS集群中的路径： /user/atguigu/.Trash/…. 通过网页上直接删除的文件也不会走回收站。 通过程序删除的文件不会经过回收站，需要调用 moveToTrash()才 进入回收站 只有在命令行利用 hadoop fs -rm命令删除的文件才会走回收站。 恢复回收站数据: $ hadoop fs mv/user/atguigu/.Trash/Current/user/atguigu/input/user/atguigu/input HDFS集群压测写性能$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB 2021 02 09 10:43:16,853 INFO fs.TestDFSIO: TestDFSIO : write 2021 02 09 10:43:16,854 INFO fs.Te stDFSIO: Date &amp; time: Tue Feb 09 10:43:16 CST 2021 2021 02 09 10:43:16,854 INFO fs.TestDFSIO: Number of files: 10 2021 02 09 10:43:16,854 INFO fs.TestDFSIO: Total MBytes processed: 1280 2021 02 09 10:43:16,854 INFO fs.TestDFSIO: Throughput mb/sec: 1.61 2021 02 09 10:43:16,854 INFO fs.TestDFSIO: Average IO rate mb/sec: 1.9 2021 02 09 10:43:16,854 INFO fs.TestDFSIO: IO rate std deviation: 0.76 2021 02 09 10:43:16,854 INFO fs.TestDFSIO: Test exec time sec: 133.05 2021 02 09 10:43:16,854 INFO fs.TestDFSIO: nrFiles 为生成 mapTask的数量，设置为 CPU核数-1 Number of files：生成 mapTask数量 ，一般是集群中 CPU核数 -1） Total MBytes processed：单个 map处理的文件大小 Throughput mb/sec：单个 mapTak的吞吐量 Average IO rate mb/sec：平均 mapTak的吞吐量 IO rate std deviation：方差、反映各个 mapTask处理的差值，越小越均衡 注意：如果测试过程中，出现异常，在 yarn-site.xml中设置虚拟内存检测为 false &lt;!-- 是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 true--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem check enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 分发配置并重启 Yarn集群 测试结果分析： 由于副本1在本地，则改副本不参与测试。共计10个文件*2个副本=20个文件。 1.61*20=32M/s 三台服务器均为100Mbps，100/8*3=30M/s 可以认为网络资源都已经用满。 如果实测速度远远小于网络，并且实测速度不能满足工作需求，可以考虑采用固态硬盘或者增加磁盘个数。 如果客服端不在集群节点，则三个副本都参与计算。 读性能$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient 3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB 2021 02 09 11:34:15,847 INFO fs.TestDFSIO: TestDFSIO : read 2021 02 09 11:34:15,847 INFO fs.TestDFSIO: Date &amp; time: Tue Feb 09 11:34:15 CST 2021 2021 02 09 11:34:15,847 INFO fs.TestDFSIO: Number of files: 10 2021 02 09 11:34:15,847 INFO fs.TestDFSIO: Total MBytes processed: 1280 2021 02 09 11:3 4:15,848 INFO fs.TestDFSIO: Throughput mb/sec: 200.28 2021 02 09 11:34:15,848 INFO fs.TestDFSIO: Average IO rate mb/sec: 266.74 2021 02 09 11:34:15,848 INFO fs.TestDFSIO: IO rate std deviation: 143.12 2021 02 09 11:34:15,848 INFO fs.TestDFSIO: Test exec time sec: 20.83 删除测试生成数据 $ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient 3.1.3-tests.jar TestDFSIO -clean 测试结果分析：为什么读取文件速度大于网络带宽 由于目前只有三台服务器，且有三个副本，数据读取就近原则，相当于都是读取的本地磁盘数据，没有走网络。 HDFS多目录NameNode多目录配置NameNode的本地目录可以配置成多个， 且每个目录存放内容相同，增加了可靠性。 hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt; file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp. dir}/dfs/name2 &lt;/value&gt; &lt;/property&gt; DataNode多目录配置DataNode可以配置成多个目录， 每个目录存储的数据不一样，数据不是副本。 hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt; file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp. dir}/dfs/data2 &lt;/value&gt; &lt;/property&gt; 集群数据均衡之磁盘间数据均衡(Hadoop3.x)生产环境，由于硬盘空间不足，往往需要增加一块硬盘。刚加载的硬盘没有数据时，可以执行磁盘数据均衡命令。 （Hadoop3.x新特性） 生成均衡计划 (我们只有一块磁盘，不会生成计划) hdfs diskbalancer -plan hadoop103 执行均衡计划 hdfs diskbalancer -execute hadoop103.plan.json 查看当前均衡任务的执行情况 hdfs diskbalancer -query hadoop103 取消均衡任务 hdfs diskbalancer -cancel hadoop103.plan.json HDFS集群扩容及缩容添加白名单白名单：表示在白名单的主机IP地址可以，用来存储数据。 企业中：配置白名单，可以尽量防止黑客恶意访问攻击。 在 NameNode节点的 /opt/module/hadoop-3.1.3/etc/hadoop目录下 分别创 建 whitelist 和blacklist文件，在 whitelist 中写入可访问节点的主机名称，blacklist 保持空即可 hdfs-site.xml 白名单 &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/whitelist&lt;/value&gt; &lt;/property&gt; 黑名单 &lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt; &lt;/property&gt; 分发配置文件 whitelist， hdfs-site.xml 第一次添加白名单必须重启集群，不是第一次，只需要刷新 NameNode节点即可。hdfs dfsadmin -refreshNodes 服役新服务器 启动DataNode和NodeManager 添加白名单，分发文件，重启集群 刷新NameNode：hdfs dfsadmin -refreshNodes 服务器间数据均衡节点之间数据不均衡。 开启数据均衡命令： sbin/start-balancer.sh-threshold 10 对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过 10%，可根据实际情况进行调整。 停止数据均衡命令: sbin/stop-balancer.sh 由于 HDFS需要启动单独的 Rebalance Server 来执行 Rebalance 操作， 所以尽量不要在 NameNode上执行 start-balancer.sh，而是找一台比较空闲的机器。 黑名单退役服务器黑名单：表示在黑名单的主机IP地址不可以，用来存储数据。 企业中：配置黑名单，用来退役服务器。 hdfs-site.xml配置文件中增加主机，分发文件。第一次添加黑名单必须重启集群，不是第一次，只需要刷新 NameNode节点即可。 黑名单 &lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/blacklist&lt;/value&gt; &lt;/property&gt; 检查 Web浏览器 ，退役节点的状态为 decommission in progress（退役中）， 说明数据节点正在复制块到其他节点 等待退役节点状态为 decommissioned （所有块已经复制完成 ），停止该节点及节点资源管理器。 注意 ：如果副本数是 3， 服役的节点小于等于 3，是不能退役成功的，需要修改副本数后才能退役 HDFS存储优化MapReduce 生产经验MR跑得慢的原因MapReduce 程序效率的瓶颈在于两点： 计算机性能：CPU、内存、磁盘健康、网络 I/O 操作优化 数据倾斜 Map和Reduce数设置不合理 Map运行时间太长，导致Reduce等待过久 小文件过多 大量的不可分块的超大文件 Spill次数过多 Merge次数过多等 MR调优参数 数据倾斜问题倾斜现象： 数据频率倾斜——某一个区域的数据量要远远大于其他区域。 数据大小倾斜——部分记录的大小远远大于平均值。 减少数据倾斜的方法： 首先检查是否空值过多造成的数据倾斜，生产环境，可以直接过滤掉空值；如果想保留空值，就自定义分区，将空值加随机数打散。最后再二次聚合 。 抽样和范围分区：可以通过对原始数据进行抽样得到的结果集来预设分区边界值。 自定义分区：基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例。 Combine：使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据。 采用Map Join，尽量避免Reduce Join MR优化方法MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。 数据输入 合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢。 采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 Map阶段 减少溢写（Spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO。 减少合并（Merge）次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间。 在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少 I/O。 Reduce阶段 合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误。 设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。 规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。 合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整。 IO传输 采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器。 使用SequenceFile二进制文件。 常用的调优参数。资源相关参数以下参数是在用户自己的MR应用程序中配置就可以生效（mapred-default.xml） 配置参数 参数说明 mapreduce.map.memory.mb 一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。 mapreduce.reduce.memory.mb 一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。 mapreduce.map.cpu.vcores 每个MapTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.cpu.vcores 每个ReduceTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.shuffle.parallelcopies 每个Reduce去Map中取数据的并行数。默认值是5 mapreduce.reduce.shuffle.merge.percent Buffer中的数据达到多少比例开始写入磁盘。默认值0.66 mapreduce.reduce.shuffle.input.buffer.percent Buffer大小占Reduce可用内存的比例。默认值0.7 mapreduce.reduce.input.buffer.percent 指定多少比例的内存用来存放Buffer中的数据，默认值是0.0 应该在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml） 配置参数 参数说明 yarn.scheduler.minimum-allocation-mb 给应用程序Container分配的最小内存，默认值：1024 yarn.scheduler.maximum-allocation-mb 给应用程序Container分配的最大内存，默认值：8192 yarn.scheduler.minimum-allocation-vcores 每个Container申请的最小CPU核数，默认值：1 yarn.scheduler.maximum-allocation-vcores 每个Container申请的最大CPU核数，默认值：32 yarn.nodemanager.resource.memory-mb 给Containers分配的最大物理内存，默认值：8192 Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml） 配置参数 参数说明 mapreduce.task.io.sort.mb Shuffle的环形缓冲区大小，默认100m mapreduce.map.sort.spill.percent 环形缓冲区溢出的阈值，默认80% 容错相关参数 配置参数 参数说明 mapreduce.map.maxattempts 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 mapreduce.reduce.maxattempts 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 mapreduce.task.timeout Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。 HDFS小文件优化方法DFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。 小文件的优化无非以下几种方式： 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。 在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。 在MapReduce处理时，可采用CombineTextInputFormat提高效率。 Hadoop Archive：是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了NameNode的内存使用。 Sequence File： Sequence File由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。 CombineFileInputFormat：CombineFileInputFormat是一种新的InputFormat，用于将多个文件合并成一个单独的Split，另外，它会考虑数据的存储位置。 开启JVM重用：对于大量小文件Job，可以开启JVM重用会减少45%运行时间。JVM重用原理：一个Map运行在一个JVM上，开启重用的话，该Map在JVM上运行完毕后，JVM继续运行其他Map。具体设置：mapreduce.job.jvm.numtasks值在10-20之间。","tags":[]},{"title":"源码解析","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop11-源码解析.html","text":"NameNode 启动源码解析 DataNode 启动源码解析 HDFS上传源码解析 Yarn 源码解析","tags":[]},{"title":"MapReduce框架原理","date":"2020-06-03T12:58:30.000Z","path":"2020/06/03/05-Hadoop/Hadoop07-MapReduce框架原理.html","text":"InputFormat数据输入数据切片理解MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。 思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ 数据块：Block是HDFS物理上把数据分成一块一块。针对物理上的储存。 数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce程序计算输入数据的单位 ，一个切片会对应启动一个 MapTask。针对程序上的读取。有多种方式切片，下图只是一种。 FileInputFormat //父类，抽象类 --NLineInputFormat //N行记录为一个切片，一次返回一行 &lt;起始字节偏移量，一行内容(不包含换行符和回车符)&gt;；输入文件总行数/N=切片数，若不整除，切片数=商+1 --TextInputFormat //默认 //按照单个文件大小切片，一次返回一行 &lt;起始字节偏移量，一行内容(不包含换行符和回车符)&gt; --CombineFileInputFormat //按照多个文件大小切片，一次返回一行 &lt;起始字节偏移量，一行内容(不包含换行符和回车符)&gt; --FixedLengthInputFormat // --KeyValueTextInputFormat //按照文件大小切片，一次返回一行，一行内容根据指定分隔符分为&#39;key \\t value&#39;, 返回&lt;key, value&gt; --SequenceFileInputFormat // --自定义InputFormat Job提交流程源码和切片源码详解Job提交流程源码详解： waitForCompletion() submit(); // 1建立连接 connect(); // 1）创建提交Job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交job submitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);//会创建临时文件目录 // 2）获取jobid ，并创建Job路径 JobID jobId = submitClient.getNewJobID();//临时文件目录/mapred/staging/use_id/.staging/ // 3）拷贝jar包到集群 submitJobDir = new Path(jobStagingArea, jobId.toString()); copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // jobSubmitDir=submitJobDir // 4）计算切片，生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);////////进行切片的函数 splitSize = Math.max(minSize, Math.min(maxSize,blockSize)); minsize=1; maxsize=Long.MAXValue; // 故，默认情况下，blockSize=128M。 // 5）向Stag路径写XML配置文件 writeConf(conf, submitJobFile); conf.writeXml(out); // 6）提交Job,返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); // 7) 清空 submitJobDir FileInputFormat切片源码解析(input.getSplits(job))： TextInputFormat 切片机制 源码中计算切片大小的公式 Math.max(minSize, Math.min(maxSize,blockSize)); mapreduce.input.fileinputformat.split.minsize=1; mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue; // 故，默认情况下，blockSize=128M。 &gt; 129M 的文件，&lt;font color = #3333ff face=&quot;宋体&quot;&gt;HDFS上存 2 块，切片只有 1 块&lt;/font&gt;。 切片大小设置 maxSize：比blockSize小，则会让切片大小等于maxSize minSize：比blockSize大，则会让切片大小等于minSize 获取切片信息API // 获取切片的文件名称 String name = inputSplit.getPath().getName(); // 根据文件类型获取切片信息 FileSplit inputSplit = (FileSplit) contex.getInputSplit(); TextInputFormat 是默认的FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，LongWritable类型。值是这行的内容，不包括任何行终止符（ 换行符和回车符）， Text类型。 CombineTextInputFormat 切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。 CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。 虚拟存储切片最大值设置： CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。 生成切片过程包括：虚拟存储过程和切片过程两部分。 虚拟存储过程 将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。 例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。 切片过程 （a）判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。 （b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。 （c）测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为： 1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M） 最终会形成3个切片，大小分别为： （1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M 案例： 需求：将输入的大量小文件合并成一个切片统一处理。 不做任何处理，运行WordCount案例程序，观察切片个数为4 在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数 驱动类中添加代码如下 // 如果不设置InputFormat，它默认用的是TextInputFormat.class job.setInputFormatClass(CombineTextInputFormat.class); //虚拟存储切片最大值设置4m CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); //运行结果为3个切片 驱动中添加代码如下 // 如果不设置InputFormat，它默认用的是TextInputFormat.class job.setInputFormatClass(CombineTextInputFormat.class); //虚拟存储切片最大值设置20m CombineTextInputFormat.setMaxInputSplitSize(job, 20971520) // 运行结果为1个切片 KeyValueTextInputFormat 使用案例需求：统计输入文件中每一行的第一个单词相同的行数。 // 输入数据 banzhang ni hao xihuan hadoop banzhang banzhang ni hao xihuan hadoop banzhang // 期望结果数据 banzhang 2 xihuan 2 写Mapper类： package com.atguigu.mapreduce.KeyValueTextInputFormat; import java.io.IOException; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class KVTextMapper extends Mapper&lt;Text, Text, Text, LongWritable&gt;{ // 1 设置value LongWritable v = new LongWritable(1); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException { // banzhang ni hao // 2 写出 context.write(key, v); } } 写Reducer类： package com.atguigu.mapreduce.KeyValueTextInputFormat; import java.io.IOException; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class KVTextReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{ LongWritable v = new LongWritable(); @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException { long sum = 0L; // 1 汇总统计 for (LongWritable value : values) { sum += value.get(); } v.set(sum); // 2 输出 context.write(key, v); } } 写Driver类： package com.atguigu.mapreduce.keyvaleTextInputFormat; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader; import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class KVTextDriver { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { Configuration conf = new Configuration(); // 设置切割符 conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot; &quot;); // 1 获取job对象 Job job = Job.getInstance(conf); // 2 设置jar包位置，关联mapper和reducer job.setJarByClass(KVTextDriver.class); job.setMapperClass(KVTextMapper.class); job.setReducerClass(KVTextReducer.class); // 3 设置map输出kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 4 设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); // 5 设置输入输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); // 设置输入格式 job.setInputFormatClass(KeyValueTextInputFormat.class); // 6 设置输出数据路径 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交job job.waitForCompletion(true); } } NLineInputFormat使用案例需求：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中。 // 输入数据 banzhang ni hao xihuan hadoop banzhang banzhang ni hao xihuan hadoop banzhang banzhang ni hao xihuan hadoop banzhang banzhang ni hao xihuan hadoop banzhang banzhang ni hao xihuan hadoop banzhang banzhang ni hao xihuan hadoop banzhang // 期望输出数据 Number of splits:4 Mapper类： package com.atguigu.mapreduce.nline; import java.io.IOException; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class NLineMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{ private Text k = new Text(); private LongWritable v = new LongWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { // 1 获取一行 String line = value.toString(); // 2 切割 String[] splited = line.split(&quot; &quot;); // 3 循环写出 for (int i = 0; i &lt; splited.length; i++) { k.set(splited[i]); context.write(k, v); } } } 写Reducer类： package com.atguigu.mapreduce.nline; import java.io.IOException; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class NLineReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{ LongWritable v = new LongWritable(); @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException { long sum = 0; // 1 汇总 for (LongWritable value : values) { sum += value.get(); } v.set(sum); // 2 输出 context.write(key, v); } } 写Driver类： package com.atguigu.mapreduce.nline; import java.io.IOException; import java.net.URISyntaxException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.NLineInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; public class NLineDriver { public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException { // 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[] { &quot;e:/input/inputword&quot;, &quot;e:/output1&quot; }; // 1 获取job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 7设置每个切片InputSplit中划分三条记录 NLineInputFormat.setNumLinesPerSplit(job, 3); // 8使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class); // 2设置jar包位置，关联mapper和reducer job.setJarByClass(NLineDriver.class); job.setMapperClass(NLineMapper.class); job.setReducerClass(NLineReducer.class); // 3设置map输出kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 4设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); // 5设置输入输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6提交job job.waitForCompletion(true); } } 自定义InputFormat步骤 继承FileInputFormat 改写RecordReader，实现一次读取一个完整文件封装为KV 在输出时使用SequenceFileOutPutFormat输出合并文件。 案例无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义InputFormat实现小文件的合并。 将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式），SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。 自定义一个类继承FileInputFormat 重写isSplitable()方法，返回false不可切割 重写createRecordReader()，创建自定义的RecordReader对象，并初始化 改写RecordReader，实现一次读取一个完整文件封装为KV 采用IO流一次读取一个文件输出到value中，因为设置了不可切片，最终把所有文件都封装到了value中 获取文件路径信息+名称，并设置key 设置Driver // （1）设置输入的inputFormat job.setInputFormatClass(WholeFileInputformat.class); // （2）设置输出的outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); 自定义InputFromat—WholeFileInputformatpackage com.atguigu.mapreduce.inputformat; import java.io.IOException; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.BytesWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.mapreduce.InputSplit; import org.apache.hadoop.mapreduce.JobContext; import org.apache.hadoop.mapreduce.RecordReader; import org.apache.hadoop.mapreduce.TaskAttemptContext; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; // 定义类继承FileInputFormat public class WholeFileInputformat extends FileInputFormat&lt;Text, BytesWritable&gt;{ @Override protected boolean isSplitable(JobContext context, Path filename) { return false; } @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException { WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; } } 自定义RecordReader类—WholeRecordReaderpackage com.atguigu.mapreduce.inputformat; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.BytesWritable; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.mapreduce.InputSplit; import org.apache.hadoop.mapreduce.RecordReader; import org.apache.hadoop.mapreduce.TaskAttemptContext; import org.apache.hadoop.mapreduce.lib.input.FileSplit; public class WholeRecordReader extends RecordReader&lt;Text, BytesWritable&gt;{ private Configuration configuration; private FileSplit split; private boolean isProgress= true; private BytesWritable value = new BytesWritable(); private Text k = new Text(); @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException { this.split = (FileSplit)split; configuration = context.getConfiguration(); } @Override public boolean nextKeyValue() throws IOException, InterruptedException { if (isProgress) { // 1 定义缓存区 byte[] contents = new byte[(int)split.getLength()]; FileSystem fs = null; FSDataInputStream fis = null; try { // 2 获取文件系统 Path path = split.getPath(); fs = path.getFileSystem(configuration); // 3 读取数据 fis = fs.open(path); // 4 读取文件内容 IOUtils.readFully(fis, contents, 0, contents.length); // 5 输出文件内容 value.set(contents, 0, contents.length); // 6 获取文件路径及名称 String name = split.getPath().toString(); // 7 设置输出的key值 k.set(name); } catch (Exception e) { }finally { IOUtils.closeStream(fis); } isProgress = false; return true; } return false; } @Override public Text getCurrentKey() throws IOException, InterruptedException { return k; } @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException { return value; } @Override public float getProgress() throws IOException, InterruptedException { return 0; } @Override public void close() throws IOException { } } SequenceFileMapper类package com.atguigu.mapreduce.inputformat; import java.io.IOException; import org.apache.hadoop.io.BytesWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.lib.input.FileSplit; public class SequenceFileMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt;{ @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException { context.write(key, value); } } SequenceFileReducer类package com.atguigu.mapreduce.inputformat; import java.io.IOException; import org.apache.hadoop.io.BytesWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; { @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException { context.write(key, values.iterator().next()); } } SequenceFileDriver类package com.atguigu.mapreduce.inputformat; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.BytesWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat; public class SequenceFileDriver { public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { // 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[] { &quot;e:/input/inputinputformat&quot;, &quot;e:/output1&quot; }; // 1 获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 设置jar包存储位置、关联自定义的mapper和reducer job.setJarByClass(SequenceFileDriver.class); job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 7设置输入的inputFormat job.setInputFormatClass(WholeFileInputformat.class); // 8设置输出的outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); // 3 设置map输出端的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); // 4 设置最终输出端的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); // 5 设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); } } MapReduce工作流程绿色框均可自定义！ 对于 MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 对于 ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 第8步，分区内 快排，只能使用字典顺序排序，无法修改。 第9步，环形缓冲区只有100M，达到80%时会溢出到磁盘。一个MapTask的数据可能很多，会多次溢出到磁盘，形成多个文件。 第10步，MapTask将溢出的多个文件合并，采用 归并排序 第11步，可选流程。采用 归并排序 第13步，将多个MapTask的中同一分区的数据合并，采用 归并排序 流程详解 上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下： MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序 ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据 ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序） 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法） Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。 缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M。 context.write(k, NullWritable.get()); output.write(key, value); collector.collect(key, value,partitioner.getPartition(key, value, partitions)); HashPartitioner(); collect() close() collect.flush() sortAndSpill() sort() QuickSort mergeParts(); collector.close(); Shuffle机制Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。 map方法和reduce方法之间的都是shuffle机制的内容。 Partition分区默认分区： public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt;{ public int getPartition(K key, V value, int numReduceTasks){ return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; } } // 用户无法控制哪个key存储在哪个分区。 自定义Partitioner分区： 自定义类继承Partitioner，重写getPartition()方法 public class Custompartitioner exends Partitioner&lt;Text, FlowBean&gt;{ @Override public int getPartition(Text key, FlowBeam value, int numPartitions){ ........... return partition; } } 在Job驱动中，设置自定义Partitioner job.setPartitionerClass(Custompartitioner.class); 自定义Partition后，根据自定义Partitioner的逻辑设置相应数量的ReduceTask job.setBumReduceTask(5); 分区总结: 如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx； 如果1","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://mtcai.github.io/tags/Hadoop/"}]},{"title":"数据结构02-复杂度分析（上）","date":"2020-06-03T11:19:39.000Z","path":"2020/06/03/01-数据结构/数据结构02-复杂度分析(上).html","text":"[toc] 复杂度分析(上)如何分析、统计算法的执行效率和资源消耗？数据结构和算法本身解决的是“快”和“省”的问题，即，如何让代码运行得更快，如何让代码更省存储空间。 事后统计法事后统计法：把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。 局限性： 测试结果非常依赖测试环境，相同代码不同机子，测试结果不一致。 测试结果受数据规模的影响很大，同一个排序算法对有序度不同的数据测试结果不一致。 大O表示法算法的执行效率，粗略地讲，就是算法代码执行的时间。但是，如何在不运行代码的情况下，用“肉眼”得到一段代码的执行时间呢？ 这里有段非常简单的代码，求 1,2,3…n 的累加和。现在，我就带你一块来估算一下这段代码的执行时间。 int cal(int n) { int sum = 0; int i = 1; for (; i &lt;= n; ++i) { sum = sum + i; } return sum; } 从 CPU 的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的 CPU 执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？ 第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 2nunit_time 的执行时间，所以这段代码总的执行时间就是 (2n+2)unit_time。可以看出来，所有代码的执行时间 T(n) 与每行代码的执行次数成正比。 按照这个分析思路，我们再来看这段代码。 int cal(int n) { int sum = 0; int i = 1; int j = 1; for (; i &lt;= n; ++i) { j = 1; for (; j &lt;= n; ++j) { sum = sum + i * j; } } 我们依旧假设每个语句的执行时间是 unit_time。那这段代码的总执行时间 T(n) 是多少呢？ 第 2、3、4 行代码，每行都需要 1 个 unit_time 的执行时间，第 5、6 行代码循环执行了 n 遍，需要 2n unit_time 的执行时间，第 7、8 行代码循环执行了 $n^2$遍，所以需要 $2n^2$ unit_time 的执行时间。所以，整段代码总的执行时间 T(n) = ($2n^2+2n+3$)*unit_time。 尽管我们不知道 unit_time 的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。 我们可以把这个规律总结成一个公式。注意，大 O 就要登场了！ T(n)=O(f(n))具体解释一下这个公式。其中，T(n) 我们已经讲过了，它表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和。因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比。 所以，第一个例子中的 T(n) = O(2n+2)，第二个例子中的 T(n) = O($2n^2+2n+3$)。这就是大 O 时间复杂度表示法。大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。 当 n 很大时，你可以把它想象成 10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大 O 表示法表示刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n)； T(n) = O(n^2)。 时间复杂度分析 只关注循环执行次数最多的一段代码 int cal(int n) { int sum = 0; int i = 1; for (; i &lt;= n; ++i) { sum = sum + i; } return sum; } 其中第 2、3 行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第 4、5 行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了 n 次，所以总的时间复杂度就是 O(n)。 加法法则：总复杂度等于量级最大的那段代码的复杂度 int cal(int n) { int sum_1 = 0; int p = 1; for (; p &lt; 100; ++p) { sum_1 = sum_1 + p; } int sum_2 = 0; int q = 1; for (; q &lt; n; ++q) { sum_2 = sum_2 + q; } int sum_3 = 0; int i = 1; int j = 1; for (; i &lt;= n; ++i) { j = 1; for (; j &lt;= n; ++j) { sum_3 = sum_3 + i * j; } } return sum_1 + sum_2 + sum_3; } 第一段的时间复杂度是多少呢？这段代码循环执行了 100 次，所以是一个常量的执行时间，跟 n 的规模无关。 这里我要再强调一下，即便这段代码循环 10000 次、100000 次，只要是一个已知的数，跟 n 无关，照样也是常量级的执行时间。当 n 无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。 那第二段代码和第三段代码的时间复杂度是多少呢？答案是 O(n) 和 O(n2)。 综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为 O(n2)。也就是说：总的时间复杂度就等于量级最大的那段代码的时间复杂度。那我们将这个规律抽象成公式就是： T1(n)=O(f(n))\\\\ T2(n)=O(g(n))\\\\ T(n)=T1(n)+T2(n)=max(O(f(n)),O(g(n)))=O(max(f(n),f(n))) 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 可以把乘法法则看成是嵌套循环 int cal(int n) { int ret = 0; int i = 1; for (; i &lt; n; ++i) { ret = ret + f(i); } } int f(int n) { int sum = 0; int i = 1; for (; i &lt; n; ++i) { sum = sum + i; } return sum; } 我们单独看 cal() 函数。假设 f() 只是一个普通的操作，那第 4～6 行的时间复杂度就是，T1(n) = O(n)。但 f() 函数本身不是一个简单的操作，它的时间复杂度是 T2(n) = O(n)，所以，整个 cal() 函数的时间复杂度就是，T(n) = T1(n) * T2(n) = O(n*n) = O(n^2)。 T1(n)=O(f(n))\\\\ T2(n)=O(g(n))\\\\ T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))几种常见时间复杂度实例分析 对于刚罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2^n) 和 O(n!)。当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。因此，关于 NP 时间复杂度我就不展开讲了。 我们主要来看几种常见的多项式时间复杂度。 O(1)首先必须明确一个概念，O(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这段代码，即便有 3 行，它的时间复杂度也是 O(1），而不是 O(3)。只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。 int i = 8; int j = 6; int sum = i + j; $O(logn)、O(nlogn)$对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。我通过一个例子来说明一下。 i=1; while (i &lt;= n) { i = i * 2; } 通过 $2^x=n$ 求解 x 这个问题我们想高中应该就学过了，我就不多说了。x=\\log_2n，所以，这段代码的时间复杂度就是 $O(\\log^2n)$。 现在，我把代码稍微改下，你再看看，这段代码的时间复杂度是多少？ i=1; while (i &lt;= n) { i = i * 3; } 这段代码的时间复杂度为 $O(\\log_3n)$。 我们可以把所有对数阶的时间复杂度都记为 O(logn)，\\log_3n=\\log_32*\\log_2n，所以O(\\log_3n) = O(C*\\log_2n)，其中C=\\log_32是一个常量。基于我们前面的一个理论：在采用大 O 标记复杂度的时候，可以忽略系数，即 O(C*f(n)) = O(f(n))。所以，O(\\log_2n)就等于 $O(\\log_3n)$。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 $O(\\log n)$。 那O(n \\log n) 就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是 $O(\\log n)$，我们循环执行 n 遍，时间复杂度就是O(n \\log n) 了。而且，O(n \\log n)也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是O(n \\log n)。 O(m+n)、O(m*n)代码的复杂度由两个数据的规模来决定。 int cal(int m, int n) { int sum_1 = 0; int i = 1; for (; i &lt; m; ++i) { sum_1 = sum_1 + i; } int sum_2 = 0; int j = 1; for (; j &lt; n; ++j) { sum_2 = sum_2 + j; } return sum_1 + sum_2; } 从代码中可以看出，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。 针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)T2(n) = O(f(m) f(n))。 空间复杂度分析时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 void print(int n) { int i = 0; int[] a = new int[n]; for (i; i &lt;n; ++i) { a[i] = i * i; } for (i = n-1; i &gt;= 0; --i) { print out a[i] } } 跟时间复杂度分析一样，我们可以看到，第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 我们常见的空间复杂度就是 $O(1)、O(n)、O(n^2 )$，像O(\\log n)、O(n\\log n)这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我说的这些内容已经足够了。 内容小结基础复杂度分析的知识到此就讲完了，总结一下。 复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(\\log n)、O(n)、O(n\\log n)、O(n^2 )。等学完整个专栏之后，就会发现几乎所有的数据结构和算法的复杂度都跑不出这几个。 O(1)","tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://mtcai.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Ubuntu16.04 安装OpenCV","date":"2020-01-20T07:18:50.000Z","path":"2020/01/20/00-环境/99-Ubuntu16.04 安装opencv（C++版本）.html","text":"[toc] Ubuntu16.04 安装OpenCV 3.4.x（C++版本） 1.安装依赖 2.编译OpenCV 3.测试 4.踩坑 安装依赖1.安装ffmpegffmpeg安装 2.安装其他依赖 sudo apt-get install build-essential sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev # 处理图像所需的包 sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev liblapacke-dev sudo apt-get install libxvidcore-dev libx264-dev # 处理视频所需的包，可选 sudo apt-get install libatlas-base-dev gfortran # 优化opencv功能，可选 sudo apt-get install ffmpeg 编译opencv3.安装 cd opencv-3.4.6 #进入opencv的目录 mkdir build &amp;&amp; cd build cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. sudo make -j4 # 使用四线程，电脑线程多可以改大 sudo mkae install 4.配置环境变量 sudo vim /etc/ld.so.conf.d/opencv.conf # 这个文件并不存在，我们新建一个 然后，在里面写入： /usr/local/lib 退出vim sudo vim /etc/ld.so.conf 添加： include /etc/ld.so.conf.d/*.conf /usr/local/lib 退出vim sudo ldconfig # 使更改生效 sudo vim /etc/bash.bashrc 在最末尾添加: PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig export PKG_CONFIG_PATH 退出vim source /etc/bash.bashrc sudo updatedb pkg-config opencv --modversion # 查看opencv版本 5.测试 进入到opencv的example目录 cd cd samples/cpp/example_cmake/ make ./opencv_example # 会输出以下内容，并打开摄像头 Built with OpenCV 3.4.6 Capture is opened 踩坑1.ippicv 在编译opencv时，会碰到ippicv_2019_lnx_intel64_general_20180723.tgz这个文件下载超级慢的问题 下载网址1：https://github.com/opencv/opencv_3rdparty/tree/ippicv/master_20180723/ippicv 下载网址2：https://links.jianshu.com/go?to=https%3A%2F%2Fraw.githubusercontent.com%2Fopencv%2Fopencv_3rdparty%2Fippicv%2Fmaster_20180723%2Fippicv%2Fippicv_2019_lnx_intel64_general_20180723.tgz 下载后放到自己方便的目录下，随便哪个都行 然后修改 ./3rdparty/ippicv/ippicv.cmake #就是这个文件的第47行 &quot;https://raw.githubusercontent.com/opencv/opencv_3rdparty/${IPPICV_COMMIT}ippicv/&quot; 修改为： &quot;file:~/Downloads/&quot; #（仅供参考，根据自己的路径填写，填写绝对路径） 2.recipe for target ‘lib/libopencv_videoio.so.3.4.6’ failed报错信息为： modules/videoio/CMakeFiles/opencv_videoio.dir/build.make:333: recipe for target ‘lib/libopencv_videoio.so.3.4.2’ failed make[2]: *** [lib/libopencv_videoio.so.3.4.2] Error 1 CMakeFiles/Makefile2:5205: recipe for target ‘modules/videoio/CMakeFiles/opencv_videoio.dir/all’ failed make[1]: *** [modules/videoio/CMakeFiles/opencv_videoio.dir/all] Error 2 Makefile:162: recipe for target ‘all’ failed make: *** [all] Error 2 经过查资料发现时ffmpeg的问题，没有编译好，编译ffmpeg时缺少参数https://github.com/DeaDBeeF-Player/deadbeef/issues/1691解决办法： 编译ffmpeg时添加 —extra-cflags=”-fPIC” 参数./configure —extra-cflags=”-fPIC” ok至此，Ubuntu16.04下安装OpenCV已全部结束，请小量食用OpenCV！","tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://mtcai.github.io/tags/OpenCV/"},{"name":"C++","slug":"C","permalink":"https://mtcai.github.io/tags/C/"}]},{"title":"ffmpeg安装","date":"2020-01-20T07:18:50.000Z","path":"2020/01/20/00-环境/99-安装ffmpeg.html","text":"[toc] ffmpeg安装下载ffmpeg 1.git下载 git clone git://source.ffmpeg.org/ffmpeg.git ffmpeg # 网速太慢 2.官网下载 http://www.ffmpeg.org/download.html # 我使用的这个 安装依赖sudo apt-get install yasm # 必选 sudo apt-get install xorg-dev # 可选 sudo apt-get install libsdl1.2-dev # 可选 开始编译安装./configure --enable-shared --extra-cflags=&quot;-fPIC&quot; sudo make sudo make install # 编译完成，查看/usr/local/lib和/usr/local/include是否生成ffmpeg的库 测试ffmpeg -version # 输出正常即可 测试用例1： 新建test.c，写入以下代码： #include &lt;stdio.h&gt; #include &lt;libavutil/avstring.h&gt; #include &lt;libavutil/eval.h&gt; #include &lt;libavutil/mathematics.h&gt; #include &lt;libavutil/pixdesc.h&gt; #include &lt;libavutil/imgutils.h&gt; #include &lt;libavutil/dict.h&gt; #include &lt;libavutil/parseutils.h&gt; #include &lt;libavutil/samplefmt.h&gt; #include &lt;libavutil/avassert.h&gt; #include &lt;libavutil/time.h&gt; #include &lt;libavformat/avformat.h&gt; #include &lt;libavdevice/avdevice.h&gt; #include &lt;libswscale/swscale.h&gt; #include &lt;libavutil/opt.h&gt; #include &lt;libavcodec/avfft.h&gt; #include &lt;libswresample/swresample.h&gt; int main(int argc, char* argv[]) { printf(&quot;this is a test program for ffmpeg\\n&quot;); av_register_all(); return 0; } 在终端输入以下指令：gcc test.c -o test -I /usr/local/include -L /usr/local/lib -lavutil -lavformat -lavcodec -lavutil -lswresample -lm -lrt -lpthread -lz./test 测试用例2 test1.c #include &lt;stdio.h&gt; #include &lt;libavutil/avstring.h&gt; #include &lt;libavutil/eval.h&gt; #include &lt;libavutil/mathematics.h&gt; #include &lt;libavutil/pixdesc.h&gt; #include &lt;libavutil/imgutils.h&gt; #include &lt;libavutil/dict.h&gt; #include &lt;libavutil/parseutils.h&gt; #include &lt;libavutil/samplefmt.h&gt; #include &lt;libavutil/avassert.h&gt; #include &lt;libavutil/time.h&gt; #include &lt;libavformat/avformat.h&gt; #include &lt;libavdevice/avdevice.h&gt; #include &lt;libswscale/swscale.h&gt; #include &lt;libavutil/opt.h&gt; #include &lt;libavcodec/avfft.h&gt; #include &lt;libswresample/swresample.h&gt; int main(int argc, char* argv[]) { printf(&quot;this is a test program for ffmpeg\\n&quot;); printf(&quot;%s&quot;, avcodec_configuration()); getchar(); return 0; } 在终端输入以下指令：gcc test1.c -o test -I /usr/local/include -L /usr/local/lib -lavutil -lavformat -lavcodec -lavutil -lswresample -lm -lrt -lpthread -lz./test 踩坑 1.ffmpeg: error while loading shared libraries: libavdevice.so.58: cannot open shared object file: No such file or directory因为我们使用源码编译，没有添加环境变量 sudo vim /etc/ld.so.conf 添加： /usr/local/lib 退出vim sudo ldconfig # 更改生效 sudo vim /etc/profile 添加： export PATH=&quot;/usr/local/bin:$PATH&quot; source /etc/profile # 更改生效 再在终端输入ffmpeg -version 就会有信息输出了","tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://mtcai.github.io/tags/ffmpeg/"}]}]